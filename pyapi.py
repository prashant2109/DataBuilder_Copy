#!/usr/bin/python
# -*- coding:utf-8 -*-
import sys, os, sets, hashlib, binascii, lmdb, copy, json, ast, datetime,sqlite3, MySQLdb, traceback
#import numpy as np
import db.get_conn as get_conn
conn_obj    = get_conn.DB()
from utils.meta_info import MetaInfo
import report_year_sort
import utils.numbercleanup as numbercleanup
numbercleanup_obj   = numbercleanup.numbercleanup()
import utils.convert as scale_convert
sconvert_obj   = scale_convert.Convert()
import utils.convert_currency as currency_convert
cconvert_obj   = currency_convert.Convert()
import gcom_operator
import tavinash
from collections import defaultdict as DD, OrderedDict as OD
from itertools import combinations
import dateutil.parser as pydateutils
#try:
#    import ParaAPI.Para_Comp_API_Back_New as Para_Comp_API_Back_New 
#    date_obj    = Para_Comp_API_Back_New.Para_Comp_API()
#except:pass
import parse_formula
frm_parse_obj   = parse_formula.Parse()

def disableprint():
    return
    sys.stdout = open(os.devnull, 'w')
    pass

def enableprint():
    return
    sys.stdout = sys.__stdout__


def print_exception():
        formatted_lines = traceback.format_exc().splitlines()
        for line in formatted_lines:
            print '<br>',line
from libgcomputation_cpp import compute as gcompute_cpp
from collections import defaultdict as dd
from nong_new_akshay import NGComp
def avg(ar):
    ar  = filter(lambda x:not isinstance(x, int) or x != 0, ar)
    if ar:
        return sum(ar)/float(len(ar))
    else:
        return 0
def percentage(ar):
        if abs(float(ar[1])) > 0.0:
            return (float(ar[0])/float(ar[1]))*100
        else:
            return 0
    

def remap_index(orig_index_dict, old_results):

    for res in old_results:
        res[1] = [orig_index_dict[i] for i in res[1]]

    return old_results


def calculate_comp_results(ph_values):

    ng_obj = NGComp()
    ph, values = ph_values
    values, xml_ids, act_values = values['values'], values['xml_ids'], values['actual_values']

    i1 = 0
    i2 = len(act_values)

    for i in range(0, len(act_values)):
        if (act_values[i] == ''):
           i1 = i+1
        else:
           break
    
    #print len(act_values)  

    if (i1 == i2):
       return (ph, [], [])

    while act_values[i2-1] == '':
          i2 = i2 - 1         

    #print i1, i2
    #sys.exit()  

    truncate_values = []
    if (i1 < i2):
       truncate_values = values[i1:i2]

    orig_index_dict = {}
    valid_values = []
    new_index = 0

    '''
    for orig_index, (x, v) in enumerate(zip(xml_ids, values)):
        if x.strip():
            orig_index_dict[new_index] = orig_index
            valid_values.append(v)
            new_index += 1
    #'''
    valid_values = values[:]

    gcomp_results = []
    nong_results = []

    '''
    if not any([x.strip() for x in xml_ids]):
        return (ph, gcomp_results, nong_results)
    '''


    '''
    if not any([x.strip() for x in xml_ids]):
        return (ph, gcomp_results, nong_results)
    '''

    pattern_file = '/mnt/eMB_db/GComp_txt_files/1/ExtnGCPatterns.txt'
    formula_file = '/mnt/eMB_db/GComp_txt_files/1/Extnadd_formula.txt'

    # Process only if there are any valid xmlids/values
    if truncate_values:
        gcompute_cpp(truncate_values, pattern_file, formula_file, gcomp_results)
        #nong_results = ng_obj.compute_nong_eqs(valid_values)

     

    
    #print valid_values
    #print gcomp_results # [[0, 0], [0, 1, 2], False]
    '''
          if result[-1]:
                       # first one is resultant
                       res_operands = result[0][1:]
                    else:
                       # last one is resultant
                       res_operands = result[0][:-1]
    '''

    add_res = []
    # Result multiplier
    for res in gcomp_results:
        if len(res[1]) >= 4:
           #print res
           #print truncate_values
           mid_indx = res[1][1:-1]
           if res[2] == False:
              # Last one is resultant
              #print 'Last one is the resultant 1:' 
              mid_opers = res[0][1:] 
              rem_opr = res[0][0] 
           elif res[2] == True:
              # First one is resultant
              #print 'First one is the resultant :-1'  
              mid_opers = res[0][:-1] 
              rem_opr = res[0][-1] 
                 
           #print mid_indx   
           val_ar = map(lambda x:truncate_values[x], mid_indx)
           #print val_ar    
           #print mid_opers 
           non_zero_indx = []
           i = -1
           for x in mid_indx:
               
               i = i + 1
               if (truncate_values[x] == 0) or (truncate_values[x] == 0.0):
                   continue
               else:
                   non_zero_indx.append((x, mid_opers[i]))
           if non_zero_indx:   
              #print '======================='
              #print res
              #print non_zero_indx  
              #new_result = []
              n_indx = map(lambda x:x[0], non_zero_indx[:])
              n_oprs = map(lambda x:x[1], non_zero_indx[:])
               
              new_opr = [ res[1][0] ] + n_indx[:] + [ res[1][-1] ]
              #print 'new_opr: ', new_opr  
              if res[2] == False:
                 new_sgn = [ rem_opr ] + n_oprs[:]
              else: 
                 new_sgn = n_oprs[:]  + [ rem_opr ]
              new_res = [ new_sgn, new_opr, res[2] ]
              #print new_res   
              add_res.append(new_res)
    gcomp_results = gcomp_results + add_res[:]
               
    #sys.exit()   


    sign_dict = {}
    for res in gcomp_results:
        mykey = tuple(res[1] + [res[2]])
        if mykey not in sign_dict:
           sign_dict[mykey] = [] 
        sign_dict[mykey].append(res[0])

    new_gcomp_results = [] 
    for k, vs in sign_dict.items():
        operands = list(k)[:-1]
        parity = k[-1]
        if len(vs) > 1:
           #print truncate_values 
           min_ar = []
           vs.sort()
           for v in vs:
               #print '++++++++++++++++++++++++++'  
               if parity == False:
                  # Last one is resultant
                  resultant = truncate_values[operands[-1]]
                  opers = map(lambda x:truncate_values[x], operands[:-1])

               else:
                  # first one is the resultant
                  resultant = truncate_values[operands[0]]
                  opers = map(lambda x:truncate_values[x], operands[1:])
               if 1:        
                  #print v
                  #print opers  
                  #print resultant 
                  comp_value = 0
                  for i, v_elm in enumerate(v):
                      if v_elm == 0:
                         comp_value = comp_value + opers[i]
                      else:
                         comp_value = comp_value - opers[i]
                  #print 'computed_value: ', comp_value 
                  diff = abs(resultant - comp_value) 
                  #print 'diff: ', diff               
                  min_ar.append((diff, v))
           min_ar.sort()
           best_v = min_ar[0][1]
           new_gcomp_results.append([ best_v, operands, parity]) 
        else:
           new_gcomp_results.append([ vs[0], operands, parity]) 

    gcomp_results = new_gcomp_results[:] 
    #print 'Final'
    #sys.exit()  
    new_results = []
    for res in gcomp_results:
        sgn = res[0]
        #print res
        #continue 
        #for sgn1 in res:
        #    print sgn1
          
        #sys.exit() 
        opr = map(lambda x:x+i1, res[1][:])
        s = res[2]
        new_results.append((sgn, opr, s))  

        #print '======================', i1
        #print res
        #print (sgn, opr, s)

    #sys.exit() 
    #print 'Old: ', gcomp_results
    #print 'New: ', new_results
    gcomp_results = new_results[:]     
    #sys.exit() 

    # If there is change in index
    #if len(valid_values) != len(values):
    #    gcomp_results = remap_index(orig_index_dict, gcomp_results)
        #nong_results = remap_index(orig_index_dict, nong_results)

    return (ph, gcomp_results, nong_results)


class PYAPI(MetaInfo):
    def __init__(self, pypath="/root/databuilder_train_ui/tenkTraining/Data_Builder_Training_Copy/pysrc"):
        MetaInfo.__init__(self, pypath)
        self.doc_path          = self.config['doc_path'] 
        self.model_db          = self.config['modeldb'] 
        self.bkup_name       = '' #_user_10_dec'
        self.db_path         =  '/mnt/yMB_db/%s_%s'+ self.bkup_name+'/'
        self.taxo_path       = '/mnt/eMB_db/%s/%s/'
        self.output_path    = self.config['table_path']
        self.bbox_path      = '/var/www/html/fill_table/'
        self.value_type_map  = {
                            'Percentage'        : '%',
                            'Percentage-1'      : '%-1',
                            'Percentage-2'      : '%-2',
                            'Percentage-3'      : '%-3',
                            'Percentage-4'      : '%-4',
                            'Percentage-5'      : '%-5',
                            'Percentage-6'      : '%-6',
                            'Percentage-7'      : '%-7',
                            'Percentage-8'      : '%-8',
                            'Percentage-9'      : '%-9',
                            'Percentage-10'      : '%-10',
                            'Percentage-11'      : '%-11',
                            'Percentage-12'      : '%-12',
                        }
        self.ph_deivation_ar    = [
                    'Q4=FY-Q1-Q2-Q3',
                    'Q4=FY-M9',
                    'Q4=FY-H1-Q3',
                    'Q4=H2-Q3',
                    'Q3=FY-Q1-Q2-Q4',
                    'Q3=H2-Q4',
                    'Q3=M9-H1',
                    'Q3=M9',
                    'Q3=FY-H1-Q4',
                    'Q2=FY-Q1-Q3-Q4',
                    'Q2=H1-Q1',
                    'Q2=H1-Q1',
                    'Q2=H1',
                    'Q1=FY-Q2-Q3-Q4',
                    'FY=Q1+Q2+Q3+Q4',
                    'FY=H1+H2',
                    'H2=FY-H1',
                    'H2=FY',
                    'H1=Q1+Q2',
                    'Q1=H1-Q2',
                    'Q4=FY',
                    'Q4=H2',
                    'FY=H2',
                    'FY=Q4',
                    'H1=Q2',
                    'H2=H1',
                    'H2=Q3+Q4',
                ]
        self.order_d     = {
                        'IS'     : 1,
                        'BS'     : 2,
                        'CF'     : 3,
                        'RBS'    : 4,
                        'RBG'    : 5,
                        'STD'    : 6,
                        'LTD'    : 7,
                        'OLSE'   : 8,
                        'CLSE'   : 9,
                        'IEXP'   : 10,
                        'IINC'   : 11,
                        'COGSS'  : 12,
                        'FE'     : 13,
                        'FI'     : 14,
                        'RNTL'   : 15,
                        'COGS'   : 16,
                        'DEBT'   : 17,
                        'COS'    : 18,
                        'LTP'    : 19,
                        'STP'    : 20,
                        'RnD'    : 21,
                        'PRV'    : 22,
                        'PSH'    : 23,
                        'GWIL'   : 24,
                        'CEQ'    : 25,
                        'EBITDA' : 26,
                        'EBIT'   : 27,
                        }
        self.scale_map_d = {
            '1':'One', 'TH':'Thousand', 'TENTHOUSAND':'TenThousand', 'Mn':'Million', 'Bn':'Billion', 'KILO':'Thousand','Ton':'Million','Tn':'Million', 'Mn/Ton':"Million",
            'TH/KILO':"Thousand",
            '1/100':'One',
        }
        self.scale_map_rev_d = {
            'One':'1', 'Thousand':"TH", 'TenThousand':'TENTHOUSAND', 'Million':'Mn', 'Billion':'Mn'
        }
        self.within_crncy_d = {
                                ('GBP', 'PENCE')    : ('GBP', 0.01),
                                ('PENCE', 'GBP')    : ('GBP', 100),
                                ('CENTS', 'USD')    : ('USD', 100),
                                ('USD','CENTS')     : ('USD', 0.01),
                                ('CENTS', 'CAD')    : ('CAD', 100),
                                ('CAD','CENTS')     : ('CAD', 0.01),
                                }
        self.report_map_d    = {
                            'Q1'    : {'Q1':1},
                            'FY'    : {'FY':1,'Q4':1,'H2':1},
                            'Q2'    : {'Q2':1,'H1':1},
                            'Q3'    : {'Q3':1, 'M9':1},
                            'Q4'    : {'Q4':1,'FY':1},
                            'H1'    : {'H1':1, 'Q2':1},
                            'H2'    : {'H2':1, 'Q4':1,'FY':1},
                            'M9'    : {'M9':1, 'Q3':1},
                            }
        self.ignore_phs = {
                            '219':{
                                    '79-72':{
                                            'Q22017':1,
                                            'Q32017':1
                                        }
                                },
                            }
        self.gen_users      = {'captain':1
                                
                                }
        self.re_state_d = {
                            ('44', 'KPI'):{
                                        'FY':['FY', 'H1'],
                                }
                            }
        self.non_financial_tt   = {
                                    'KPI'   : 1,
                                    'KPI1'   : 1,
                                    'KPI2'   : 1,
                                    'KPI3'   : 1,
                                    'KPI4'   : 1,
                                    'KPI5'   : 1,
                                    'KPI6'   : 1,
                                    'KPI7'   : 1,
                                    'KPI8'   : 1,
                                    'KPI9'   : 1,
                                    'KPI10'   : 1,
                                    'NKPI'  : 1,
                                    }

        self.ph_f_cond          = {}
        self.ph_grp_d       = {
                                ('97', 'RBS'):1,
                                ('44', 'OS'):1,
                                ('93', 'DM'):1,
                                ('114', 'PSH'):1,
                                ('85', 'FI'):1,
                                }
        self.run_date_diff_tmp  = {
                                    ('DEBT', '86'):1,
                                    ('Long', '120'):2,
                                }
        self.kpi_types   = {
                    'PassengerTransportation-Airline':1
                        }
        self.ph_constant    = {
                                'FY'    : '365.0',
                                'H1'    : '182.5',
                                'H2'    : '182.5',
                                'Q1'    : '91.25',
                                'Q2'    : '91.25',
                                'Q3'    : '91.25',
                                'Q4'    : '91.25',
                                'M9'    : '273.75',
                                }
        self.sign_compute_d = {}
        self.return_fomula  = 'N'
        self.priority_ks    = ['v', 'f', 'clr', 'c_s', 'data_overlap']
        self.key_map = {
                        1   : 'actual_value',
                        2   : 'clean_value',
                        3   : 'd',
                        4   : 'pno',
                        5   : 'bbox',
                        6   : 'gv_ph',
                        7   : 'scale',
                        8   : 'value_type',
                        9   : 'currency',
                        10  : 'ph',
                        11  : 'label change',
                        12  : 're_stated',
                        13  : 'period',
                        14  : 'description',
                        15  : 'f_col',
                        16  : 'f_row',
                        17  : 'operator',
                        18  : 'taxo',
                        19  : 'row_id',
                        20  : 'col_id',
                        21  : 'taxo_id',
                        22  : 'maturity_date',
                        23  : 'h_or_t',
                        24  : 'table_id',
                        25  : 'xml_id',
                        26  : 'tn',
                        27  : 'f_ph_col',
                        28  : 'checksum',
                        29  : 'f_ph_str',
                        30  : 'calc_value',
                        31  : 're_stated_all',
                        32  : 'data_available',
                        33  : 'level_id',
                        34  : 'edited_value',
                        35  : 'translated_text',
                        36  : 'available_formula',
                        37  : 'available_in_template',
                        38  : 'as_reported_value',
                        39  : 'as_reported_phcsv',
                        40  : 're_stated_cell',
                        41  : 'calc_value_db',
                        42  : 'actual_value_flg',
                        43  : 'mnemonic_id',
                        44  : 'TASNODE',
                    }
        pass
    def create_seq(self, ijson):
        import create_table_seq
        obj = create_table_seq.TableSeq()
        return obj.create_seq(ijson)

    def clean_txts(self, txt):
        alpha      = {'a':1, 'b':1, 'c':1, 'd':1, 'e':1, 'f':1, 'g':1, 'h':1, 'i':1, 'j':1, 'k':1, 'l':1, 'm':1, 'n':1, 'o':1, 'p':1, 'q':1, 'r':1, 's':1, 't':1 ,'u':1, 'v':1, 'w':1, 'x':1, 'y':1, 'z':1}
        tmptxt  = txt.lower()
        if 'note' in tmptxt:
            rind    = tmptxt.rfind('note')
            f   = 0
            for c in tmptxt[rind+4:]:
                if c in alpha:
                    f   = 1
                    break
            if f == 0:
                return txt[:rind]
        return txt

    def form_yoy_formula(self, row, rr, all_phs, table_type, to_dealid):
        form_d  = {}
        for ph in all_phs.keys():
            pt, year    = ph[:-4], int(ph[-4:])
            prev_year   = year-1
            p_y         = pt+str(prev_year)
            if p_y in all_phs:
                c_y = pt+str(year)
                #((c_y - p_y)/p_y)*100
                form_ar = []
                dd      = {'vt': '', 'rid': 'RID-0', 'c': '', 'g_id': u'', 't_type': table_type, 'txid': rr['t_id'], 'fyear': year, 's': '', 'type': u't', 'op': u'=', 'to_dealid':to_dealid}
                form_ar.append(dd)
                dd      = {'vt': '', 'rid': '', 'c': '', 'g_id': u'', 't_type': u'', 'txid': u'', 'fyear': '', 's': '', 'type': u'v', 'op': u'((', 'to_dealid':to_dealid}
                form_ar.append(dd)

                dd      = {'k': c_y, 'ph':c_y, 'vt': '', 'rid': 'RID-0', 'c': '', 'g_id': u'', 't_type': table_type, 'txid': row['t_id'], 's': '', 'type': u't', 'op': u'+', 'to_dealid':to_dealid}
                form_ar.append(dd)


                dd      = {'k': p_y, 'ph':p_y,'vt': '', 'rid': 'RID-0', 'c': '', 'g_id': u'', 't_type': table_type, 'txid': row['t_id'], 's': '', 'type': u't', 'op': u'-', 'to_dealid':to_dealid}
                form_ar.append(dd)

                dd      = {'vt': '', 'rid': 'RID-133', 'c': '', 'g_id': u'', 't_type': u'', 'txid': u'', 'fyear': '', 's': '', 'type': u'v', 'op': u')', 'to_dealid':to_dealid}
                form_ar.append(dd)

                dd      = {'k': p_y, 'ph':p_y, 'vt': '', 'rid': 'RID-0', 'c': '', 'g_id': u'', 't_type': table_type, 'txid': row['t_id'], 's': '', 'type': u't', 'op': u'/', 'to_dealid':to_dealid}
                form_ar.append(dd)

                dd      = {'period': u'', 'yd': 0, 'vt': '', 'rid': 'RID-0', 'c': '', 'g_id': u'', 't_type': u'', 'txid': u'', 'fyear': '', 's': '', 'type': u'v', 'op': u')', 'to_dealid':to_dealid}
                form_ar.append(dd)

                dd      = {'period': u'', 'yd': 0, 'vt': '', 'rid': 'RID-0', 'c': '', 'g_id': u'', 't_type': u'', 'txid': u'100', 'fyear': '', 's': '', 'type': u'v', 'op': u'*', 'to_dealid':to_dealid}
                form_ar.append(dd)
                form_d[ph]  = ('RID-0', form_ar)
        return form_d

    def get_year(self, ijson):
        if ijson.get('project_name', ''):
            company_name    = ijson['company_name']
            mnumber         = ijson['model_number']
            db_file         = self.get_db_path(ijson)
            conn, cur   = conn_obj.sqlite_connection(db_file)
            for col in ['rsp_period', 'rsp_year', 'rfr', 'restated_period', 'rfr_tt', 'scale_vt', 'formula_derivation', 'excel_start_period', 'excel_start_year', 'doc_type_info', 'mt_set', 'r_as_r']:
                try:
                    sql = 'alter table company_config add column %s TEXT'%(col)
                    cur.execute(sql)
                except:pass
            sql         = "select reporting_type, total_years, derived_ph, periods, ind_group_flg, taxo_flg, excel_config_str, rsp_period, rsp_year, rfr, r_as_r, restated_period, rfr_tt, scale_vt, formula_derivation, excel_start_period, excel_start_year, doc_type_info, mt_set from company_config where project_name='%s'"%(ijson['project_name'])
            try:
                cur.execute(sql)
                res = cur.fetchall()
            except:
                res = []
            conn.close()
            for rr in res:
                reporting_type, total_years, derived_ph, periods, ind_group_flg, taxo_flg, excel_config_str, rsp_period, rsp_year, rfr, r_as_r, restated_period, rfr_tt, scale_vt, formula_derivation, excel_start_period, excel_start_year, doc_type_info, mt_set  = rr
                #print rr
                scale_vt_info   = {}
                if scale_vt:
                    for ts1 in scale_vt.split('#'):
                        ts1 = ts1.strip().split('~')
                        if len(ts1) == 3:
                            scale_vt_info[ts1[1]]   = ts1[0]
                            igtids  = {}
                            for k, v in eval(ts1[2]).items():
                                tkps    = k.split('$$')
                                if len(tkps ) != 2:
                                    tkps    = [tkps[0], ''] 
                                igtids[tuple(tkps)]   = map(lambda x:str(x), v)+igtids.get(tuple(tkps), [])
                            scale_vt_info[('IGNORE', ts1[1])]   = igtids
                            continue
                        if len(ts1) != 2:continue
                        scale_vt_info[ts1[1]]   = ts1[0]
                if reporting_type == 'Reported':
                    rfr, restated_period    ='',''
                    r_as_r  = ''
                if rfr_tt and rfr == 'Y':
                    rfr_tt  = rfr_tt.split('##')
                else:
                    rfr_tt  = []
                ex_d    = {}
                for pc in excel_config_str.split('^!!^'):
                    if not pc:continue
                    p, cs    = pc.split('$$')
                    p_sp = p.split('@:@')
                    text, group_id, tmpy, ttype = p_sp[:4]
                    t_ids   = []
                    if len(p_sp) > 4:
                        t_ids   = filter(lambda x:x, p_sp[4].split('&&'))
                    ignore_dphs   = []
                    if len(p_sp) > 5:
                        ignore_dphs   = filter(lambda x:x, p_sp[5].split('##'))
                    remove_not_restated   = []
                    if len(p_sp) > 6:
                        remove_not_restated   = filter(lambda x:x, p_sp[6].split('##'))
                    try:
                        p_ig = p_sp[7]
                    except:p_ig = ''
                    try:
                        p_db = p_sp[8]
                    except:p_db = ''
                    try:
                        p_rfr = p_sp[9]
                    except:p_rfr = ''
                    try:
                        p_phd = p_sp[10]
                    except:p_phd = ''
                    try:
                        p_offset_year = p_sp[11]
                    except:p_offset_year = ''
                    if p_rfr == 'within':
                        ex_d[('RFR', ttype)] = remove_not_restated+ex_d.get(('RFR', ttype), [])
                    elif p_rfr == 'across':
                            ex_d[('RFR', 'DEAL')] = remove_not_restated+ex_d.get(('RFR', 'DEAL'), [])
                        
                    if tmpy == '0' and ttype:
                        if group_id:
                            ex_d[(ttype, group_id)] = (t_ids, ignore_dphs, remove_not_restated, p_offset_year)
                        else:
                            ex_d[(ttype, '')] = (t_ids, ignore_dphs, remove_not_restated, p_offset_year)
                    for c in cs.split('@@'):
                        if not c:continue
                        c_sp    = c.split('^^')
                        group_id, table_type    = c_sp[:2]
                        t_ids   = []
                        if len(c_sp) > 2:
                            t_ids   = filter(lambda x:x, c_sp[2].split('&&'))
                        ignore_dphs   = []
                        if len(c_sp) > 3:
                            ignore_dphs   = filter(lambda x:x, c_sp[3].split('##'))
                        remove_not_restated   = []
                        if len(c_sp) > 4:
                            remove_not_restated   = filter(lambda x:x, c_sp[4].split('##')) 
                        try:
                            c_ig = c_sp[5]
                        except:c_ig = ''
                        try:
                            c_db = c_sp[6]
                        except:c_db = ''
                        try:
                            c_rfr = c_sp[7]
                        except:c_rfr = ''
                        try:
                            c_phd = c_sp[8]
                        except:c_phd = ''
                        try:
                            c_offset_year = c_sp[9]
                        except:c_offset_year = ''
                        if c_rfr == 'within':
                            ex_d[('RFR', table_type)] = remove_not_restated+ex_d.get(('RFR', table_type), [])
                        elif c_rfr == 'across':
                            ex_d[('RFR', 'DEAL')] = remove_not_restated+ex_d.get(('RFR', 'DEAL'), [])


                        if group_id:
                            ex_d[(table_type, group_id)] = (t_ids, ignore_dphs, remove_not_restated, c_offset_year)
                        else:
                            ex_d[(table_type, '')] = (t_ids, ignore_dphs, remove_not_restated, c_offset_year)
                if not formula_derivation:
                    formula_derivation  = '0'
                doc_grps    = []
                if doc_type_info and doc_type_info != 'None':
                    for rss in doc_type_info.split('@@'):
                        grp_info, dptype = rss.split('##')          
                        doc_grps.append(dptype.split('~'))
                ignore_ttypes    = {}
                if mt_set and mt_set != 'None':
                    for mt_i in mt_set.split('@@'):
                        rmt_id, rmt_data = mt_i.split('~')
                        itd = {}
                        for e in rmt_data.split('##'):
                            tmpttp  = tuple(e.split('$$'))
                            if len(tmpttp) == 1:
                                tmpttp  = (tmpttp[0], '')
                            itd[tmpttp]  = 1
                        ignore_ttypes[rmt_id]   = itd
                cinfo   = {'reporting_type':reporting_type, 'total_years':total_years, 'derived_ph':derived_ph, 'periods':periods, 'rsp_period':rsp_period,'rsp_year':rsp_year, 'rfr':rfr, 'r_as_r':r_as_r, 'restated_period':restated_period, 'rfr_tt':rfr_tt, 'ex_d':ex_d, 'scale_info_m':scale_vt_info, 'formula_derivation':formula_derivation, 'es_p':excel_start_period, 'es_yr':excel_start_year,'doc_type_info':doc_grps, 'ignore_ttypes':ignore_ttypes}
                ijson['g_cinfo']   = cinfo
                ijson['scale_info_m']   = scale_vt_info
                try:
                    ijson['year']   = int(total_years)
                except:pass
        return ijson

    def split_by_camel(self, txt):
        if isinstance(txt, unicode) :
            txt = txt.encode('utf-8')  
        if ' ' in txt:
            return txt
        txt_ar  = []
        for c in txt:
            if c.upper() == c:
                txt_ar.append(' ')
            txt_ar.append(c)
        txt = ' '.join(''.join(txt_ar).split())
        return txt
                
                
                
    def read_kpi_data(self, ijson, return_data=None):
        #import utils.convert_currency_across as currency_convert_across
        #self.cconvert_across_obj   = currency_convert_across.Convert()
        if ijson.get("PRINT") != "Y":
            disableprint()
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        ph_formula_d    = self.read_ph_user_formula(ijson, '')
        db_file         = self.get_db_path(ijson)
        ptype_d = {
                    'quarterly': {
                                    'Q1':1,
                                    'Q2':1,
                                    'Q3':1,
                                    'Q4':1,
                                    }, 
                    'annualized'    :{
                                        'FY':1
                                    }
                    }
        conn, cur       = conn_obj.sqlite_connection(db_file)
        print db_file
        sql = "select row_id, taxonomy, c_id, ph from mt_data_builder where table_type='%s'"%(ijson['table_type'])
        cur.execute(sql)
        res = cur.fetchall()
        edited_value    = {}
        for rr in res:
            row_id, taxonomy, c_id, ph  = rr
            if c_id and ph:
                edited_value.setdefault(taxonomy, {})[ph]   = (row_id, c_id)
            
        #for k, v in ph_formula_d.items():
        #    print k, v
        display_name_d  = {}
        ttype_d = {}
        phs_d   = {}
        all_phs = {}
        if ijson.get('INPUT_DB_DATA'):
            display_name_d, ttype_d, phs_d, all_phs = ijson['INPUT_DB_DATA']
        time_series = ''
        empty_line_display  = 'Y'
        if ijson.get('from_merge') != 'Y':
            if not ijson.get("template_id"):
                db_file     = '/mnt/eMB_db/page_extraction_global.db'
                conn, cur   = conn_obj.sqlite_connection(db_file)
                sql = "select industry_type, industry_id from industry_type_storage"
                cur.execute(sql)
                res = cur.fetchall()
                conn.close()
                exist_indus = {}
                for rr in res:
                    industry_type, industry_id  = rr
                    industry_type   = industry_type.lower()
                    exist_indus[industry_type]  = industry_id
                industry_id = exist_indus[industry_type]
                db_file     = "/mnt/eMB_db/industry_kpi_taxonomy.db"
                conn, cur   = conn_obj.sqlite_connection(db_file)
            else:
                industry_id = ijson['template_id']
                db_file     = "/mnt/eMB_db/template_info.db"
                conn, cur   = conn_obj.sqlite_connection(db_file)
                sql = "select time_series, display_name, empty_line_display from meta_info where row_id=%s"%(industry_id)
                cur.execute(sql)
                tmpres  = cur.fetchone()
                display_name_d[(int(deal_id), ijson['table_type'], '')]   = tmpres[1]
                time_series = tmpres[0]
                if tmpres[2] == 'N':
                    empty_line_display  = 'N'
            

            
        tindustry_type  = ijson['table_type'].lower()
        self.kpi_types[ijson['table_type']]  = 1
        #sys.exit()
        data_d          = {}
        grp_d           = {}
        all_table_types = {}
        taxo_f_d        = {}
        kpi_taxo_f_d    = {}
        f_ar    = []
        yoy_d   = {}
        all_taxoids     = {}
        table_type  = ijson['table_type']
        p_c_d       = {}
        rev_map = {}
        if ijson.get('from_merge') != 'Y':
            try:
                    sql = "alter table industry_kpi_template add column target_currency TEXT"
                    cur.execute(sql)
            except:pass
            sql         = "select taxo_id, prev_id, parent_id, taxonomy, taxo_label, scale, value_type, client_taxo, yoy, editable, formula_str, sign, target_currency, period_type, required_period, mnemonic_id from industry_kpi_template where industry_id=%s"%(industry_id)
            cur.execute(sql)
            res         = cur.fetchall()
            for rr in res:
                taxo_id, prev_id, parent_id, taxonomy, taxo_label, scale, value_type, client_taxo, yoy, editable, formula_str, sign, target_currency, period_type, required_period, mnemonic_id   = rr
                p_c_d[taxo_id]       = parent_id
                p_c_d[('TAXO', taxo_id)]       = taxonomy
                p_c_d[taxonomy]       = taxo_id
    
                if client_taxo and ijson.get('gen_output') == 'Y':
                    taxo_label = client_taxo
                if scale == '1.0':
                    scale   = '1'
                #if str(deal_id) == '51':
                #    scale   = ''
                grp_d.setdefault(parent_id, {})[prev_id]    = taxo_id
                data_d[taxo_id]  = (taxo_id, taxonomy, taxo_label, scale, value_type, client_taxo, yoy, editable, formula_str, sign, target_currency, period_type, required_period, mnemonic_id)
                all_taxoids[taxonomy]   = period_type
            final_ar    = []
            def form_tree_data(dd, level, pid):
                prev_id = -1
                iDs = []
                pid = -1
                done_d  = {}
                if (pid not in dd) and dd:
                    ks  = dd.keys()
                    ks.sort()
                    pid = ks[0]

                while 1:
                    if pid not in dd:break
                    ID  = dd[pid]
                    if (ID, pid) in done_d:break #AVOID LOOPING
                    done_d[(ID, pid)]  = 1
                    pid = ID
                    iDs.append(ID)
                tmp_ar  = []
                prev_id = -1
                for iD in iDs:
                    final_ar.append(data_d[iD]+(level, pid, prev_id))
                    c_ids   = grp_d.get(iD, {})
                    if c_ids:
                        form_tree_data(c_ids, level+1, iD)
                    prev_id = iD
            root    = grp_d[-1]
            form_tree_data(root, 1, -1)

            for rr in final_ar:
                taxo_id, taxonomy, taxo_label, scale, value_type, client_taxo, yoy, editable, formula_str, sign, target_currency, period_type, required_period, mnemonic_id, level_id, parrent_id, prev_id  = rr
                req_ptypes  = {}
                if required_period:
                    for ptype in required_period.split(','):
                        ptype   = ' '.join(ptype.split()).strip().lower()
                        req_ptypes.update(ptype_d.get(ptype, {}))
                    pass
                if ijson.get('gen_output') == 'Y':
                    taxo_label  = self.split_by_camel(taxo_label)
                if formula_str:
                    tmpf_ar  = []
                    ttaxo   = []
                    #print '\n-----------------------------------'
                    #print (taxonomy, taxo_label)
                    for ft in formula_str.split(':$$:'):
                        if not ft:continue
                        tname, op, ftype = ft.split('@@@')
                        #print '\t', ft
                        ttaxo.append(tname)
                        dd  = {'txid':tname, 'type':ftype, 't_type':table_type, 'g_id':'', 'op':op, 'c':'', 's':'','vt':'', 'rid':'RID-0', 'to_dealid':int(deal_id), 'i_f_type':'EXCELFORMULA'}
                        tmpf_ar.append(dd)
                    if tmpf_ar:
                        dd  = {'txid':taxonomy, 'type':'t', 't_type':table_type, 'g_id':'', 'op':'=', 'c':'', 's':'','vt':'', 'rid':'RID-0', 'to_dealid':int(deal_id), 'i_f_type':'EXCELFORMULA'}
                        tmpf_ar = [dd]+tmpf_ar
            
                        ph_formula_d.setdefault(('ALL_F', taxonomy), {})[tuple(ttaxo)]  = ('RID-0', tmpf_ar)
                row = {'t_id':taxonomy, 't_l':taxo_label, 'parent_txt':scale+'~'+value_type, 'th_flg':"", "d":"", "x":"x-1_1","bbox":[], 'p_id':parrent_id, 'prev_id':prev_id, '$$treeLevel':level_id, 'scale':scale, 'value_type':value_type, 'yoy':yoy, 'editable':editable, 'kpi_row_sign':sign, 'target_currency':target_currency, 'req_ptypes':req_ptypes, 'ptypes':period_type, 'mnemonic_id':mnemonic_id}
                if yoy == 'Y':
                    trow = {'t_id':taxonomy+'YoY', 't_l':taxo_label+' Growth (%)', 'parent_txt':'1~Percentage', 'th_flg':"", "d":"", "x":"x-1_1","bbox":[], 'p_id':taxo_id, 'prev_id':-1, '$$treeLevel':level_id+1, 'scale':'1', 'value_type':'Percentage'}
                    ttype_d.setdefault((int(deal_id), table_type, ''), {})[str(taxonomy+'YoY')]    = trow
                    yoy_d[taxonomy] = trow
                if row['t_id'] in edited_value:
                    for ph, vtup in edited_value[row['t_id']].items():
                        row[ph]  = {'v':vtup[1], 'save_id':vtup[0], 'edited':'Y', 'x':'x-1_1', 't':'-1', 'd':'-1', 'phcsv':{'p':ph[-4:], 'pt':ph[:-4], 's':scale, 'c':'', 'vt':value_type},'rid':1, 'bbox':[]}
                ttype_d.setdefault((int(deal_id), table_type, ''), {})[str(taxonomy)]    = row
                rev_map[taxonomy] = len(f_ar)
                #print 'RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR'
                #print row , '\n'
                f_ar.append(row)
        else:
            ijson_c                     = copy.deepcopy(ijson)
            del ijson_c['from_merge']
            ijson_c['taxo_flg']         = 1
            ijson_c['table_type']       = ijson_c['table_type'].split('@MERGE')[0]
            ijson_c['without_c_s']      = 'Y'
            import data_builder.db_data as db_data
            obj = db_data.PYAPI()
            tres = obj.read_db_data(ijson_c)
            for rr in tres[0]['data']:
                taxo_id, taxonomy, taxo_label, scale, value_type, client_taxo, yoy, editable, level_id, parrent_id, prev_id  = '', '', '', '', '', '', '', '', '', '', ''
                p_c_d[taxo_id]       = parrent_id
                p_c_d[('TAXO', taxo_id)]       = taxonomy
                p_c_d.setdefault(('CHILD', parrent_id), {})[taxo_id]    = 1
                p_c_d[taxonomy]       = taxo_id
                taxonomy    = str(rr['t_id'])
                taxo_label  = rr['t_l']
                row = {'t_id':taxonomy, 't_l':taxo_label, 'parent_txt':scale+'~'+value_type, 'th_flg':"", "d":"", "x":"x-1_1","bbox":[], 'p_id':parrent_id, 'prev_id':prev_id, '$$treeLevel':level_id, 'scale':scale, 'value_type':value_type, 'yoy':yoy, 'editable':editable, 'o_tid':taxo_id}
                all_taxoids[taxonomy]   = 1
                ttype_d.setdefault((int(deal_id), table_type, ''), {})[str(taxonomy)]    = row
                rev_map[taxonomy] = len(f_ar)
                f_ar.append(row)
        import data_builder.db_data as db_data
        obj = db_data.PYAPI()
        m_tables, rev_m_tables, doc_m_d, table_type_m = self.get_main_table_info(company_name, model_number)
        ijson_c = copy.deepcopy(ijson)
        ijson_c['pos_ids']  = all_taxoids.keys()
        taxo_pos_d = {} #self.read_pos_mapping(ijson_c)
        
        form_cls_d  = {}
        ptype_taxo  = {}
        ttype_grp_d = {}
        rev_form_d  = {}
        for taxonomy, period_type in all_taxoids.items():
                t_row_f     = ph_formula_d.get(('USER F', taxonomy), ())
                t_row_f_all = ph_formula_d.get(('ALL_USER F', taxonomy), {})
                fflag       = 'USER F'
                if not t_row_f:
                    fflag       = 'F'
                    t_row_f     = ph_formula_d.get(('F', taxonomy), ())
                    t_row_f_all = ph_formula_d.get(('ALL_F', taxonomy), {})
                for flg in  ['CELL F', 'PTYPE F']:
                    for cell_id, t_row_f in ph_formula_d.get((flg, taxonomy), {}).items():
                        ptype_taxo[taxonomy]  = 1
                if t_row_f_all:
                    #print '\n============================='
                    #print taxonomy
                    if len(t_row_f_all.keys()) >= 1:
                        done_rem  = {}
                        for f_taxos, t_row_ft in t_row_f_all.items():
                            found_other = 0
                            ttype_cl    = {}
                            taxo_cl     = {}
                            found_M     = 0
                            #print
                            for ft in t_row_ft[1]:
                                #print '\t', ft
                                if ft['type'] != 'v':
                                    if ft['op'] != '=':
                                        if ft['op'] == 'M':
                                            found_M = 1
                                        if ft['t_type'] != ijson['table_type']:
                                            ttype_cl[(ft['t_type'], ft['g_id'])]    = 1
                                            if period_type:
                                                form_key    = '%s^^%s^^%s'%(ft['t_type'], ft['g_id'], ft['txid'])
                                                rev_form_d.setdefault(form_key, {}).setdefault(period_type, {})[taxonomy]    = 1
                                        if ft['g_id'] == '':
                                            if ft['t_type'] not in ttype_grp_d:
                                                g_ar    = obj.read_all_vgh_groups(ft['t_type'], company_name, model_number, doc_m_d, {})
                                                ttype_grp_d[ft['t_type']]   = g_ar
                                            else:
                                                g_ar    = ttype_grp_d[ft['t_type']]   
                                            if g_ar:
                                                print 'Error  Mapped From Main ', (company_name, taxonomy, ft['t_type'], ft['txid'])
                                            
                                        taxo_cl[(ft['t_type'], ft['g_id'], ft['txid'])]    = 1
                                        all_table_types.setdefault(ft['to_dealid'], {}).setdefault(ft['t_type'], {}).setdefault(ft['g_id'], {})[ft['txid']]  = 1
                                    if ft['t_type'] != ijson['table_type'] and ft['op'] != '=':
                                        found_other = 1
                            if found_other ==0:
                                taxo_f_d[taxonomy]  = t_row_ft
                            else:
                                done_rem[f_taxos]  = 1
                            if len(taxo_cl.keys()) == 1 or found_M:
                                form_cls_d.setdefault(taxonomy, {})['DIRECT']    = t_row_ft
                            elif len(ttype_cl.keys()) == 0:
                                form_cls_d.setdefault(taxonomy, {})['DIRECT']    = t_row_ft
                            else:
                                form_cls_d.setdefault(taxonomy, {})['DIRECT']    = t_row_ft
                                
                        for f_taxos in done_rem.keys():
                                ph_formula_d[(fflag, taxonomy)]   = t_row_f_all[f_taxos]
                    else:
                        form_cls_d.setdefault(taxonomy, {})['DIRECT']    = t_row_f
                        pass
                    t_row_f     = ph_formula_d.get((fflag, taxonomy), ())
                    t_kpi_taxo_f_d  = {}
                    if t_row_f:
                        for ft in t_row_f[1]:
                            if ft['type'] != 'v':
                                if ft['t_type'] == ijson['table_type'] and ft['op'] != '=':# and taxonomy not in taxo_f_d:
                                    t_kpi_taxo_f_d[taxonomy]  = t_kpi_taxo_f_d.get(taxonomy, 0)+1
                                    taxo_f_d[taxonomy]   = {}
                                    if period_type:
                                        form_key    = '%s^^%s^^%s'%(ft['t_type'], ft['g_id'], ft['txid'])
                                        rev_form_d.setdefault(form_key, {}).setdefault(period_type, {})[taxonomy]    = 1
                                all_table_types.setdefault(ft['to_dealid'], {}).setdefault(ft['t_type'], {}).setdefault(ft['g_id'], {})[ft['txid']]  = 1
                    if t_kpi_taxo_f_d.get(taxonomy, 0) > 1:
                        kpi_taxo_f_d[taxonomy]  = 1
                    for flg in  ['CELL F', 'PTYPE F']:
                        for cell_id, t_row_f in ph_formula_d.get((flg, taxonomy), {}).items():
                            ptype_taxo[taxonomy]  = 1
                            t_kpi_taxo_f_d  = {}
                            for ft in t_row_f[1]:
                                if ft['type'] != 'v':
                                    if ft['t_type'] == ijson['table_type'] and ft['op'] != '=':# and taxonomy not in taxo_f_d:
                                        t_kpi_taxo_f_d[taxonomy]  = t_kpi_taxo_f_d.get(taxonomy, 0)+1
                                        taxo_f_d[taxonomy]   = {}
                                    all_table_types.setdefault(ft['to_dealid'], {}).setdefault(ft['t_type'], {}).setdefault(ft['g_id'], {})[ft['txid']]  = 1
                            if t_kpi_taxo_f_d.get(taxonomy, 0) > 1:
                                kpi_taxo_f_d[taxonomy]  = 1
            
        #print all_table_types.keys()
        #sys.exit()
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()
        ijson_c = copy.deepcopy(ijson)
        if table_type in all_table_types.get(int(deal_id), {}):
            del all_table_types[int(deal_id)][table_type]
        if ijson.get('PRINT') != 'Y':
            disableprint()
        dealijson   = self.read_company_info({"cids":map(lambda x:str(x), all_table_types.keys())+[str(deal_id)]})
        reported    = 'N'
        if ijson.get('reported') == 'Y':
            reported    = 'Y'
        cinfo           = ijson.get('g_cinfo', {})
        if ijson.get('reported') == 'Y' or  cinfo.get('reporting_type', '') == 'Reported':
            reported    = 'Y'
        for to_dealid, tinfo in all_table_types.items():
            tmpjson    = copy.deepcopy(dealijson['%s_%s'%(ijson['project_id'], to_dealid)])
            tmpjson['project_name']    = ijson['project_name']
            tmpjson['model_number']    = ijson['model_number']
            tmpjson['store_flg']        = ijson.get('store_flg', '')
            tmpjson['norm_scale']        = ijson.get('norm_scale', 'Y')
            tmpjson                    = self.get_year(tmpjson)
            tmpjson['reported'] =  reported
            if table_type in tinfo:
                del tinfo[table_type]
            if ijson.get('gen_output') == 'Y' and ijson.get('GEN_DEPEND') == 'Y':
                self.gen_table_type_data(tinfo, tmpjson, company_name, model_number, doc_m_d, obj, tmpjson, to_dealid, display_name_d, ttype_d, phs_d, all_phs, rev_form_d)
                import model_view.data_builder_new_excel_pr3 as PH_LABEL
                tobj = PH_LABEL.MRD_excel()
                res = tobj.gen_output(tmpjson) 
            self.get_table_type_data(tinfo, tmpjson, company_name, model_number, doc_m_d, obj, tmpjson, to_dealid, display_name_d, ttype_d, phs_d, all_phs)
        t_col_grp_d = ttype_d.get('t_col_grp_d', {})
        if t_col_grp_d and ijson.get('gen_output') == 'Y':
            tmp_path            = '/mnt/eMB_db/%s/%s/PREVIEW_OUTPUT/%s_%s/'%(company_name, model_number, 'TEMPLATE_PH_UPDATE', ijson['template_id'])
            os.system("rm -rf '%s'"%(tmp_path))
            os.system("mkdir -p '%s'"%(tmp_path))
            env1        = lmdb.open(tmp_path, map_size=2**39)
            print '\tSTORE ', tmp_path
            with env1.begin(write=True) as txn1:
                txn1.put('res', str(t_col_grp_d))
        #sys.exit()
                
        phs = [{"k":"PH", "g":"PH", "n":"PH"}]
        if all_phs:
            phs = report_year_sort.year_sort(all_phs.keys())
            phs.reverse()
            phs = map(lambda x:{'k':x, 'n':x,'g':x,  'dph':(x[:-4], int(x[-4:]))}, phs)
        elif time_series == 'N':
            phs             = map(lambda x:{'k':x, 'n':x,'g':x}, [ijson['company_name']])
        else:
            phs             = []
            pt_arr          = report_year_sort.year_sort(['FY2018', 'Q12018', 'Q22018', 'Q32018', 'Q42018', 'H12018', 'H22018', 'M92018'])
            pt_arr.reverse()
            phs             = map(lambda x:{'k':x, 'n':x,'g':x,  'dph':(x[:-4], int(x[-4:]))}, pt_arr)
        cmeta_info  = {}
        txt_path = '/mnt/eMB_db/%s/%s/company_meta_info.txt'%(ijson['company_name'], ijson['model_number'])
        if os.path.exists(txt_path):
            f       = open(txt_path)
            lines   = f.readlines() 
            f.close()
            if len(lines) > 1:
                header = map(lambda x:x.strip(), lines[0].split('\t'))
                meta   = map(lambda x:x.strip(), lines[1].split('\t'))
                if len(header) == len(meta):
                    for i, h in enumerate(header):
                        cmeta_info[h]    = meta[i]
        if self.return_fomula == 'Y':
            for rr in f_ar:
                for ph in all_phs.keys():
                    rr.setdefault(ph, {}).setdefault('phcsv', {})
                    rr[ph]['phcsv']['s']   = rr['scale'] 
                    rr[ph]['phcsv']['vt']   = rr['value_type'] 
                    rr[ph]['x']             = 'x-1_-1'
                ttype_d.setdefault((int(deal_id), table_type, ''), {})[str(rr['t_id'])]    = rr
        pc_d    = {}
        if 1:#ijson.get('PRINT') == 'Y':
            level_d = {}
            def form_recursive_formula(resid, opers, done_t_d, level_id):
                c_level_d   = {resid:{}}
                #print '\t'*level_id, [resid]
                c_ar    = []
                for oper in opers:
                    op_txid     = oper['txid']
                    op          = oper['op']
                    op_type     = oper['type']
                    ttype       = oper['t_type']
                    grpid       = oper['g_id']
                    if op_type == 'v':
                        #tmpopers.append(oper)
                        continue
                    to_dealid= oper['to_dealid']
                    if (ttype, grpid) != (table_type, ''):continue
                    #print '\t'*(level_id+1), [op_txid]
                        
                    t_row_f = ()
                    for cnt, ftype in enumerate(['DIRECT']):#, 'ACROSS', 'WITHIN']):
                        t_row_f     = form_cls_d.get(str(op_txid), {}).get(ftype, ()) #ph_formula_d.get(('USER F', rr['t_id']), ())  
                        break
                    if t_row_f:
                        c_level_d[resid][level_id]    = 1
                        c_ar.append(str(op_txid))
                        if ((ttype, grpid, str(op_txid)) not in done_t_d):
                                done_t_d[(ttype, grpid, str(op_txid))]= []
                                tmplevel_d, tmpc_ar = form_recursive_formula(op_txid, filter(lambda x:x['op'] != '=', t_row_f[1]),  done_t_d, level_id+1)
                                done_t_d[(ttype, grpid, str(op_txid))]= tmpc_ar
                                #print '\t'*(level_id+1), 'N', tmpc_ar 
                                c_ar    += tmpc_ar
                                for k, v in tmplevel_d.items():
                                    c_level_d.setdefault(resid,  {}).update(v)
                                    #c_level_d.setdefault(k,  {}).update(v)
                        else:
                            tmpc_ar = done_t_d[(ttype, grpid, str(op_txid))]
                            #print '\t'*(level_id+1), tmpc_ar 
                            c_ar    += tmpc_ar
                #print '\t'*(level_id), 'F', c_ar 
                return c_level_d, c_ar
            done_t_d    = {}
            for cnt, ftype in enumerate(['DIRECT']):#, 'ACROSS', 'WITHIN']):
                for ti, rr in enumerate(f_ar[:]):
                    t_row_f     = form_cls_d.get(str(rr['t_id']), {}).get(ftype, ()) #ph_formula_d.get(('USER F', rr['t_id']), ())
                    if t_row_f:
                        c_level_d, c_ar   = form_recursive_formula(str(rr['t_id']), filter(lambda x:x['op'] != '=',t_row_f[1]), done_t_d, 1)
                        done_t_d[(table_type, '', str(rr['t_id']))]    = c_ar
                        pc_d[str(rr['t_id'])]   = c_ar
                        for k, v in c_level_d.items():
                            level_d.setdefault(k,  {}).update(v)
            print 'PC_D'
            for k, v in level_d.items():
                print k, v, pc_d.get(k, [])
            #sys.exit()
        final_form_ar   = []
        cnt = 0
        done_f_d    = {}
        #while cnt < 2:
        sn      = 0
        terr    = {}
        #print 'ptype_taxo ', ptype_taxo
        found_d = {}
        done_form_d = {}
        done_taxo_d = {}
        t_ar    = range(len(f_ar))
        #pc_d    = {}
        tmppc_d = {}
        if pc_d:# and update_form != 'DN':
            t_ar    = []
            for ti, rr in enumerate(f_ar[:]):
                t_ar.append((ti, len(pc_d.get(rr['t_id'], []))))
            t_ar.sort(key=lambda x:x[1], reverse=True)
            t_ar    = map(lambda x:x[0], t_ar)
            tmppc_d = copy.deepcopy(pc_d)
            
        for cnt, ftype in enumerate(['DIRECT']):#, 'ACROSS', 'WITHIN']):
            #for ti, rr in enumerate(data[:]):
            for m_ti in t_ar:
                if m_ti in done_taxo_d:continue
                tmp_ar  = f_ar
                rr  = f_ar[m_ti]
                #print 'PC ', [update_form, rr['t_id'], ti, rr['t_l'], tmppc_d.get(str(rr['t_id']), [])]
                tmptar  = map(lambda x:rev_map[x], tmppc_d.get(str(rr['t_id']), []))
                tmptar  = [m_ti]+tmptar
                tmptar.reverse()
                #print tmptar
                for ti in tmptar:
                    #print '\t', ti
                    if ti in done_taxo_d:continue
                    rr  = f_ar[ti]
                    #rr['m_currency']  = 'USD'
                    #if cnt > 1 and rr['t_id'] not in taxo_f_d:continue
                    t_row_f     = form_cls_d.get(rr['t_id'], {}).get(ftype, ()) #ph_formula_d.get(('USER F', rr['t_id']), ())
                    #if not t_row_f:
                    #    t_row_f         = ph_formula_d.get(('F', rr['t_id']), ())
                    if 1:#ftype == 'WITHIN':
                        t_row_f_ptype   = ph_formula_d.get(('PTYPE F', rr['t_id']), {})
                        t_row_f_cell    = ph_formula_d.get(('CELL F', rr['t_id']), {})
                        if cnt > 1 and taxo_f_d.get(rr['t_id']):
                            t_row_f = taxo_f_d[rr['t_id']]
                    else:
                        t_row_f_ptype   = ()
                        t_row_f_cell    = ()
                    if (t_row_f and t_row_f[1]) or (t_row_f_ptype) or t_row_f_cell:
                        if 0:
                            print '\n', [cnt, rr['t_l'], kpi_taxo_f_d.get(rr['t_id']), ftype]
                            if t_row_f:
                                for ft in t_row_f[1]:
                                   print '\t', ft
                            if t_row_f_ptype:
                                for pk, pv in t_row_f_ptype.items():
                                    print '\n\t-----------------------------------'
                                    print '\tPTYPE ', pk
                                    for ft in pv[1]:
                                        print '\t\t', ft
                                     
                        val_dict, form_d, flip_sign, mismatch_scale, mismatch_vt, taxo_grp = self.get_formula_evaluation_across(rr, t_row_f, ttype_d, all_phs.keys(), phs_d, rr['scale'], rr['value_type'], 'Y', t_row_f_ptype, t_row_f_cell, None, cmeta_info)
                        if self.return_fomula == 'Y':
                            for ph, formulaar in form_d.items():
                                final_form_ar   += formulaar
                            continue
                        update_pk   = {}
                        for k, v in val_dict.get(rr['t_id'], {}).items():
                            #print '\t', [k, v['v']]
                            if rr.get('req_ptypes', {}) and k[:-4] not in rr['req_ptypes']:continue
                                
                            if rr.get(k, {}).get('v', None) != None and rr.get(k, {}).get('v', '') != '' and rr.get(k, {}).get('FORM_VALUE') != 'Y':
                                v_d = rr.get(k, {})
                                try:
                                    clean_value = numbercleanup_obj.get_value_cleanup(v_d['v'])
                                except:
                                    clean_value = '0'
                                    pass
                                if clean_value == '':
                                    clean_value = '0'
                                clean_value = float(clean_value)
                                sum_val     = sum(v['v_ar'])
                                if v['ftype'] == 'NG':
                                    sum_val  = v['v'].replace(',', '')
                                n_value     = abs(abs(clean_value) - abs(float(sum_val))) #v['v'].replace(',', '')) 
                                n_value     = self.convert_floating_point(n_value).replace(',', '')
                                if n_value not in ['0', '0.00', '0.0'] and ftype == 'WITHIN':
                                    rr[k]['c_s']    = n_value
                                else:
                                    rr[k]['c_s']    = ''
                                continue
                            phs_d.setdefault((int(deal_id), table_type, ''), {})[k] = k
                            if ftype !='DIRECT':#kpi_taxo_f_d.get(rr['t_id']) or ftype == 'WITHIN':
                                v['FORM_VALUE'] = 'Y'
                                v['expr_str'] += str(['FORM_VALUE ', 'Y'])
                            v['FORM_VALUE_KPI'] = 'Y'
                            update_pk[k]   = 1
                            v['rid']    = sn
                            sn  += 1
                            rr[k]   = v 
                            v.setdefault('phcsv', {})['vt'] = rr['value_type']
                            #print '\t', v
                            if v.get('v'): 
                                rr['da']    = 'Y'
                        #update_pk   = {}
                        ttype_d.setdefault((int(deal_id), table_type, ''), {})[str(rr['t_id'])]    = rr
                        #print '\tupdate_pk',update_pk, form_d.keys()
                        res_row = self.read_sys_form_across(t_row_f, rr, phs, ttype_d, table_type, {}, phs_d, ijson, display_name_d, update_pk, form_d, dealijson)
                        s, vt   = rr['scale'], rr['value_type']
                        error   = {}
                        m_scale = {}
                        for sc, tinfo in mismatch_scale.items():
                            if sc   == s:continue
                            #m_scale.setdefault(sc, {}).update(tinfo)
                            for ttup in tinfo.keys():
                                ttup    = '@@'.join(ttup)
                                error.setdefault(ttup, {}).setdefault('Scale Missmatch', {})[sc]  = 1
                        m_vt = {}
                        for vtc, tinfo in mismatch_vt.items():
                            if vtc   == vt:continue
                            for ttup in tinfo.keys():
                                ttup    = '@@'.join(ttup)
                                error.setdefault(ttup, {}).setdefault('Value Type Missmatch', {})[sc]  = 1
                        if error:
                            terr[rr['t_id']]    = error
                        if 0:#len(taxo_grp.keys()) == 1 and taxo_grp[taxo_grp.keys()[0]] and ijson.get("gen_output") == 'Y':
                            ttype, tgrpid, ttaxo    = taxo_grp.keys()[0]
                            #print 'ONE ', rr['t_l'], ttype, tgrpid, ttaxo
                    
                            for ph in phs:
                                tph         = phs_d.get((ttype, grpid), {}).get(ph['n'], '')
                                if tph in  ttype_d.get((ttype, tgrpid), {}).get(ttaxo, {}):
                                    v   = copy.deepcopy(ttype_d.get((ttype, tgrpid), {}).get(ttaxo, {})[tph])
                                    v['rid']    = sn
                                    sn  += 1
                                    v['ttype']      = ttype
                                    v['tgrpid']      = tgrpid
                                    if 'rest_ar' in v:
                                        del v['rest_ar']
                                    if 'rest_ar_all' in v:
                                        del v['rest_ar_all']
                                    if 'PH_FORM' in v:
                                        del v['PH_FORM']
                                    op_val_fnd  = 0
                                    for ii, ft in enumerate(v['f_col'][0]):
                                        #print ft
                                        ft['disp_name'] = ttype
                                        if ft['operator'] == '=':
                                            v['f_col'][0][ii]   = res_row[ph['n']]
                                            #print 'F', v['f_col'][0][ii]
                                            #break
                                        elif ft.get('actual_value', '').strip():
                                            op_val_fnd = 1
                                    if op_val_fnd == 1:
                                        rr[ph['n']]['f_col_o']   = copy.deepcopy(v['f_col'])
                                        rr[ph['n']]['f_col']   = copy.deepcopy(v['f_col'])
                        #if cnt > 1 and taxo_f_d.get(rr['t_id']):
                        for pk in update_pk.keys():
                            rr[pk]['f_col_o']   = copy.deepcopy(rr[pk]['f_col'])
                        
        #if ijson.get('EXIT') == 'Y':
        #    return
        if self.return_fomula == 'Y':
            return final_form_ar
        final_ar    = []
        #print 'YOY ',yoy_d.keys()
        taxo_ind_d  = {}
        for ii, tmprow in enumerate(f_ar):
            taxo_ind_d[str(tmprow['t_id'])]  = ii
            final_ar.append(tmprow)
            if tmprow['t_id'] in yoy_d and tmprow.get('da') == 'Y':
                rr              = yoy_d[tmprow['t_id']]
                t_row_f_cell   = self.form_yoy_formula(tmprow, rr, all_phs, table_type, int(deal_id))
                #print '\n', ['YOY', rr['t_l'], kpi_taxo_f_d.get(rr['t_id'])]
                t_row_f_ptype   = {}
                t_row_f         = ()
                if (t_row_f and t_row_f[1]) or (t_row_f_ptype or t_row_f_cell):
                    #if t_row_f:
                    #    for ft in t_row_f[1]:
                    #       print '\t', ft
                    #if t_row_f_ptype:
                    #    for pk, pv in t_row_f_ptype.items():
                    #        print '\n\t-----------------------------------'
                    #        print '\tPTYPE ', pk
                    #        for ft in pv[1]:
                    #            print '\t\t', ft
                    #if t_row_f_cell:
                    #    for pk, pv in t_row_f_cell.items():
                    #        print '\n\t-----------------------------------'
                    #        print '\tPTYPE ', pk
                    #        for ft in pv[1]:
                    #            print '\t\t', ft
                             
                    val_dict, form_d, flip_sign, mismatch_scale, mismatch_vt, taxo_grp = self.get_formula_evaluation_across(rr, t_row_f, ttype_d, all_phs.keys(), phs_d, rr['scale'], rr['value_type'], 'Y', {}, t_row_f_cell)
                    update_pk   = {}
                    f_value = 0
                    for k, v in val_dict.get(rr['t_id'], {}).items():
                        #print '\t', [k, v['v']]
                        if rr.get(k, {}).get('v', None) != None and rr.get(k, {}).get('v', '') != '':
                            v_d = rr.get(k, {})
                            try:
                                clean_value = numbercleanup_obj.get_value_cleanup(v_d['v'])
                            except:
                                clean_value = '0'
                                pass
                            if clean_value == '':
                                clean_value = '0'
                            clean_value = float(clean_value)
                            sum_val     = sum(v['v_ar'])
                            if v['ftype'] == 'NG':
                                sum_val  = v['v'].replace(',', '')
                            n_value     = abs(abs(clean_value) - abs(float(sum_val))) #v['v'].replace(',', '')) 
                            n_value     = self.convert_floating_point(n_value).replace(',', '')
                            if n_value not in ['0', '0.00', '0.0']:
                                rr[k]['c_s']    = n_value
                            else:
                                rr[k]['c_s']    = ''
                            continue
                        phs_d.setdefault((int(deal_id), table_type, ''), {})[k] = k
                        if kpi_taxo_f_d.get(rr['t_id']):
                            v['FORM_VALUE'] = 'Y'
                            v['expr_str'] += str(['FORM_VALUE ', 'Y'])
                        update_pk[k]   = 1
                        v['rid']    = sn
                        sn  += 1
                        rr[k]   = v
                        if v.get('v'): 
                            f_value = 1
                    #update_pk   = {}
                    ttype_d.setdefault((int(deal_id), table_type, ''), {})[str(rr['t_id'])]    = rr
                    #print '\tupdate_pk',update_pk
                    res_row = self.read_sys_form_across(t_row_f, rr, phs, ttype_d, table_type, {}, phs_d, ijson, display_name_d, update_pk, form_d, dealijson)
                    if f_value == 1:
                        final_ar.append(rr)
        table_ar    = [{'dt':[], 't':''}]
        if ijson.get('ptype_info') == 'Y':
            new_ar  = []
            n_phs   = {}
            t_col_d = {}
            table_vgh_gh_d  = {}
            def get_vghh_gh(t):
                txt_ar  = []
                for l in ['GH', 'VGH', 'HGH']:
                    c_ids    = txn_m.get(l+'_'+str(t))
                    if c_ids:
                        for c_id in c_ids.split('#'):
                            txt       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                            txt       = ''.join(txt.split())
                            txt_ar.append(txt)
                return ''.join(txt_ar).strip()
                
            for rr in f_ar:
                dd  = {}
                ignore_k    = {}
                for ph in phs:
                    if ph['k'] not in rr:continue
                    ignore_k[ph['k']]   = 1
                    if ijson.get('ptype_key') and ph['n'][:-4] != ijson.get('ptype_key'):continue
                    if not ijson.get('ptype_key') and (ph['n'][:-4] not in ('Q2', 'Q3')):continue
                    v_d = rr[ph['k']]
                    t   = v_d.get('t')
                    x   = v_d.get('x')
                    if (not t) or (not x):continue
                    if t not in table_vgh_gh_d:
                        table_vgh_gh_d[t]   = get_vghh_gh(t)
                    ttxt    = ''.join(table_vgh_gh_d[t].split()).lower()
                    if ('threemonths' in ttxt) or ('quarter' in ttxt):continue
                    tk      = self.get_quid(t+'_'+x)
                    c_id    = txn_m.get('XMLID_MAP_'+tk)
                    if ijson.get('ptype_table') == 'Y':
                        tk      = '%s-%s'%(t, c_id.split('_')[2])
                    else:
                        tk      = '%s-%s'%(t, c_id.split('_')[2])
                    if rr.get('ptypes'):
                        t_col_d.setdefault(tk, {})[rr['ptypes']]   = 1
                    if ijson.get('ptype_table') == 'Y':
                        n_phs.setdefault(t, {})[tk]   = '-'+ph['n']
                    else:
                        n_phs.setdefault(ph['n'], {})[tk]   = ''
                    if v_d.get('v'):
                        dd['parent_txt']    = rr.get('ptypes', '')
                        dd[tk]  = v_d
                        if ijson.get('ptype_table') == 'Y':
                            t_col_d.setdefault(('COL', t), {})[c_id.split('_')[2]]  = 1
                if dd:
                    for k, v in rr.items():
                        if k in dd:continue
                        if k in ignore_k:continue
                        dd[k]   = v
                    new_ar.append(dd)
            tmphs   = []
            if ijson.get('ptype_table') == 'Y':
                lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
                env         = lmdb.open(lmdb_path1, readonly=True)
                txn         = env.begin()
                ttphs   = []
                for t in  n_phs.keys():
                    tmpdd   = {'n':t}
                    cols    = txn_m.get('COLS_'+str(t))
                    print t, cols, t_col_d[('COL', t)]
                    for c in cols.split('#'):
                        if c not in t_col_d[('COL', t)]:
                            
                            tk      = '%s-%s'%(t, c)
                            tmp_phd = {}
                            for c_id in txn_m.get('GV_'+str(t)).split('#'):
                                if c_id.split('_')[2] == c:
                                    x       = txn_m.get('XMLID_'+c_id)
                                    key     = t+'_'+self.get_quid(x)
                                    ph_map  = txn.get('PH_MAP_'+str(key))
                                    if ph_map:
                                        tperiod_type, tperiod, tcurrency, tscale, tvalue_type    = ph_map.split('^')
                                    else:
                                        tperiod_type, tperiod, tcurrency, tscale, tvalue_type   = '', '', '', '', ''
                                    if ijson.get('ptype_key') and tperiod_type != ijson.get('ptype_key'):continue
                                    tmp_phd['%s%s'%(tperiod_type, tperiod)] = 1
                            n_phs.setdefault(t, {})[tk]   = '-'+'~'.join(tmp_phd.keys())
                    ttphs.append(tmpdd)
                phs = ttphs
                    
            for ph in phs:
                if ph['n'] in n_phs:
                    t_ar    = n_phs[ph['n']].keys()
                    t_ar.sort(key=lambda x:map(lambda x1:int(x1), x.split('-')))
                    for tph in t_ar:
                        color   = ''
                        if len(t_col_d.get(tph, {}).keys()) > 1:
                            color   = 'R'
                        dd  = {'k':tph, 'g':ph['n'], 'n':'%s%s'%(tph, n_phs[ph['n']][tph]), 'color':color}
                        tmphs.append(dd)
                    
            #print tmphs
            res = [{"message":"done", "data":new_ar, 'phs':tmphs, "table_ar":table_ar, "table_type":ijson['table_type'],"grpid":ijson.get("grpid", ""),"from_KPI":"Y", 'main_header':"KPI", "g_ar":[], 'load_grps':"Y"}]
            key_d   = {
                        'F'     : ["description", "operator", "taxo_id", "ph","pno","clean_value", "actual_value","tt", 'f_col', 'type', 'conv_value','x', 'bbox', 't', 'pno', 'd', 'rid', 'grpid'],
                        'CELL'    : ['f_col_o', 'expr_val', 'ftype', 'v_ar', 'FORM_VALUE_KPI', 'kpi_re_stated_all'],
                    }
            self.clean_preview_data(res, {}, {}, None, [], key_d)
            return res
            
        f_ar    = final_ar
        if taxo_pos_d:
            for rr in f_ar:
                if rr['t_id'] in taxo_pos_d:
                    rr['taxo_pos']  = 'Y'
        if ijson.get('type') == 'all':
            tmpijson_c     = copy.deepcopy(ijson)
            tmpijson_c['INPUT_DB_DATA'] = (display_name_d, ttype_d, phs_d, all_phs)
            tmpijson_c['t_id']          =  'ALL'
            taxo_pos_d = self.read_pos_mapping(tmpijson_c)[0]['data']
            for rr in f_ar:
                if rr['t_id'] in taxo_pos_d:
                    for ph in phs:
                        if ph['k'] in rr:continue
                        frm_ar  = taxo_pos_d[rr['t_id']]
                        tmpfrm_ar   = []
                        for ft in frm_ar:
                            if ft['ftype'] == 'PTYPEFORMULA' and ph['n'][:-4] != ft[0]['ph'][:-4]:continue
                            tmpfrm_ar.append(ft)
                        if tmpfrm_ar:
                            rr.setdefault(ph['k'], {})['pos_formula']   = tmpfrm_ar
                            rr.setdefault(ph['k'], {})['x']   = 'x-1_-1'
                            rr.setdefault(ph['k'], {})['t']   = '-1'
                            rr.setdefault(ph['k'], {})['d']   = '-1'
                            rr.setdefault(ph['k'], {})['v']   = ''
                            rr.setdefault(ph['k'], {})['phcsv']   = {'c':'', 'vt':rr['value_type'], 's':rr['scale'], 'p':'', 'pt':''}
        #for rr in f_ar:
        #    if rr['t_id'] in edited_value:
        #        for ph, vtup in edited_value[rr['t_id']].items():
        #            rr[ph]  = {'v':vtup[1], 'save_id':vtup[0], 'edited':'Y'}

                
        if ijson.get('gen_output') == 'Y' and empty_line_display == 'N':
            found_d = {}
            for ii, rr in enumerate(f_ar):
                tid = p_c_d[rr['t_id']]
                if rr.get('da') != 'Y':continue
                found_d[rr['t_id']]   = 1
                done_d  = {}
                done_d[tid]   = 1
                pnode   = p_c_d.get(tid, -1)
                while pnode != -1 and pnode not in done_d:
                    found_d[taxonomy]   = 1
                    done_d[pnode]   = 1
                    pnode   = p_c_d.get(tid, -1)
                    
            n_ar    = []
            for ii, rr in enumerate(f_ar):
                done_d  = {}    
                if rr['t_id'] in found_d:
                    n_ar.append(rr)
            f_ar    = n_ar
                
                
        res = [{'message':'done', 'data':f_ar, 'phs':phs, "table_ar":table_ar, "table_type":ijson['table_type'],"grpid":ijson.get("grpid", ""),"from_KPI":"Y", 'main_header':"KPI", "g_ar":[], 'load_grps':"Y", "error_d":terr}]
        s_grp   = []
        #SCALE FACTOR
        #print sconvert_obj.scale_factor_map
        if return_data == 'Y':
            return res, (display_name_d, ttype_d, phs_d, all_phs)
        if 1:
            sgrps   = sconvert_obj.scale_factor_map
            res_lst_na, not_available_scale_lst = self.get_scale_factor_info(sgrps) 
            if res_lst_na:
                self.update_db_data(res_lst_na, f_ar)
                    
            res[0]['s_grps']  = res_lst_na
            res[0]['c_grps']  = not_available_scale_lst
        if ijson.get('type') == 'all':
            res[0]['INPUT_DB_DATA'] = (display_name_d, ttype_d, phs_d, all_phs)
        if ijson.get('gen_output') != 'Y':
            key_d   = {
                        'F'     : ["description", "operator", "taxo_id", "ph","pno","clean_value", "actual_value","tt", 'f_col', 'type', 'conv_value','x', 'bbox', 't', 'pno', 'd', 'rid', 'grpid'],
                        'CELL'    : ['f_col_o', 'expr_val', 'ftype', 'v_ar', 'FORM_VALUE_KPI', 'kpi_re_stated_all'],
                    }
            self.clean_preview_data(res, {}, {}, None, [], key_d)
        #for r in res[0]['data']:
        #    if r['t_l'] == 'Number of Operating Aircraft':
        #        print 'HHHH'
        #        for ft in  r['Q42018']['f_col'][0]:
        #            print '\t', ft
        return res


    def update_db_data(self, res_lst_na, f_ar, without_taxo=None):
        #taxo_ind_d  = {}
        #for ii, rr in enumerate(f_ar ):
        #    taxo_ind_d[str(rr['t_id'])]= ii
        for t in res_lst_na:
            #print '\t', t
            for ktup in sconvert_obj.no_scale_factor_map.get(tuple(t['op']), {}).keys():
                #print '\t', ktup, ktup[0] in taxo_ind_d, isinstance(ktup, tuple), len(ktup) == 2
                if isinstance(ktup, tuple) and len(ktup) == 2:# and (str(ktup[0]) in taxo_ind_d or without_taxo == 'Y'):
                    #f_ar[taxo_ind_d[ktup[0]]].setdefault('s_gids', {}).setdefault(t['g_id'], {})[ktup[1]]   = 'N'
                    #print '\t\t', f_ar[taxo_ind_d[ktup[0]]]['t_l'], f_ar[taxo_ind_d[ktup[0]]]['s_grps']
                    t.setdefault('ref', {}).setdefault(str(ktup[0]), {})[ktup[1]] = 'N'
                elif isinstance(ktup, tuple) and len(ktup) == 3 and ktup[2] in ['Y']:# and (str(ktup[0]) in taxo_ind_d or without_taxo == 'Y'):
                    t.setdefault('ref', {}).setdefault(str(ktup[0]), {})[ktup[1]] = 'Y'

    def get_scale_factor_info(self, sgrps):
        res_lst_na, not_available_scale_lst = [], []
        g_id = 0
        for key_elm in ['Not-Available', 'Available', 'Not-Available-Scale']:
            get_dt_dct = sgrps.get(key_elm, {})
            if not get_dt_dct:continue 
            for frm_to_tup, fctr in get_dt_dct.iteritems():     
                if key_elm in ('Not-Available', 'Available'):
                    ph_s = '-'.join(frm_to_tup)
                    op   = list(frm_to_tup)
                    g_id += 1 
                    done = 'N'
                    if key_elm == 'Available':
                        done = 'Y'   
                    row_dct = {'ph':ph_s, 'op':op, 'g_id':g_id, 'done':done, 'factor':fctr}
                    res_lst_na.append(row_dct)
                elif key_elm in ('Not-Available-Scale', ):
                    row_dct = {'s':frm_to_tup, 'v':fctr, 's_label':frm_to_tup} 
                    not_available_scale_lst.append(row_dct)
        return res_lst_na, not_available_scale_lst
        

    def read_sys_form_across(self, t_row_f, rr, f_phs, taxo_id_dict, table_type, op_d, phs_d, ijson, display_name_d, update_pk, form_d, dealijson, overwrite=None):
        del_ar  = []
        if ijson.get('AS_REPORTED_VIEW') == 'Y':
            del_ar  =  ['f_col_o', 're_stated', 'company_info', 'f_col']
        else:
            #if table_type not in self.kpi_types:
            #    del_ar  =  ['f_col_o', 're_stated', 'company_info']
            #else:
            #    del_ar  =  ['f_col_o', 'company_info']
            del_ar  =  ['f_col_o', 're_stated', 'company_info']
        res_row = {}
        for ph in f_phs[:]:
            if update_pk and (ph['k'] not in update_pk):
                if ijson.get('gen_output') != 'Y':
                    if ph['k'] not in form_d:
                        continue
                else:continue
            if ph['k'] not in rr and ph['k'] in form_d and ijson.get('gen_output') != 'Y':
                rr.setdefault(ph['k'], {}) #{'v':'', 't':'', 'd':'','x':'', 'phcsv':{'vt':'', 's':'','c':'', 'pt':'', 'p':''}})
            if ph['k'] not in rr and ijson.get('gen_output') != 'Y':
                rr[ph['k']] = {'v':'', 't':'', 'd':'','x':'', 'phcsv':{'vt':'', 's':'','c':'', 'pt':'', 'p':''}}
            if ph['k'] in rr:
                k   = ph['k']
                v   = rr.get(ph['k'], {})
                if v.get('f_col', []) and overwrite == None:continue
                #print '\tIN', k, v
                
                f_ar    = []
                f   = 1
                op_inds = op_d.get((str(rr['t_id']), ph['k']), [])
                op_ind  = 0
                #print '\n', [rr['t_l']], ph['k'], ph['k'] in form_d
                form_ar = form_d.get(ph['k'], [])
                if not form_ar and t_row_f:
                    form_ar = t_row_f[1]
                for ft in form_ar:
                    #print '\t',ft
                    if ft['type'] == 'v':
                        dd  = {}
                        dd['clean_value']   = ft['txid']
                        dd['i_f_type']      = ft.get('i_f_type', '')
                        dd['disp_name']     = ''
                        dd['description']   = ''
                        dd['txid']          = ''
                        dd['v']             = ft['txid']
                        dd['operator']      = ft['op']
                        dd['type']          = 'v'
                        dd['op']            = ft['op']
                        dd['ph']            = ''
                        dd['label']         = ''
                    else:
                        ttype    = ft['t_type']
                        grpid    = ft['g_id']
                        to_dealid= ft['to_dealid']
                        if ft.get('pk'):
                            k       =  ft['pk']
                        else:
                            k       =  phs_d.get((to_dealid, ttype, grpid), {}).get(ph['n'], k)
                        if ft['txid'] not in taxo_id_dict.get((to_dealid, ttype, grpid), {}):# and table_type == ft['t_type']:
                            dd  = {}
                            f   = 0
                            dd['i_f_type']      = ft.get('i_f_type', '')
                            dd['type']          = 't'
                            dd['clean_value']   = ''
                            dd['description']   = 'Not Exists'
                            dd['ph']            = ph['n']
                            dd['to_dealid']     = ft['to_dealid']
                            dd['taxo_id']       = ft['txid']
                            dd['tt']            = ft['t_type']
                            dd['grpid']         = ft['g_id']
                            dd['s']             = ft.get('s', '')
                            dd['c']             = ft.get('c', '')
                            dd['vt']             = ft.get('vt', '')
                            dd['operator']          = ft['op']
                            if dd['operator'] != '=':
                                if op_inds:
                                    dd['operator']          = op_inds[op_ind]
                                op_ind  += 1
                            if ft['op'] == '=':
                                dd['R']    = 'Y'
                                dd['rid']    = t_row_f[0].split('-')[-1]
                            dd['disp_name'] = display_name_d.get((ttype, grpid), ttype)
                            f_ar.append(dd)
                            continue
                            #break
                        dd  = copy.deepcopy(taxo_id_dict.get((to_dealid, ttype, grpid), {}).get(ft['txid'], {}).get(k, {}))
                        if table_type not in self.kpi_types:
                            del_ar  += ['kpi_re_stated_all']
                        for dky in del_ar:
                                if dky in dd:
                                    del dd[dky]
                        dd['i_f_type']      = ft.get('i_f_type', '')
                        #print '\t\t', (to_dealid, ttype, grpid, ft['txid'], k), dd.get('FORM_VALUE')
                        dd['type']          = 't'
                        dd['clean_value']   = dd.get('v', '')
                        dd['description']   = taxo_id_dict.get((to_dealid, ttype, grpid), {}).get(ft['txid'], {'t_l':''})['t_l']
                        dd['ph']            = ft['ph'] if ft.get('ph') else ph['n']
                        dd['taxo_id']       = ft['txid']
                        dd['s']             = ft.get('s', '')
                        dd['c']             = ft.get('c', '')
                        dd['vt']             = ft.get('vt', '')
                        dd['tt']            = ft['t_type']
                        dd['grpid']         = ft['g_id']
                        dd['to_dealid']     = ft['to_dealid']
                        dd['to_company']    = dealijson['%s_%s'%(ijson['project_id'], ft['to_dealid'])]['company_name']
                        dd['conv_value']    = ft.get('conv_value', {})
                        dd['disp_name'] = display_name_d.get((ft['to_dealid'], ttype, grpid), ttype)
                        dd['factors'] = ft.get('factors', [])
                    dd['operator']          = ft['op']
                    if dd['operator'] != '=':
                        if op_inds:
                            dd['operator']          = op_inds[op_ind]
                        op_ind  += 1
                        
                    dd['k']      = k
                    if ft['op'] == '=':
                        dd['R']    = 'Y'
                        dd['rid']    = ft['rid'].split('-')[-1]
                        res_row[ph['n']] = copy.deepcopy(dd)
                        res_row['ALL'] = copy.deepcopy(dd)
                    dd['company_info']  = {'deal_id':ft['to_dealid'], 'company_name':dealijson['%s_%s'%(ijson['project_id'], ft['to_dealid'])]['org_company_name']}
                    f_ar.append(dd)
                #if f == 0:
                #    break
                v['f_col']  = [f_ar]
                if ijson.get('gen_output') == 'Y':
                    v['f_col_o']  = copy.deepcopy([f_ar])
                v['fv']  = 'Y'
                rr['f']  = 'Y'
        return res_row

        
        
    def read_table_builder(self, ijson):
        import data_builder.db_data as db_data
        obj = db_data.PYAPI()
        res = obj.read_table_builder(ijson)
        return res

    def create_seq_across(self, ijson):
        if ijson.get('AUTO_DB') == 'Y':
            del ijson['AUTO_DB']
            t_ijson = copy.deepcopy(ijson)
            t_ijson['auto_DB']    = 'Y'
            t_ijson['table_types'] = [ijson['table_type']]
            import data_builder.taxo_group_update as py
            tObj = py.TaxoGroup()
            tObj.create_taxo_grp(t_ijson)
        if ijson.get("from_merge") == 'Y':
            return self.read_kpi_data(ijson)
        if ijson.get('err_flg') == 'Y' and ijson.get('from_lmdb') == 'Y':
            company_name    = ijson['company_name']
            mnumber         = ijson['model_number']
            model_number    = mnumber
            lmdb_path   = '/mnt/eMB_db/%s/%s/%s'%(company_name, model_number, ijson['table_type'])
            grpid   = ijson.get('grpid')
            if not grpid:
                grpid   = 'ALL'
            env         = lmdb.open(lmdb_path, readonly=True)
            txn       = env.begin()
            res = eval(txn.get(str(grpid)))
            return [res]
            
            
        if ijson['table_type']  == 'PassengerTransportation-Airline' or ijson.get("template_id"):
            return self.read_kpi_data(ijson)
        import data_builder.db_data as db_data
        obj = db_data.PYAPI()
        res = obj.read_db_data(ijson)
        if ijson.get('err_flg') == 'Y' and ijson.get('gen_output') != 'Y':
            ijson['type']   = 'display'
            DB_DATA = {}
            DB_DATA[(ijson['table_type'], ijson.get('grpid', ''), 'P')] = copy.deepcopy(res)
            res = self.clean_response(res)
            #print ijson
            ijson_c = copy.deepcopy(ijson)
            if ijson.get('PRINT') != 'Y':
                disableprint()
            tmpres  = self.create_final_output(ijson_c, DB_DATA)
            enableprint()
            if tmpres:
                if tmpres[0]['message'] != 'done':
                    tmpres  = tmpres[0]['res']
                #for rr in tmpres[0]['data']:
                #    print '\n=============================='
                #    print rr['t_l']
                #    for ph in tmpres[0]['phs']:
                #        if ph['k'] in rr:
                #            print '\t', [ph['k'], ph['n'], rr[ph['k']]['v']]
                res[0]['preview']   = tmpres
            if ijson.get('prev_rep') == 'Y':
                disableprint()
                ijson_c = copy.deepcopy(ijson)
                ijson_c['reported']   = 'Y'
                tmpres  = self.create_final_output(ijson_c, DB_DATA)
                enableprint()
                if tmpres:
                    if tmpres[0]['message'] != 'done':
                        tmpres  = tmpres[0]['res']
                    #for rr in tmpres[0]['data']:
                    #    print '\n=============================='
                    #    print rr['t_l']
                    #    for ph in tmpres[0]['phs']:
                    #        if ph['k'] in rr:
                    #            print '\t', [ph['k'], ph['n'], rr[ph['k']]['v']]
                    res[0]['preview_reported']   = tmpres
        return res
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        if ijson.get('vids', {}) and isinstance(ijson.get('vids', {}), list):
            ijson['vids']   = ijson['vids'][0]
        else:
            ijson['vids']   = {}
        #m_tables, rev_m_tables, doc_m_d = self.get_main_table_info(company_name, model_number)
        ph_d        = {}
        path    = "%s/%s/%s/1_1/21/sdata/doc_map.txt"%(self.doc_path, project_id, deal_id)
        if os.path.exists(path):
            fin = open(path, 'r')
            lines   = fin.readlines()
            fin.close()
        else:
            lines   = []
        def form_ph_csv_taxo(ph, c_ph):
            return ph
        def form_ph_csv_mt(ph, c_ph):
            return c_ph
        db_name = ''
        if ijson.get('taxo_flg', '') == 1:
            form_ph_csv = form_ph_csv_taxo
            db_name = 'taxo_data_builder'
        else:
            form_ph_csv = form_ph_csv_mt
            db_name = 'mt_data_builder'
        doc_d       = {}
        dphs        = {}
        c_year      = self.get_cyear(lines)
        #start_year  = c_year - int(ijson['year'])
        start_year  = c_year - int(ijson.get('year', 5))
        for line in lines[1:]:
            line    = line.split('\t')
            if len(line) < 8:continue
            line    = map(lambda x:x.strip(), line)
            ph      = line[3]+line[7]
            try:
                year    = int(ph[-4:])
            except:continue
            if ijson.get('consider_docs', []):
                if line[0] in ijson.get('consider_docs', []):
                    doc_id  = line[0]
                    doc_d[doc_id]   = (ph, line[2])
                    dphs[ph]        = 1
                continue
            if ph and (start_year < year or ijson.get('taxo_flg', '') == 1):
                doc_id  = line[0]
                doc_d[doc_id]   = (ph, line[2])
                dphs[ph]        = 1

                    

        i_table_type    = ijson['table_type']
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, [i_table_type])
        main_header     = table_type_m['main_header']
        #m_path          = self.taxo_path%(company_name, model_number)
        #path            = m_path+'/TAXO_RESULT/'
        #env1        = lmdb.open(path, max_dbs=27)
        #db_name1    = env1.open_db('other')
        #txn         = env1.begin(db=db_name1)

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn         = env.begin()

        lmdb_path   = '/var/www/html/Rajeev/BBOX/'+str(project_id)+'_'+str(deal_id)
        lmdb_path    = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        env1    = lmdb.open(lmdb_path, readonly=True)
        txn1    = env1.begin()
        table_ph_d  = {}
        all_ph_d    = {}
        table_type  = str(i_table_type)
        f_taxo_arr  = [] #json.loads(txn.get('TABLE_RESULTS_'+table_type, "[]"))
        taxo_d      = {} #json.loads(txn.get('TABLE_RESULTS_'+table_type, "[]"))
        table_ids   = {}
        g_ar        = []
        table_col_phs   = {}
        consider_tables = {}
        taxo_complete_map = {}
        vgh_id_d, vgh_id_d_all, docinfo_d   = {}, {}, {}
        if 'HGHGROUP' in ijson.get('grpid', ''):
            table_type  = ijson['grpid']
        g_isvisible   = 'Y'
        if ijson.get('show_delete', '') == 'Y':
            g_isvisible = 'N'
        if not taxo_d:
            db_file         = self.get_db_path(ijson)
            conn, cur   = conn_obj.sqlite_connection(db_file)
            e_lbl_d     = {}
            sql         = "select label, taxo_id, table_type, table_id, xml_id, parent_xml_id from label_changing_map where table_type='%s'"%(table_type)
            try:
                cur.execute(sql)
                res = cur.fetchall()
            except:
                res =[]
            for rr in res:
                label, taxo_id, table_type, table_id, xml_id, parent_xml_id = rr
                e_lbl_d[(int(taxo_id), str(table_id), xml_id, parent_xml_id)]    = label

            ph_der_stmt = 'select ph, formula from ph_derivation where table_type = "%s" and group_id = "%s" and formula_type="TH"'%(table_type, ijson.get('grpid', ''))
            ph_der_rows = []
            try:
                cur.execute(ph_der_stmt)
                ph_der_rows = cur.fetchall()
            except:pass
            ph_der_map_dict = {}
            for ph_der_row in ph_der_rows:
                ph, formula = ph_der_row
                ph = int(ph)
                formula = str(formula)
                ph_der_map_dict[ph] = formula 

            if not ijson.get('vids', {}):
                sql         = "select row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, doc_id, m_rows, vgh_text, vgh_group, xml_id, period, period_type, scale, currency, value_type from mt_data_builder where table_type='%s' and isvisible='%s'"%(table_type, g_isvisible)
            else:
                sql         = "select row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, doc_id, m_rows, vgh_text, vgh_group, xml_id, period, period_type, scale, currency, value_type from mt_data_builder where table_type='%s' and isvisible='%s' and vgh_text in (%s)"%(table_type, g_isvisible, ', '.join(ijson['vids'].keys()))
            try:
                cur.execute(sql)
                res         = cur.fetchall()
            except:
                res = []
            g_vids  = ijson.get('vids', {})
            for rr in res:
                row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, gv_xml, ph, ph_label,gcom, ngcom, doc_id,m_rows, vgh_text, vgh_group, xml_id, period, period_type, scale, currency, value_type    = rr
                #if ijson.get('gen_output', '') == 'Y' and ijson.get('t_ids', []) and int(taxo_id) not in ijson.get('t_ids', []):continue
                th_flg = ''

                if th_flg == None:
                    th_flg = ''
                doc_id      = str(doc_id)
                if doc_id not in doc_d:continue
                table_id    = str(table_id)
                if table_id not in m_tables:continue # or (vgh_group and 'COPY_' not in vgh_group and ):continue
                tk          = table_id+'_'+self.get_quid(xml_id)
                c_id        = txn_m.get('TXMLID_MAP_'+tk)
                #print [table_id, xml_id, c_id]
                if not c_id:
                    tk   = self.get_quid(table_id+'_'+xml_id)
                    c_id        = txn_m.get('XMLID_MAP_'+tk)
                if not c_id:continue
                #if str(deal_id) == '214' and str(table_id) == '1184':
                #print [taxo_id, xml_id, c_id]
                c   = int(c_id.split('_')[2])
                if g_vids.get(vgh_text, {}) and table_id+'-'+str(c) not in g_vids.get(vgh_text, {}):continue
                key     = table_id+'_'+self.get_quid(xml_id)
                ph_map  = txn.get('PH_MAP_'+str(key))
                if ph_map:
                    tperiod_type, tperiod, tcurrency, tscale, tvalue_type    = ph_map.split('^')
                else:
                    tperiod_type, tperiod, tcurrency, tscale, tvalue_type   = '', '', '', '', ''
                if not period:
                    period      = tperiod
                if not period_type:
                    period_type      = tperiod_type
                if not scale:
                    scale      = tscale
                if not currency:
                    currency     = tcurrency
                if  not value_type:
                    value_type      = tvalue_type
                if period and period_type:
                    ph  = period_type+period
                gv_xml  = c_id
                consider_tables[table_id] = 1
                table_col_phs.setdefault((table_id, c), {})[ph]   = table_col_phs.setdefault((table_id, c), {}).get(ph, 0) +1
                table_ids[table_id]   = 1
                comp    = ''
                #if gcom == 'Y' or ngcom == 'Y':
                #    comp    = 'Y'
                vgh_id_d.setdefault(vgh_text, {})[(table_id, c_id, row_id, doc_id)]        = 1
                vgh_id_d_all[vgh_text]  = 1
                doc_id      = str(doc_id)
                table_id    = str(table_id)
                docinfo_d.setdefault(doc_id, {})[(table_id, c_id, vgh_text)]   = 1
                taxo_d.setdefault(taxo_id, {'l_change':{},'order_id':order_id, 'rid':row_id, 'u_label':user_taxonomy, 't_l':taxonomy, 'missing':missing_taxo, 'comp':comp, 'ks':[], 'm_rows':m_rows, 'th_flg':th_flg, 'ds':'N'})['ks'].append((table_id, gv_xml, ph, ph_label,gcom, ngcom, doc_id, xml_id, row_id, period, period_type, scale, currency, value_type, vgh_text))
                #if ijson.get('taxo_flg', '') == 1 and vgh_text:
                #    tk   = self.get_quid(table_id+'_'+vgh_text)
                #    c_id        = txn_m.get('XMLID_MAP_'+tk)
                #    taxo_d[taxo_id]['ks'].append((table_id, c_id, 'Parent', ph_label,gcom, ngcom, doc_id, vgh_text, row_id, period, period_type, scale, currency, value_type))
                #print (c_id, xml_id, vgh_group)
                if vgh_group == 'N':
                    taxo_d[taxo_id]['l_change'][table_id+':$$:'+xml_id]  = 1
            conn.close()

        #print table_ids
        #print taxo_d
        ph_csv_error_map_dict, table_gv_info = {}, {}
        if ijson.get('taxo_flg', '') == 1:
            ph_csv_error_map_dict, table_gv_info = self.validate_taxo_ph_scv_info(company_name, model_number, company_id, txn_m, txn, table_ids, taxo_d)

        r_ld    = {}
        rp_ld   = {}
        for table_id in table_ids.keys():
            k       = 'HGH_'+str(table_id)
            ids     = txn_m.get(k)
            if ids:
                ids     = ids.split('#')
                row_d   = {}
                for c_id in ids:
                    r       = int(c_id.split('_')[1])
                    c       = int(c_id.split('_')[2])
                    x       = txn_m.get('XMLID_'+c_id)
                    key     = table_id+'_'+self.get_quid(x)
                    t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                    t       = ' '.join(t.split())
                    rs      = int(txn_m.get('rowspan_'+c_id))
                    for tr in range(rs): 
                        #r_ld.setdefault(table_id, {}).setdefault(r+tr, []).append((c, t))
                        #print [table_id, r+tr, c_id, t]
                        row_d.setdefault(r+tr, []).append((c, t, x))
                r_ld[table_id]  = {}
                for r, c_ar in row_d.items():
                    c_ar.sort()
                    if len(c_ar) == 2 and c_ar[-1][1].strip():
                        rp_ld[(table_id, r)]    = copy.deepcopy(c_ar)
                        for mtup in c_ar[::-1]:
                            if mtup[1]:
                                c_ar    = [mtup]
                                break
                    txt = []
                    xml = []
                    for tr in c_ar:
                        txt.append(tr[1])
                        xml.append(tr[2])
                    bbox        = self.get_bbox_frm_xml(txn1, table_id, ':@:'.join(xml))
                    r_ld[table_id][r]  = (' '.join(txt), ':@:'.join(xml), bbox)
        rc_ld    = {}
        if ijson.get('vids', []) and 0:
            for table_id in table_ids.keys():
                k       = 'VGH_'+str(table_id)
                ids     = txn_m.get(k)
                if ids:
                    col_d   = {}
                    ids     = ids.split('#')
                    for c_id in ids:
                        r       = int(c_id.split('_')[1])
                        c       = int(c_id.split('_')[2])
                        cs      = int(txn_m.get('colspan_'+c_id))
                        for tr in range(cs): 
                            col_d.setdefault(c+tr, {})[r]   = c_id
                    for c, rows in col_d.items():
                        rs= rows.keys()
                        rs.sort(reverse=True)
                        for r in rs:
                            c_id        = str(table_id)+'_'+str(r)+'_'+str(c)
                            if not c_id or not txn_m.get('TEXT_'+c_id):continue
                            x       = txn_m.get('XMLID_'+c_id)
                            t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                            t       = ' '.join(t.split())
                            rc_ld.setdefault(table_id, {})[c]   = (x, t, self.get_bbox_frm_xml(txn1, table_id, x))
                            break
                        
        f_ar        = []
        done_d      = {}
        #for dd in f_taxo_arr:
        tmptable_col_phs    = {}
        not_found_ph    = {}
        if 0:
            for k, v in table_col_phs.items():
                phs = v.keys()
                phs.sort(key=lambda x:v[x], reverse=True)
                if len(phs) == 1 and phs[0] == '':
                    table_id    = k[0]
                    tk       = 'VGH_'+str(table_id)
                    ids     = txn_m.get(tk)
                    if ids:
                        col_d   = {}
                        ids     = ids.split('#')
                        for c_id in ids:
                            r       = int(c_id.split('_')[1])
                            c       = int(c_id.split('_')[2])
                            cs      = int(txn_m.get('colspan_'+c_id))
                            for tr in range(cs): 
                                col_d.setdefault(c+tr, {})[r]   = c_id
                        for c, rows in col_d.items():
                            if c != k[1]:continue
                            rs= rows.keys()
                            rs.sort(reverse=True)
                            for r in rs:
                                c_id        = str(table_id)+'_'+str(r)+'_'+str(c)
                                if not c_id or not txn_m.get('TEXT_'+c_id):continue
                                x       = txn_m.get('XMLID_'+c_id)
                                t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                                t       = ' '.join(t.split())
                                tmptable_col_phs[k]   = t
                                not_found_ph.setdefault(k[0], {})[k[1]] = 1
                                break
                    pass
                if k not in tmptable_col_phs:
                    tmptable_col_phs[k] = phs[0]
        row_ids = taxo_d.keys()
        row_ids.sort(key=lambda x:len(taxo_d[x]['ks']), reverse=True)
        #for row_id, dd in taxo_d.items():
        if ijson.get('gen_output', '') == 'Y':
            tmprows = []
            done_d  = {}
            for row_id in row_ids:
                dd      = taxo_d[row_id]
                ks      = dd['ks']
                tmp_arr = []
                for table_id, c_id, ph, tlabel, gcom, ngcom, doc_id, xml_id, trid, period, period_type, scale, currency, value_type, vgh_text in ks:
                    if (table_id, xml_id) in done_d:continue
                    tmp_arr.append((table_id, c_id, ph, tlabel, gcom, ngcom, doc_id, xml_id, trid))
                    done_d[(table_id, xml_id)]=1
                if not tmp_arr:continue
                tmprows.append(row_id)
            row_ids = tmprows[:]
        
        ph_formula_d                            = {}
        if ijson.get('NO_FORM', '') != 'Y' and ijson.get('taxo_flg', '') != 1:
            ph_formula_d                            = self.read_ph_user_formula(ijson, '')
            if ijson.get('grpid', ''):
                ph_formula_d                            = self.read_ph_user_formula(ijson, ijson.get('grpid', ''), ph_formula_d)
        dphs = report_year_sort.year_sort(dphs.keys())
        dphs.reverse()

        table_xml_d     = {}
        taxo_id_dict    = {}
        fs_d            = {}
        rp_done_d       = {}
        n_taxo          = -1
        row_ids.sort(key=lambda x:taxo_d[x]['order_id'])
        for row_id in row_ids:
            dd      = taxo_d[row_id]
            db_comp_flg = 0
            ks      = dd['ks']
            th_flg  = ph_der_map_dict.get(row_id, '')
            taxos   = dd['t_l'].split(' / ')[0]
            row     = {'t_id':row_id} #'t_l':taxo}
            f_dup   = ''
            label_d = {}
            label_r_d = {}
            f_phs   = []
            if len(ks) < len(table_ids.keys()):
                db_comp_flg = 1
            ks.sort(key=lambda x:tuple(map(lambda x1:int(x1), x[1].split('_'))))
            ph_ind  = {}
            scale_d = {}
            r_label_d   = {}
            rp_p_d      = {}
            table_lxml_d    = {}
            f_rp_done   = 'N'
            pre_rp_done = copy.deepcopy(rp_done_d)
            ks.sort(key=lambda x:tuple(map(lambda x1:int(x1), x[1].split('_'))))
            for table_id, c_id, ph, tlabel, gcom, ngcom, doc_id, xml_id, trid, period, period_type, scale, currency, value_type, vgh_text in ks:
                #tk   = self.get_quid(table_id+'_'+xml_id)
                #c_id        = txn_m.get('XMLID_MAP_'+tk)
                #if not c_id:continue
                row.setdefault('tids', {})[table_id]    = 1
                table_id    = str(table_id)
                c_id        = str(c_id)
                r           = int(c_id.split('_')[1])
                c           = int(c_id.split('_')[2])
                x           = txn_m.get('XMLID_'+c_id)
                xml_id      = x
                t           = self.convert_html_entity(binascii.a2b_hex(txn_m.get('TEXT_'+c_id)))
                if t: 
                    row['da']   = 'Y'
                if tlabel:
                    tlabel      = self.convert_html_entity(tlabel)
                if (table_id, r) in rp_ld:
                    c_ar    = rp_ld[(table_id, r)]
                    if c_ar[0][1:] not in rp_done_d:
                        x_c    = c_ar[0][2].split(':@:')[0].split('#')[0]
                        table_lxml_d.setdefault(table_id, {})[(int(x_c.split('_')[1]), int(x_c.split('_')[0].strip('x')))]   = 1
                        rp_p_d[(doc_id, table_id)]  = c_ar[0][1:] 
                        rp_done_d[c_ar[0][1:]]   = ''
                    if c_ar[0][1:] in pre_rp_done:
                        if pre_rp_done[c_ar[0][1:]] != '':
                            f_ar[pre_rp_done[c_ar[0][1:]]]['org_t_id'][row['t_id']] = 1
                        f_rp_done   = 'Y'
                
                #print [row_id, table_id, c_id, t, ph, tlabel, xml_id]
                if (deal_id, table_type) in self.ph_grp_d or 'HGHGROUP' in table_type:
                    c_ph        = ph
                    c_tlabel    = ''
                else:
                    c_ph        = str(c)
                    c_tlabel    = ''
                c_ph    = form_ph_csv(ph, c_ph)
                pc_error_flg = 0
                if c_ph != 'Parent':
                    pc_error_flg = ph_csv_error_map_dict.get((table_id, x), 0)
                gv_ph_csv_info  = table_gv_info.get((table_id, x), [])
                bbox, x_c       = self.get_bbox_frm_xml(txn1, table_id, x, 'Y')
                #f_ph            =  tmptable_col_phs[(table_id, c)]
                if(table_id, c_ph) in ph_ind:
                    c_tlabel    = str(ph_ind[(table_id, c_ph)])
                    ph_ind[(table_id, c_ph)]    += 1
                else:
                    ph_ind[(table_id, c_ph)]    = 1
                c_key           = table_id+'-'+c_ph+'-'+c_tlabel
                if t: 
                    scale_d.setdefault(scale, {})[(c_key, trid)] = 1
                tmptable_col_phs.setdefault(c_key, {})
                if ph:
                    tmptable_col_phs[c_key][ph] = tmptable_col_phs[c_key].get(ph, 0)+1
                #print table_id, c_id
                txts, xml_ar, bbox1    = r_ld[table_id].get(r, ('', '', ''))
                if (row_id, table_id, xml_id, xml_ar) in e_lbl_d:
                    txts = e_lbl_d[(row_id, table_id, xml_id, xml_ar)]
                txts        = self.convert_html_entity(txts)
                grm_txts    = txts.lower() #self.remove_grm_mrks(txts).lower()
                if ijson.get('gen_output') == 'Y':
                    grm_txts    = self.gen_taxonomy_alpha_num(grm_txts)
                if ijson.get('taxo_flg', '') != 1 or ph != 'Parent' :
                    label_r_d[grm_txts] = 1
                    r_label_d[(doc_id, table_id)]   = {'id':xml_id, 'txt':txts, 'bbox':bbox1, 'x':xml_ar, 'd':doc_id, 't':table_id}
                    label_d.setdefault(grm_txts, {'id':{}, 'txt':txts, 'bbox':bbox1, 'x':xml_ar, 'd':doc_id, 't':table_id, 'v':{}, 's':"Y"})['v'][doc_d[doc_id][0]]    = {'bbox':bbox1, 'x':xml_ar, 'd':doc_id, 't':table_id}
                    label_d[grm_txts]['id'].setdefault(table_id, {})[xml_id]    = xml_ar
                    if ijson.get('gen_output') == 'Y':
                        row[table_id+':$$:'+xml_id] = {'v':txts, 'bbox':bbox1, 'x':xml_ar, 'd':doc_id, 't':table_id}

                    if table_id+':$$:'+xml_id in dd.get('l_change', {}):
                        label_d[grm_txts]['s']  = 'N'
                if ijson.get('ignore_doc_ids', {}) and doc_id in  ijson.get('ignore_doc_ids', {}):
                    x_c = self.clean_xmls(xml_ar)
                    if x_c:
                        x_c    = x_c.split(':@:')[0].split('#')[0]
                        table_xml_d.setdefault(table_id, {})[(int(x_c.split('_')[1]), int(x_c.split('_')[0].strip('x')))]   = 1
                    continue
                row[c_key]      = {'v':t, 'x':x, 'bbox':bbox, 'd':doc_id, 't':table_id, 'r':r, 'rid':trid, 'phcsv':{'p':period, 'pt':period_type, 's':scale, 'c':currency, 'vt':value_type}, 'pce':pc_error_flg, 'vt':gv_ph_csv_info, 'ph_f':c_tlabel}
                
                
                table_xml_d.setdefault(table_id, {})[(int(x_c.split('_')[1]), int(x_c.split('_')[0].strip('x')))]   = 1
                #if c_id in done_d:
                #    f_dup   = 'Y'
                #    row[table_id+'-'+c_ph+'-'+c_tlabel]['m']    = 'Y'
                if gcom == 'Y':
                    row[c_key]['g_f']    = 'Y'
                    row['f']    = 'Y'
                if ngcom == 'Y':
                    row[c_key]['ng_f']    = 'Y'
                    row['f']    = 'Y'
                done_d[c_id]    = 1
                if (deal_id, table_type) in self.ph_grp_d  or ('HGHGROUP' in table_type):
                    table_ph_d.setdefault((doc_id, table_id), {})[(c_ph, c_tlabel)]   = (c, ph)
                else:
                    if ijson.get('taxo_flg', '') == 1:
                        table_ph_d.setdefault((doc_id, table_id), {})[(c_ph, c_tlabel)]   = (0 if not c_tlabel else int(c_tlabel), ph)
                    else:
                        table_ph_d.setdefault((doc_id, table_id), {})[(c_ph, c_tlabel)]   = ((c, 0 if not c_tlabel else int(c_tlabel)), ph)
                col_txt = rc_ld.get(table_id, {}).get(c, ())
                if col_txt:
                    txts        = self.convert_html_entity(col_txt[1])
                    grm_txts    = txts.lower() #self.remove_grm_mrks(txts).lower()
                    label_d.setdefault(grm_txts, {'id':{}, 'txt':txts, 'bbox':col_txt[2], 'x':col_txt[0], 'd':doc_id, 't':table_id, 'v':{},"s":"Y"})['v'][doc_d[doc_id][0]]    = {'bbox':col_txt[2], 'x':col_txt[0], 'd':doc_id, 't':table_id}
                    label_d[grm_txts]['id'].setdefault(table_id, {})[xml_id]    = 1
                    if table_id+':$$:'+xml_id  in dd.get('l_change', {}):
                        label_d[grm_txts]['s']  = 'N'
                if vgh_text and ijson.get('taxo_flg', '') == 1:
                    x   = vgh_text
                    tk   = self.get_quid(table_id+'_'+x)
                    c_id        = txn_m.get('XMLID_MAP_'+tk)
                    if c_id:
                        c_id        = str(c_id)
                        r           = int(c_id.split('_')[1])
                        c           = int(c_id.split('_')[2])
                        x           = txn_m.get('XMLID_'+c_id)
                        t           = self.convert_html_entity(binascii.a2b_hex(txn_m.get('TEXT_'+c_id)))
                        c_ph        = 'Parent'
                        c_key           = table_id+'-'+c_ph+'-'+c_tlabel
                        bbox            = self.get_bbox_frm_xml(txn1, table_id, x)
                        row[c_key]  = {'v':t, 'x':x, 'bbox':bbox, 'd':doc_id, 't':table_id, 'r':r, 'rid':trid, 'phcsv':{'p':period, 'pt':period_type, 's':scale, 'c':currency, 'vt':value_type}}
                        table_ph_d.setdefault((doc_id, table_id), {})[(c_ph, c_tlabel)]   = (0 if not c_tlabel else int(c_tlabel), 'Parent')
                        ph  = 'Parent'
                        tmptable_col_phs.setdefault(c_key, {})
                        tmptable_col_phs[c_key][ph] = tmptable_col_phs[c_key].get(ph, 0)+1
                    
                        
            if len(label_d.keys()) > 1:
                row['ldata']    = label_d.values()
                row['l_change']    = dd.get('l_change', {})
                if ijson.get('gen_output') == 'Y' and dd.get('l_change', {}):
                    row['ldata']    = filter(lambda x:x.get('s', '') == 'Y', label_d.values())
                    if len(row['ldata']) <=1:
                        row['ldata']    = []
                if len(row['ldata']) >1:
                    row['lchange']  = 'Y'
            lble    = label_r_d.keys()
            lble.sort(key=lambda x:len(x), reverse=True)
            row['taxo']  = dd['t_l']
            if dd.get('u_label', ''):
                row['t_l']  = dd.get('u_label', '')
                doc_t   = r_label_d.keys()
                doc_t.sort(key=lambda x:(dphs.index(doc_d[x[0]][0]), sorted(table_xml_d[x[1]].keys())[0]))
                row['x']    = r_label_d[doc_t[0]]['x']
                row['bbox']    = r_label_d[doc_t[0]]['bbox']
                row['t']    = r_label_d[doc_t[0]]['t']
                row['d']    = r_label_d[doc_t[0]]['d']
            else:
                doc_t   = r_label_d.keys()
                doc_t.sort(key=lambda x:(dphs.index(doc_d[x[0]][0]), sorted(table_xml_d[x[1]].keys())[0]))
                row['x']    = r_label_d[doc_t[0]]['x']
                row['bbox']    = r_label_d[doc_t[0]]['bbox']
                row['t']    = r_label_d[doc_t[0]]['t']
                row['d']    = r_label_d[doc_t[0]]['d']
                row['t_l']  = r_label_d[doc_t[0]]['txt']
            xml_ar  = label_d[ lble[0]]['x'].split(':@:')
            if ijson.get('taxo_flg', '') != 1 and xml_ar and xml_ar[0] and (not row.get('parent_txt', '')):
                    table_id    =  label_d[ lble[0]]['t']
                    doc_id      = label_d[ lble[0]]['d']
                    p_key   = txn.get('TRPLET_HGH_PINFO_'+table_id+'_'+self.get_quid(xml_ar[0]))
                    if p_key:
                        tmp_xar  = []
                        t_ar    = []
                        for pkinfo in p_key.split(':!:'):
                            pxml, ptext = pkinfo.split('^')
                            tmp_xar.append(pxml)
                            t_ar.append(binascii.a2b_hex(ptext))
                        pxml    = ':@:'.join(tmp_xar)
                        row['parent_txt']   = ' '.join(t_ar) # {'txt':' '.join(t_ar), 'bbox':self.get_bbox_frm_xml(txn1, table_id, pxml), 'x':pxml, 'd':doc_id, 't':table_id}
                        if not row['t_l']:
                            row['t_l']  = row['parent_txt']+' (Null)'

            if 0:#len(scale_d.keys()) > 1:
                scale_grps  = scale_d.keys()
                scale_grps.sort()
                scale_grps  = tuple(scale_grps)
                fs_d.setdefault(scale_grps, {})[row_id] = scale_d
                row['scale_error']  = scale_grps
                pass

            row['l']    = len(ks)
            row['fd']   = f_dup
            row['order']   = dd['order_id']
            row['rid']     = dd['rid']
            row['th_flg']  = th_flg
            row['dbf']     = db_comp_flg
            if dd.get('m_rows', ''):
                row['merge']   = 'Y'
            if dd.get('missing', ''):
                row['missing']  = dd['missing']
            taxo_id_dict[str(row_id)]    = row
            row['t_l']  = row['t_l'].strip('-/+')
            row['t_l']  = row['t_l'].strip('+/-')
            row['t_l']  = row['t_l'].strip('=')
            #print '\n',[row['t_l']]

            if rp_p_d and f_rp_done != 'Y':
                doc_t   = rp_p_d.keys()
                doc_t.sort(key=lambda x:(dphs.index(doc_d[x[0]][0]), sorted(table_lxml_d[x[1]].keys())[0]))
                doc_id, table_id    = doc_t[0]
                t_l, x              = rp_p_d[doc_t[0]]
                t_l                 = t_l[0].upper()+t_l[1:].lower()
                trow                = {}
                trow['taxo']         = 'Header-'+str(n_taxo)
                trow['t_id']         = n_taxo
                trow['org_t_id']          = {row['t_id']:1}
                #print '\t ', [t_l, len(f_ar)]
                for ktup in doc_t:
                    rp_done_d[rp_p_d[doc_t[0]]] = len(f_ar)
                trow['t_l']          =  self.convert_html_entity(t_l)
                trow['x']            =  x
                trow['order']       =  dd['order_id']
                trow['rid']         = n_taxo
                bbox, x_c       = self.get_bbox_frm_xml(txn1, table_id, x, 'Y')
                trow['t']            =  table_id
                trow['d']            =  doc_id
                trow['bbox']         =  bbox
                n_taxo              -= 1
                f_ar.append(trow)
            f_ar.append(row)
        if ijson.get('NO_FORM', '') == 'Y':
            fs_d    = {}
        s_ar    = []
        for stup, taxo_d  in fs_d.items():
            k   = '_'.join(stup)
            scale_d = {}
            for t_id, sc_d in taxo_d.items():
                for scale, rid_d in sc_d.items():
                    scale_d.setdefault(scale, {'t':0, 'a':0, 'rids':{}})
                    for (c_key, rid) in rid_d.keys():
                        scale_d[scale]['rids'].setdefault(t_id, {})[rid] = ph_formula_d.get(('SCALE', c_key), 'N')
                        scale_d[scale]['t'] += 1
                        if ph_formula_d.get(('SCALE', c_key), 'N') == 'Y':
                            scale_d[scale]['a'] += 1
            sgrp    = []
            for k, v in scale_d.items():
                dd  = {'s':k, 't':v['t'],'a':v['a'], 'sids':v['rids']}
                dd['done']  = 'N'
                if v['a'] == v['t']:
                    dd['done']  = 'Y'
                elif v['a'] < v['t']:
                    dd['done']  = 'P'
                sgrp.append(dd)
            dd  = {'info':sgrp}
            s_ar.append(dd)
            
                    

        #alphs = report_year_sort.year_sort(all_ph_d.keys())
        #alphs.reverse()
        table_ids   = table_ph_d.keys()
        #try:
        #table_ids.sort(key=lambda x:(dphs.index(doc_d[x[0]][0]), x[1]))
        table_ids.sort(key=lambda x:(dphs.index(doc_d[x[0]][0]), sorted(table_xml_d[x[1]].keys())[0]))
        #except:
        #    table_ids.sort(key=lambda x:(x[0], x[1]))
        #    pass
        phs = []
        t_ar    = []
        for table_id in table_ids:
            #print table_id, sorted(table_xml_d[table_id[1]].keys())
            t_ar.append({'t':table_id[1], 'l':len(table_ph_d[table_id].keys()), 'd':table_id[0], 'dt':doc_d.get(table_id[0], '')})
            all_phs = table_ph_d[table_id].keys()
            all_phs.sort(key=lambda x:table_ph_d[table_id][x][0])
            for ph, tlabel in all_phs:
                #tph = table_ph_d[table_id][(ph, tlabel)][1]
                c_key   = table_id[1]+'-'+ph+'-'+tlabel
                t_phs   = tmptable_col_phs[c_key].keys()
                t_phs.sort(key=lambda x:tmptable_col_phs[c_key][x], reverse=True)
                tph     = t_phs[0] if t_phs else ''
                d_tph   = doc_d[table_id[0]][0]
                phs.append({'k':table_id[1]+'-'+ph+'-'+tlabel, 'n':tph, 'g':table_id[0]+'-'+table_id[1]+' ( '+'_'.join(doc_d.get(table_id[0], []))+' )', 'ph':ph, 'dph':(d_tph[:-4], int(d_tph[-4:]))})
        not_done = list(sets.Set(m_tables.keys()) - sets.Set(consider_tables.keys()))
        t_rem_ar    = []
        for t in not_done:
            if table_type != m_tables[t]:continue
            table_id    = (doc_m_d[t], t)
            if table_id[0] not in doc_d:continue
            t_rem_ar.append({'t':table_id[1], 'd':table_id[0], 'dt':doc_d.get(table_id[0], ['',''])})
        
        # f_ar.sort(key=lambda x:(0 if x.get('missing', '') != 'Y' else 1, x.get(phs[0]['k'], {'r':999999999})['r'], 99999 - x['l']))
        f_ar.sort(key=lambda x:(x['order'], x['rid']))
        if  ijson.get('taxo_flg', '') != 1  and ijson.get('NO_FORM', '') != 'Y':
            for row in f_ar:
                t_row_f     = ph_formula_d.get(('F', str(row['t_id'])), ())
                op_d    = {}
                ttype   = ''
                if not t_row_f:
                    t_row_f     = ph_formula_d.get(('SYS F', str(row['t_id'])), ())
                    if t_row_f:
                        ttype   = 'SYSTEM'
                        op_d    = ph_formula_d.get(("OP", t_row_f[0]), {})
                else:
                    ttype   = 'USER'
                if t_row_f:
                    #print [row['t_l'], ttype]
                    self.read_sys_form(t_row_f, row, phs, taxo_id_dict, table_type, op_d)
        res = [{'message':'done', 'data':f_ar, 'phs':phs, 'table_ar':t_ar, 'table_ar_rem':t_rem_ar, 'total':len(rev_m_tables.get(table_type, {}).keys()), 'nph':not_found_ph, 'main_header':main_header,'s_arr':s_ar}] #, 'm_tables':m_tables, 'm':rev_m_tables}]
        if not ijson.get('vids', []) and 'HGHGROUP' not in table_type:
            if ijson.get('taxo_flg', '') != 1 and ijson.get('NO_FORM', '') != 'Y':
                res[0]['g_ar']  = self.read_all_vgh_groups(table_type, company_name, model_number,vgh_id_d, vgh_id_d_all, docinfo_d)
                res[0]['load_grps']  = 'Y'
                
        else:
            res[0]['FROM_GRP']  = 'Y'
        if ijson.get('taxo_flg', '') == 1:
            res[0]['taxo_flg']  = 1
            res[0]['g_ar']      = []
            
        return res

    def read_sys_form(self, t_row_f, rr, f_phs, taxo_id_dict, table_type, op_d, company_info={}):
        for ph in f_phs[:]:
            if ph['k'] in rr:
                k   = ph['k']
                v   = rr[ph['k']]
                if v.get('f_col', []):continue
                #print '\tIN', k, v
                
                f_ar    = []
                f   = 1
                op_inds = op_d.get((str(rr['t_id']), ph['k']), [])
                op_ind  = 0
                for ft in t_row_f[1]:
                    if ft['type'] == 'v':
                        dd  = {}
                        dd['clean_value']   = ft['txid']
                    else:
                        if ft['txid'] not in taxo_id_dict:# and table_type == ft['t_type']:
                            dd  = {}
                            f   = 0
                            dd['clean_value']   = ''
                            dd['description']   = 'Not Exists'
                            dd['ph']            = ph['n']
                            dd['taxo_id']       = ft['txid']
                            dd['tt']            = ft['t_type']
                            dd['operator']          = ft['op']
                            dd['s']             = ft.get('s', '')
                            dd['c']             = ft.get('c', '')
                            dd['vt']             = ft.get('vt', '')
                            if dd['operator'] != '=':
                                if op_inds:
                                    dd['operator']          = op_inds[op_ind]
                                op_ind  += 1
                            f_ar.append(dd)
                            continue
                            #break
                        dd  = copy.deepcopy(taxo_id_dict.get(ft['txid'], {}).get(k, {}))
                        dd['clean_value']   = dd.get('v', '')
                        dd['description']   = taxo_id_dict.get(ft['txid'], {'t_l':''})['t_l']
                        dd['ph']            = ph['n']
                        dd['taxo_id']       = ft['txid']
                        dd['tt']            = ft['t_type']
                        dd['s']             = ft.get('s', '')
                        dd['c']             = ft.get('c', '')
                        dd['vt']             = ft.get('vt', '')
                    dd['operator']          = ft['op']
                    if dd['operator'] != '=':
                        if op_inds:
                            dd['operator']          = op_inds[op_ind]
                        op_ind  += 1
                        
                    dd['k']      = k
                    if ft['op'] == '=':
                        dd['R']    = 'Y'
                        dd['rid']    = t_row_f[0].split('-')[-1]
                    dd['company_info']  = company_info
                    f_ar.append(dd)
                #if f == 0:
                #    break
                v['f_col']  = [f_ar]
                v['fv']  = 'Y'
                rr['f']  = 'Y'

    def read_all_vgh_groups(self, table_type, company_name, model_number,vgh_id_d, vgh_id_d_all, docinfo_d):
        group_d     = {}
        revgroup_d  = {}
        d_group_d     = {}
        d_revgroup_d  = {}
        ijson   = {'company_name':company_name, 'model_number':model_number}
        db_file         = self.get_db_path(ijson)
        conn, cur   = conn_obj.sqlite_connection(db_file)
        grp_doc_map_d   = {}
        rev_doc_map_d   = {}
        sql = "select vgh_group_id, doc_group_id, group_txt from vgh_doc_map where table_type='%s'"%(table_type)
        try:
            cur.execute(sql)
            res = cur.fetchall()
        except:
            res = []
        doc_vgh_map = {}
        for rr in res:
            vgh_group_id, doc_group_id, group_txt   = rr
            doc_vgh_map[(vgh_group_id, doc_group_id)]   = group_txt

        sql = "select group_id, table_type, group_txt from vgh_group_info where table_type like '%s'"%('HGHGROUP-'+table_type+'%')
        #if company_name == 'ADIENT PLC':
        #    print sql
        try:
            cur.execute(sql)
            res1 = cur.fetchall()
        except:
            res1 = []
        hgh_grp = []
        for rr in res1:
            group_id, ttable_type, group_txt = rr
            group_txt = group_txt.replace('\t', '')
            hgh_grp.append({'n':group_txt, 'vids':[{}], 'grpid':ttable_type, 'doc_ids':{}, 'doc_grpid':'', 'hgh':"Y"})
        #if company_name == 'ADIENT PLC':
        #    sys.exit()
        sql = "select group_id, table_type, group_txt from vgh_group_info where table_type='%s'"%(table_type)
        try:
            cur.execute(sql)
            res = cur.fetchall()
        except:
            res = []
        grp_info    = {}
        for rr in res:
            #print rr
            group_id, table_type, group_txt = rr
            group_txt = group_txt.replace('\t', '')
            grp_info[str(group_id)]   = group_txt
        

        sql         = "select row_id, vgh_id, group_txt, table_str, doc_vgh from vgh_group_map where table_type='%s'"%(table_type)
        res         = []
        try:
            cur.execute(sql)
            res         = cur.fetchall()
        except:pass
        for rr in res:
            row_id, vgh_id, group_txt, table_str, doc_vgh   = rr
            if doc_vgh == 'DOC':
                d_group_d.setdefault(group_txt, {})[vgh_id]   = (row_id, table_str)
                d_revgroup_d.setdefault(vgh_id, {})[group_txt]    = (row_id, table_str)
                pass
            else:
                group_d.setdefault(group_txt, {})[vgh_id]   = (row_id, table_str)
                revgroup_d.setdefault(vgh_id, {})[group_txt]    = (row_id, table_str)

        vghs = vgh_id_d.keys()
        vghs.sort(key=lambda x:len( vgh_id_d[x].keys()), reverse=True)
        f_arr       = []
        max_tlen    = 0
        grp_doc_ids = {}
        for ii, vgh_id in enumerate(vghs):
            vids        = vgh_id_d[vgh_id].keys()
            #vgh_id, doc_grp = vgh_id
            tmparr  = []
            t_d     = {}
            tc_d     = {}
            doc_id_d    = {}
            for rtup in vids:
                table_id, c_id, row_id, doc_id  = rtup
                c                       = c_id.split('_')[2]
                t_d.setdefault((table_id+'-'+c, doc_id), {})[(row_id, c_id)] = 1
                tc_d[table_id+'-'+c]    = doc_id
                doc_id_d.setdefault(doc_id, {})[table_id+'-'+c]    = 1
            if vgh_id in revgroup_d:
                dd_g         = revgroup_d.get(vgh_id, {}).keys()
                for grp_id in dd_g:
                    tmpd            = {}
                    doc_tcd = {}
                    if not revgroup_d[vgh_id][grp_id][1]:
                        tmpd    = tc_d
                    else:
                        for tsr in revgroup_d[vgh_id][grp_id][1].split('#'):
                            if tsr not in tc_d:continue
                            tmpd[tsr]   = tc_d[tsr]
                    if not revgroup_d[vgh_id][grp_id][1]:
                        for tkey, doc_k in tc_d.items():
                            doc_tcd.setdefault(doc_k, {})[tkey]     = 1
                    else:
                        for tsr in revgroup_d[vgh_id][grp_id][1].split('#'):
                            if tsr not in tc_d:continue
                            tkey    = tsr
                            doc_k   = tc_d[tkey]
                            doc_tcd.setdefault(doc_k, {})[tkey]     = 1
                    for doc_id in doc_id_d.keys():
                        grp_doc_ids.setdefault(grp_id, {}).setdefault(d_revgroup_d.get(doc_id, {'':''}).keys()[0], {}).setdefault(vgh_id, {}).update(doc_tcd.get(doc_id, {}))
                    group_d[grp_id][vgh_id] = (ii, revgroup_d[vgh_id][grp_id][0], tmpd, revgroup_d[vgh_id][grp_id][1])
        g_ar    = []
        for k, v in group_d.items():
            #print [k]
            doc_grp = grp_doc_ids.get(k, {})
            for tk, tvids in doc_grp.items():
                tmp_grp_name    = ''
                if tk in grp_info:
                    tmp_grp_name    = ' - '+grp_info[tk]
                tmp_grp_name    = grp_info.get(k, k)+tmp_grp_name
                tmp_grp_name    = doc_vgh_map.get((k, tk), tmp_grp_name)    
                tmp_grpid       = k
                if tk:
                    tmp_grpid   = k+'-'+tk
                dd  = {'n':tmp_grp_name, 'vids':[{}], 'grpid':tmp_grpid, 'doc_ids':{}, 'doc_grpid':tk}
                table_ids   = {}
                for vid, i in v.items():
                    if vid not in tvids:continue
                    if len(i) <3:
                        continue
                    table_ids[vid]  = tvids[vid]
                dd['vids'] = [table_ids]
                g_ar.append(dd)
        g_ar    += hgh_grp
        return g_ar


    def get_computation(self, key, txn_m, txn, r, p_xml_d, pxml, tks):
        for gflg in ['G', 'NG']:
            gcom        = txn.get('COM_'+gflg+'_ROWMAP_'+key)
            if gcom:
                for eq in gcom.split('|'):
                    formula, eq_str = eq.split(':$$:')
                    f_taxo_arr[ii].setdefault('gcom', {})[c_id] = formula
                    for xml in eq_str.split('^'):
                        k   = self.get_quid(table_id+'_'+xml)
                        tmpcid  = txn_m.get('XMLID_MAP_'+k)
                        if tmpcid.split('_')[1] == r and tmpcid in tks:
                            p_xml_d[xml]  = pxml

    def cal_bobox_data_new(self, ijson):
        project_id  = ijson['project_id']
        deal_id     = ijson['deal_id']
        company_id  = str(project_id) + '_'+str(deal_id)
        #self.output_path = '/var/www/html/fundamentals_intf/output/'
        lmdb_folder      = os.path.join(self.output_path, company_id, 'doc_page_adj_cords')
        print lmdb_folder
        doc_page_dict    = {}
        if os.path.exists(lmdb_folder):
            env = lmdb.open(lmdb_folder, readonly=True)
            txn = env.begin()
            if 1:
                cursor = txn.cursor()
                for doc_id, res_str in cursor:
                    if res_str:
                        page_dict = ast.literal_eval(res_str)
                        doc_page_dict[doc_id] = page_dict
        return [{'message':'done', 'data':doc_page_dict}]

    def get_scale_info(self):
        db_path = '/mnt/eMB_db/unit_conversion/conversion_factor.db'
        conn  = sqlite3.connect(db_path)
        cur   = conn.cursor()
        read_qry = 'SELECT scale, disp_name FROM scale_info;'
        cur.execute(read_qry)
        scale_info =[{'s':'', 's_label':''}] + [{'s':row[0], 's_label':row[1]} for row in cur.fetchall()]
        conn.close() 
        return scale_info
        
    def read_custom_node_mapping(self):
        db_path = '/mnt/eMB_db/node_mapping.db' 
        conn = sqlite3.connect(db_path)
        cur  = conn.cursor()
        read_qry = """ SELECT sheet_id, node_name FROM custom_node_mapping; """
        cur.execute(read_qry)
        t_data = cur.fetchall()
        conn.close()
        sid_map_nm = {str(row[0]):str(row[1]) for row in t_data}
        return sid_map_nm
        
    def prepare_custom_table_group_map_info_mtlist(self, company_name, model_number):
        sid_map_nm   = self.read_custom_node_mapping()
        db_path = '/mnt/eMB_db/%s/%s/tas_company.db'%(company_name, model_number)
        #print db_path
        conn = sqlite3.connect(db_path)
        cur  = conn.cursor()
        read_qry = """ SELECT sheet_id FROM custom_table_group_mapping;  """
        try:
            cur.execute(read_qry)
            t_data = cur.fetchall()
        except:
            t_data  = []
        conn.close()
        res_lst = []
        mst_lst_all = []
        for row in t_data:
            sid = str(row[0])
            node_name = sid_map_nm[sid]
            dt_dct = {'l': node_name, 'k': node_name, 'f': False, 'grpid':'', 'sub_grps':[], 'custom':'Y'}
            mst_all_dct = {'f': False, 'k':node_name, 'l':node_name, 'grpid': "", '$$treeLevel': 0, 'level_id': 0, 'custom':'Y'}
            res_lst.append(dt_dct)
            mst_lst_all.append(mst_all_dct)
        return res_lst, mst_lst_all
    def read_all_classification(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        if 1:#company_id in ('1_3', ):
            mst_lst, mst_lst_all = self.prepare_custom_table_group_map_info_mtlist(company_name, model_number)
            #pass

        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number)
        map_dic = {
                    'IS'    :0,
                    'BS'    :1,
                    'CF'    :2,
                    'RBS'    :3,
                    'RBG'    :4,
                    }
        
        ks = rev_m_tables.keys()
        ks.sort(key=lambda x:map_dic.get(x, 9999))
        list_mt = []
        for k in ks:
            dic = {'l': k, 'k': k, 'f': False, 'grpid':''}
            list_mt.append(dic)
        list_mt = list_mt + mst_lst
        mst_lst_all = list_mt
        data    = []
        for i, r in enumerate(mst_lst_all):
            dd  = {'sn':{'v':str(i+1)}, 'l':{'v':r['k']}}
            data.append(dd)
        proj_info = self.getProjectId_details_company(ijson, {})
        phs = [{'k':'sn', 'n':'SL No', 'type':'SL'}, {'k':'l', 'n':'Table Type'}]
        return [{'message':'done', 'data':data, 'col_def': phs, 'project_info':proj_info,'map':{}}]

    def cp_read_all_classification(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        if 1:#company_id in ('1_3', ):
            mst_lst, mst_lst_all = self.prepare_custom_table_group_map_info_mtlist(company_name, model_number)
            #pass

        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number)
        map_dic = {
                    'IS'    :0,
                    'BS'    :1,
                    'CF'    :2,
                    'RBS'    :3,
                    'RBG'    :4,
                    }
            
        import data_builder.db_data as db_data
        obj = db_data.PYAPI()
        #cp_ijson = copy.deepcopy(ijson)
        ks = rev_m_tables.keys()
        ks.sort(key=lambda x:map_dic.get(x, 9999))
        list_mt = []
        for k in ks:
            dic = {'l': k, 'k': k, 'f': False, 'grpid':'', '$$treeLevel':0}
            list_mt.append(dic)
            #cp_ijson['table_type'] = k
            g_ar    = obj.read_all_vgh_groups(k, company_name, model_number, doc_m_d, {})
            #print 'GGGGGG', g_ar
            if g_ar:
                for rr in g_ar:
                    dic_e = {'l': k+'-'+rr['n'], 'k': k, 'f': False, 'grpid':rr['grpid'], 'vids':rr['vids'], 'level_id':1,'$$treeLevel':1}
                    list_mt.append(dic_e)

        list_mt = list_mt + mst_lst
        mst_lst_all = list_mt[:]
        data    = []
        for i, r in enumerate(mst_lst_all):
            #print 'RRRRRRRRRR', r, '\n'
            dd  = {'sn':{'v':str(i+1)}, 'l':{'v':r['l'], 'k':r['k']}, 'vids':r.get('vids', [])}
            data.append(dd)
        #for ss in data:
        #    print 'SSSSSS', ss, '\n'
        proj_info = self.getProjectId_details_company(ijson, {})
        phs = [{'k':'sn', 'n':'SL No', 'type':'SL'}, {'k':'l', 'n':'Table Type'}]
        return [{'message':'done', 'data':data, 'col_def': phs, 'project_info':proj_info,'map':{}}]

    def getProjectId_details_company(self, ijson, grp_map_d={}):
        company_name = ijson["company_name"]
        model_number = ijson["model_number"]
        deal_id      = ijson["deal_id"]
        project_id   = ijson["project_id"]
        company_id   = "%s_%s"%(project_id, deal_id)
        db_path      = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn         = sqlite3.connect(db_path)
        cur          = conn.cursor() 
        
        sql = "CREATE TABLE IF NOT EXISTS f_vgh_group_info(row_id INTEGER PRIMARY KEY AUTOINCREMENT, group_id VARCHAR(20), table_type VARCHAR(100), group_txt VARCHAR(20), table_str TEXT,user_name VARCHAR(100), datetime TEXT);"
        cur.execute(sql)

        try:
            sql     = "CREATE TABLE IF NOT EXISTS mt_data_builder(row_id INTEGER PRIMARY KEY AUTOINCREMENT, taxo_id INTEGER, prev_id INTEGER,order_id INTEGER, table_type, taxonomy TEXT, user_taxonomy TEXT, missing_taxo varchar(1), table_id TEXT, c_id TEXT, gcom TEXT, ngcom TEXT, ph TEXT, ph_label TEXT, user_name TEXT, datetime TEXT, isvisible varchar(1), m_rows TEXT, vgh_text TEXT, vgh_group TEXT, doc_id INTEGER, xml_id TEXT, period VARCHAR(50), period_type VARCHAR(50), scale VARCHAR(50), currency VARCHAR(50), value_type VARCHAR(50))"
            cur.execute(sql)
            cur.commit()
        except:pass
        read_qry = 'select table_type, row_id, group_txt from f_vgh_group_info;'
        cur.execute(read_qry)
        tt_grp_data = {tuple(map(str, row[:2])):row[0]+' - '+row[2] for row in cur.fetchall()}
        for k in tt_grp_data.keys():
            table_type  = k[0]
            sql = "select row_id, table_type, group_txt from f_vgh_group_info where table_type like '%s'"%('HGHGROUP-'+table_type+'%')
            try:
                cur.execute(sql)
                res1 = cur.fetchall()
            except:
                res1 = []
            for rr in res1:
                group_id, ttable_type, group_txt = rr
                group_txt = group_txt.replace('\t', '')
                tt_grp_data[(table_type, ttable_type)]  = table_type+' - '+group_txt
        
        # taxo_data_builder
        d_pt = '/mnt/eMB_db/%s/%s/taxo_data_builder.db'%(company_name, model_number) 
        conn1         = sqlite3.connect(d_pt)
        cur1          = conn1.cursor() 
        sql     = "CREATE TABLE IF NOT EXISTS mt_data_builder(row_id INTEGER PRIMARY KEY AUTOINCREMENT, taxo_id INTEGER, prev_id INTEGER,order_id INTEGER, table_type, taxonomy TEXT, user_taxonomy TEXT, missing_taxo varchar(1), table_id TEXT, c_id TEXT, gcom TEXT, ngcom TEXT, ph TEXT, ph_label TEXT, user_name TEXT, datetime TEXT, isvisible varchar(1), m_rows TEXT, vgh_text TEXT, vgh_group TEXT, doc_id INTEGER, xml_id TEXT, period VARCHAR(50), period_type VARCHAR(50), scale VARCHAR(50), currency VARCHAR(50), value_type VARCHAR(50))"
        cur1.execute(sql)
        conn1.commit()
        read_qry = 'select table_type, taxo_id from mt_data_builder where isvisible="Y"'
        cur1.execute(read_qry)
        tx_ids_data = {(row[0], int(row[1])):1 for row in cur1.fetchall()}
        conn1.close()


        #check table_type
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number)

        comp_qry     = 'select project_id, project_name, periods, excel_config_str from company_config;'
        try:
            cur.execute(comp_qry)
            comp_data    = cur.fetchall()
        except:
            comp_data = []
        conn.close()

        res_lst      = []
        required_lst      = []
        for row in comp_data:
            project_id, project_name, derived_ph, excel_config_str = map(str, row)
            res_lst.append({'n':project_name, 'pid':project_id})
        #print res_lst;sys.exit()
        return res_lst
        
    def read_source_doc_type(self, company_name, model_number):
        db_path = '/mnt/eMB_db/%s/%s/tas_company.db'%(company_name, model_number)
        conn = sqlite3.connect(db_path)
        cur  = conn.cursor()
        try:
            alter_qry = """ ALTER table company_meta_info ADD column source_doc_type TEXT; """  
            cur.execute(alter_qry)  
        except:pass
        read_qry = """ SELECT doc_id, source_doc_type FROM company_meta_info; """
        cur.execute(read_qry)
        t_data = cur.fetchall()
        conn.close()
        res_dct = {}
        for row_data in t_data:
            doc_id, source_doc_type = row_data
            if not source_doc_type:
                source_doc_type = 'pdf'
            source_doc_type = source_doc_type.lower()
            if source_doc_type == 'html2pdf':
                source_doc_type = 'html'
            res_dct[doc_id] = source_doc_type
        return res_dct 

    def read_doc_info(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        try:
            src_doc_info = self.read_source_doc_type(company_name, model_number)
        except:src_doc_info = {}
        if 1:#company_id in ('1_3', ):
            mst_lst, mst_lst_all = self.prepare_custom_table_group_map_info_mtlist(company_name, model_number)
            #pass

        path    = "%s/%s/%s/1_1/21/sdata/doc_map.txt"%(self.doc_path, project_id, deal_id)
        if os.path.exists(path):
            fin = open(path, 'r')
            lines   = fin.readlines()
            fin.close()
        else:
            lines   = []
        doc_ar  = []
        for line in lines[1:]:
            line    = line.split('\t')
            if len(line) < 8:
                doc_ar.append({'d':line[0].strip()})
                continue
            line    = map(lambda x:x.strip(), line)
            ph      = line[3]+line[7]
            doc_id  = line[0]
            doc_ar.append({'d':doc_id, 'ph':ph, 'type':line[2]})
        doc_ar.sort(key=lambda x:int(x['d']))
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number)
        print m_tables, '\n', rev_m_tables, '\n', doc_m_d, '\n', table_type_m
        map_dic = {
                    'IS'    :0,
                    'BS'    :1,
                    'CF'    :2,
                    'RBS'    :3,
                    'RBG'    :4,
                    }
        
        ks = rev_m_tables.keys()
        ks.sort(key=lambda x:map_dic.get(x, 9999))
        list_mt = []
        for k in ks:
            dic = {'l': k, 'k': k, 'f': False, 'grpid':''}
            list_mt.append(dic)
        meta_data = self.read_comp_info_txt(ijson)
        list_mt = []
        list_mt_all = []
        #lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        #env         = lmdb.open(lmdb_path1)
        #txn_m       = env.begin()
        import data_builder.db_data as db_data
        obj = db_data.PYAPI()
        grp_map_d   = {}
        for k in ks:
            ijson['table_type'] = k
            #g_ar    = self.create_group_ar(ijson, txn_m)
            g_ar    = obj.read_all_vgh_groups(k, company_name, model_number, doc_m_d, {})
            dic_m = {'l': k, 'k': k, 'f': False, 'grpid':'', 'sub_grps':[]}
            dic_e = {'l': k, 'k': k, 'f': False, 'grpid':'', 'level_id':0,'$$treeLevel':0, 'sub_grp': 0}
            if project_id == '40':
                ijson_c = copy.deepcopy(ijson)
                ijson_c['i_formula_types']  = map(lambda x:'"'+x+'"', ['WITHINFORMULA', 'SFORMULA', 'ACROSSFORMULA', 'CELLFORMULA', 'FORMULA', 'PTYPEFORMULA', 'USERFORMULA', 'DIRECTFORMULA', 'RELFORMULA'])
                f_d = self.read_ph_user_formula(ijson_c)
                if f_d:
                    dic_m['f_d']    = len(f_d.keys()) 
                    dic_e['f_d']    = len(f_d.keys()) 
            list_mt.append(dic_m)
            if g_ar:
                dic_e['sub_grp'] = 1
                list_mt_all.append(dic_e)
                for rr in g_ar:
                    grp_map_d[(k, str(rr['grpid']))]   = rr['vids']
                    dic_e = {'l': k+'-'+rr['n'], 'k': k, 'f': False, 'grpid':rr['grpid'], 'vids':rr['vids'], 'level_id':1,'$$treeLevel':1}
                    list_mt_all.append(dic_e)
                    #dic_m['sub_grps'].append(dic_e)
            else:
                list_mt_all.append(dic_e)
        proj_info = self.getProjectId_details_comp(ijson, grp_map_d)
        scale_info = self.get_scale_info()
        meta_info_list = self.meta_info_txt_data(ijson)[0]['data']
        if ijson.get('from_merge') == 'Y':
            list_mt     = []
            temp_info   = list_mt_all 
            for tr in temp_info:
                tr['k'] = tr['k']+'@MERGE'
        else:
            temp_info = self.all_industry_type_data(ijson) 
        if 1:#company_id in ('1_3', ):
            list_mt = list_mt + mst_lst
            list_mt_all  = list_mt_all + mst_lst_all
            
        return [{'message':'done', 'data':doc_ar, 'mt_list': list_mt, 'proj_info':proj_info, 'md':meta_data, 'mt_list_all':list_mt_all, 'scale_info':scale_info, 'meta_info_list':meta_info_list, 'temp_info':temp_info, 'source_doc_info':src_doc_info}]

    def all_industry_type_data(self, ijson):
        db_path = '/mnt/eMB_db/template_info.db'
        conn = sqlite3.connect(db_path)    
        cur  = conn.cursor()
            
        read_qry = 'SELECT row_id, sheet_name, project_name, display_name, industry, time_series from meta_info where (industry like "%s" or industry == "All")'%(ijson.get('industry_type', ''))
        cur.execute(read_qry)
        table_data = cur.fetchall()
        conn.close()
        res_lst = []
        for row in table_data:
            row_id, sheet_name, project_name, display_name, industry, time_series = row[:]
            project_name = ''.join(project_name.split())
            industry     = ''.join(industry.split())
            tsheet_name  = ''.join(sheet_name.split())
            k = '-'.join([industry, project_name]) 
            if k in ['FoodProcessing-EquityBuyout', 'PassengerTransportation-Airline'] and sheet_name in ['PassengerTransportation-Airline', 'Equity_Buyout_KraftHeinzCompany']:
                k = '-'.join([industry, project_name]) 
            else:
                k = '-'.join([industry, project_name, tsheet_name]) 
                
            if time_series == 'N':
                k = '-'.join([k, 'without_time_series'])
            data_dct = {'k':k, 'l':display_name, 'template_id':row_id, 'sheet_name':sheet_name, 'pname':project_name, 'time_series':time_series}
            res_lst.append(data_dct)
        return res_lst          

    def get_industry_id(self):
        db_path = '/mnt/eMB_db/industry_kpi_taxonomy.db'
        conn  = sqlite3.connect(db_path)
        cur   = conn.cursor()

        read_qry = 'SELECT DISTINCT(industry_id) FROM industry_kpi_template;'
        cur.execute(read_qry)
        dis_industry_id = cur.fetchall()
        conn.close()
        i_ids = ' ,'.join(map(lambda x:str(x[0]), dis_industry_id))
    
        d_p = '/mnt/eMB_db/page_extraction_global.db'    
        conn  = sqlite3.connect(d_p)
        cur   = conn.cursor() 
        rd_qry = 'SELECT industry_type FROM industry_type_storage WHERE industry_id IN (%s)'%(i_ids)
        #print rd_qry;sys.exit()
        cur.execute(rd_qry)
        t_data = cur.fetchall()
        conn.close() 
        res_lst = [{'k':row[0], 'l':row[0]} for row in t_data]    
        return res_lst

    def oldread_comp_info_txt(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        
        rfname = '/mnt/eMB_db/%s/%s/company_meta_info.txt'%(company_name, model_number)
        fin = open(rfname, 'r')
        rows = fin.readlines()
        fin.close()
        if len(rows[-1].split('\t')) == 15:
            er_date, q1, q2, q3, q4, h1, h2, rep_year, m9, filing_freq, ticker, acc_std, industry, tx1, tx2 = map(lambda x:str(x).strip(), rows[-1].split('\t'))
        elif len(rows[-1].split('\t')) != 14:
            er_date, q1, q2, q3, q4, h1, h2, rep_year, filing_freq, ticker, acc_std, industry = map(lambda x:str(x).strip(), rows[-1].split('\t'))
        else:
            er_date, q1, q2, q3, q4, h1, h2, rep_year, filing_freq, ticker, acc_std, industry, tx1, tx2 = map(lambda x:str(x).strip(), rows[-1].split('\t'))
        try:mn = m9
        except:mn = '' 
        ydict = {'Q1': q1, 'Q2': q2, 'Q3': q3, 'Q4': q4, 'H2': h2, 'FY':rep_year, 'H1': h1, 'M9':mn}
        return ydict

    def read_comp_info_txt(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        check_data = ['Q1', 'Q2', 'H1', 'Q3', 'M9', 'Q4', 'H2', 'FY']
        txt_path = '/mnt/eMB_db/%s/%s/company_meta_info.txt'%(company_name, model_number)
        print txt_path
        f = open(txt_path)
        txt_data = f.readlines() 
        f.close()
        if len(txt_data) < 2:
            return {}
        header = txt_data[0].split('\t')
        meta   = txt_data[1].split('\t')
        ydict = {}
        for pt in check_data:
            if pt not in header:continue
            get_idx = header.index(pt)
            ydict[pt] = meta[get_idx]
        return ydict

    def add_new_taxonomy(self, ijson):
        import create_table_seq
        obj = create_table_seq.TableSeq()
        disableprint()
        obj.insert_new_taxonomy(ijson)
        enableprint()
        return self.create_seq_across(ijson)
    def update_label(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        ijson['taxo_flg']   = 1
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)

        table_type      = ijson['table_type']

        label           = ijson['lbl']
        t_id            = ijson['t_id']
        sql = "alter table mt_data_builder add column user_label TEXT"
        try:
            cur.execute(sql)
        except:pass
        sql             = 'update mt_data_builder set user_label="%s" where taxo_id=%s and table_type="%s"'%(label, t_id, table_type)
        cur.execute(sql)
        conn.commit()
        res             = [{'message':'done'}]
        return res 

    def sh_merge(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type      = ijson['table_type']
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        sql             = "select distinct(user_taxonomy) from mt_data_builder where table_type='%s'"%(table_type)
        cur.execute(sql)
        res             = cur.fetchall()
        e_taxo  = {}
        for rr in res:
            e_taxo[rr[0].lower()]  = 1
        f_ar    = []
        for t_ids, user_taxonomy in ijson['merge_taxo']:   
            ijson['t_ids']  = t_ids 
            ijson['target_taxo']  = user_taxonomy
            t_ids            = map(lambda x:int(x), ijson['t_ids'])
            f_taxo_id       = t_ids[0]
            taxo_d      = {} #json.loads(txn.get('TABLE_RESULTS_'+table_type, "[]"))
            table_ids   = {}
            g_ar        = []
            table_col_phs   = {}
            consider_tables = {}
            f_ph        = {}
            lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
            env         = lmdb.open(lmdb_path1, readonly=True)
            txn_m       = env.begin()

            sql             = "select prev_id, row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, doc_id, m_rows, vgh_text, vgh_group, xml_id from mt_data_builder where table_type='%s' and isvisible='Y' and taxo_id in (%s)"%(table_type, ', '.join(map(lambda x:str(x), ijson['t_ids'])))
            cur.execute(sql)
            res             = cur.fetchall()
            new_row_id  = {}
            ol_taxoids  = {}
            for rr in res:
                prev_id, row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, gv_xml, ph, ph_label,gcom, ngcom, doc_id,m_rows, vgh_text, vgh_group, xml_id    = rr
                
                if not prev_id or int(prev_id) != -10000:
                    ol_taxoids[int(taxo_id)] = 1
                doc_id      = str(doc_id)
                table_id    = str(table_id)
                tk   = self.get_quid(table_id+'_'+xml_id)
                c_id        = txn_m.get('XMLID_MAP_'+tk)
                if not c_id:continue
                gv_xml  = c_id
                if ijson.get('taxo_flg', '') != 1 and ((deal_id, table_type) in self.ph_grp_d  or (deal_id == '44' and table_type == 'OS')):
                    f_ph.setdefault((table_id, ph, ph_label), {})[(gv_xml, ph, ph_label)]    = 1
                elif ijson.get('taxo_flg', '') == 1:
                    f_ph.setdefault(table_id, {})[(taxo_id, xml_id, row_id)] = 1
                else:
                    f_ph.setdefault((table_id, c_id.split('_')[2], ''), {}).setdefault((gv_xml, ph, ph_label), {})[(taxo_id, xml_id)] = 1
                new_row_id[str(row_id)] = 1
            if ijson.get('taxo_flg', '') == 1 and str(deal_id) != '43' and ijson.get('wt_group') != 'Y':
                lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
                env         = lmdb.open(lmdb_path1, readonly=True)
                txn         = env.begin()
                res = self.check_ph_overlap(f_ph, txn_m, txn)
                if res:
                    return res
            #print f_ph
            error_tables    = []
            if ijson.get('taxo_flg', '') != 1:
                for ph, cnt in f_ph.items():
                    if len(cnt.keys()) > 1:
                         error_tables   += cnt.keys()
            if error_tables:
                tmpar   = []
                for rr in error_tables:
                    gv_xml, ph, ph_label    = rr
                    table_id    = gv_xml.split('_')[0]
                    tmpar.append(table_id+'-'+gv_xml.split('_')[2]+'-'+ph_label)
                error_tables    = tmpar[:]
                    
            
            if error_tables:
                error_tables    = list(sets.Set(error_tables))
                res             = [{'message':'PH Overlap', 'data':error_tables, 'taxo_d':[]}]
                return res
            d_ids   = {}
            prev_id = None
            if int(f_taxo_id) not in ol_taxoids and ol_taxoids:
                f_taxo_id   = ol_taxoids.keys()[0]
                prev_id = -1
            for t in ijson['t_ids']:
                if int(t) != f_taxo_id:
                    d_ids[t]    = 1
            
            f_ar.append((ijson.get('target_taxo', ''), f_taxo_id, '_'.join(map(lambda x:str(x), ijson['t_ids'])), ','.join(map(lambda x:str(x), filter(lambda x1:int(x1) != f_taxo_id, ijson['t_ids']))), d_ids, prev_id))
        tmp_ar  = []
        for (target_taxo, f_taxo_id, tids, rids, d_ids, prev_id) in f_ar:
            if target_taxo.lower() in e_taxo:
                tid = e_taxo[target_taxo.lower()]
                target_taxo += '-N-'+str(tid)
                e_taxo[target_taxo.lower()] = tid+1
            else:
                e_taxo[target_taxo.lower()] = 1
            if prev_id == None:
                sql     = "update mt_data_builder set user_taxonomy='%s', taxo_id=%s, m_rows='%s' where taxo_id in (%s)"%(target_taxo, f_taxo_id, tids, rids)
            else:
                sql     = "update mt_data_builder set user_taxonomy='%s', taxo_id=%s, m_rows='%s', prev_id=-1 where taxo_id in (%s)"%(target_taxo, f_taxo_id, tids, rids)
            #print sql
            cur.execute(sql)
            tmp_ar.append({'s':int(f_taxo_id), 'd_ids':d_ids, 'merge':"New", "t_taxo":target_taxo, 'message':'done'})
        conn.commit()
        conn.close()
        if ijson.get('dtime'):
            res = [{"message":'done', 'dtime':ijson['dtime'], 'data':tmp_ar, 'db_file':db_file}]
            return res
        else:
            return self.create_seq_across(ijson)

    def merge_taxo(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type      = ijson['table_type']
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        t_ids            = map(lambda x:int(x), ijson['t_ids'])
        f_taxo_id       = t_ids[0]
        taxo_d      = {} #json.loads(txn.get('TABLE_RESULTS_'+table_type, "[]"))
        table_ids   = {}
        g_ar        = []
        table_col_phs   = {}
        consider_tables = {}
        f_ph        = {}
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        sql             = "select row_id, prev_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, doc_id, m_rows, vgh_text, vgh_group, xml_id from mt_data_builder where table_type='%s' and isvisible='Y' and taxo_id in (%s)"%(table_type, ', '.join(map(lambda x:str(x), ijson['t_ids'])))
        cur.execute(sql)
        res             = cur.fetchall()
        new_row_id  = {}
        ol_taxoids  = {}
        for rr in res:
            row_id, prev_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, gv_xml, ph, ph_label,gcom, ngcom, doc_id,m_rows, vgh_text, vgh_group, xml_id    = rr
            if not prev_id or int(prev_id) != -10000:
                ol_taxoids[int(taxo_id)] = 1
            doc_id      = str(doc_id)
            table_id    = str(table_id)
            tk   = self.get_quid(table_id+'_'+xml_id)
            c_id        = txn_m.get('XMLID_MAP_'+tk)
            if not c_id:continue
            gv_xml  = c_id
            if ijson.get('taxo_flg', '') != 1 and ((deal_id, table_type) in self.ph_grp_d  or (deal_id == '44' and table_type == 'OS')):
                f_ph.setdefault((table_id, ph, ph_label), {})[(gv_xml, ph, ph_label)]    = 1
            elif ijson.get('taxo_flg', '') == 1:
                f_ph.setdefault(table_id, {})[(taxo_id, xml_id, row_id)] = 1
            else:
                f_ph.setdefault((table_id, c_id.split('_')[2], ''), {}).setdefault((gv_xml, ph, ph_label), {})[(taxo_id, xml_id)] = 1
            new_row_id[str(row_id)] = 1
        if ijson.get('taxo_flg', '') == 1 and str(deal_id) != '43' and ijson.get('wt_group') != 'Y':
            lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
            env         = lmdb.open(lmdb_path1, readonly=True)
            txn         = env.begin()
            res = self.check_ph_overlap(f_ph, txn_m, txn)
            if res:
                return res
        #print f_ph
        error_tables    = []
        if ijson.get('taxo_flg', '') != 1:
            for ph, cnt in f_ph.items():
                if len(cnt.keys()) > 1:
                     error_tables   += cnt.keys()
        if error_tables:
            tmpar   = []
            for rr in error_tables:
                gv_xml, ph, ph_label    = rr
                table_id    = gv_xml.split('_')[0]
                tmpar.append(table_id+'-'+gv_xml.split('_')[2]+'-'+ph_label)
            error_tables    = tmpar[:]
                
        
        if error_tables:
            error_tables    = list(sets.Set(error_tables))
            res             = [{'message':'PH Overlap', 'data':error_tables, 'taxo_d':[]}]
            return res
        #print 'YES'
        #sys.exit()
        d_ids   = {}
        for t in t_ids[1:]:
            d_ids[t]    = 1
        if ijson.get('target_taxo', ''):
            if int(f_taxo_id) not in ol_taxoids and ol_taxoids:
                f_taxo_id   = ol_taxoids.keys()[0]
                d_ids   = {}
                for t in t_ids[:]:
                    if int(t) != f_taxo_id:
                        d_ids[t]    = 1
                
                
                sql     = "update mt_data_builder set user_taxonomy='%s', taxo_id=%s, m_rows='%s', prev_id=-1 where row_id in (%s)"%(ijson.get('target_taxo', ''), f_taxo_id, '_'.join(map(lambda x:str(x), ijson['t_ids'])), ', '.join(new_row_id.keys()))
            else:
                sql     = "update mt_data_builder set user_taxonomy='%s', taxo_id=%s, m_rows='%s' where row_id in (%s)"%(ijson.get('target_taxo', ''), f_taxo_id, '_'.join(map(lambda x:str(x), ijson['t_ids'])), ', '.join(new_row_id.keys()))
        elif ijson.get("taxo_flg", '') == 1:
            res = [{"message":"Error Not allowed without target taxo"}]
            conn.close()
            return res
        else:
            if int(f_taxo_id) not in ol_taxoids and ol_taxoids:
                f_taxo_id   = ol_taxoids.keys()[0]
                d_ids   = {}
                for t in t_ids[:]:
                    if int(t) != f_taxo_id:
                        d_ids[t]    = 1
                sql     = "update mt_data_builder set taxo_id=%s, m_rows='%s', prev_id=-1 where row_id in (%s)"%(f_taxo_id, '_'.join(map(lambda x:str(x), ijson['t_ids'])), ', '.join(new_row_id.keys()))
            else:
                sql     = "update mt_data_builder set taxo_id=%s, m_rows='%s' where row_id in (%s)"%(f_taxo_id, '_'.join(map(lambda x:str(x), ijson['t_ids'])), ', '.join(new_row_id.keys()))
        cur.execute(sql)
        conn.commit()
        
        res = [{'message':'done', 's':f_taxo_id, 'd_ids':d_ids, 'merge':'New', 't_taxo':ijson.get('target_taxo', '')}]
        return res


    def check_ph_overlap(self, f_ph, txn_m, txn):
        t_d     = {}
        table_d = {}
        for ph, cnt in f_ph.items():
            #print ph, cnt
            tmpcnt  = {}
            for taxo_id, xml_id, rid in cnt.keys():
                tmpcnt.setdefault(taxo_id, {})[(xml_id, rid)]   = 1
            cnt = tmpcnt
            if len(cnt.keys()) > 1:
                t_d[ph]   = 1
                for taxo_id, xml_d in cnt.items():
                    for xml_id, rid in xml_d.keys():
                        table_d.setdefault(ph, {})[xml_id]   = (taxo_id, rid)
        t_rc_d  = {}
        for table_id in t_d.keys():
            #if table_id != '1215':continue
            k       = 'HGH_'+table_id
            ids     = txn_m.get(k)
            r_ld    = {}
            if ids:
                ids     = ids.split('#')
                for c_id in ids:
                    r       = int(c_id.split('_')[1])
                    c       = int(c_id.split('_')[2])
                    x       = txn_m.get('XMLID_'+c_id)
                    if x in table_d.get(table_id, {}):
                        rs      = int(txn_m.get('rowspan_'+c_id))
                        for tr in range(rs):
                            r_ld.setdefault(r+tr, {})[x]  = 1
            k   = 'GV_'+table_id
            ids = txn_m.get(k)
            if not ids:continue
            ids         = ids.split('#')
            for c_id in ids:
                r       = int(c_id.split('_')[1])
                c       = int(c_id.split('_')[2])
                #print c_id
                #print 'YESY'
                x       = txn_m.get('XMLID_'+c_id)
                key     = table_id+'_'+self.get_quid(x)
                x_txts    = r_ld.get(r, {}).keys()
                for x_p in x_txts:
                    t_rc_d.setdefault((table_id, x_p), {})[x]   = 1
        error_d = {}
        for table_id, px_d in table_d.items():
            e_flg   = 0
            prev_phs    = ''
            for px in px_d.keys():
                phs = []
                xd  = t_rc_d.get((table_id, px), {})
                #print (table_id, px, xd)
                for xml_id in xd.keys():
                    key     = table_id+'_'+self.get_quid(xml_id)
                    ph_map  = txn.get('PH_MAP_'+str(key))
                    if ph_map:
                        tperiod_type, tperiod, tcurrency, tscale, tvalue_type    = ph_map.split('^')
                    else:
                        tperiod_type, tperiod, tcurrency, tscale, tvalue_type   = '', '', '', '', ''
                    if tperiod_type and tperiod:
                        phs.append(tperiod_type+tperiod)
                if prev_phs != '' and (sets.Set(prev_phs).intersection(sets.Set(phs)) or not phs):
                    e_flg   = 1
                    #break
                #print phs
                prev_phs    = phs
                if e_flg == 1:
                    for px, (tid, rid) in px_d.items():
                        error_d[rid] = 1
        if error_d:
            res             = [{'message':'PH Overlap', 'taxo_d':error_d.keys(), 'data':[]}]
            return res
        return []

    def form_row(self, table_type, txn_m, txn, table_ids, taxo_d):
        r_ld    = {}
        for table_id in table_ids.keys():
            k       = 'HGH_'+str(table_id)
            ids     = txn_m.get(k)
            if ids:
                ids     = ids.split('#')
                row_d   = {}
                for c_id in ids:
                    r       = int(c_id.split('_')[1])
                    c       = int(c_id.split('_')[2])
                    x       = txn_m.get('XMLID_'+c_id)
                    key     = table_id+'_'+self.get_quid(x)
                    t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                    t       = ' '.join(t.split())
                    rs      = int(txn_m.get('rowspan_'+c_id))
                    for tr in range(rs): 
                        #r_ld.setdefault(table_id, {}).setdefault(r+tr, []).append((c, t))
                        row_d.setdefault(r+tr, []).append((c, t, x))
                r_ld[table_id]  = {}
                for r, c_ar in row_d.items():
                    c_ar.sort()
                    txt = []
                    xml = []
                    for tr in c_ar:
                        txt.append(tr[1])
                        xml.append(tr[2])
                    bbox        = self.get_bbox_frm_xml(txn1, table_id, ':@:'.join(xml))
                    r_ld[table_id][r]  = (' '.join(txt), ':@:'.join(xml), bbox)
        rc_ld    = {}
        if ijson.get('vids', []):
            for table_id in table_ids.keys():
                k       = 'VGH_'+str(table_id)
                ids     = txn_m.get(k)
                if ids:
                    col_d   = {}
                    ids     = ids.split('#')
                    for c_id in ids:
                        r       = int(c_id.split('_')[1])
                        c       = int(c_id.split('_')[2])
                        cs      = int(txn_m.get('colspan_'+c_id))
                        for tr in range(cs): 
                            col_d.setdefault(c+tr, {})[r]   = c_id
                    for c, rows in col_d.items():
                        rs= rows.keys()
                        rs.sort(reverse=True)
                        for r in rs:
                            c_id        = str(table_id)+'_'+str(r)+'_'+str(c)
                            if not c_id or not txn_m.get('TEXT_'+c_id):continue
                            x       = txn_m.get('XMLID_'+c_id)
                            t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                            t       = ' '.join(t.split())
                            rc_ld.setdefault(table_id, {})[c]   = (x, t, self.get_bbox_frm_xml(txn1, table_id, x))
                            break
                        
        f_ar        = []
        done_d      = {}
        #for dd in f_taxo_arr:
        tmptable_col_phs    = {}
        for k, v in table_col_phs.items():
            phs = v.keys()
            phs.sort(key=lambda x:v[x], reverse=True)
            tmptable_col_phs[k] = phs[0]
        row_ids = taxo_d.keys()
        row_ids.sort(key=lambda x:len(taxo_d[x]['ks']), reverse=True)
        #for row_id, dd in taxo_d.items():
        if ijson.get('gen_output', '') == 'Y':
            tmprows = []
            done_d  = {}
            for row_id in row_ids:
                dd      = taxo_d[row_id]
                ks      = dd['ks']
                tmp_arr = []
                for table_id, c_id, ph, tlabel, gcom, ngcom, doc_id, xml_id in ks:
                    if (table_id, xml_id) in done_d:continue
                    tmp_arr.append((table_id, c_id, ph, tlabel, gcom, ngcom, doc_id, xml_id))
                    done_d[(table_id, xml_id)]=1
                if not tmp_arr:continue
                tmprows.append(row_id)
            row_ids = tmprows[:]
                    
        
        for row_id in row_ids:
            dd      = taxo_d[row_id]
            ks      = dd['ks']
            taxos   = dd['t_l'].split(' / ')[0]
            row     = {'t_id':row_id} #'t_l':taxo}
            f_dup   = ''
            label_d = {}
            label_r_d = {}
            for table_id, c_id, ph, tlabel, gcom, ngcom, doc_id, xml_id in ks:
                #tk   = self.get_quid(table_id+'_'+xml_id)
                #c_id        = txn_m.get('XMLID_MAP_'+tk)
                #if not c_id:continue
                row.setdefault('tids', {})[table_id]    = 1
                table_id    = str(table_id)
                c_id        = str(c_id)
                r           = int(c_id.split('_')[1])
                c           = int(c_id.split('_')[2])
                x           = txn_m.get('XMLID_'+c_id)
                t           = self.convert_html_entity(binascii.a2b_hex(txn_m.get('TEXT_'+c_id)))
                if tlabel:
                    tlabel  = self.convert_html_entity(tlabel)
                #print [taxo, table_id, c_id, t, ph, tlabel]
                if (deal_id, table_type) in self.ph_grp_d  or (deal_id == '44' and table_type == 'OS'):
                    c_ph        = ph
                    c_tlabel    = tlabel
                else:
                    c_ph        = str(c)
                    c_tlabel    = ''
                row[table_id+'-'+c_ph+'-'+c_tlabel]    = {'v':t, 'x':x, 'bbox':self.get_bbox_frm_xml(txn1, table_id, x), 'd':doc_id, 't':table_id, 'r':r}
                #if c_id in done_d:
                #    f_dup   = 'Y'
                #    row[table_id+'-'+c_ph+'-'+c_tlabel]['m']    = 'Y'
                if gcom == 'Y':
                    row[table_id+'-'+c_ph+'-'+c_tlabel]['g_f']    = 'Y'
                    row['f']    = 'Y'
                if ngcom == 'Y':
                    row[table_id+'-'+c_ph+'-'+c_tlabel]['ng_f']    = 'Y'
                    row['f']    = 'Y'
                done_d[c_id]    = 1
                if (deal_id, table_type) in self.ph_grp_d  or (deal_id == '44' and table_type == 'OS'):
                    table_ph_d.setdefault((doc_id, table_id), {})[(c_ph, c_tlabel)]   = (c, ph)
                else:
                    table_ph_d.setdefault((doc_id, table_id), {})[(c_ph, c_tlabel)]   = (c, tmptable_col_phs[(table_id, c)])
                #print table_id, c_id
                txts, xml_ar, bbox    = r_ld[table_id].get(r, ('', '', ''))
                txts        = self.convert_html_entity(txts)
                grm_txts    = txts.lower() #self.remove_grm_mrks(txts).lower()
                label_r_d[grm_txts] = 1
                label_d.setdefault(grm_txts, {'id':xml_id, 'txt':txts, 'bbox':bbox, 'x':xml_ar, 'd':doc_id, 't':table_id, 'v':{}})['v'][doc_d[doc_id][0]]    = {'bbox':bbox, 'x':xml_ar, 'd':doc_id, 't':table_id}

                if xml_id not in dd.get('l_change', {}):
                    label_d[grm_txts]['s']  = 'Y'
                col_txt = rc_ld.get(table_id, {}).get(c, ())
                if col_txt:
                    txts        = self.convert_html_entity(col_txt[1])
                    grm_txts    = txts.lower() #self.remove_grm_mrks(txts).lower()
                    label_d.setdefault(grm_txts, {'id':xml_id, 'txt':txts, 'bbox':col_txt[2], 'x':col_txt[0], 'd':doc_id, 't':table_id, 'v':{}})['v'][doc_d[doc_id][0]]    = {'bbox':col_txt[2], 'x':col_txt[0], 'd':doc_id, 't':table_id}
                    if xml_id not in dd.get('l_change', {}):
                        label_d[grm_txts]['s']  = 'Y'
            if len(label_d.keys()) > 1:
                row['lchange']  = 'Y'
                row['ldata']    = label_d.values()
            lble    = label_r_d.keys()
            lble.sort(key=lambda x:len(x), reverse=True)
            row['taxo']  = dd['t_l']
            if dd.get('u_label', ''):
                row['t_l']  = dd.get('u_label', '')
            else:
                row['t_l']  = label_d[lble[0]]['txt']
            xml_ar  = label_d[ lble[0]]['x'].split(':@:')
            if xml_ar and xml_ar[0]:
                    table_id    =  label_d[ lble[0]]['t']
                    doc_id      = label_d[ lble[0]]['d']
                    p_key   = txn.get('TRPLET_HGH_PINFO_'+table_id+'_'+self.get_quid(xml_ar[0]))
                    if p_key:
                        tmp_xar  = []
                        t_ar    = []
                        for pkinfo in p_key.split(':!:'):
                            pxml, ptext = pkinfo.split('^')
                            tmp_xar.append(pxml)
                            t_ar.append(binascii.a2b_hex(ptext))
                        pxml    = ':@:'.join(tmp_xar)
                        row['parent_txt']   = ' '.join(t_ar) # {'txt':' '.join(t_ar), 'bbox':self.get_bbox_frm_xml(txn1, table_id, pxml), 'x':pxml, 'd':doc_id, 't':table_id}

            row['x']    = label_d[ lble[0]]['x']
            row['bbox'] = label_d[ lble[0]]['bbox']
            row['t']    = label_d[ lble[0]]['t']
            row['d'] = label_d[ lble[0]]['d']
            row['l']    = len(ks)
            row['fd']   = f_dup
            row['order']   = dd['order_id']
            row['rid']   = dd['rid']
            if dd.get('m_rows', ''):
                row['merge']   = 'Y'
            if dd.get('missing', ''):
                row['missing']  = dd['missing']
            f_ar.append(row)
        return f_ar



    def merge_label(self, ijson):
        if ijson.get('st_ids', []):
            return self.merge_table_to_row(ijson)
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        if 1:#str(deal_id) == '214':
            return self.merge_taxo(ijson)
            


        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        t_ids           = ijson['t_ids']
        table_type      = ijson['table_type']
        sql             = "select taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, vgh_text, vgh_group, doc_id, xml_id from mt_data_builder where table_type='%s' and taxo_id in (%s) and isvisible='Y'"%(table_type , ', '.join(map(lambda x:str(x), t_ids)))
        cur.execute(sql)
        res             = cur.fetchall()
        ks_d            = {}
        taxo_ar         = []
        uset_taxo       = {}
        order_ids       = {}
        table_ids       = {}
        f_ph            = {}
        for rr in res:
            taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, gv_xml, ph, ph_label,gcom, ngcom, vgh_text, vgh_group, doc_id, xml_id    = rr
            doc_id      = str(doc_id)
            table_id    = str(table_id)
            table_ids[table_id]       = 1
            vgh_text     = '' if not vgh_text else vgh_text
            vgh_group    = '' if not vgh_group else vgh_group
            order_ids[order_id] = 1
            tk   = self.get_quid(table_id+'_'+xml_id)
            c_id        = txn_m.get('XMLID_MAP_'+tk)
            if not c_id:continue
            gv_xml  = c_id
            if (deal_id, table_type) in self.ph_grp_d:
                f_ph.setdefault((table_id, ph, ph_label), {})[(gv_xml, ph, ph_label)]    = 1
            else:
                f_ph.setdefault((table_id, c_id.split('_')[2], ''), {}).setdefault((gv_xml, ph, ph_label), {})[(taxo_id, xml_id)] = 1
            ks_d[(table_id, gv_xml, ph, ph_label, gcom, ngcom, vgh_text, vgh_group, doc_id, xml_id)]            = 1
            taxo_ar     += taxonomy.split(' / ')
            if user_taxonomy:
                uset_taxo[user_taxonomy]    = 1
        #print f_ph
        error_tables    = []
        for ph, cnt in f_ph.items():
            if len(cnt.keys()) > 1:
                 error_tables   += cnt.keys()
        if ((deal_id, table_type) in self.ph_grp_d) and error_tables:
            tmpar   = []
            for rr in error_tables:
                gv_xml, ph, ph_label    = rr
                table_id    = gv_xml.split('_')[0]
                tmpar.append(table_id+'-'+ph+'-'+ph_label)
            error_tables    = tmpar[:]
        elif error_tables:
            tmpar   = []
            for rr in error_tables:
                gv_xml, ph, ph_label    = rr
                table_id    = gv_xml.split('_')[0]
                tmpar.append(table_id+'-'+gv_xml.split('_')[2]+'-'+ph_label)
            error_tables    = tmpar[:]
                
        
        if error_tables:
            error_tables    = list(sets.Set(error_tables))
            res             = [{'message':'PH Overlap', 'data':error_tables}]
            return res
        if not order_ids:
            sql = "select max(order_id) from mt_data_builder"
            cur.execute(sql)
            r   = cur.fetchone()
            if r:
                order_ids[r[0]] = 1 
        order_ids    = order_ids.keys()
        order_ids.sort()
        order_id    = order_ids[0]
        #t_ids.sort()
        sql             = "update mt_data_builder set isvisible='N' where table_type='%s' and taxo_id in (%s)"%(table_type , ', '.join(map(lambda x:str(x), t_ids)))
        cur.execute(sql)


        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        lmdb_path   = '/var/www/html/Rajeev/BBOX/'+str(project_id)+'_'+str(deal_id)
        lmdb_path    = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        env1    = lmdb.open(lmdb_path, readonly=True)
        txn1    = env1.begin()
        r_ld    = {}
        for table_id in table_ids.keys():
            k       = 'HGH_'+str(table_id)
            ids     = txn_m.get(k)
            if ids:
                ids     = ids.split('#')
                for c_id in ids:
                    r       = int(c_id.split('_')[1])
                    c       = int(c_id.split('_')[2])
                    x       = txn_m.get('XMLID_'+c_id)
                    key     = table_id+'_'+self.get_quid(x)
                    t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id)).replace('\xc2\xa0', '')
                    t       = ' '.join(t.split())
                    rs      = int(txn_m.get('rowspan_'+c_id))
                    for tr in range(rs): 
                        r_ld.setdefault(table_id, {}).setdefault(r+tr, []).append((c, t))

        i_ar    = []
        taxo_ar = list(sets.Set(taxo_ar))
        taxo_ar.sort()
        taxo    = ' / '.join(taxo_ar)
        try:
            taxo    = taxo.decode('utf-8')
        except:pass
        uset_taxo   = uset_taxo.keys()
        uset_taxo.sort(key=lambda x:len(x))
        if uset_taxo:
            uset_taxo   = uset_taxo[-1]
        else:
            uset_taxo   = ''

        path    = "%s/%s/%s/1_1/21/sdata/doc_map.txt"%(self.doc_path, project_id, deal_id)
        if os.path.exists(path):
            fin = open(path, 'r')
            lines   = fin.readlines()
            fin.close()
        else:
            lines   = []
        doc_d       = {}
        dphs        = {}
        for line in lines[1:]:
            line    = line.split('\t')
            if len(line) < 8:continue
            line    = map(lambda x:x.strip(), line)
            ph      = line[3]+line[7]
            doc_id  = line[0]
            doc_d[doc_id]   = (ph, line[2])
            dphs[ph]        = 1
        dtime   = str(datetime.datetime.now()).split('.')[0]
        row     = {'t_id':t_ids[0]} #'t_l':taxo}
        label_d = {}
        for (table_id, c_id, ph, tlabel, gcom, ngcom, vgh_text, vgh_group, doc_id, xml_id) in ks_d.keys():
            c_id    = str(c_id)
            try:
                tlabel    = tlabel.decode('utf-8')
            except:pass
            tk   = self.get_quid(table_id+'_'+xml_id)
            c_id    = txn_m.get('XMLID_MAP_'+tk)
            i_ar.append((order_id, table_type, taxo, '', '', table_id, c_id, ph, tlabel, gcom, ngcom, 'Y', vgh_text, vgh_group, '_'.join(map(lambda x:str(x), t_ids)), doc_id, xml_id, ijson['user'], dtime))
            t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
            x       = txn_m.get('XMLID_'+c_id)
            r       = int(c_id.split('_')[1])
            c       = int(c_id.split('_')[2])
            if (deal_id, table_type) in self.ph_grp_d :
                c_ph        = ph
                c_tlabel    = tlabel
            else:
                c_ph    = str(c)
                c_tlabel    = ''
            row[table_id+'-'+c_ph+'-'+c_tlabel]    = {'v':t, 'x':x, 'bbox':self.get_bbox_frm_xml(txn1, table_id, x), 'd':doc_id, 't':table_id, 'r':r}
            row.setdefault('tids', {})[table_id]    = 1
            if gcom == 'Y':
                row[table_id+'-'+c_ph+'-'+c_tlabel]['g_f']    = 'Y'
            if ngcom == 'Y':
                row[table_id+'-'+c_ph+'-'+c_tlabel]['ng_f']    = 'Y'
            txts    = r_ld.get(table_id, {}).get(r, [])
            txts.sort(key=lambda x:x[0])
            txts_ar = []
            xml_ar  = []
            for x in txts:
                txts_ar.append(x[1])
                pkey    = table_id+'_'+str(r)+'_'+str(x[0])
                t_x = txn_m.get('XMLID_'+pkey)
                if t_x:
                    xml_ar.append(t_x)
            txts    = ' '.join(txts_ar)
            grm_txts    = txts.lower() #self.remove_grm_mrks(txts).lower()
            label_d.setdefault(grm_txts, {'id':xml_id, 'txt':txts, 'bbox':self.get_bbox_frm_xml(txn1, table_id, ':@:'.join(xml_ar)), 'x':':@:'.join(xml_ar), 'd':doc_id, 't':table_id, 'v':{}})['v'][doc_d[doc_id][0]]    = {'bbox':self.get_bbox_frm_xml(txn1, table_id, ':@:'.join(xml_ar)), 'x':':@:'.join(xml_ar), 'd':doc_id, 't':table_id}
            if vgh_group != 'N':
                label_d[grm_txts]['s']  = 'Y'
        if len(label_d.keys()) > 1:
            row['lchange']  = 'Y'
            row['ldata']    = label_d.values()
        lble    = label_d.keys()
        lble.sort(key=lambda x:len(x), reverse=True)
        if uset_taxo:
            row['t_l']  = uset_taxo
        else:
            #row['t_l']  = self.convert_html_entity(lble[0])
            row['t_l']  = self.convert_html_entity(label_d[lble[0]]['txt'])
        row['x']    = label_d[ lble[0]]['x']
        row['bbox'] = label_d[ lble[0]]['bbox']
        row['t']    = label_d[ lble[0]]['t']
        row['d'] = label_d[ lble[0]]['d']
        row['merge'] = 'Y'
        with conn:
            #sql = "select max(taxo_id) from kpi_input"
            sql = "select seq from sqlite_sequence WHERE name = 'mt_data_builder'"
            cur.execute(sql)
            r       = cur.fetchone()
            g_id    = int(r[0])+1
            sql     = "select max(taxo_id) from mt_data_builder" 
            cur.execute(sql)
            r       = cur.fetchone()
            tg_id    = int(r[0])+1
            g_id    = max(g_id, tg_id)
            row['t_id'] = g_id
            i_ar    = map(lambda x:(g_id, )+x, i_ar)
            cur.executemany("insert into mt_data_builder(taxo_id, order_id, table_type, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label, gcom, ngcom, isvisible, vgh_text, vgh_group, m_rows, doc_id, xml_id, user_name, datetime)values(?,?, ?, ?, ?, ?, ?,?, ?, ?, ?, ?, ?,?, ?, ?, ?,?,?, ?)", i_ar)
            conn.commit()
        conn.close()
        dd  = {}
        dd[t_ids[0]]    = row
        del_ids = {}
        for t in t_ids[1:]:
            del_ids[t]  = 1
        res             = [{'message':'done', 'data':dd, 'd_ids':del_ids}]
        return res

    def read_all_vgh_texts(self, ijson):
        return self.read_user_saved_vgh_data_new(ijson)
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        i_table_type    = ijson['table_type']
        ph_d        = {}
        path    = "%s/%s/%s/1_1/21/sdata/doc_map.txt"%(self.doc_path, project_id, deal_id)
        if os.path.exists(path):
            fin = open(path, 'r')
            lines   = fin.readlines()
            fin.close()
        else:
            lines   = []
        doc_d       = {}
        dphs        = {}
        c_year      = self.get_cyear(lines)
        #start_year  = c_year - int(ijson['year'])
        start_year  = c_year - int(ijson.get('year', 5))
        for line in lines[1:]:
            line    = line.split('\t')
            if len(line) < 8:continue
            line    = map(lambda x:x.strip(), line)
            if ijson.get('grp_doc_ids', {}) and str(line[0]) not in ijson['grp_doc_ids']:continue
            ph      = line[3]+line[7]
            if ph and start_year<int(ph[2:]):
                doc_id  = line[0]
                doc_d[doc_id]   = (ph, line[2])
                dphs[ph]        = 1

        i_table_type    = ijson['table_type']
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, [i_table_type])

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        lmdb_path   = '/var/www/html/Rajeev/BBOX/'+str(project_id)+'_'+str(deal_id)
        lmdb_path    = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        env1    = lmdb.open(lmdb_path, readonly=True)
        txn1    = env1.begin()


        table_type  = str(i_table_type)
        group_d     = {}
        revgroup_d  = {}

        d_group_d     = {}
        d_revgroup_d  = {}

        vgh_id_d    = {}
        db_file         = self.get_db_path(ijson)
        conn, cur   = conn_obj.sqlite_connection(db_file)
        #sql         = "select vgh_id, doc_id, table_id, group_txt from doc_group_map where table_type='%s'"%(table_type)
        #try:
        #    cur.execute(sql)
        #    res = cur.fetchall()
        #except:
        #    res = []
        grp_doc_map_d   = {}
        rev_doc_map_d   = {}
        #for rr in res:
        #    vgh_id, doc_id, table_id, group_txt = rr
        #    grp_doc_map_d.setdefault(group_txt, {})[table_id]   = doc_id #.setdefault(doc_id, {})[table_id]    = 1
        #    rev_doc_map_d.setdefault((doc_id, table_id), {})[group_txt] = 1
        #sql = "CREATE TABLE IF NOT EXISTS vgh_doc_map(row_id INTEGER PRIMARY KEY AUTOINCREMENT, vgh_group_id TEXT, doc_group_id TEXT, table_type VARCHAR(100), group_txt TEXT, user_name VARCHAR(100), datetime TEXT);"
        sql = "select vgh_group_id, doc_group_id, group_txt from vgh_doc_map where table_type='%s'"%(table_type)
        doc_vgh_map = {}
        try:
            cur.execute(sql)
            res = cur.fetchall()
        except:
            res = []
        for rr in res:
            vgh_group_id, doc_group_id, group_txt   = rr
            doc_vgh_map[(vgh_group_id, doc_group_id)]   = group_txt

        sql = "select group_id, table_type, group_txt from vgh_group_info where table_type='%s'"%(table_type)
        try:
            cur.execute(sql)
            res = cur.fetchall()
        except:
            res = []
        grp_info    = {}
        for rr in res:
            group_id, table_type, group_txt = rr
            group_txt = group_txt.replace('\t', '')
            grp_info[str(group_id)]   = group_txt

        sql         = "select row_id, vgh_id, group_txt, table_str, doc_vgh from vgh_group_map where table_type='%s'"%(table_type)
        try:
            cur.execute(sql)
            res = cur.fetchall()
        except:
            res = []
        for rr in res:
            row_id, vgh_id, group_txt, table_str, doc_vgh   = rr
            #print rr
            if doc_vgh == 'DOC':
                d_group_d.setdefault(group_txt, {})[vgh_id]   = (row_id, table_str)
                d_revgroup_d.setdefault(vgh_id, {})[group_txt]    = (row_id, table_str)
                pass
            else:
                group_d.setdefault(group_txt, {})[vgh_id]   = (row_id, table_str)
                revgroup_d.setdefault(vgh_id, {})[group_txt]    = (row_id, table_str)

        sql         = "select row_id, table_id, c_id, vgh_text, doc_id, xml_id from mt_data_builder where table_type='%s'"%(table_type)
        try:
            cur.execute(sql)
            res = cur.fetchall()
        except:
            res = []
        docinfo_d   = {}
        vgh_id_d_all    = {}
        for rr in res:
            row_id, table_id, c_id, vgh_text, doc_id, xml_id    = rr
            tk   = self.get_quid(table_id+'_'+xml_id)
            c_id        = txn_m.get('XMLID_MAP_'+tk)
            if not c_id:continue
            doc_id  = str(doc_id)
            if vgh_text != None and str(doc_id) in doc_d and str(table_id) in m_tables:
                vgh_id_d.setdefault(vgh_text, {})[(table_id, c_id, row_id, doc_id)]        = 1
                vgh_id_d_all[vgh_text]  = 1
                doc_id      = str(doc_id)
                table_id    = str(table_id)
                docinfo_d.setdefault(doc_id, {})[(table_id, c_id, vgh_text)]   = 1

        
        sql         = "select vgh, vgh_id from vgh_info where table_type='%s'"%(table_type)
        try:
            cur.execute(sql)
            res = cur.fetchall()
        except:
            res = []
        vgh_d       = {}
        for rr in res:
            vgh_text, vgh_id    = rr
            if vgh_id in vgh_id_d_all:
                try:
                    vgh_text    = binascii.a2b_hex(vgh_text)
                except:pass
                vgh_d[vgh_id]       = self.convert_html_entity(vgh_text)

        dghs = docinfo_d.keys()
        dghs.sort(key=lambda x:len( docinfo_d[x].keys()), reverse=True)
        df_arr       = []
        dmax_tlen    = 0
        for ii, dgh_id in enumerate(dghs[:]):
            dgh         = docinfo_d[dgh_id]
            dd          = {}
            dd['txt']   = '_'.join(doc_d[dgh_id])
            dd['v_id']  = dgh_id
            vids        = docinfo_d[dgh_id].keys()
            vids.sort()
            doc_id      = dgh_id
            table_id, c_id, row_id        = vids[0]
            x           = txn_m.get('COLXMLID_'+str(table_id)+'_'+str(c_id.split('_')[2]))
            if not x:
                x       = txn_m.get('XMLID_'+str(c_id))
            dd['x']     = x
            dd['d']     = doc_id
            dd['t']     = table_id
            dd['bbox']  = self.get_bbox_frm_xml(txn1, table_id, x)
            tmparr  = []
            t_d     = {}
            vgh_ids = {}
            tc_d    = {}
            for rtup in vids:
                table_id, c_id, row_id  = rtup
                c                       = c_id.split('_')[2]
                t_d.setdefault(table_id+'-'+c, {})[(row_id, c_id)] = 1
                vgh_ids[row_id] = 1
                tc_d[table_id+'-'+c]    = doc_id
            dmax_tlen    = max(dmax_tlen, len(t_d.keys()))
            for table_id, rows in t_d.items():
                rows    = rows.keys()
                c_id    = rows[0][1]
                x       = txn_m.get('COLXMLID_'+str(table_id)+'_'+str(c_id.split('_')[2]))
                if not x:
                    x       = txn_m.get('XMLID_'+str(c_id))
                rd      = {'t':table_id, 'x':x, 'bbox':self.get_bbox_frm_xml(txn1, table_id, x), 'd':doc_id, 'rid':map(lambda x:x[0], rows)}
                tmparr.append(rd)
                #if (doc_id, table_id) in rev_doc_map_d:
                #    dd['g']         = rev_doc_map_d[(doc_id, table_id)].keys()
                #    for grp_id in dd['g']:
                #        dgroup_d[grp_id][(doc_id, table_id)] = ii
            dd['table_str']     = ', '.join(t_d.keys())
            dd['table_ids']     = tmparr  
            dd['i']             = ii
            dd['vgh_ids']       = vgh_ids

            if doc_id in d_revgroup_d:
                dd['g']         = d_revgroup_d.get(doc_id, {}).keys()
                for grp_id in dd['g']:
                    tmpd            = {}
                    if not d_revgroup_d[doc_id][grp_id][1]:
                        tmpd    = tc_d
                    else:
                        for tsr in d_revgroup_d[doc_id][grp_id][1].split('#'):
                            if tsr not in tc_d:continue
                            tmpd[tsr]   = tc_d[tsr]
                    d_group_d[grp_id][doc_id] = (ii, d_revgroup_d[doc_id][grp_id][0], tmpd, d_revgroup_d[doc_id][grp_id][1])
            df_arr.append(dd)

        vghs = vgh_id_d.keys()
        vghs.sort(key=lambda x:len( vgh_id_d[x].keys()), reverse=True)
        f_arr       = []
        max_tlen    = 0
        grp_doc_ids = {}
        for ii, vgh_id in enumerate(vghs):
            vids        = vgh_id_d[vgh_id].keys()
            #vgh_id, doc_grp = vgh_id
            vgh         = vgh_d[vgh_id]
            dd          = {}
            dd['txt']   = vgh
            dd['v_id']  = vgh_id
            vids.sort()
            table_id, c_id, row_id, doc_id        = vids[0]
            x           = txn_m.get('COLXMLID_'+str(table_id)+'_'+str(c_id.split('_')[2]))
            if not x:
                x       = txn_m.get('XMLID_'+str(c_id))
            dd['x']     = x
            dd['d']     = doc_id
            dd['t']     = table_id
            dd['bbox']  = self.get_bbox_frm_xml(txn1, table_id, x)
            tmparr  = []
            t_d     = {}
            tc_d     = {}
            doc_id_d    = {}
            for rtup in vids:
                table_id, c_id, row_id, doc_id  = rtup
                c                       = c_id.split('_')[2]
                t_d.setdefault((table_id+'-'+c, doc_id), {})[(row_id, c_id)] = 1
                tc_d[table_id+'-'+c]    = doc_id
                doc_id_d.setdefault(doc_id, {})[table_id+'-'+c]    = 1
            max_tlen    = max(max_tlen, len(t_d.keys()))
            for (table_id, doc_id), rows in t_d.items():
                rows    = rows.keys()
                c_id    = rows[0][1]
                x       = txn_m.get('COLXMLID_'+str('_'.join(table_id.split('-'))))
                if not x:
                    x       = txn_m.get('XMLID_'+str(c_id))
                rd      = {'t':table_id, 'x':x, 'bbox':self.get_bbox_frm_xml(txn1, table_id.split('-')[0], x), 'd':doc_id, 'rid':map(lambda x:x[0], rows)}
                tmparr.append(rd)
            dd['table_ids']     = tmparr  
            dd['table_str']     = ', '.join(tc_d.keys())
            dd['i']             = ii
            dd['dont_tables']   = {}
            if vgh_id in revgroup_d:
                dd['g']         = revgroup_d.get(vgh_id, {}).keys()
                for grp_id in dd['g']:
                    tmpd            = {}
                    doc_tcd = {}
                    if not revgroup_d[vgh_id][grp_id][1]:
                        tmpd    = tc_d
                    else:
                        for tsr in revgroup_d[vgh_id][grp_id][1].split('#'):
                            if tsr not in tc_d:continue
                            tmpd[tsr]   = tc_d[tsr]
                    if revgroup_d[vgh_id][grp_id][1]:
                        for tsr in revgroup_d[vgh_id][grp_id][1].split('#'):
                            if tsr not in tc_d:continue
                            doc_tcd.setdefault(tc_d[tsr], {})[tsr]     = 1
                    else:
                        for tkey, doc_k in tc_d.items():
                            #if tkey not in tc_d:continue
                            doc_tcd.setdefault(doc_k, {})[tkey]     = 1
                    for ck, tv in tmpd.items():
                            dd['dont_tables'].setdefault(ck, {})[grp_id]   = 1
                    for doc_id in doc_id_d.keys():
                        grp_doc_ids.setdefault(grp_id, {}).setdefault(d_revgroup_d.get(doc_id, {'':''}).keys()[0], {}).setdefault(vgh_id, {}).update(doc_tcd.get(doc_id, {}))
                    #print (grp_id, grp_id, tmpd, revgroup_d[vgh_id][grp_id][1])
                    group_d[grp_id][vgh_id] = (ii, revgroup_d[vgh_id][grp_id][0], tmpd, revgroup_d[vgh_id][grp_id][1])
            f_arr.append(dd)



        g_ar    = []
        for k, v in group_d.items():
            #print [k]
            doc_grp = grp_doc_ids.get(k, {})
            for tk, tvids in doc_grp.items():
                tmp_grp_name    = ''
                if tk in grp_info:
                    tmp_grp_name    = ' - '+grp_info[tk]
                tmp_grp_name    = grp_info.get(k, k)+tmp_grp_name
                tmp_grp_name    = doc_vgh_map.get((k, tk), tmp_grp_name)    
                dd  = {'grp':tmp_grp_name, 'vids':{}, 'grpid':k, 'doc_ids':{}, 'doc_grpid':tk}
                table_ids   = {}
                for vid, i in v.items():
                    if vid not in tvids:continue
                    #print vid, i
                    dd['vids'][i[0]]   = str(i[1])
                    if len(i) <3:
                        dd['vids'][i[0]]   = str(i[0])
                        continue
                    table_ids.update(tvids[vid])
                    for tk, dv in tvids[vid].items(): 
                        dd['doc_ids'][dv]  = 1
                    #for ktup in vgh_id_d[vid].keys():
                    #    table_ids[ktup[0]]  = ktup[3]
                #print k in grp_doc_map_d, grp_doc_map_d.get(k, {})
                #if k in grp_doc_map_d:
                #    table_ids   = grp_doc_map_d[k]
                #    dd['doc_filter'] = 'Y'
                #    dd['doc_ids'] = {}
                #    for k, v in grp_doc_map_d[k].items():
                #        dd['doc_ids'][v] = 1
                dd['table_ids'] = table_ids
                g_ar.append(dd)
        if not g_ar:
            g_ar.append({'grp':'', 'vids':{}, 'grpid':'new'})
        dg_ar   = []
        for k, v in d_group_d.items():
            #print [k]
            dd  = {'grp':grp_info.get(k, k), 'vids':{}, 'grpid':k, 'doc_ids':{}}
            table_ids   = {}
            for vid, i in v.items():
                dd['vids'][i[0]]   = str(i[1])
                if len(i) <3:
                    dd['vids'][i[0]]   = str(i[0])
                    continue
                table_ids.update(i[2])
                for tk, dv in i[2].items(): 
                    dd['doc_ids'][dv]  = 1
                #for ktup in vgh_id_d[vid].keys():
                #    table_ids[ktup[0]]  = ktup[3]
            #print k in grp_doc_map_d, grp_doc_map_d.get(k, {})
            #if k in grp_doc_map_d:
            #    table_ids   = grp_doc_map_d[k]
            #    dd['doc_filter'] = 'Y'
            #    dd['doc_ids'] = {}
            #    for k, v in grp_doc_map_d[k].items():
            #        dd['doc_ids'][v] = 1
            dd['table_ids'] = table_ids
            dg_ar.append(dd)
        if not dg_ar:
            dg_ar.append({'grp':'', 'vids':{}, 'grpid':'new'})
        #sys.exit()
        res = [{'message':'done', 'data':f_arr, 'groups':g_ar, 'max_tlen':max_tlen, 'docs':df_arr, 'dmax_tlen':dmax_tlen, 'dgroups':dg_ar}]
        return res

    def add_new_line_itesm(self, ijson):
        pass

    def unmerge_label(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        t_id           = ijson['t_id']
        table_type      = ijson['table_type']
        sql             = "select m_rows from mt_data_builder where table_type='%s' and taxo_id in (%s)"%(table_type , t_id)
        cur.execute(sql)
        res             = cur.fetchone()
        t_ids           = res[0].split('_')
        sql             = "select taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, doc_id,m_rows from mt_data_builder where table_type='%s' and taxo_id in (%s)"%(table_type , ', '.join(map(lambda x:str(x), t_ids)))
        cur.execute(sql)
        res         = cur.fetchall()
        taxo_d  = {}
        table_ids   = {}
        for rr in res:
            taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, gv_xml, ph, ph_label,gcom, ngcom, doc_id,m_rows    = rr
            doc_id      = str(doc_id)
            table_id    = str(table_id)
            table_ids[table_id]   = 1
            comp    = ''
            if gcom == 'Y' or ngcom == 'Y':
                comp    = 'Y'
            taxo_d.setdefault(taxo_id, {'order_id':order_id, 'u_label':user_taxonomy, 't_l':taxonomy, 'missing':missing_taxo, 'comp':comp, 'ks':[], 'm_rows':m_rows})['ks'].append((table_id, gv_xml, ph, ph_label,gcom, ngcom, doc_id))


        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        lmdb_path   = '/var/www/html/Rajeev/BBOX/'+str(project_id)+'_'+str(deal_id)
        lmdb_path    = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        env1    = lmdb.open(lmdb_path, readonly=True)
        txn1    = env1.begin()
        r_ld    = {}
        for table_id in table_ids.keys():
            k       = 'HGH_'+str(table_id)
            ids     = txn_m.get(k)
            if ids:
                ids     = ids.split('#')
                for c_id in ids:
                    r       = int(c_id.split('_')[1])
                    c       = int(c_id.split('_')[2])
                    x       = txn_m.get('XMLID_'+c_id)
                    key     = table_id+'_'+self.get_quid(x)
                    t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id)).replace('\xc2\xa0', '')
                    t       = ' '.join(t.split())
                    r_ld.setdefault(table_id, {}).setdefault(r, []).append((c, t))
        f_ar        = []
        done_d      = {}
        #for dd in f_taxo_arr:
        for row_id, dd in taxo_d.items():
            ks      = dd['ks']
            taxos   = dd['t_l'].split(' / ')[0]
            row     = {'t_id':row_id} #'t_l':taxo}
            f_dup   = ''
            label_d = {}
            for table_id, c_id, ph, tlabel, gcom, ngcom, doc_id in ks:
                table_id    = str(table_id)
                c_id        = str(c_id)
                r           = int(c_id.split('_')[1])
                c           = int(c_id.split('_')[2])
                x           = txn_m.get('XMLID_'+c_id)
                t           = self.convert_html_entity(binascii.a2b_hex(txn_m.get('TEXT_'+c_id)))
                if tlabel:
                    tlabel  = self.convert_html_entity(tlabel)
                #print [taxo, table_id, c_id, t, ph, tlabel]
                row[table_id+'-'+ph+'-'+tlabel]    = {'v':t, 'x':x, 'bbox':self.get_bbox_frm_xml(txn1, table_id, x), 'd':doc_id, 't':table_id, 'r':r}
                if c_id in done_d:
                    f_dup   = 'Y'
                    row[table_id+'-'+ph+'-'+tlabel]['m']    = 'Y'
                if gcom == 'Y':
                    row[table_id+'-'+ph+'-'+tlabel]['g_f']    = 'Y'
                    row['f']    = 'Y'
                if ngcom == 'Y':
                    row[table_id+'-'+ph+'-'+tlabel]['ng_f']    = 'Y'
                    row['f']    = 'Y'
                done_d[c_id]    = 1
                txts    = r_ld[table_id][r]
                txts.sort(key=lambda x:x[0])
                txts_ar = []
                xml_ar  = []
                for x in txts:
                    txts_ar.append(x[1])
                    pkey    = table_id+'_'+str(r)+'_'+str(x[0])
                    xml_ar.append(txn_m.get('XMLID_'+pkey))
                txts    = self.convert_html_entity(' '.join(txts_ar))
                tph     = ph
                if tlabel:
                    tph = ph+'('+self.value_type_map.get(tlabel, tlabel)+')'
                grm_txts    = txts.lower() #self.remove_grm_mrks(txts).lower()
                label_d.setdefault(grm_txts, {'txt':txts,'bbox':self.get_bbox_frm_xml(txn1, table_id, ':@:'.join(xml_ar)), 'x':':@:'.join(xml_ar), 'd':doc_id, 't':table_id, 'v':{}})['v'][table_id+'-'+ph+'-'+tlabel]    = 1
            if len(label_d.keys()) > 1:
                row['lchange']  = 'Y'
                row['ldata']    = label_d.values()
            lble    = label_d.keys()
            lble.sort(key=lambda x:len(x), reverse=True)
            if dd.get('u_label', ''):
                row['t_l']  = dd.get('u_label', '')
            else:
                row['t_l']  = label_d[lble[0]]['txt']
            row['x']    = label_d[lble[0]]['x']
            row['bbox'] = label_d[ lble[0]]['bbox']
            row['t']    = label_d[ lble[0]]['t']
            row['d'] = label_d[ lble[0]]['d']
            row['l']    = len(ks)
            row['fd']   = f_dup
            row['order']   = dd['order_id']
            if dd.get('m_rows', ''):
                row['merge']   = 'Y'
            if dd.get('missing', ''):
                row['missing']  = dd['missing']
            f_ar.append(row)
        sql             = "update mt_data_builder set isvisible='Y' where table_type='%s' and taxo_id in (%s)"%(table_type , ', '.join(map(lambda x:str(x), t_ids)))
        cur.execute(sql)
        sql             = "update mt_data_builder set isvisible='N' where table_type='%s' and taxo_id in (%s)"%(table_type , t_id)
        cur.execute(sql)
        conn.commit()
        conn.close()
        res             = [{'message':'done', 'data':f_ar, 'd_ids':{t_id:1}}]
        return res

    def update_vgh_group_text(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type    = ijson['table_type']
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        i_ar        = []
        dtime   = str(datetime.datetime.now()).split('.')[0]
        for rr in ijson['data']:
            sql     = "update vgh_group_map set group_txt=%s where vgh_id in (%s) and table_type ='%s'"%(rr['grpid'], ','.join(rr['vids']), table_type)
            cur.execute(sql)
        conn.commit()
        conn.close()
        res = [{'message':'done'}]
        return self.read_all_vgh_texts(ijson)

    def update_vgh_group(self, ijson):
        return self.save_user_selected_table_column_wrapper(ijson)  
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type    = ijson['table_type']
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)

        sql = "CREATE TABLE IF NOT EXISTS vgh_group_info(row_id INTEGER PRIMARY KEY AUTOINCREMENT, group_id VARCHAR(20), table_type VARCHAR(100), group_txt VARCHAR(20), user_name VARCHAR(100), datetime TEXT);"
        cur.execute(sql)

        #sql = "CREATE TABLE IF NOT EXISTS doc_group_map(row_id INTEGER PRIMARY KEY AUTOINCREMENT, doc_id TEXT, table_id TEXT, table_type VARCHAR(100), group_txt VARCHAR(20), vgh_id TEXT, user_name VARCHAR(100), datetime TEXT);"
        #cur.execute(sql)

        doc_vgh     = ijson.get('doc_vgh', 'VGH')

        i_ar        = []
        dtime   = str(datetime.datetime.now()).split('.')[0]
        doc_vgh_map = []
        u_doc_vgh_map = []
        sql = "select vgh_group_id, doc_group_id, group_txt from vgh_doc_map where table_type='%s'"%(table_type)
        cur.execute(sql)
        doc_vgh_map_d = {}
        res = cur.fetchall()
        for rr in res:
            vgh_group_id, doc_group_id, group_txt   = rr
            doc_vgh_map_d[(vgh_group_id, doc_group_id)]   = group_txt
        if 1:
            new_grp = []
            for ii, rr in enumerate(ijson['data']):
                if rr.get('txt_update', '') != 'Y':continue
                if rr['grpid'] == 'new':
                    new_grp.append((rr['grp'], ii))
                else:
                    if rr.get('doc_grpid', '') != '':
                        if(rr['grpid'], rr['doc_grpid']) in doc_vgh_map_d:
                            u_doc_vgh_map.append((rr['grp'], table_type, rr['grpid'], rr['doc_grpid']))
                        else:
                            doc_vgh_map.append((rr['grp'], table_type, rr['grpid'], rr['doc_grpid']))
                        cur.execute(sql)
                    else:
                        sql     = "update vgh_group_info set group_txt='%s' where table_type ='%s' and group_id='%s'"%(rr['grp'], table_type, rr['grpid'])
                        cur.execute(sql)
            if new_grp:
                i_ar    = []
                dtime   = str(datetime.datetime.now()).split('.')[0]
                with conn:
                    #sql = "select max(taxo_id) from kpi_input"
                    sql = "select seq from sqlite_sequence WHERE name = 'vgh_group_info'"
                    cur.execute(sql)
                    r       = cur.fetchone()
                    g_id    = 1
                    if r and r[0]:
                        g_id    = int(r[0])+1
                    
                    sql = "select max(group_id) from vgh_group_info"
                    cur.execute(sql)
                    r       = cur.fetchone()
                    tg_id   = 1
                    if r and r[0]:
                        tg_id    = int(r[0])+1
                    g_id     = max(g_id, tg_id)
                    for ng in new_grp:
                        ijson['data'][ng[1]]['grpid']   = g_id
                        i_ar.append((g_id, table_type, ng[0], ijson['user'], dtime))
                        g_id    += 1
                cur.executemany('insert into vgh_group_info(group_id, table_type, group_txt, user_name, datetime) values(?,?,?,?,?)', i_ar)
        if u_doc_vgh_map:
            cur.executemany("update vgh_doc_map set group_txt=? where table_type=? and vgh_group_id=? and doc_group_id=?", u_doc_vgh_map)
        if doc_vgh_map:
            cur.executemany("insert into vgh_doc_map(group_txt, table_type , vgh_group_id, doc_group_id)values(?,?,?,?)", doc_vgh_map)
        conn.commit()
        exists      = {}
        sql         = "select vgh_id, group_txt from vgh_group_map where table_type='%s'"%(table_type)
        cur.execute(sql)
        res         = cur.fetchall()
        for rr in res:
            vgh_id, group_txt   = rr
            exists[(str(vgh_id), group_txt)] = 1

        #dexists      = {}
        #sql         = "select doc_id, table_id, group_txt from doc_group_map where table_type='%s'"%(table_type)
        #try:
        #    cur.execute(sql)
        #    res         = cur.fetchall()
        #except:
        #    res = []
        #for rr in res:
        #    doc_id, table_id, group_txt   = rr
        #    dexists[(str(doc_id), str(table_id), group_txt)] = 1
        i_ar    = []
        di_ar   = []
        u_ar    = []
        if doc_vgh == 'DOC':
            for rr in ijson['data']:
                for rid, doc_tables in rr['vids'].items():
                    if(rid, rr['grpid']) not in exists:
                        i_ar.append((table_type, rr['grpid'], rid, '', doc_vgh, ijson['user'], dtime))
                    #else:
                    #    u_ar.append(('#'.join(doc_tables), ijson['user'], dtime, table_type, rr['grpid'], rid, doc_vgh))
        else:
            for rr in ijson['data']:
                for rid, doc_tables in rr['vids'].items():
                    #print rid, doc_tables
                    if(rid, rr['grpid']) not in exists:
                        i_ar.append((table_type, rr['grpid'], rid, '#'.join(doc_tables), doc_vgh, ijson['user'], dtime))
                    else:
                        u_ar.append(('#'.join(doc_tables), ijson['user'], dtime, table_type, rr['grpid'], rid, doc_vgh))
                        
                    #if rr.get('doc_filter', '') == 'Y':
                    #    for doc_table in doc_tables:
                    #        doc_id, table_id    = doc_table.split('-')
                    #        if(doc_id, table_id, str(rr['grpid'])) not in dexists:
                    #            di_ar.append((table_type, rr['grpid'], rid, doc_id, table_id, ijson['user'], dtime))
                    
        #print di_ar
        #sys.exit()
        cur.executemany("insert into vgh_group_map(table_type, group_txt, vgh_id, table_str, doc_vgh, user_name, datetime)values(?,?,?,?,?,?,?)", i_ar)
        cur.executemany("update vgh_group_map set table_str=?, user_name=?, datetime=? where table_type=? and  group_txt=? and  vgh_id=? and doc_vgh=?", u_ar)
        conn.commit()

        #cur.executemany("insert into doc_group_map(table_type, group_txt, vgh_id, doc_id, table_id, user_name, datetime)values(?,?,?,?,?,?,?)", di_ar)
        #conn.commit()
        #for rr in ijson['data']:
        #    sql     = "update vgh_group_map set group_txt='%s' where vgh_id in (%s) and table_type ='%s'"%(rr['grp'], ','.join(rr['vids']), table_type)
        #    cur.execute(sql)
        conn.close()
        res = [{'message':'done'}]
        return self.read_all_vgh_texts(ijson)

    def save_user_selected_table_column_wrapper(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        user_name       = ijson["user"] 
        save_data       = ijson["data"]
        table_type      = ijson["table_type"]
        #sql = "CREATE TABLE IF NOT EXISTS f_vgh_group_info(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_type TEXT, group_txt TEXT, table_str TEXT, user_name VARCHAR(100), datetime TEXT);"
        #txn_m.get('COLTEXT_'+table_id+'_'+colid)
        #txn_m.get('COLXMLID_'+table_id+'_'+colid)
        comp_db_path = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn  = sqlite3.connect(comp_db_path)
        cur   = conn.cursor()
        crt_qry = "CREATE TABLE IF NOT EXISTS f_vgh_group_info(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_type TEXT, group_txt TEXT, table_str TEXT, user_name VARCHAR(100), datetime TEXT);"
        cur.execute(crt_qry)
            
        from itertools import chain
        
        for row_dct in  save_data:
            row_id, group_txt, vids = row_dct["grpid"], row_dct["grp"], row_dct["vids"]
            table_str = '#'.join(list(chain(*vids.values())))
            entry_time = str(datetime.datetime.now())
            if not row_id or row_id == 'new':
                cur.execute('INSERT INTO f_vgh_group_info(table_type, group_txt, table_str, user_name, datetime) VALUES("%s", "%s", "%s", "%s", "%s")'%(table_type, group_txt, table_str, user_name, entry_time))
                conn.commit()
            elif row_id:
                row_id = int(row_id)
                cur.execute('UPDATE f_vgh_group_info SET table_str="%s", group_txt="%s" WHERE row_id=%d'%(table_str, group_txt,int(row_id)))
                conn.commit()
        conn.close()
        res_dat = self.read_user_saved_vgh_data_new(ijson)[0] 
        groups = res_dat['groups']
        dt = res_dat['data']
        mx_ln = res_dat['max_tlen'] 
        self.prepare_taxo_dct(ijson)
        return [{'message':'done', 'data':dt, 'groups':groups, 'max_tlen':mx_ln}]
    def prepare_taxo_dct(self, ijson):
        res_dct = self.get_hgh_group_ids(ijson)
        for grpid, td in res_dct.items():
            ijson_c = copy.deepcopy(ijson)
            ijson_c['t_ids'] = td
            self.create_HGH_group(ijson_c)
        return        


    def get_hgh_group_ids(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type      = ijson['table_type']
        '''
            sql = "select row_id, table_type, group_txt from f_vgh_group_info where table_type like '%s'"%('HGHGROUP-'+table_type+'%')
            try:
                cur.execute(sql)
                res = cur.fetchall()
            except:
                res = []
            for rr in res:
                #print rr
                tgroup_id, table_type, group_txt = rr
                group_txt = group_txt.replace('\t', '')
                grp_info[str(table_type)]   = group_txt
        '''
        db_path = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)   
        conn = sqlite3.connect(db_path)
        cur  = conn.cursor()
        read_qry = """select table_type, group_txt from f_vgh_group_info where table_type like '%s'"""%('HGHGROUP-'+table_type+'%')
        try:
                cur.execute(read_qry)
                res = cur.fetchall()
        except:
            res = []
        res_dct = {}
        for rr in res:
            group_id, group_txt = rr
            group_txt = group_txt.replace('\t', '')
            for tid in group_id.split('-')[-2].split('_'):
                if tid:
                    res_dct.setdefault(group_id, {})[int(tid)] = group_txt
        return res_dct
            
            
                



    def delete_vgh_group(self, ijson):
        return self.delete_vgh_info_new(ijson)
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type    = ijson['table_type']
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        i_ar        = []
        di_ar       = []
        rr  = ijson
        row_d   = rr['r_ids']
        #for rid, doc_tables in rr['vids'].items():
        #    i_ar.append((table_type, rr['grpid'], rid))
            #if rr.get('doc_filter', '') == 'Y':
            #    for doc_table in doc_tables:
            #        doc_id, table_id    = doc_table.split('-')
            #        di_ar.append((table_type, rr['grpid'], rid, doc_id, table_id))
        #print i_ar
        cur.execute("delete from vgh_group_map where row_id in (%s)"%(', '.join(filter(lambda x:x != 'None', row_d))))
        #cur.executemany("delete from vgh_group_map where table_type=? and group_txt=? and  vgh_id=?", i_ar)
        #cur.executemany("delete from doc_group_map where table_type=? and group_txt=? and  vgh_id=? and doc_id=? and table_id=?", di_ar)
        conn.commit()
        conn.close()
        res = [{'message':'done'}]
        return res

    def update_order_multiple(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type      = ijson['table_type']
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        if 'HGHGROUP' in ijson.get('grpid'):
            sql             = "select row_id, taxo_id, order_id from mt_data_builder where table_type='%s' and isvisible='Y' group by taxo_id"%(ijson['grpid'])
        else:
            sql             = "select row_id, taxo_id, order_id from mt_data_builder where table_type='%s' and isvisible='Y' group by taxo_id"%(table_type)
        cur.execute(sql)
        res             = cur.fetchall()
        order_d         = {}
        taxo_o_d        = {}
        for rr in res:
            row_id, taxo_id, order_id   = rr
            taxo_o_d[taxo_id]           = order_id
            order_d.setdefault(order_id, {})[taxo_id]   = min(row_id, order_d.get(order_id, {}).get(taxo_id, row_id))
        order_ids   = order_d.keys()
        order_ids.sort()
        tid1    = int(ijson['t_id1'])
        tid2    = map(lambda x:int(x), ijson['t_ids'])
        direction = ijson['pos']
        od1     = taxo_o_d[tid1]
        new_order   = []
        new_order_d = {}
        for order_id in order_ids:
            if order_id < od1:
                taxo_ids    = order_d[order_id].keys()
                for taxo_id in taxo_ids:
                    if taxo_id in tid2:continue
                    new_order_d[taxo_id] = order_id
                    new_order.append((order_id, taxo_id))
            elif order_id > od1:
                taxo_ids    = order_d[order_id].keys()
                order_id    += len(tid2)+1
                for taxo_id in taxo_ids:
                    if taxo_id in tid2:continue
                    new_order_d[taxo_id] = order_id
                    new_order.append((order_id, taxo_id))
            elif order_id == od1:
                if direction == 'after':
                    new_order_d[tid1] = od1
                    new_order.append((od1, tid1))
                    tod1 = od1+1
                    for tid in tid2:
                        new_order_d[tid] = tod1
                        new_order.append((tod1, tid))
                        tod1 += 1
                else:
                    tod1 = od1
                    for tid in tid2:
                        new_order_d[tid] = tod1
                        new_order.append((tod1, tid))
                        tod1 += 1
                    new_order_d[tid1] = tod1
                    new_order.append((tod1, tid1))
        if 'HGHGROUP' in ijson.get('grpid'):
            new_order   = map(lambda x: x+(ijson['grpid'], ), new_order)
        else:
            new_order   = map(lambda x: x+(table_type, ), new_order)
        cur.executemany("update mt_data_builder set order_id=? where taxo_id=? and table_type=?", new_order)
        conn.commit()
        conn.close()
        res = [{'message':'done', 'order_d':new_order_d, 'tid1':tid1, 'tid2':tid2}]
        return res

    def update_order(self, ijson):
        if ijson.get('t_ids', []):
            return self.update_order_multiple(ijson)
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type      = ijson['table_type']
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        sql             = "select row_id, taxo_id, order_id from mt_data_builder where table_type='%s' and isvisible='Y' group by taxo_id"%(table_type)
        cur.execute(sql)
        res             = cur.fetchall()
        order_d         = {}
        taxo_o_d        = {}
        for rr in res:
            row_id, taxo_id, order_id   = rr
            taxo_o_d[taxo_id]           = order_id
            order_d.setdefault(order_id, {})[taxo_id]   = min(row_id, order_d.get(order_id, {}).get(taxo_id, row_id))
        order_ids   = order_d.keys()
        order_ids.sort()
        tid1    = int(ijson['t_id1'])
        tid2    = int(ijson['t_id2'])
        direction = ijson['pos']
        od1     = taxo_o_d[tid1]
        new_order   = []
        new_order_d = {}
        for order_id in order_ids:
            if order_id < od1:
                taxo_ids    = order_d[order_id].keys()
                for taxo_id in taxo_ids:
                    if taxo_id == tid2:continue
                    new_order_d[taxo_id] = order_id
                    new_order.append((order_id, taxo_id))
            elif order_id > od1:
                taxo_ids    = order_d[order_id].keys()
                order_id    += 2
                for taxo_id in taxo_ids:
                    if taxo_id == tid2:continue
                    new_order_d[taxo_id] = order_id
                    new_order.append((order_id, taxo_id))
            elif order_id == od1:
                if direction == 'after':
                    new_order_d[tid1] = od1
                    new_order_d[tid2] = od1+1
                    new_order.append((od1, tid1))
                    new_order.append((od1+1, tid2))
                else:
                    new_order_d[tid2] = od1
                    new_order_d[tid1] = od1+1
                    new_order.append((od1, tid2))
                    new_order.append((od1+1, tid1))
        new_order   = map(lambda x: x+(table_type, ), new_order)
        cur.executemany("update mt_data_builder set order_id=? where taxo_id=? and table_type=?", new_order)
        conn.commit()
        conn.close()
        res = [{'message':'done', 'order_d':new_order_d, 'tid1':tid1, 'tid2':tid2}]
        return res

    def merge_table_to_row(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type      = ijson['table_type']
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        t_ids            = map(lambda x:int(x), ijson['t_ids'])
        t_id_d          = {}
        fnd_taxo        = {}
        for t in t_ids:
            t_id_d[t]   = 1
            fnd_taxo[t]  = 1
        table_ids       = map(lambda x:x.split('-')[1], ijson['st_ids'])
        tmp_tble_d      = {}
        for t in table_ids:
            tmp_tble_d[t]   = 1
        
        sql             = "select row_id, taxo_id, order_id, table_id, xml_id from mt_data_builder where table_type='%s' and isvisible='Y' and taxo_id in (%s)"%(table_type, ', '.join(map(lambda x:str(x), ijson['t_ids'])))
        cur.execute(sql)
        res             = cur.fetchall()
        order_d         = {}
        taxo_o_d        = {}
        new_row_id      = {}
        table_taxo_d    = {}
        f_ph            = {}
        for rr in res:
            row_id, taxo_id, order_id, table_id, xml_id         = rr
            taxo_o_d[taxo_id]                           = order_id
            if int(taxo_id) in t_id_d and str(table_id) in tmp_tble_d:
                f_ph.setdefault(str(taxo_id), {})[(taxo_id, xml_id, row_id)]    = 1
                table_taxo_d.setdefault(str(table_id), {})[taxo_id] = 1
                new_row_id[str(row_id)]      = 1
                if int(taxo_id) in fnd_taxo:
                    del fnd_taxo[int(taxo_id)]
            order_d.setdefault(order_id, {})[taxo_id]   = min(row_id, order_d.get(order_id, {}).get(taxo_id, row_id))
        if ijson.get('taxo_flg', '') == 1:
            lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
            env         = lmdb.open(lmdb_path1, readonly=True)
            txn_m       = env.begin()
            lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
            env         = lmdb.open(lmdb_path1, readonly=True)
            txn         = env.begin()
            res = self.check_ph_overlap(f_ph, txn_m, txn)
            if res:
                return res
        error   = {}
        if ijson.get('taxo_flg', '') != 1:
            for taxo_id, cnt in table_taxo_d.items():
                if len(cnt.keys()) > 1:
                    error.update(cnt)
        if error:
            res = [{'message':'Table Overlap '+json.dumps(error.keys()), 'data':[], 'taxo_d':[]}]
            return res
        rem_taxos   = fnd_taxo.keys()
        if len(rem_taxos) != 1:
            res = [{'message':'Error Nothing to merge'}]
            return res
        if ijson.get('target_taxo', ''):
            sql     = "update mt_data_builder set user_taxonomy='%s', taxo_id=%s where row_id in (%s)"%(ijson.get('target_taxo', ''), rem_taxos[0], ', '.join(new_row_id.keys()))
        elif ijson.get("taxo_flg", '') == 1:
            res = [{"message":"Error Not allowed without target taxo"}]
            conn.close()
            return res
        else:
            sql     = "update mt_data_builder set taxo_id=%s where row_id in (%s)"%(rem_taxos[0], ', '.join(new_row_id.keys()))
        cur.execute(sql)
        conn.commit()
        res = [{'message':'done', 'sd':t_id_d, 'd':rem_taxos[0], 'table_ids':tmp_tble_d}]
        return res

    def split_row_multi(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type      = ijson['table_type']

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        sql             = "select row_id, taxo_id, order_id, table_id, xml_id from mt_data_builder where table_type='%s' and isvisible='Y'"%(table_type)
        cur.execute(sql)
        res             = cur.fetchall()
        order_d         = {}
        taxo_o_d        = {}
        new_row_id      = {}
        table_taxo_d    = {}
        for rr in res:
            row_id, taxo_id, order_id, table_id, xml_id         = rr
            taxo_o_d[taxo_id]                           = order_id
            tk   = self.get_quid(table_id+'_'+xml_id)
            c_id        = txn_m.get('XMLID_MAP_'+tk)
            if not c_id:continue
            #if c_id.split('_')[1]   != '5':continue
            gv_xml  = c_id
            new_row_id[str(row_id)]      = 1
            table_taxo_d.setdefault(taxo_id, {}).setdefault(table_id, {}).setdefault(int(c_id.split('_')[1]), {})[str(row_id)]   = 1
        with conn:
            #sql = "select max(taxo_id) from kpi_input"
            sql = "select seq from sqlite_sequence WHERE name = 'mt_data_builder'"
            cur.execute(sql)
            r       = cur.fetchone()
            g_id    = int(r[0])+1
            sql     = "select max(taxo_id) from mt_data_builder" 
            cur.execute(sql)
            r       = cur.fetchone()
            tg_id    = int(r[0])+1
            g_id    = max(g_id, tg_id)
            for taxo_id, table_d in table_taxo_d.items():
                for table_id, row_d in table_d.items():
                    if len(row_d.keys()) <= 1:continue
                    rows    = row_d.keys()
                    rows.sort() 
                    for r in rows[1:]:
                        sql     = "insert into mt_data_builder(taxo_id)values(-1)"
                        #cur.execute(sql)
                        #conn.commit()
                        sql     = "update mt_data_builder set taxo_id=%s where row_id in (%s)"%(g_id, ', '.join(row_d[r].keys()))
                        print [table_id, sql]
                        #cur.execute(sql)
                        #conn.commit()
                        g_id    += 1

    def split_taxo_cell(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type      = ijson['table_type']

        lmdb_path2      =  "/mnt/eMB_db/%s/%s/table_phcsv_data"%(company_name, model_number)
        env1             = lmdb.open(lmdb_path2, readonly=True)
        txn_trip             = env1.begin()

        lmdb_path2      =  "/mnt/eMB_db/%s/%s/default_table_phcsv_data"%(company_name, model_number)
        env1             = lmdb.open(lmdb_path2, readonly=True)
        txn_trip_default             = env1.begin()

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        t_ids            = ijson['t_id']
        sql             = "select distinct(user_taxonomy) from mt_data_builder where table_type='%s'"%(table_type)
        cur.execute(sql)
        res             = cur.fetchall()
        e_taxo  = {}
        for rr in res:
            e_taxo[rr[0]]  = 1
            
        
        sql             = "select row_id, taxo_id, order_id, table_id, xml_id from mt_data_builder where table_type='%s' and isvisible='Y' and row_id in (%s) and taxo_id in (%s)"%(table_type, ', '.join(map(lambda x:str(x), ijson['r_ids'])), t_ids)
        cur.execute(sql)
        res             = cur.fetchall()
        taxo_o_d        = {}
        new_row_id      = {}
        u_taxo          = {}
        for rr in res:
            row_id, taxo_id, order_id, table_id, xml_id = rr
            table_id    = str(table_id)
            xml_id      = str(xml_id)
            tk   = self.get_quid(table_id+'_'+xml_id)
            c_id        = txn_m.get('XMLID_MAP_'+tk)
            if not c_id:continue
            taxo_o_d[taxo_id]                           = order_id
            new_row_id[str(row_id)]                     = 1
            u_taxo[self.gen_taxonomy(binascii.a2b_hex(txn_m.get('TEXT_'+c_id)))] = 1
        if u_taxo:
            u_taxo  = u_taxo.keys()
            u_taxo.sort(key=lambda x:len(x))
            u_taxo  = u_taxo[-1]
            if u_taxo in e_taxo :
                u_taxo  += '-N-1'
        else:
            u_taxo  = ''
        with conn:
            #sql = "select max(taxo_id) from kpi_input"
            sql = "select seq from sqlite_sequence WHERE name = 'mt_data_builder'"
            cur.execute(sql)
            r       = cur.fetchone()
            g_id    = int(r[0])+1
            sql     = "select max(taxo_id) from mt_data_builder" 
            cur.execute(sql)
            r       = cur.fetchone()
            tg_id    = int(r[0])+1
            g_id    = max(g_id, tg_id)
            r       = cur.fetchone()
            sql     = "insert into mt_data_builder(taxo_id)values(-1)"
            cur.execute(sql)
            conn.commit()
            sql     = "update mt_data_builder set taxo_id=%s, user_taxonomy='%s' where row_id in (%s)"%(g_id, u_taxo, ', '.join(new_row_id.keys()))
            cur.execute(sql)
            conn.commit()
        conn.close()
        res = [{'message':'done'}]
        return res


    def split_row(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type      = ijson['table_type']

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        t_ids            = map(lambda x:int(x), ijson['t_ids'])
        t_id_d          = {}
        for t in t_ids:
            t_id_d[t]   = 1
        table_ids       = map(lambda x:x.split('-')[1], ijson['st_ids'])
        tmp_tble_d      = {}
        for t in table_ids:
            tmp_tble_d[t]   = 1
        
        sql             = "select row_id, taxo_id, order_id, table_id, xml_id from mt_data_builder where table_type='%s' and isvisible='Y' and taxo_id in (%s)"%(table_type, ', '.join(map(lambda x:str(x), ijson['t_ids'])))
        cur.execute(sql)
        res             = cur.fetchall()
        order_d         = {}
        taxo_o_d        = {}
        new_row_id      = {}
        for rr in res:
            row_id, taxo_id, order_id, table_id, xml_id         = rr
            taxo_o_d[taxo_id]                           = order_id
            if int(taxo_id) in t_id_d and str(table_id) in tmp_tble_d:# and xml_id in {'x2868_4#x1000428_4':1, 'x2872_4':1}:
                tk   = self.get_quid(table_id+'_'+xml_id)
                c_id        = txn_m.get('XMLID_MAP_'+tk)
                if not c_id:continue
                #if c_id.split('_')[1]   != '5':continue
                gv_xml  = c_id
                new_row_id[str(row_id)]      = 1
            order_d.setdefault(order_id, {})[taxo_id]   = min(row_id, order_d.get(order_id, {}).get(taxo_id, row_id))
        with conn:
            #sql = "select max(taxo_id) from kpi_input"
            sql = "select seq from sqlite_sequence WHERE name = 'mt_data_builder'"
            cur.execute(sql)
            r       = cur.fetchone()
            g_id    = int(r[0])+1
            sql     = "select max(taxo_id) from mt_data_builder" 
            cur.execute(sql)
            r       = cur.fetchone()
            tg_id    = int(r[0])+1
            g_id    = max(g_id, tg_id)
            r       = cur.fetchone()
            sql     = "insert into mt_data_builder(taxo_id)values(-1)"
            cur.execute(sql)
            conn.commit()
            sql     = "update mt_data_builder set taxo_id=%s where row_id in (%s)"%(g_id, ', '.join(new_row_id.keys()))
            cur.execute(sql)
            conn.commit()
        order_ids   = order_d.keys()
        order_ids.sort()
        tid1    = t_ids[0]
        tid2    = g_id
        od1     = taxo_o_d[tid1]
        new_order   = []
        new_order_d = {}
        for order_id in order_ids:
            if order_id < od1:
                taxo_ids    = order_d[order_id].keys()
                for taxo_id in taxo_ids:
                    if taxo_id == tid2:continue
                    new_order_d[taxo_id] = order_id
                    new_order.append((order_id, taxo_id))
            elif order_id > od1:
                taxo_ids    = order_d[order_id].keys()
                order_id    += 2
                for taxo_id in taxo_ids:
                    if taxo_id == tid2:continue
                    new_order_d[taxo_id] = order_id
                    new_order.append((order_id, taxo_id))
            elif order_id == od1:
                new_order_d[tid1] = od1
                new_order_d[tid2] = od1+1
                new_order.append((od1, tid1))
                new_order.append((od1+1, tid2))
        new_order   = map(lambda x: x+(table_type, ), new_order)
        cur.executemany("update mt_data_builder set order_id=? where taxo_id=? and table_type=?", new_order)
        conn.commit()
        conn.close()
        res = [{'message':'done', 'order_d':new_order_d, 'sd':t_id_d, 's':tid1, 'd':tid2, 'table_ids':tmp_tble_d}]
        return res
    
    def read_main_tables(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        m_tables, rev_m_tables, doc_m_d, table_type_m = self.get_main_table_info(company_name, model_number)
        map_dic = {
                    'IS'    :0,
                    'BS'    :1,
                    'CF'    :2,
                    'RBS'    :3,
                    'RBG'    :4,
                    }
        
        ks = rev_m_tables.keys()
        ks.sort(key=lambda x:map_dic.get(x, 9999))
        list_mt = []
        for k in ks:
            dic = {'l': k, 'k': k, 'f': False}
            list_mt.append(dic)
        res = [{'message': 'done', 'data': list_mt}]
        return res

    def sheet_id_map(self):
        db_file     = '/mnt/eMB_db/node_mapping.db'
        conn, cur   = conn_obj.sqlite_connection(db_file)
        sql   = "select main_header, sub_header, sheet_id, node_name, description from node_mapping where review_flg = 0"
        cur.execute(sql)
        tres        = cur.fetchall()
        #print rr, len(tres)
        ddict = dd(set)
        for tr in tres:
            #print tr
            main_header, sub_header, sheet_id, node_name, description = map(str, tr)
            ddict[sheet_id] = [main_header, sub_header, node_name, description]
        return ddict

    def update_label_change(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type      = ijson['table_type']
        ijson['taxo_flg']   = 1
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        t_id            = ijson['t_id']
        u_ar            = []
        for rr in ijson['data']:
            for table_id, xmld in rr['id'].items():
                for xml in xmld.keys():
                    u_ar.append((rr['sts'], table_id, xml, t_id))
        cur.executemany("update mt_data_builder set vgh_group=? where table_id=? and xml_id=? and taxo_id=?", u_ar)
        conn.commit()
        conn.close()
        res = [{'message':'done'}]
        return res

    def update_cid_xml_id(self, ijson):
        #sys.exit()
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        db_file         = self.get_db_path(ijson)
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        conn, cur       = conn_obj.sqlite_connection(db_file)
        try:
            sql = "alter table mt_data_builder add column xml_id TEXT"
            cur.execute(sql)
        except:pass
        sql = "select row_id, c_id from mt_data_builder"
        cur.execute(sql)
        res = cur.fetchall()
        u_ar    = []
        for rr in res:
            row_id, c_id    = rr
            if c_id:
                x   = txn_m.get("XMLID_"+str(c_id))
                if x:
                    u_ar.append((x, row_id))
        cur.executemany("update mt_data_builder set xml_id=? where row_id=?", u_ar)
        conn.commit()
        conn.close()
        pass

    def update_ph(self, ijson):
        #sys.exit()
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        db_file         = self.get_db_path(ijson)
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        conn, cur       = conn_obj.sqlite_connection(db_file)
        try:
            sql = "alter table mt_data_builder add column xml_id TEXT"
            cur.execute(sql)
        except:pass
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn         = env.begin()
        if ijson.get('table_ids', []):
            sql = "select row_id, c_id, xml_id, table_id, ph from mt_data_builder where table_id in (%s)"%(', '.join(map(lambda x:str(x), ijson.get('table_ids', []))))
        elif ijson.get('table_types', []):
            sql = "select row_id, c_id, xml_id, table_id, ph from mt_data_builder where table_type in (%s)"%(', '.join(map(lambda x:'"'+str(x)+'"', ijson.get('table_types', []))))
        else:
            sql = "select row_id, c_id, xml_id, table_id, ph from mt_data_builder"
        cur.execute(sql)
        res = cur.fetchall()
        u_ar    = []
        for rr in res:
            row_id, c_id, xml_id, table_id, ph    = rr
            if table_id and xml_id:
                table_id = str(table_id)
                key     = table_id+'_'+self.get_quid(xml_id)
                ph_map  = txn.get('PH_MAP_'+str(key))
                #print c_id, ph_map
                if ph_map:
                    period_type, period, currency, scale, value_type    = ph_map.split('^')
                    if 1:#ph != period_type+period:
                        u_ar.append((period_type+period, row_id))
        #for rr in u_ar:
        #    print rr
        #sys.exit()
        print 'Total ', len(u_ar)
        cur.executemany("update mt_data_builder set ph=? where row_id=?", u_ar)
        conn.commit()
        conn.close()
        pass

    def add_missing_c_id(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        m_tables, rev_m_tables, doc_m_d, table_type_m = self.get_main_table_info(company_name, model_number)
        db_file         = self.get_db_path(ijson)
        print db_file
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn         = env.begin()

        conn, cur       = conn_obj.sqlite_connection(db_file)
        vgh_text_d      = {}
        sql             = "select table_type, vgh, vgh_id from vgh_info"
        cur.execute(sql)
        res = cur.fetchall()
        for rr in res:
            table_type, vgh, vgh_id = rr
            vgh_text_d.setdefault(table_type, {})[vgh.lower()] = (vgh_id, vgh)
        sql             = "select table_type, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, vgh_text, vgh_group, doc_id, xml_id,m_rows from mt_data_builder where isvisible='Y' and table_type='%s'"%(ijson['table_type'])
        cur.execute(sql)
        res             = cur.fetchall()
        table_row_d     = {}
        table_ds        = {}
        t_col_vgh       = {}
        t_row_ph        = {}
        xml_row_d       = {}
        taxo_d          = {}
        table_row_overlap_d     = {}
        exists_table    = {}
        for rr in res:
            table_type, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, vgh_text, vgh_group, doc_id, xml_id, m_rows = rr
            if xml_id:
                tk   = self.get_quid(table_id+'_'+xml_id)
                c_id    = txn_m.get('XMLID_MAP_'+tk)
                if not c_id:continue
                if ijson.get('table_ids', '') and int(table_id) not in ijson.get('table_ids', []):continue
                r       = int(c_id.split('_')[1])
                xml_row_d.setdefault((table_id, xml_id), {}).setdefault((r, taxo_id), {})[xml_id]  = 1
                table_row_d.setdefault((table_id, r), {}).setdefault(xml_id, {})[taxo_id]    = rr
                table_ds[table_id]        = (table_type, doc_id)
                t_col_vgh[(table_id, int(c_id.split('_')[2]))]  = vgh_text
                t_row_ph.setdefault((table_id, r), {})
                t_row_ph[(table_id, r)][(table_id, ph, '')]  = t_row_ph[(table_id, r)].get((table_id, ph, ''), 0)+1
                taxo_d.setdefault(taxo_id, {'l_change':{},'order_id':order_id, 'u_label':user_taxonomy, 't_l':taxonomy, 'missing':missing_taxo, 'ks':[], 'm_rows':m_rows})
                exists_table[str(table_id)] = 1
                t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                clean_value = t
                try:
                    clean_value = numbercleanup_obj.get_value_cleanup(t)
                except:
                    clean_value = ''
                    pass
                table_row_overlap_d.setdefault(taxo_id, {'l_change':{},'order_id':order_id, 'u_label':user_taxonomy, 't_l':taxonomy, 'missing':missing_taxo, 'ks':[], 'm_rows':m_rows})
                key     = table_id+'_'+self.get_quid(xml_id)
                ph_map  = txn.get('PH_MAP_'+str(key))
                if ph_map and clean_value:
                        period_type, period, currency, scale, value_type    = ph_map.split('^')
                        table_row_overlap_d.setdefault((table_type, period_type+period, clean_value, value_type), {})[taxo_id]  = 1

        lmdb_path2      =  "/mnt/eMB_db/%s/%s/table_phcsv_data"%(company_name, model_number)
        env1             = lmdb.open(lmdb_path2, readonly=True)
        txn_trip             = env1.begin()

        lmdb_path2      =  "/mnt/eMB_db/%s/%s/default_table_phcsv_data"%(company_name, model_number)
        env1             = lmdb.open(lmdb_path2, readonly=True)
        txn_trip_default             = env1.begin()
        i_ar                = []
        new_vgh_text_d      = {}
        vgh_triplet_data    = {}
        dtime               = str(datetime.datetime.now()).split('.')[0]
        f_taxo_d            = {}
        table_type_d        = {}
        for table_id, (table_type, doc_id) in table_ds.items():
            if ijson.get('table_types', '') and table_type not in ijson.get('table_types', []) :continue
            table_id    = str(table_id)
            if ijson.get('table_ids', '') and int(table_id) not in ijson.get('table_ids', []):continue
            k   = 'GV_'+str(table_id)
            ids = txn_m.get(k)
            if not ids:continue
            ids         = ids.split('#')
            rd          = {}
            for c_id in ids:
                r       = int(c_id.split('_')[1])
                c       = int(c_id.split('_')[2])
                x       = txn_m.get('XMLID_'+c_id)
                t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                if not t:continue
                key     = table_id+'_'+self.get_quid(x)
                ph_map  = txn.get('PH_MAP_'+str(key))
                if ph_map:
                        period_type, period, currency, scale, value_type    = ph_map.split('^')
                        if period_type and period:
                            rd.setdefault(r, {})[(c, x)] = (c_id, period_type, period, currency, scale, value_type)
            rows    = rd.keys()
            rows.sort()
            new_rows    = {}
            for r in rows:
                col_xmls    = rd[r].keys()
                col_xmls.sort(key=lambda x:x[0])
                ph_ind      = t_row_ph.get((table_id, r), {})
                old_rows    = {}
                #print '\n=================================='
                for c, xml in col_xmls:
                    if(table_id, xml) in xml_row_d:
                        #print ((table_id, xml))
                        old_rows.update(xml_row_d[(table_id, xml)])
                #print 'r ',r, old_rows
                if not old_rows:
                    new_rows[r] = 1
                if len(old_rows.keys()) != 1:
                    continue
                o_r, taxo_id = old_rows.keys()[0]
                #print taxo_d[taxo_id]
                o_d = table_row_d[(table_id, o_r)]
                nrows   = {}
                for c, xml in col_xmls:
                    #print c, xml, xml in o_d
                    if xml in o_d:continue
                    c_id, period_type, period, currency, scale, value_type    = rd[r][(c, xml)]
                    ph  = period_type+period
                    tlabel  = ''
                    if (table_id, ph, tlabel) in ph_ind:
                        cind    = ph_ind[(table_id, ph, tlabel)]
                        ph_ind[(table_id, ph, tlabel)]    = cind+1
                        tlabel  = tlabel+'-'+str(cind) if tlabel else str(cind)
                    else:
                        ph_ind[(table_id, ph, tlabel)]    = 1
                    t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                    nrows[(c_id, xml, ph, tlabel)]   = t
                if nrows:
                    print '\n==========================================='
                    print table_id, table_type
                    for k, v in nrows.items():
                        c_id, xml, ph, tlabel   = k
                        print '\tNEW CELL',(table_type, doc_id), taxo_id, k, v
                        x       = xml
                        triplet =  self.read_triplet(table_id, txn_trip, xml, txn_trip_default)
                        vgh_triplet_data[(table_id, xml)] = triplet.get('VGH', {}).keys()
                        vgh_txt = vgh_triplet_data.get((table_id, xml), []) 
                        if not vgh_txt:
                            vgh_txt = [[txn_m.get('COLTEXT_'+table_id+'_'+str(c_id.split('_')[2]))]]
                        vgh_txt = ' '.join(vgh_txt[0])
                        gcom    = ''
                        ngcom   = ''
                        comp    = ''
                        try:
                            tlabel    = tlabel.decode('utf-8')
                        except:pass
                        try:
                            vgh_txt    = vgh_txt.decode('utf-8')
                        except:pass
                        if vgh_txt.lower() in vgh_text_d.get(table_type, {}):
                            vgh_id  = vgh_text_d[table_type][vgh_txt.lower()][0]
                        else:
                            vgh_id  = len(vgh_text_d.get(table_type, {}).keys())+1
                            vgh_text_d.setdefault(table_type, {})[vgh_txt.lower()] = (vgh_id, vgh_txt)
                            new_vgh_text_d.setdefault(table_type, {})[vgh_txt.lower()] = (vgh_id, vgh_txt)
                        #taxo_d.setdefault(taxo_id, {'l_change':{},'order_id':order_id, 'rid':row_id, 'u_label':user_taxonomy, 't_l':taxonomy, 'missing':missing_taxo, 'comp':comp, 'ks':[], 'm_rows':m_rows})
                        i_ar.append((taxo_id, taxo_d[taxo_id]['order_id'], table_type, taxo_d[taxo_id]['t_l'], taxo_d[taxo_id]['u_label'], taxo_d[taxo_id]['missing'], table_id, c_id, ph, tlabel, gcom, ngcom, 'Y', vgh_id, doc_id, -1, x, 'SYSTEM', dtime))
            rows    = new_rows.keys()
            rows.sort()
            for r in rows:
                table_type_d.setdefault(table_type, {}).setdefault(table_id, {})[r] = 1
                continue
                col_xmls    = rd[r].keys()
                col_xmls.sort(key=lambda x:x[0])
                ph_ind      = {}
                o_d         = {}
                nrows   = {}
                for c, xml in col_xmls:
                    if xml in o_d:continue
                    c_id, period_type, period, currency, scale, value_type    = rd[r][(c, xml)]
                    ph  = period_type+period
                    tlabel  = ''
                    if (table_id, ph, tlabel) in ph_ind:
                        cind    = ph_ind[(table_id, ph, tlabel)]
                        ph_ind[(table_id, ph, tlabel)]    = cind+1
                        tlabel  = tlabel+'-'+str(cind) if tlabel else str(cind)
                    else:
                        ph_ind[(table_id, ph, tlabel)]    = 1
                    t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                    nrows[(table_id, table_type, doc_id, c_id, xml, ph, tlabel)]   = t
                if nrows:
                    print '\n==========================================='
                    ks  = {}
                    dd  = {}
                    for k, v in nrows.items():
                        table_id, table_type, doc_id, c_id, xml, ph, tlabel = k
                        ks[(table_id, c_id, ph, tlabel, doc_id)]  = 1
                        triplet =  self.read_triplet(table_id, txn_trip, xml, txn_trip_default)
                        vgh_triplet_data[(table_id, xml)] = triplet.get('VGH', {}).keys()
                        print '\tNEW ROW ',k, v
                        key         = table_id+'_'+self.get_quid(xml)
                        for rc in ['COL', 'ROW']:
                            ngcom        = txn.get('COM_NG_'+rc+'MAP_'+str(key))
                            if ngcom:
                                for eq in ngcom.split('|'):
                                    formula, eq_str = eq.split(':$$:')
                                    dd.setdefault('ngcom', {})[c_id] = formula
                    
                            gcom        = txn.get('COM_G_'+rc+'MAP_'+key)
                            if gcom:
                                for eq in gcom.split('|'):
                                    formula, eq_str = eq.split(':$$:')
                                    dd.setdefault('gcom', {})[c_id] = formula
                    taxo    = 'TAS_TAXO_'+str(len(f_taxo_d.get(table_type, [])))
                    dd['t_l']   = [taxo]
                    dd['ks']    = ks.keys() 
                    f_taxo_d.setdefault(table_type, []).append(dd)
                    
        if table_type_d:
            #conn, cur       = conn_obj.sqlite_connection(db_file)
            #sql = "delete from mt_data_builder where table_id in (%s)"%(', '.join(map(lambda x:str(x), table_ids)))
            #print sql
            #cur.execute(sql)
            dtime   = str(datetime.datetime.now()).split('.')[0]
            #i_ar    = []
            #sql = "select max(taxo_id) from kpi_input"
            sql = "select seq from sqlite_sequence WHERE name = 'mt_data_builder'"
            cur.execute(sql)
            r       = cur.fetchone()
            g_id    = int(r[0])+1
            sql     = "select max(taxo_id) from mt_data_builder" 
            cur.execute(sql)
            r       = cur.fetchone()
            tg_id    = int(r[0])+1
            g_id    = max(g_id, tg_id)
            self.form_new_row(ijson, g_id, table_type_d, new_vgh_text_d, vgh_text_d, vgh_triplet_data, i_ar, doc_m_d, table_row_overlap_d)
        vgh_groups  = []
        for k, v in new_vgh_text_d.items():
            for t, vtup in v.items():
                vgh_groups.append((k, vtup[1], vtup[0]))
        print vgh_groups
        for rr in i_ar:
            print rr
        print 'Total ', len(i_ar)
        #cur.executemany("insert into mt_data_builder(taxo_id, order_id, table_type, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label, gcom, ngcom, isvisible, vgh_text, doc_id, prev_id, xml_id, user_name, datetime)values(?,?, ?, ?, ?, ?, ?,?, ?, ?, ?, ?, ?,?, ?, ?, ?, ?, ?)", i_ar)

        #cur.executemany("insert into vgh_info(table_type, vgh, vgh_id)values(?,?, ?)", vgh_groups)
        #conn.commit()
        conn.close()
        pass


    def add_new_table(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        vgh_text_d      = {}
        sql             = "select table_type, vgh, vgh_id from vgh_info"
        cur.execute(sql)
        res = cur.fetchall()
        for rr in res:
            table_type, vgh, vgh_id = rr
            vgh_text_d.setdefault(table_type, {})[vgh.lower()] = (vgh_id, vgh)

            
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn         = env.begin()
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        sql             = "select table_type, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, vgh_text, vgh_group, doc_id, xml_id,m_rows from mt_data_builder where isvisible ='Y'"
        cur.execute(sql)
        res             = cur.fetchall()
        table_row_d     = {}
        exists_table    = {}
        for rr in res:
            table_type, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, vgh_text, vgh_group, doc_id, xml_id, m_rows = rr
            if xml_id:
                tk   = self.get_quid(table_id+'_'+xml_id)
                c_id    = txn_m.get('XMLID_MAP_'+tk)
                if not c_id:continue
                r       = int(c_id.split('_')[1])
                exists_table[str(table_id)] = 1
                t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                clean_value = t
                try:
                    clean_value = numbercleanup_obj.get_value_cleanup(t)
                except:
                    clean_value = ''
                    pass
                key     = table_id+'_'+self.get_quid(xml_id)
                ph_map  = txn.get('PH_MAP_'+str(key))
                table_row_d.setdefault(taxo_id, {'l_change':{},'order_id':order_id, 'u_label':user_taxonomy, 't_l':taxonomy, 'missing':missing_taxo, 'ks':[], 'm_rows':m_rows})
                if ph_map and clean_value:
                        period_type, period, currency, scale, value_type    = ph_map.split('^')
                        table_row_d.setdefault((table_type, period_type+period, clean_value, value_type), {})[taxo_id]  = 1
        conn.close()


        lmdb_path2      =  "/mnt/eMB_db/%s/%s/table_phcsv_data"%(company_name, model_number)
        env1             = lmdb.open(lmdb_path2, readonly=True)
        txn_trip             = env1.begin()

        lmdb_path2      =  "/mnt/eMB_db/%s/%s/default_table_phcsv_data"%(company_name, model_number)
        env1             = lmdb.open(lmdb_path2, readonly=True)
        txn_trip_default             = env1.begin()

        table_ids    = ijson['table_ids']
        m_tables, rev_m_tables, doc_m_d, table_type_m = self.get_main_table_info(company_name, model_number)
        #print m_tables
        #print '3136' in m_tables
        vgh_triplet_data                = {}
        f_taxo_d                      = {}
        new_vgh_text_d              = {}
        table_type_d                = {}
        for table_id in table_ids:
            table_id   = str(table_id)
            table_type  = m_tables[table_id]
            table_type_d.setdefault(table_type, {})[table_id]   = {'ALL'}
                    
        if table_type_d:
            conn, cur       = conn_obj.sqlite_connection(db_file)
            i_ar    = []
            #sql = "select max(taxo_id) from kpi_input"
            sql = "select seq from sqlite_sequence WHERE name = 'mt_data_builder'"
            cur.execute(sql)
            r       = cur.fetchone()
            g_id    = int(r[0])+1
            sql     = "select max(taxo_id) from mt_data_builder" 
            cur.execute(sql)
            r       = cur.fetchone()
            tg_id    = int(r[0])+1
            g_id    = max(g_id, tg_id)
            self.form_new_row(ijson, g_id, table_type_d, new_vgh_text_d, vgh_text_d, vgh_triplet_data, i_ar, doc_m_d, table_row_d)
            vgh_groups  = []
            for k, v in new_vgh_text_d.items():
                for t, vtup in v.items():
                    vgh_groups.append((k, vtup[1], vtup[0]))
            print 'Total ', len(i_ar)
            print vgh_groups
            #for rr in i_ar:
            #    print rr
            for table_type, table_ids in  table_type_d.items():
                sql = "delete from mt_data_builder where table_id in (%s) and table_type ='%s'"%(', '.join(map(lambda x:str(x), table_ids)), table_type)
                print sql
                cur.execute(sql)
            cur.executemany("insert into mt_data_builder(taxo_id, order_id, table_type, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label, gcom, ngcom, isvisible, vgh_text, doc_id, prev_id, xml_id, user_name, datetime)values(?,?, ?, ?, ?, ?, ?,?, ?, ?, ?, ?, ?,?, ?, ?, ?, ?, ?)", i_ar)

            cur.executemany("insert into vgh_info(table_type, vgh, vgh_id)values(?,?, ?)", vgh_groups)
            conn.commit()
            conn.close()

    def validate_cell(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type       = str(ijson['table_type'])

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        lmdb_path   = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        env1        = lmdb.open(lmdb_path, readonly=True)
        txn1        = env1.begin()

        db_file         = self.get_db_path(ijson)
        conn, cur   = conn_obj.sqlite_connection(db_file)
        
        sql         = "select taxo_id, table_id, doc_id, xml_id, ph from mt_data_builder where table_type='%s' and isvisible='Y'"%(table_type)
        cur.execute(sql)
        res         = cur.fetchall()
        conn.close()
        invalid_cell_dict = {}
        dmap = {'0':1, '1':1, '2':1, '3':1, '4':1, '5':1, '6':1, '7':1, '8':1, '9':1}
        for row in res:
            row = map(str, row)
            taxo_id, table_id, doc_id, xml_id, ph = row
            tk   = self.get_quid(table_id+ '_' +xml_id)
 
            c_id        = txn_m.get('XMLID_MAP_' + tk)
            if not c_id:continue

            ctable, crow, ccol = c_id.split('_')
            mkey = '%s-%s-'%(ctable, ccol)           

            cell_txt    = self.convert_html_entity(binascii.a2b_hex(txn_m.get('TEXT_'+c_id)))
            numFlg = 0

            final_cell_txt = ''
            for t in cell_txt:
                if t in dmap:
                    final_cell_txt += t

            if final_cell_txt.strip():
                clean_value = cell_txt
                try:
                    clean_value = numbercleanup_obj.get_value_cleanup(cell_txt)
                except:
                    clean_value = ''
                if (not clean_value.strip()):
                    #print [taxo_id, table_id, doc_id, xml_id, ph, final_cell_txt, clean_value]
                    invalid_cell_dict.setdefault(taxo_id, {})[mkey] = 1

            x       = xml_id
            cell_bbox   = self.get_bbox_frm_xml(txn1, table_id, x)
            if not cell_bbox:
                if invalid_cell_dict.get(taxo_id, {}).get(mkey, '') != 1:
                    invalid_cell_dict.setdefault(taxo_id, {})[mkey] = 2
                else:
                    invalid_cell_dict.setdefault(taxo_id, {})[mkey] = 3
        return invalid_cell_dict

    def form_new_row(self, ijson, g_id, table_type_d, new_vgh_text_d, vgh_text_d, vgh_triplet_data, i_ar, doc_m_d, table_row_overlap_d):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn         = env.begin()

        lmdb_path2      =  "/mnt/eMB_db/%s/%s/table_phcsv_data"%(company_name, model_number)
        env1             = lmdb.open(lmdb_path2, readonly=True)
        txn_trip             = env1.begin()

        lmdb_path2      =  "/mnt/eMB_db/%s/%s/default_table_phcsv_data"%(company_name, model_number)
        print lmdb_path2
        env1             = lmdb.open(lmdb_path2, readonly=True)
        txn_trip_default             = env1.begin()
        #for k, v in txn_trip_default.cursor():
        #    print k, v
        
        import create_table_seq
        obj = create_table_seq.TableSeq()
        dtime   = str(datetime.datetime.now()).split('.')[0]
        for table_type, table_d in table_type_d.items():
            print table_type, table_d.keys()
            ijson['table_type'] = table_type
            ijson['ret']        = 'Y'
            #print table_d
            f_taxo_arr  = obj.create_seq_across_mt_taxo(ijson, {}, table_d)
            prev_id     = -1
            n_taxos_cnt     = 0
            for ii, dd in enumerate(f_taxo_arr):
                ks      = dd['ks']
                taxos   = dd['t_l']
                taxo    = ' / '.join(list(sets.Set(taxos))) 
                missing = 'Y'
                try:
                    taxo    = taxo.decode('utf-8')
                except:pass
                #print '\n===================================================='
                #print taxo
                n_taxos    = {}
                tmp_i_ar    = []
                for (table_id, c_id, ph, tlabel) in ks:
                    doc_id  = doc_m_d[table_id]
                    x       = txn_m.get('XMLID_'+c_id)
                    t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                    if 1:#str(deal_id) == '220' and table_id == '1264':
                        triplet =  self.read_triplet(table_id, txn_trip_default, x, txn_trip_default)
                    else:
                        triplet =  self.read_triplet(table_id, txn_trip, x, txn_trip_default)
                    vgh_triplet_data[(table_id, x)] = triplet.get('VGH', {}).keys()
                    vgh_txt = vgh_triplet_data.get((table_id, x), []) 
                    if not vgh_txt:
                        #vgh_txt = [[binascii.a2b_hex(txn_m.get('COLTEXT_'+table_id+'_'+str(c_id.split('_')[2])))]]
                        print 'NO Triplet ', (table_id, x), [vgh_txt]
                        sys.exit()
                    vgh_txt = ' '.join(vgh_txt[0])
                    gcom    = ''
                    ngcom   = ''
                    if c_id in dd.get('gcom', {}):
                        gcom    = 'Y'
                    if c_id in dd.get('ngcom', {}):
                        ngcom    = 'Y'
                    try:
                        tlabel    = tlabel.decode('utf-8')
                    except:pass
                    try:
                        vgh_txt    = vgh_txt.decode('utf-8')
                    except:pass
                    if vgh_txt.lower() in vgh_text_d.get(table_type, {}):
                        vgh_id  = vgh_text_d[table_type][vgh_txt.lower()][0]
                    else:
                        vgh_id  = len(vgh_text_d.get(table_type, {}).keys())+1
                        vgh_text_d.setdefault(table_type, {})[vgh_txt.lower()] = (vgh_id, vgh_txt)
                        new_vgh_text_d.setdefault(table_type, {})[vgh_txt.lower()] = (vgh_id, vgh_txt)
                    #print '\t', [c_id, ph, tlabel, t]    
                    t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                    clean_value = t
                    try:
                        clean_value = numbercleanup_obj.get_value_cleanup(t)
                    except:
                        clean_value = ''
                        pass
                    key     = table_id+'_'+self.get_quid(x)
                    ph_map  = txn.get('PH_MAP_'+str(key))
                    if ph_map and clean_value and len(clean_value.split('.')[0].strip('-')) > 2:
                            period_type, period, currency, scale, value_type    = ph_map.split('^')
                            #print (table_type, period_type+period, clean_value, value_type), table_row_overlap_d.get((table_type, period_type+period, clean_value, value_type), {})
                            n_taxos.update(table_row_overlap_d.get((table_type, period_type+period, clean_value, value_type), {}))
                    tmp_i_ar.append((g_id, g_id, table_type, taxo, '', 'I', table_id, c_id, ph, tlabel, gcom, ngcom, 'Y', vgh_id, doc_id, prev_id, x, 'SYSTEM', dtime))
                if n_taxos:
                    #print 'n_taxos ',len(n_taxos.keys()), n_taxos.keys()
                    if len(n_taxos.keys()) == 1:
                        n_id    = n_taxos.keys()[0]
                        t_ar    = []
                        tmpdd  = table_row_overlap_d[n_id]
                        for trr in tmp_i_ar:
                            ttlis   = list(trr)
                            ttlis[0]    = n_id
                            ttlis[1]    = tmpdd['order_id']
                            ttlis[3]    = tmpdd['t_l']
                            ttlis[4]    = tmpdd['u_label']
                            ttlis[5]    = tmpdd['missing']
                            t_ar.append(tuple(trr))
                        tmp_i_ar    =  t_ar
                    else:
                        n_taxos_cnt += 1
                else:   
                    n_taxos_cnt += 1
                i_ar    +=  tmp_i_ar
                g_id    += 1
                prev_id = ii+1
            print 'RES ', table_type, len(f_taxo_arr), n_taxos_cnt

    def show_triplet(self, table_id, txn1, xml, txn1_default):
        triplet = {}
        gv_tree = txn1.get(self.get_quid(table_id+'^!!^'+xml))
        if not gv_tree:
            gv_tree = txn1_default.get(self.get_quid(table_id+'^!!^'+xml))
        if gv_tree:
            gv_tree = eval(gv_tree)
            triplet = gv_tree['triplet']
        print triplet
        dd  = {}
        for tk, tv in triplet.items():
            print '\n', tk
            for ii, tv1 in enumerate(tv):  
                print 
                for tv2 in tv1:
                    print '\t', tv2
        return dd
        

    
    def read_triplet(self, table_id, txn1, xml, txn1_default):
        triplet = {}
        flip    = txn1.get('FLIP_'+table_id)
        gv_tree = txn1.get(self.get_quid(table_id+'^!!^'+xml))
        if not gv_tree:
            flip    = txn1_default.get('FLIP_'+table_id)
            gv_tree = txn1_default.get(self.get_quid(table_id+'^!!^'+xml))
        if gv_tree:
            gv_tree = eval(gv_tree)
            triplet = gv_tree['triplet']
        dd  = {}
        rev_map = {}
        if flip == 'Y':
            rev_map = {
                        'HGH':'VGH',
                        'VGH':'HGH',
                        }
        for tk, tv in triplet.items():
            if tk not in ['HRP']:continue
            for ii, tv1 in enumerate(tv):  
                for tv2 in tv1:
                    tflg    = ''
                    if tv2:
                        tflg    = tv2[0][2]
                        dd.setdefault(rev_map.get(tflg, tflg), {})[tuple(map(lambda x:' '.join(x[0].replace('\xc2\xa0', '').split()).strip(), tv2))] = 1
        return dd


    def get_order_phs(self, phs, dphs, report_map_d, ijson):
        ucol_m              = self.read_user_selected_col_map_info(ijson)
        if 0:#ijson.get('consider_docs', []):
            tmp_phs = []
            for ii, ph in enumerate(copy.deepcopy(phs)):
                ph['ph']    = ph['n']
                tmp_phs.append(ph)
            return tmp_phs, {}
        doc_ph_d    = {}
        pk_map_d    = {}
        #print 'FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF', phs
        doctype_d  = {}
        for ii, ph in enumerate(phs):
            pk_map_d[ph['k']]    = ph['dph']
            if ijson['table_type'] in self.kpi_types:
                doc_id, table_id = ph['n'], ph['n']
            else:
                doc_id, table_id = ph['g'].split()[0].split('-')
            if ph['n']:
                doctype_d.setdefault(ph['g'], {})[(doc_id, ph['n'], ph['k'])]  = 1
                doc_ph_d.setdefault(doc_id, {}).setdefault(ph['n'], {})[ph['k']]   = 1
        reported_phs    = {}
        done_doc    = {}
        for doctype, key_d in doctype_d.items():
            if 'ConsolidatedHistoricalStatement' in doctype:
                done_doc[doc_id]    = 1
                for k in key_d.keys():
                    doc_id, ph, pk  = k
                    reported_phs.setdefault(ph, {})[pk] = ph
                
        dphs_ar         = report_year_sort.year_sort(dphs.keys())
        #print dphs_ar
        for dph in dphs_ar:
            doc_ids = dphs[dph].keys()
            doc_ids.sort()
            d_ph    = dph[:-4]
            d_year  = dph[-4:]
            #print '\n+++++++++++++++++++++++++++++++++++'
            #print dph, ijson
            for doc_id in doc_ids:
                if doc_id in done_doc:continue
                if doc_id not in doc_ph_d:continue
                doc_phs = doc_ph_d[doc_id]
                #print 'PPPPPPPPPPPPPPPPPPPPPPPP', doc_phs
                #print '\t\t', doc_ph_d
                #print 'DDDDDDDDDDDDDDDDDDDDDDDD', [doc_id]
                doc_phs_ar = report_year_sort.year_sort(doc_phs.keys())
                f   = 0
                for ph in doc_phs_ar:
                    t_ph    = ph[:-4]
                    t_year  = ph[-4:]
                    #print '\t', ph
                    if dph == ph:
                        reported_phs.setdefault(ph, {}).update(doc_phs[ph]) 
                        f   = 1
                        #break
                if 1:#f == 0:
                    for ph in doc_phs_ar:
                        t_ph    = ph[:-4]
                        t_year  = ph[-4:]
                        if d_year == t_year and t_ph in report_map_d:
                            if d_ph in report_map_d[t_ph]:
                                ''' # Invalid Condition  #2020-Jan-14
                                if t_ph == 'H1':
                                    tmpph   = 'Q2'+t_year
                                    if tmpph not in doc_ph_d[doc_id] and (ph not in reported_phs):
                                        reported_phs.setdefault(ph, {}).update(doc_phs[ph]) 
                                elif t_ph == 'H2':
                                    tmpph   = 'Q4'+t_year
                                    if tmpph not in doc_ph_d[doc_id] and (ph not in reported_phs):
                                        reported_phs.setdefault(ph, {}).update(doc_phs[ph]) 
                                else:
                                '''
                                reported_phs.setdefault(ph, {}).update(doc_phs[ph]) 
                                #break
        fphs    = report_year_sort.year_sort(reported_phs.keys())
        phs = []
        for ph in fphs:
            allks   = reported_phs[ph].keys()
            allks.sort(key=lambda x:0 if ph[:-4] == pk_map_d[x][0] else 1)
            u_phs   = filter(lambda x:x in pk_map_d, ucol_m.get(ph, {}).get('phs', []))
            if u_phs and allks[0] not in u_phs:
                for pk in u_phs:
                    if pk in allks:
                        idx = allks.index(pk)
                        felm    = allks[0]
                        allks[0]    = pk
                        allks[idx]  = felm
                        break
            tph = pk_map_d[allks[0]]
            if reported_phs[ph][allks[0]] != 1:
                tph = (ph[:-4], int(ph[-4:]))
            phs.append({'k':allks[0], 'ph':ph, 'dph':tph})
        return phs, reported_phs

    def get_overlap_rows(self, grp):
        table_d     = {}
        table_cd    = {}
        rev_map     = {}
        value_types = grp.keys()
        new_grp     = {}
        value_types.sort(key=lambda x:len(x), reverse=True)
        if len(value_types) == 1:
            return value_types
        #print '\n',value_types
        done_grp    = {}
        for value_type in value_types:
            exist_grp   = {}
            v_set   = sets.Set(value_type)
            for v in done_grp.keys():
                if sets.Set(v).intersection(v_set):
                    exist_grp[v]    = 1
            if exist_grp:
                nv  = ()
                for e in exist_grp:
                    del done_grp[e]
                    nv  += e
                nv  += value_type
                nv  = list(sets.Set(nv))
                nv.sort()
                nv  = tuple(nv)
                done_grp[nv]    = 1
            else:
                done_grp[value_type]    = 1
        return done_grp.keys()

    def validate_pt_value(self, pt, pyear, key_value, f_phs_d, rr, ph_derived):
        def c_f_n(n):
            return float(self.convert_floating_point(n).replace(',', ''))
        v_d     = {
                    'H2'    : [
                                'H2>max(FY,H1)',
                                'H2<c_f_n(Q3+Q4)',
                                ],
                    'Q4'    : [
                                'Q4>max(FY,Q1,Q2,Q3)',
                                'Q4>c_f_n(FY-Q1+Q2+Q3)',
                                ],
                    'H1'    : [
                                'H1>max(FY,H2)',
                                'H1<c_f_n(Q1+Q2)',
                                ],
                    'FY'    : [
                                    'FY<c_f_n(H1+Q3+Q4)',
                                    'FY<c_f_n(H1+H2)',
                                    'FY<c_f_n(Q1+Q2+H2)',
                                    'FY<c_f_n(Q1+Q2+Q3+Q4)'
                                ],
                    'Q2'    : [
                                    'Q2>c_f_n(H1-Q1)',
                                ],
                    'Q1'    : [
                                    'Q1>c_f_n(H1-Q2)',
                                ],
                    'Q4'    : [
                                    'Q4>c_f_n(H2-Q3)',
                                ],
                    'Q3'    : [
                                    'Q3>c_f_n(H2-Q4)',
                                ],
                        
                    }
        for expr in v_d.get(pt, []):
            expr_str, expr, conver_arr    = self.form_expr_str(expr, pyear, key_value, f_phs_d, rr, ph_derived)
            ot          = eval(expr_str)
            if ot == True:
                return ot, ' ('+expr+' == '+expr_str+')' +' -- '+str(conver_arr)
        return False, ''


    def form_expr_str(self, expr, pyear, key_value, f_phs_d, rr, ph_derived):
        def c_f_n(n):
            return float(self.convert_floating_point(n).replace(',', ''))
        opr     = {
                    '=' : 1,
                    '+' : 1,
                    '-' : 1,
                    '*' : 1,
                    '/' : 1,
                    '(' : 1,
                    ')' : 1,
                    '>' : 1,
                    '<' : 1,
                    ',' : 1,
                    ',' : 1,
                    ' ' : 1,
                    }
        pt_d    = {
                    'FY':1,
                    'H1':1,
                    'H2':1,
                    'M9':1,
                    'Q1':1,
                    'Q2':1,
                    'Q3':1,
                    'Q4':1,
                    }
        operands    = []
        tmp_ar      = []
        scal_ind    = {}
        scale_d     = {}
        #print '\n================='
        #print expr
        for c in expr:
            #print '\t',[c, c in opr]
            if c in opr:
                if tmp_ar:
                    t_pt = ''.join(tmp_ar)
                    if t_pt in pt_d:
                        phk = f_phs_d.get(t_pt+pyear, '')
                        v   = rr.get(phk, {}).get('v', '')
                        try:
                            v = numbercleanup_obj.get_value_cleanup(v)
                        except:
                            v = ''
                            pass
                        cv  = v
                    
                        
                        if v == '':
                            v   = '0'
                        else:
                            s   = rr.get(phk, {}).get('phcsv', {}).get('s', '')
                            if s:
                                scale_d[s]  = 1
                                scal_ind[len(operands)] = s
                        if ph_derived == 'Y':
                            if cv == '' and  rr.get(phk, {}).get('v', ''):
                                operands.append('"'+rr.get(phk, {}).get('v', '')+'"')
                            else:
                                operands.append(str(self.convert_floating_point(abs(float(v))).replace(',', '')))
                        else:
                            if cv == '' and  rr.get(phk, {}).get('v', ''):
                                operands.append('"'+rr.get(phk, {}).get('v', '')+'"')
                            else:
                                operands.append(str(self.convert_floating_point(float(v)).replace(',', '')))
                    else:
                        operands.append(t_pt)
                tmp_ar  = []
                operands.append(c)
            elif c:
                tmp_ar.append(c)
        if tmp_ar:
            t_pt = ''.join(tmp_ar)
            if t_pt in pt_d:
                phk = f_phs_d.get(t_pt+pyear, '')
                v   = rr.get(phk, {}).get('v', '')
                try:
                    v = numbercleanup_obj.get_value_cleanup(v)
                except:
                    v = ''
                    pass
                cv  = v
                if v == '':
                    v   = '0'
                else:
                    s   = rr.get(phk, {}).get('phcsv', {}).get('s', '')
                    if s:
                        scale_d[s]  = 1
                        scal_ind[len(operands)] = s
                if ph_derived == 'Y':
                    if cv == '' and  rr.get(phk, {}).get('v', ''):
                        operands.append('"'+rr.get(phk, {}).get('v', '')+'"')
                    else:
                        operands.append(str(self.convert_floating_point(abs(float(v))).replace(',', '')))
                else:
                    if cv == '' and  rr.get(phk, {}).get('v', ''):
                        operands.append('"'+rr.get(phk, {}).get('v', '')+'"')
                    else:
                        operands.append(str(self.convert_floating_point(float(v)).replace(',', '')))
            else:
                operands.append(t_pt)
        conver_arr  = []
        m_scale = ''
        if len(scale_d.keys()) > 1:
            num_obj = {'One': 1, 'Dozen': 12, 'Hundred': 100, 'Thousand': 1000, 'Million': 1000000, 'Billion': 1000000000, 'Trillion': 1000000000000}
            scales  = map(lambda x:(sconvert_obj.num_obj.get(sconvert_obj.scale_map_d.get(x, 0), 0), x), scale_d.keys())
            scales.sort(key=lambda x:x[0])
            m_scale = scales[-1][1]
        if m_scale:
            for ind, s in scal_ind.items():
                if s != m_scale:
                    tv, factor  = sconvert_obj.convert_frm_to_1(s, m_scale, operands[ind])
                    conver_arr.append((operands[ind], tv, sconvert_obj.scale_map_d.get(s, s)+' - '+sconvert_obj.scale_map_d.get(m_scale, m_scale)))
                    operands[ind]   = str(tv)
        tmp_ar      = []
        expr_str    = ''.join(operands)
        return expr_str, expr, conver_arr 


    def read_doc_type_wise_doc_ids_dict(self, company_name, model_number):
        db_path = '/mnt/eMB_db/%s/%s/tas_company.db'%(company_name, model_number)
        conn = sqlite3.connect(db_path)
        cur  = conn.cursor()
        read_qry = """ SELECT document_type, doc_id FROM company_meta_info; """
        cur.execute(read_qry)
        t_data = cur.fetchall() 
        conn.close()    
        res_dct = {}
        for row in t_data:
            document_type, doc_id = map(str, row)    
            res_dct.setdefault(document_type, {})[doc_id]   = 1
            
        return res_dct            


    def create_final_output(self, ijson, DB_DATA={}):
        #print [ ijson.get('ACROSS'), ijson.get('type')]
        if ijson.get('ACROSS') == 'Y' and ijson.get('type') == 'display':
            cinfo           = ijson.get('g_cinfo', {})
            company_name    = ijson['company_name']
            mnumber         = ijson['model_number']
            model_number    = mnumber
            table_type  = ijson['table_type']
            grpid       = ijson.get('grpid', '')
            if not grpid:
                grpid   = ''
            reported    = 'N'
            if ijson.get('reported') == 'Y' or  cinfo.get('reporting_type', '') == 'Reported':
                reported    = 'Y'
            key             = ''.join(table_type)+'_'+''.join(str(grpid).split())+'_'+''.join(ijson.get('project_name', '').split())+'_'+reported
            path            = '/mnt/eMB_db/%s/%s/PREVIEW_OUTPUT/%s/'%(company_name, model_number, key)
            print path
            if os.path.exists(path):
                env1            = lmdb.open(path, readonly=True)
                txn             = env1.begin()
                tdata           = txn.get('res')
                if tdata:
                    res = eval(tdata)
                    if 't_col_grp_d' in res[0] and ijson.get('from_template_gen') != 'Y':
                        del res[0]['t_col_grp_d']
                    return res
        if  ijson.get('type') == 'display':
            return self.create_final_output_doc_types(ijson, DB_DATA)
        cinfo           = ijson.get('g_cinfo', {})
        template_dct = self.read_template_info_dic()
        templates       = template_dct.get((ijson['project_name'], ijson['industry_type']), {})
        m_line_items    = {}
        tmp_ttypes      = {}
        ignore_ttypes   = cinfo.get('ignore_ttypes', {})
        #print templates.keys()
        for tmpid in ignore_ttypes.keys():
            #print 'TEMP ', tmpid, tmpid in  templates
            if tmpid not in templates:continue
            tmp_ttypes.update(ignore_ttypes[tmpid])
            self.read_mapped_ttypes(copy.deepcopy(ijson), tmpid, templates[tmpid], m_line_items)
        #print tmp_ttypes
        #print m_line_items
        #print ignore_ttypes
        #sys.exit()
        ktup    = (ijson['table_type'], ijson.get('grpid', ''))
        if ignore_ttypes and 0:
            if ktup not in tmp_ttypes and ktup not in m_line_items:
                return [{"message":"Nothing to do"}]
        if m_line_items.get(ktup) and ktup not in tmp_ttypes:
            ijson['template_ids']  = m_line_items.get(ktup, {})
        if ktup in tmp_ttypes:
            ijson['non_template_db']  = 'Y'
        #print ijson['t_ids']
        #sys.exit()
        #print '1'
        ex_d            = cinfo.get('ex_d', {})
        deal_id         = str(ijson['deal_id'])
        report_pt       = ''
        report_year     = ''
        rsp_period      = cinfo.get('rsp_period', '')
        rsp_year        = cinfo.get('rsp_year', '')
        if rsp_period:
            report_pt       = rsp_period
        if rsp_year:
            report_year     = int(rsp_year)
        if ijson.get('PRINT') != 'Y':
            disableprint()
        ijson['report_pt']  = report_pt
        ijson['report_year']  = report_year
        es_period      = cinfo.get('es_p', '')
        es_year        = cinfo.get('es_yr', '')
        if es_period:
            es_period       = es_period
        if es_year:
            es_year     = int(es_year)
        ijson['es_p']  = es_period
        ijson['es_yr']  = es_year
        doctypes    = cinfo.get('doc_type_info', [])
        if not doctypes:
            doctypes    = ['']
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        doc_type_doc_lst_map = self.read_doc_type_wise_doc_ids_dict(company_name, model_number)
        res = []
        for doctype_ar in  doctypes:
            if doctype_ar:
                grp_doc_ids = {}
                for doctype in doctype_ar:
                    #print doctype, doc_type_doc_lst_map.get(doctype, {})
                    grp_doc_ids.update(doc_type_doc_lst_map.get(doctype, {}))
                if grp_doc_ids:
                    ijson['grp_doc_ids']  = grp_doc_ids
                    ijson['grp_doc_types']  = doctype_ar
                else:continue
            res = self.create_final_output_doc_types(ijson, DB_DATA)
        return res


    def create_final_output_doc_types(self, ijson, DB_DATA={}):
        print [ ijson.get('ACROSS'),  ijson.get('type')]
        if ijson.get('ACROSS') == 'Y' and ijson.get('type') == 'display':
            cinfo           = ijson.get('g_cinfo', {})
            company_name    = ijson['company_name']
            mnumber         = ijson['model_number']
            model_number    = mnumber
            table_type  = ijson['table_type']
            grpid       = ijson.get('grpid', '')
            if not grpid:
                grpid   = ''
            reported    = 'N'
            if ijson.get('reported') == 'Y' or  cinfo.get('reporting_type', '') == 'Reported':
                reported    = 'Y'
            key             = ''.join(table_type)+'_'+''.join(str(grpid).split())+'_'+''.join(ijson.get('project_name', '').split())+'_'+reported
            path            = '/mnt/eMB_db/%s/%s/PREVIEW_OUTPUT/%s/'%(company_name, model_number, key)
            if os.path.exists(path):
                env1            = lmdb.open(path, readonly=True)
                txn             = env1.begin()
                tdata           = txn.get('res')
                if tdata:
                    res = eval(tdata)
                    return res
        cinfo           = ijson.get('g_cinfo', {})
        ex_d            = cinfo.get('ex_d', {})
        deal_id         = str(ijson['deal_id'])
        report_pt       = ''
        report_year     = ''
        rsp_period      = cinfo.get('rsp_period', '')
        rsp_year        = cinfo.get('rsp_year', '')
        if rsp_period:
            report_pt       = rsp_period
        if rsp_year:
            report_year     = int(rsp_year)
        if ijson.get('PRINT') != 'Y':
            disableprint()
        ijson['report_pt']  = report_pt
        ijson['report_year']  = report_year
        es_period      = cinfo.get('es_p', '')
        es_year        = cinfo.get('es_yr', '')
        if es_period:
            es_period       = es_period
        if es_year:
            es_year     = int(es_year)
        ijson['es_p']  = es_period
        ijson['es_yr']  = es_year
        if ex_d.get((ijson['table_type'], ijson.get('grpid', '')), []) and (not ijson.get('consider_docs')):
            if not ijson.get('t_ids'):
                ijson['t_ids']  = map(lambda x:int(x), ex_d.get((ijson['table_type'], ijson.get('grpid', '')), [])[0])
                ijson['t_ids']  = filter(lambda x:x >=0, ijson['t_ids'])
            ijson['ignore_dphs']  = ex_d.get((ijson['table_type'], ijson.get('grpid', '')), [])[1]
            ijson['offset_year']  = ex_d.get((ijson['table_type'], ijson.get('grpid', '')), [])[3]
            ijson['remove_not_restated']  = ex_d.get((ijson['table_type'], ijson.get('grpid', '')), [])[2]
        ijson_c = copy.deepcopy(ijson)
        if ijson.get('reported', '') == 'Y' or cinfo.get('reporting_type', '') == 'Reported':
            enableprint()
            return self.create_final_output_with_ph(ijson_c, 'P', 'N', DB_DATA)
        if 0:#ijson.get('project_name', '').lower() == 'schroders':
            enableprint()
            return [{'message':'Error Not Valid'}]
            
        if cinfo.get('reporting_type', '') == 'Reported':
            enableprint()
            return [{'message':'Error Not Valid'}]
        ijson_c = copy.deepcopy(ijson)
        res = self.create_final_output_with_ph(ijson_c, 'P', 'Y', DB_DATA)
        if res[0]['message'] != 'done':
            enableprint()
            return res
        if ijson.get('type' ,'') == 'display':
            enableprint()
            #sgrps   = sconvert_obj.scale_factor_map
            #res_lst_na, not_available_scale_lst = self.get_scale_factor_info(sgrps) 
            #res[0]['s_grps']  = res_lst_na
            #res[0]['c_grps']  = not_available_scale_lst
            return res
        if ijson['table_type'] not in self.kpi_types and 0:
            ijson_c = copy.deepcopy(ijson)
            res1 = self.create_final_output_with_ph(ijson_c, 'P-1')
            gen_type    = ijson.get('type','')
        ijson_c = copy.deepcopy(ijson)
        #print '>>>>', ijson_c
        if cinfo.get('reporting_type', '') == 'Both':
            res1 = self.create_final_output_with_ph(ijson_c, 'P', 'N')
        enableprint()
        return res

    def re_gen_all_final_output(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        db_file         = self.get_db_path(ijson)
        conn, cur   = conn_obj.sqlite_connection(db_file)

        sql = "select group_id, table_type, group_txt from vgh_group_info"
        try:
            cur.execute(sql)
            res = cur.fetchall()
        except:
            res = []
        grp_info    = {}
        for rr in res:
            group_id, table_type, group_txt = rr
            group_txt = group_txt.replace('\t', '')
            grp_info[(table_type, str(group_id))]   = group_txt

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        sql         = "select row_id, table_type, gen_type, taxo_ids, group_text, datetime from final_output"
        cur.execute(sql)
        res = cur.fetchall()
        final_ar    = []
        sql         = "select table_type, vgh_id, group_txt, table_str from vgh_group_map"
        cur.execute(sql)
        tres        = cur.fetchall()
        vgh_grp     = {}
        for rr in tres:
            table_type, vgh_id, group_txt, table_str   = rr
            table_type  = str(table_type)
            group_txt   = str(group_txt)
            tc_d    = {}
            if table_str:
                for tcid in table_str.split('#'):
                    tc_d[tcid]  = 1
            vgh_grp.setdefault((table_type, group_txt), {})[vgh_id] = tc_d
        grp_ar_d    = {}
        for rr in res:
            row_id, table_type, gen_type, taxo_ids, group_text, tdatetime  = rr
            if '2019-01-10 ' not in tdatetime:continue
            grp_ar_d.setdefault(table_type, {})[group_text] = 1
        g_d = {}
        for table_type in grp_ar_d.keys():
            ijson['table_type'] = table_type
            g_ar    = self.create_group_ar(ijson, txn_m)
            for rr in g_ar:
                g_d[(table_type, rr['grpid'])] =  rr
        for rr in res:
            row_id, table_type, gen_type, taxo_ids, group_text, tdatetime  = rr
            if '2019-01-10 ' not in tdatetime:continue
            table_type  = str(table_type)
            group_text   = str(group_text)
            ijson_c   = copy.deepcopy(ijson)
            ijson_c['table_type']   = table_type
            ijson_c['type']         = gen_type
            ijson_c['row_id']         = row_id
            ijson_c['print']         = 'Y'
            if taxo_ids:
                ijson_c['t_ids']         = map(lambda x:int(x), taxo_ids.split(','))
            if gen_type == 'group' and  group_text:
                #print (table_type, group_text), (table_type, group_text) not in vgh_grp
                if (table_type, group_text) not in g_d:continue
                ijson_c['data']     = [g_d[(table_type, group_text)]['n']]
                ijson_c['grpid']    = group_text
                ijson_c['vids']     = g_d[(table_type, group_text)]['vids']
            #else:continue
            #print ijson_c
            final_ar.append(copy.deepcopy(ijson_c))
        conn.close()
        #sys.exit()
        txtpath      = '/var/www/html/DB_Model/%s/'%(company_name)
        os.system("mkdir -p "+txtpath)
        btxtpath      = '/var/www/html/DB_Model_backup/%s/'%(company_name)
        os.system("mkdir -p "+btxtpath)
        os.system('cp -r %s %s/%s'%(txtpath, btxtpath, datetime.datetime.now().strftime('%Y-%m-%d-%H_%M_%S')))
        os.system("rm -rf "+txtpath)
        row_error   = []
        for ii, ijson_c in enumerate(final_ar):
            if ijson_c['table_type'] in ['RBG', 'RBS']:continue
            print 'Running ', ii, ' / ', len(final_ar), [ijson_c['table_type'], ijson_c['type'], ijson_c.get('data', []), ijson_c.get('grpid', ''), ijson_c.get('vids', [])]

            tijson_c    = copy.deepcopy(ijson_c)
            res = self.create_final_output_with_ph(tijson_c, 'P')
            if res[0]['message'] != 'done':
                print 'Error ',res[0]['message']
                row_error.append((res[0]['message'], ijson_c['row_id']))
                continue
            tijson_c    = copy.deepcopy(ijson_c)
            res1 = self.create_final_output_with_ph(tijson_c, 'P-1')
        conn, cur   = conn_obj.sqlite_connection(db_file)
        cur.executemany('update final_output set error_txt=? where row_id=?', row_error)
        conn.commit()
        conn.close()
        self.store_bobox_data(ijson)
        import model_view.data_builder_new_excel as PH_LABEL
        obj = PH_LABEL.MRD_excel()
        res = obj.gen_output(ijson) 
        return res

    def read_all_tinfo(self, ijson, ret_flg=None):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        db_file         = self.get_db_path(ijson)
        conn, cur   = conn_obj.sqlite_connection(db_file)

        sql = "select group_id, table_type, group_txt from vgh_group_info"
        try:
            cur.execute(sql)
            res = cur.fetchall()
        except:
            res = []
        grp_info    = {}
        for rr in res:
            group_id, table_type, group_txt = rr
            group_txt = group_txt.replace('\t', '')
            grp_info[(table_type, str(group_id))]   = group_txt

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()
        final_ar    = []
        sql         = "select table_type, vgh_id, group_txt, table_str from vgh_group_map"
        cur.execute(sql)
        tres        = cur.fetchall()
        vgh_grp     = {}
        total_years = 10
        grp_ar_d    = {}
        for rr in tres:
            table_type, vgh_id, group_txt, table_str   = rr
            table_type  = str(table_type)
            group_txt   = str(group_txt)
            tc_d    = {}
            if table_str:
                for tcid in table_str.split('#'):
                    tc_d[tcid]  = 1
            vgh_grp.setdefault((table_type, group_txt), {})[vgh_id] = tc_d
            grp_ar_d.setdefault(table_type, {})[group_txt] = 1
        ar          = []
        deriv_phs   = {}
        cinfo       = {} 
        g_d          = {}
        for table_type in grp_ar_d.keys():
            ijson['table_type'] = table_type
            g_ar    = self.create_group_ar(ijson, txn_m)
            for rr in g_ar:
                g_d[(table_type, rr['grpid'])] =  rr
                g_d.setdefault(table_type, {})[rr['grpid']] =  rr
        sql = "select group_id, table_type, group_txt from vgh_group_info where table_type like 'HGHGROUP%'"
        try:
            cur.execute(sql)
            res = cur.fetchall()
        except:
            res = []
        for rr in res:
            group_id, table_type, group_txt = rr
            group_txt = group_txt.replace('\t', '')
            g_d[(table_type, group_id)] = group_txt
            grp_ar_d.setdefault(table_type, {})[group_id] = 1
        conn.close()
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, [])
        ar  = []
        for table_type in rev_m_tables.keys():
            if table_type.strip() == '':continue
            #print table_type, g_d.get(table_type, {}).keys()
            if table_type in g_d:
                for grpid, grpdata in g_d[table_type].items():
                    #print (1, table_type, 'group', '', grpdata['grpid'], '', [], [])
                    ar.append((1, table_type, 'group', '', grpdata['grpid'], '', [], []))
            else:
                ar.append((1, table_type, 'all', '', '', '', [], []))
        for rr in ar:
            row_id, table_type, gen_type, taxo_ids, group_text, tdatetime, t_ids, ignore_phs  = rr
            table_type  = str(table_type)
            group_text   = str(group_text)
            ijson_c   = copy.deepcopy(ijson)
            ijson_c['table_type']   = table_type
            ijson_c['type']         = gen_type
            ijson_c['t_ids']        = map(lambda x:int(x), t_ids)
            ijson_c['t_ids']          = filter(lambda x:x >=0, ijson_c['t_ids'])
            ijson_c['ignore_dphs']          = ignore_phs
            ijson_c['row_id']       = [table_type, gen_type, group_text]
            #ijson_c['print']       = 'Y'
            if taxo_ids:
                ijson_c['t_ids']         = map(lambda x:int(x), taxo_ids.split(','))
            if gen_type == 'group' and  group_text:
                #print (table_type, group_text), (table_type, group_text) not in vgh_grp
                if (table_type, group_text) not in g_d:continue
                ijson_c['data']     = [g_d[(table_type, group_text)]['n']]
                ijson_c['grpid']    = group_text
                ijson_c['vids']     = g_d[(table_type, group_text)]['vids']
            #else:continue
            #print ijson_c
            final_ar.append(copy.deepcopy(ijson_c))
        return final_ar


    def read_mapped_ttypes(self, ijson, industry_id, row, taxo_cl):
        tmpid, sheet_name, project_name, display_name, industry, time_series = row
        project_name = ''.join(project_name.split())
        industry     = ''.join(industry.split())
        tsheet_name  = ''.join(sheet_name.split())
        k = '-'.join([industry, project_name]) 
        if k in ['FoodProcessing-EquityBuyout', 'PassengerTransportation-Airline'] and sheet_name in ['PassengerTransportation-Airline', 'Equity_Buyout_KraftHeinzCompany']:
            k = '-'.join([industry, project_name]) 
        else:
            k = '-'.join([industry, project_name, tsheet_name]) 
            
        if time_series == 'N':
            k = '-'.join([k, 'without_time_series'])
        ijson['table_type'] = k
        ph_formula_d    = self.read_ph_user_formula(ijson, '')
        db_file         = "/mnt/eMB_db/template_info.db"
        conn, cur       = conn_obj.sqlite_connection(db_file)
        sql         = "select taxo_id, prev_id, parent_id, taxonomy, taxo_label, scale, value_type, client_taxo, yoy, editable, formula_str, sign, target_currency, period_type from industry_kpi_template where industry_id=%s"%(industry_id)
        cur.execute(sql)
        res         = cur.fetchall()
        conn.close()
        ptype_taxo  = {}
        for rr in res:
            taxo_id, prev_id, parent_id, taxonomy, taxo_label, scale, value_type, client_taxo, yoy, editable, formula_str, sign, target_currency, period_type   = rr
            t_row_f     = ph_formula_d.get(('USER F', taxonomy), ())
            t_row_f_all = ph_formula_d.get(('ALL_USER F', taxonomy), {})
            fflag       = 'USER F'
            if not t_row_f:
                fflag       = 'F'
                t_row_f     = ph_formula_d.get(('F', taxonomy), ())
                t_row_f_all = ph_formula_d.get(('ALL_F', taxonomy), {})
            for flg in  ['CELL F', 'PTYPE F']:
                for cell_id, t_row_f in ph_formula_d.get((flg, taxonomy), {}).items():
                    ptype_taxo[taxonomy]  = 1
                    for ft in t_row_f[1]:
                        if ft['type'] != 'v':
                            if ft['op'] != '=':
                                if ft['t_type'] != ijson['table_type']:
                                    #print (ft['t_type'], ft['g_id'], ft['txid'])
                                    taxo_cl.setdefault((ft['t_type'], ft['g_id']),{}).setdefault(ft['txid'], {})[taxonomy]    = period_type
            if t_row_f_all:
                #print '\n============================='
                #print taxonomy
                if len(t_row_f_all.keys()):
                    done_rem  = {}
                    for f_taxos, t_row_ft in t_row_f_all.items():
                        for ft in t_row_ft[1]:
                            if ft['type'] != 'v':
                                if ft['op'] != '=':
                                    if ft['t_type'] != ijson['table_type']:
                                        #print (ft['t_type'], ft['g_id'], ft['txid'])
                                        taxo_cl.setdefault((ft['t_type'], ft['g_id']),{}).setdefault(ft['txid'], {})[taxonomy]    = period_type


    def gen_final_output(self, ijson, ret_flg=None):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        db_file         = self.get_db_path(ijson)
        #ijson['only_index'] = 'Y'
        #ijson['run_flip'] = 'Y'
        if ijson.get('PRINT', '') != 'Y':
            disableprint()
        #self.run_ph_csv(ijson)
        #enableprint()
        conn, cur   = conn_obj.sqlite_connection(db_file)
        crt_qry = 'CREATE TABLE IF NOT EXISTS tt_group_display_name_info(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_type TEXT, group_id TEXT, group_name TEXT, display_name TEXT)'
        cur.execute(crt_qry)
        conn.commit()
        read_qry = 'SELECT table_type, group_id, display_name FROM tt_group_display_name_info;'
        cur.execute(read_qry)
        table_data = cur.fetchall()

        tt_group_disp_nm_info = {}
        for row in table_data:
            table_type, group_id, display_name = row[:]
            if display_name:
                if group_id:
                    tt_group_disp_nm_info.setdefault(table_type, {})[str(group_id)] = display_name
                else:
                    tt_group_disp_nm_info.setdefault(table_type, {})[''] = display_name

        import data_builder.db_data as db_data
        obj = db_data.PYAPI()

        sql = "select group_id, table_type, group_txt from vgh_group_info"
        try:
            cur.execute(sql)
            res = cur.fetchall()
        except:
            res = []
        grp_info    = {}
        for rr in res:
            group_id, table_type, group_txt = rr
            group_txt = group_txt.replace('\t', '')
            grp_info[(table_type, str(group_id))]   = group_txt

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()
        for col in ['rsp_period', 'rsp_year', 'rfr', 'restated_period', 'rfr_tt', 'scale_vt', 'formula_derivation', 'excel_start_period', 'excel_start_year', 'doc_type_info', 'mt_set', 'r_as_r']:
            try:
                sql = 'alter table company_config add column %s TEXT'%(col)
                cur.execute(sql)
            except:pass

        sql         = "select reporting_type, total_years, derived_ph, periods, ind_group_flg, taxo_flg, excel_config_str,rsp_period, rsp_year, rfr, r_as_r, restated_period, rfr_tt, scale_vt, formula_derivation, excel_start_period, excel_start_year, doc_type_info, mt_set from company_config where project_name='%s'"%(ijson["project_name"])
        cur.execute(sql)
        res = cur.fetchall()
        alter_msg = self.alter_table_coldef(conn, cur, 'vgh_group_map', ['table_str'])
        final_ar    = []
        tres        = [] #cur.fetchall()
        vgh_grp     = {}
        total_years = 10
        for rr in tres:
            table_type, vgh_id, group_txt, table_str   = rr
            table_type  = str(table_type)
            group_txt   = str(group_txt)
            tc_d    = {}
            if table_str:
                for tcid in table_str.split('#'):
                    tc_d[tcid]  = 1
            vgh_grp.setdefault((table_type, group_txt), {})[vgh_id] = tc_d
        grp_ar_d    = {}
        ar  = []
        deriv_phs   = {}
        cinfo       = {} 
        if not res:
            return [{"message":"Error select valid project"}]
        config_ttt  = {}
        ex_d    = {}
        for rr in res:
            reporting_type, total_years, derived_ph, periods, ind_group_flg, taxo_flg, excel_config_str, rsp_period, rsp_year, rfr, r_as_r, restated_period, rfr_tt, scale_vt, formula_derivation, excel_start_period, excel_start_year, doc_type_info, mt_set  = rr
            if reporting_type == 'Reported':
                rfr, restated_period    ='',''
            if rfr_tt and rfr  == 'Y':
                rfr_tt  = rfr_tt.split('##')
            else:
                rfr_tt  = []
            scale_vt_info   = {}
            if scale_vt:
                for ts1 in scale_vt.split('#'):
                    ts1 = ts1.strip().split('~')
                    if len(ts1) == 3:
                        scale_vt_info[ts1[1]]   = ts1[0]
                        igtids  = {}
                        for k, v in eval(ts1[2]).items():
                            tkps    = k.split('$$')
                            if len(tkps ) != 2:
                                tkps    = [tkps[0], ''] 
                            igtids[tuple(tkps)]   = map(lambda x:str(x), v)+igtids.get(tuple(tkps), [])
                        scale_vt_info[('IGNORE', ts1[1])]   = igtids
                        continue
                    if len(ts1) != 2:continue
                    scale_vt_info[ts1[1]]   = ts1[0]
            doc_grps    = []
            if doc_type_info and doc_type_info != 'None':
                for rss in doc_type_info.split('@@'):
                    grp_info, dptype = rss.split('##')          
                    doc_grps.append(dptype.split('~'))
            ignore_ttypes    = {}
            if mt_set and mt_set != 'None':
                for mt_i in mt_set.split('@@'):
                    if not mt_i:continue
                    rmt_id, rmt_data = mt_i.split('~')
                    if not rmt_id:continue
                    itd = {}
                    for e in rmt_data.split('##'):
                        tmpttp  = tuple(e.split('$$'))
                        if len(tmpttp) == 1:
                            tmpttp  = (tmpttp[0], '')
                        itd[tmpttp]  = 1
                    ignore_ttypes[rmt_id]   = itd
                
                        
            cinfo   = {'reporting_type':reporting_type, 'total_years':total_years, 'derived_ph':derived_ph, 'periods':periods, 'rfr':rfr, 'r_as_r':r_as_r, 'restated_period':restated_period, 'rfr_tt':rfr_tt, 'scale_info_m':scale_vt_info, 'formula_derivation':formula_derivation, 'doc_type_info':doc_grps, 'ignore_ttypes':ignore_ttypes}
            total_years = int(total_years)
            if derived_ph:
                for dph in derived_ph.split('##'):
                    if dph.strip():
                        deriv_phs[dph]  = 1
                    
            for pc in excel_config_str.split('^!!^'):
                if not pc:continue
                p, cs    = pc.split('$$')
                p_sp = p.split('@:@')
                text, group_id, tmpy, ttype = p_sp[:4]
                t_ids   = []
                if len(p_sp) > 4:
                    t_ids   = filter(lambda x:x, p_sp[4].split('&&'))
                ignore_phs  = []
                if len(p_sp) > 5:
                    ignore_phs   = filter(lambda x:x, p_sp[5].split('##'))
                remove_not_restated = []
                if len(p_sp) > 6:
                    remove_not_restated = filter(lambda x:x, p_sp[6].split('##'))

                
                if tmpy == '0' and ttype:
                    try:
                        p_ig = p_sp[7]
                    except:p_ig = ''
                    try:
                        p_db = p_sp[8]
                    except:p_db = ''
                    try:
                        p_rfr = p_sp[9]
                    except:p_rfr = ''
                    try:
                        p_phd = p_sp[10]
                    except:p_phd = ''
                    try:
                        p_offset_year = p_sp[11]
                    except:p_offset_year = ''
                    if p_rfr == 'within':
                        ex_d[('RFR', ttype)] = remove_not_restated+ex_d.get(('RFR', ttype), [])
                    elif p_rfr == 'across':
                            ex_d[('RFR', 'DEAL')] = remove_not_restated+ex_d.get(('RFR', 'DEAL'), [])
                    config_ttt[ttype]  = 1
                    if group_id:
                        ar.append((1, ttype, 'group', '', group_id, '', t_ids, ignore_phs, remove_not_restated, p_offset_year))
                        grp_ar_d.setdefault(ttype, {})[group_id] = 1
                    else:
                        ar.append((1, ttype, 'all', '', '', '', t_ids, ignore_phs, remove_not_restated, p_offset_year))
                for c in cs.split('@@'):
                    if not c:continue
                    c_sp    = c.split('^^')
                    group_id, table_type    = c_sp[:2]
                    t_ids   = []
                    if len(c_sp) > 2:
                        t_ids   = filter(lambda x:x, c_sp[2].split('&&'))
                    ignore_phs   = []
                    if len(c_sp) > 3:
                        ignore_phs   = filter(lambda x:x, c_sp[3].split('##'))
                    remove_not_restated = []
                    if len(c_sp) > 4:
                        remove_not_restated = filter(lambda x:x, c_sp[4].split('##'))

                    config_ttt[table_type]  = 1
                    try:
                        c_ig = c_sp[5]
                    except:c_ig = ''
                    try:
                        c_db = c_sp[6]
                    except:c_db = ''
                    try:
                        c_rfr = c_sp[7]
                    except:c_rfr = ''
                    try:
                        c_phd = c_sp[8]
                    except:c_phd = ''
                    try:
                        c_offset_year = c_sp[9].strip()
                    except:c_offset_year = ''
                    if c_rfr == 'within':
                        ex_d[('RFR', table_type)] = remove_not_restated+ex_d.get(('RFR', table_type), [])
                    elif c_rfr == 'across':
                        ex_d[('RFR', 'DEAL')] = remove_not_restated+ex_d.get(('RFR', 'DEAL'), [])

                    if group_id:
                        ar.append((1, table_type, 'group', '', group_id, '', t_ids, ignore_phs, remove_not_restated, c_offset_year))
                        grp_ar_d.setdefault(table_type, {})[group_id] = 1
                    else:
                        ar.append((1, table_type, 'all', '', group_id, '', t_ids, ignore_phs, remove_not_restated, c_offset_year))
        cinfo['ex_d']   = ex_d
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, [])
        ijson_tt        = copy.deepcopy(ijson)
        ijson_tt['taxo_flg']    = 1
        tdb_file         = self.get_db_path(ijson_tt)
        tconn, tcur       = conn_obj.sqlite_connection(tdb_file)
        sql             = "select table_id, xml_id, table_type from mt_data_builder where isvisible='Y'"
        res = []
        try:
            tcur.execute(sql)
            res = tcur.fetchall()
        except:
            res = []
        tconn.close()
        exists_table    = {}
        for rr in res:
            exists_table.setdefault(str(rr[0]), {})[rr[2]]    = 1
        doc_d1       = {}
        doc_d       = {}
        dphs        = {}
        path    = "/var/www/html/TASFundamentalsV2/tasfms/data/output/%s/%s/1_1/21/sdata/doc_map.txt"%(project_id, deal_id)
        if os.path.exists(path):
            fin = open(path, 'r')
            lines   = fin.readlines()
            fin.close()
        else:
            lines   = []
        doc_ph_d    = {}
        c_year      = self.get_cyear(lines)
        start_year  = c_year - int(ijson.get('year', 10))
        for line in lines[1:]:
            line    = line.split('\t')
            if len(line) < 8:continue
            line    = map(lambda x:x.strip(), line)
            try:
                year    = int(line[7])
            except:continue
            if not line[3]:continue
            ph      = line[3]+line[7]
            if ph:# and start_year<int(ph[-4:]):
                doc_id          = line[0]
                doc_d[doc_id]   = (line[3], year)
                doc_d1[doc_id]   = (ph, line[2])
                dphs[line[3]+str(year)]        = 1
                doc_ph_d[doc_id]    = line[3]+str(year)
        errortypes  = {}
        for table_type, table_ids in  rev_m_tables.items():
            if table_type not in config_ttt:continue
            for table_id in table_ids.keys():
                if doc_m_d[table_id] in doc_d:
                    if table_type not in exists_table.get(table_id, {}):
                        errortypes.setdefault(table_type, {})[table_id]  = 1
        if errortypes and ret_flg != 'Y':
            enableprint()
            res = [{"message":"Error table missing in Taxo Builder "+str(errortypes)}]
            return res
                    
        
            
        g_d = {}
        for table_type in grp_ar_d.keys():
            ijson['table_type'] = table_type
            #g_ar    = self.create_group_ar(ijson, txn_m)
            g_ar    = obj.read_all_vgh_groups(table_type, company_name, model_number, doc_m_d, {})
            for rr in g_ar:
                g_d[(table_type, rr['grpid'])] =  rr
        sql = "select group_id, table_type, group_txt from vgh_group_info where table_type like 'HGHGROUP%'"
        try:
            cur.execute(sql)
            res = cur.fetchall()
        except:
            res = []
        for rr in res:
            group_id, table_type, group_txt = rr
            group_txt = group_txt.replace('\t', '')
            g_d[(table_type, group_id)] = group_txt
        ijson['deriv_phs']  = deriv_phs
        report_pt       = ''
        report_year     = ''
        if rsp_period:
            report_pt       = rsp_period
        if rsp_year:
            report_year     = int(rsp_year)
        ijson['report_pt']  = report_pt
        ijson['report_year']    = report_year

        es_pt       = ''
        es_yr     = ''
        if excel_start_period:
            es_pt       = excel_start_period
        if excel_start_year:
            es_yr     = int(excel_start_year)
        ijson['es_p']  = es_pt
        ijson['es_yr']    = es_yr
        ijson['year']           = total_years
        ijson['g_cinfo']        = cinfo
        ijson['scale_info_m']        = cinfo['scale_info_m']
        ignore_ttypes   = cinfo['ignore_ttypes']
        template_dct = self.read_template_info_dic()
        templates       = template_dct.get((ijson['project_name'], ijson.get('industry_type', '')), {})
        m_line_items    = {}
        tmp_ttypes      = {}
        for tmpid in ignore_ttypes.keys():
            print 'TEMP ', tmpid, tmpid in  templates
            if tmpid not in templates:continue
            tmp_ttypes.update(ignore_ttypes[tmpid])
            self.read_mapped_ttypes(copy.deepcopy(ijson), tmpid, templates[tmpid], m_line_items)
        

        print ignore_ttypes
        print m_line_items
        for rr in ar:
            row_id, table_type, gen_type, taxo_ids, group_text, tdatetime, t_ids, ignore_phs, remove_not_restated, offset_year  = rr
            table_type  = str(table_type)
            group_text   = str(group_text)
            ijson_c   = copy.deepcopy(ijson)
            ijson_c['table_type']   = table_type
            ijson_c['offset_year']   = offset_year
            ijson_c['type']         = gen_type
            ijson_c['t_ids']        = map(lambda x:int(x), t_ids)
            ijson_c['t_ids']          = filter(lambda x:x >=0, ijson_c['t_ids'])
            ijson_c['ignore_dphs']          = ignore_phs
            ijson_c['remove_not_restated']          = remove_not_restated
            ijson_c['row_id']       = [table_type, gen_type, group_text]
            if ijson_c.get('project_name', '') == 'Airlines': 
                ijson_c['store_flg']    = 'Y'
            #ijson_c['print']       = 'Y'
            if taxo_ids:
                ijson_c['t_ids']         = map(lambda x:int(x), taxo_ids.split(','))
            if gen_type == 'group' and  group_text:
                #print (table_type, group_text), (table_type, group_text) not in vgh_grp
                if (table_type, group_text) not in g_d:continue
                ijson_c['data']     = [g_d[(table_type, group_text)]['n']]
                ijson_c['grpid']    = group_text
                ijson_c['vids']     = g_d[(table_type, group_text)]['vids']
                if tt_group_disp_nm_info.get(table_type, {}).get(group_text):
                    ijson_c['m_disp_name']  = tt_group_disp_nm_info[table_type][group_text]
            else:
                if tt_group_disp_nm_info.get(table_type, {}).get(''):
                    ijson_c['m_disp_name']  = tt_group_disp_nm_info[table_type]['']
            #else:continue
            #print ijson_c
            final_ar.append(copy.deepcopy(ijson_c))
        conn.close()
        if ret_flg == 'Y':
            return final_ar
        i_project_name  = ''.join(ijson['project_name'].split())
        #sys.exit()
        excel_path  = '/var/www/html/DB_Model_xls/%s/%s.xlsx'%(i_project_name, company_name)
        print "rm -rf "+excel_path
        os.system("rm -rf "+excel_path)
        row_error   = []
        if ijson.get('PRINT', '') != 'Y':
            disableprint()
        DB_DATA_D   = {}
        doctypes    = cinfo.get('doc_type_info', [])
        if not doctypes:
            doctypes    = ['']
        doc_type_doc_lst_map = self.read_doc_type_wise_doc_ids_dict(company_name, model_number)
        for doctype_ar in  doctypes:
            doctype = ''
            doctype_str = ''
            if doctype_ar:
                doctype = '_'.join(doctype_ar)+'/'
                doctype_str = '_'.join(doctype_ar)
            if ijson.get('norm_scale') == 'Y':
                txtpath      = '/var/www/html/DB_Model/%s/%s/%sNorm_Scale'%(company_name, i_project_name,doctype)
                if not ijson.get('i_table_types'):
                    os.system("rm -rf "+txtpath)
                os.system("mkdir -p "+txtpath)
                txtpath      = '/var/www/html/DB_Model_Reported/%s/%s/%sNorm_Scale'%(company_name, i_project_name, doctype)
                if not ijson.get('i_table_types'):
                    os.system("rm -rf "+txtpath)
                os.system("mkdir -p "+txtpath)
            else:
                txtpath      = '/var/www/html/DB_Model_Missing/%s/%s/%s'%(company_name, i_project_name, doctype)
                if not ijson.get('i_table_types'):
                    os.system("rm -rf "+txtpath)
                txtpath      = '/var/www/html/DB_Model_Reported_Missing/%s/%s/%s'%(company_name, i_project_name, doctype)
                if not ijson.get('i_table_types'):
                    os.system("rm -rf "+txtpath)
                txtpath      = '/var/www/html/DB_Model/%s/%s/%s'%(company_name, i_project_name, doctype)


                if not ijson.get('i_table_types'):
                    btxtpath      = '/var/www/html/DB_Model_backup/%s/%s/%s'%(company_name, i_project_name, doctype)
                    os.system("mkdir -p "+btxtpath)
                    os.system('cp -r %s %s/%s'%(txtpath, btxtpath, datetime.datetime.now().strftime('%Y-%m-%d-%H_%M_%S')))
                    os.system("rm -rf "+txtpath)
                os.system("mkdir -p "+txtpath)


                txtpath      = '/var/www/html/DB_Model_Reported/%s/%s/%s'%(company_name, i_project_name, doctype)
                if not ijson.get('i_table_types'):
                    btxtpath      = '/var/www/html/DB_Model_Reported_backup/%s/%s/%s'%(company_name, i_project_name, doctype)
                    os.system("mkdir -p "+btxtpath)
                    os.system('cp -r %s %s/%s'%(txtpath, btxtpath, datetime.datetime.now().strftime('%Y-%m-%d-%H_%M_%S')))
                    os.system("rm -rf "+txtpath)
                os.system("mkdir -p "+txtpath)
            for ii, ijson_c in enumerate(final_ar):
                if ijson.get('i_table_types') and ijson_c['table_type'] not in ijson.get('i_table_types'):continue
                ktup    = (ijson_c['table_type'], ijson_c.get('grpid', ''))
                #if ignore_ttypes and (ktup not in  tmp_ttypes) and (ktup not in m_line_items):continue
                
                if doctype_ar:
                    grp_doc_ids = {}
                    for doctype in doctype_ar:
                        grp_doc_ids.update(doc_type_doc_lst_map.get(doctype, {}))
                    if grp_doc_ids:
                        ijson_c['grp_doc_ids']  = grp_doc_ids
                        ijson_c['grp_doc_types']  = doctype_ar
                    else:continue
            
                #if ijson_c.get('PRINT') == 'Y' and ijson_c['table_type'] not in ['DIV']:continue
                print 'Running ', ii, ' / ', len(final_ar), [ijson_c['table_type'], ijson_c['type'], ijson_c.get('data', []), ijson_c.get('grpid', '')]
                #print json.dumps(ijson_c)
                tijson_c    = copy.deepcopy(ijson_c)
                tijson_c['gen_output'] = 'Y'
                if m_line_items.get(ktup, {}) and ktup not in tmp_ttypes:
                    ijson_c['template_ids']    = m_line_items.get(ktup, {})
                if ktup in tmp_ttypes:
                    ijson_c['non_template_db']  = 'Y'
                mres                 = [] #self.create_seq_across(tijson_c)
                if reporting_type == 'Reported':#ijson.get('project_name', '').lower() == 'schroders':
                    if  ijson.get('norm_scale') != 'Y':
                        tijson_c    = copy.deepcopy(ijson_c)
                        tijson_c['DB_DATA']  = copy.deepcopy(mres)
                        res = self.create_final_output_with_ph(tijson_c, 'P', 'N', DB_DATA_D)
                        if res[0]['message'] != 'done':
                            #print 'Error ',res[0]['message']
                            row_error.append({'error':res[0]['message'], 'table_type':ijson_c['table_type'], 'type':ijson_c['type'], 'grpid':ijson_c.get('grpid', ''), 'vids':ijson_c.get('vids', []), 'data':ijson_c.get('data', [])})
                    if ijson_c.get("NS") != 'N':
                        tijson_c    = copy.deepcopy(ijson_c)
                        tijson_c['DB_DATA']  = copy.deepcopy(mres)
                        tijson_c['norm_scale']  = 'Y'
                        #print 'SSSSSSSSSSSSSSSSSS', tijson_c
                        res1 = self.create_final_output_with_ph(tijson_c, 'P', 'N', DB_DATA_D)
                        if 'Empty Scale' in res1[0]['message']:
                            enableprint()
                            return res1
                    continue

                if reporting_type in ['Restated', 'Both']:
                    if  ijson.get('norm_scale') != 'Y':
                        tijson_c    = copy.deepcopy(ijson_c)
                        tijson_c['DB_DATA']  = copy.deepcopy(mres)
                        res = self.create_final_output_with_ph(tijson_c, 'P', 'Y', DB_DATA_D)
                        if res[0]['message'] != 'done':
                            #print 'Error ',res[0]['message']
                            row_error.append({'error':res[0]['message'], 'table_type':ijson_c['table_type'], 'type':ijson_c['type'], 'grpid':ijson_c.get('grpid', ''), 'vids':ijson_c.get('vids', []), 'data':ijson_c.get('data', [])})
                            #continue
                    if ijson_c.get("NS") != 'N':
                        tijson_c    = copy.deepcopy(ijson_c)
                        tijson_c['DB_DATA']  = copy.deepcopy(mres)
                        tijson_c['norm_scale']  = 'Y'
                        res = self.create_final_output_with_ph(tijson_c, 'P', 'Y', DB_DATA_D)
                        if 'Empty Scale' in res[0]['message']:
                            enableprint()
                            return res


                    if  ijson.get('norm_scale') != 'Y':
                        tijson_c    = copy.deepcopy(ijson_c)
                        res1 = self.create_final_output_with_ph(tijson_c, 'P-1', 'Y', DB_DATA_D)

                    if ijson_c.get("NS") != 'N' and 0:
                        tijson_c    = copy.deepcopy(ijson_c)
                        tijson_c['norm_scale']  = 'Y'
                        res1 = self.create_final_output_with_ph(tijson_c, 'P-1', 'Y', DB_DATA_D)
                        if 'Empty Scale' in res1[0]['message']:
                            enableprint()
                            return res1
                #if reporting_type in ['Both']:
                if  ijson.get('norm_scale') != 'Y':
                    tijson_c    = copy.deepcopy(ijson_c)
                    tijson_c['DB_DATA']  = copy.deepcopy(mres)
                    res1    = self.create_final_output_with_ph(tijson_c, 'P', 'N', DB_DATA_D)

                if ijson_c.get("NS") != 'N':
                    tijson_c    = copy.deepcopy(ijson_c)
                    tijson_c['DB_DATA']  = copy.deepcopy(mres)
                    tijson_c['norm_scale']  = 'Y'
                    res1    = self.create_final_output_with_ph(tijson_c, 'P', 'N', DB_DATA_D)
                    if 'Empty Scale' in res1[0]['message']:
                        enableprint()
                        return res1
        if ijson.get('PRINT', '') == 'Y':
            print 'DONE TEXT'
        #enableprint()
        #if row_error:
        #    res = [{"message":"Error", 'data':row_error}]
        #    return res
        self.store_bobox_data(ijson)
        import model_view.data_builder_new_excel_pr3 as PH_LABEL
        obj = PH_LABEL.MRD_excel()
        if 1:
        #try:
            res = obj.gen_output(ijson)
            res = json.loads(res)
        #except:
        #    res = [{"message":"Error while Creating Excel"}]
        enableprint()
        #print 'RES ', res
        res[0]['DB_ERR']    = row_error
        sgrps   = sconvert_obj.scale_factor_map
        res_lst_na, not_available_scale_lst = self.get_scale_factor_info(sgrps) 
        self.update_db_data(res_lst_na, [], 'Y')
        res[0]['s_grps']  = res_lst_na
        res[0]['c_grps']  = not_available_scale_lst
        return res



    def gen_taxonomy(self, txt):
        alpha      = {'a':1, 'b':1, 'c':1, 'd':1, 'e':1, 'f':1, 'g':1, 'h':1, 'i':1, 'j':1, 'k':1, 'l':1, 'm':1, 'n':1, 'o':1, 'p':1, 'q':1, 'r':1, 's':1, 't':1 ,'u':1, 'v':1, 'w':1, 'x':1, 'y':1, 'z':1}
        tmp_ar  = []
        for c in txt:
            if c.lower() in alpha:
                tmp_ar.append(c)
        return ''.join(tmp_ar)

    def gen_taxonomy_alpha_num(self, txt):
        alpha      = {'a':1, 'b':1, 'c':1, 'd':1, 'e':1, 'f':1, 'g':1, 'h':1, 'i':1, 'j':1, 'k':1, 'l':1, 'm':1, 'n':1, 'o':1, 'p':1, 'q':1, 'r':1, 's':1, 't':1 ,'u':1, 'v':1, 'w':1, 'x':1, 'y':1, 'z':1, '0':1,'1':1,'2':1,'3':1,'4':1, '5':1,'6':1,'7':1, '8':1, '9':1}
        tmp_ar  = []
        for c in txt:
            if c.lower() in alpha:
                tmp_ar.append(c)
        return ''.join(tmp_ar)


    def flip_sign(self, t_res, ph_formula_d, table_type):
        t_res, t_data, t_f_phs, t_f_phs_d, taxo_value_grp, t_re_stated_cols, t_taxo_id_dict    = t_res
        flip_d  = {}
        for ti, rr in enumerate(t_data[:]):
            t_row_f     = ph_formula_d.get(('F', str(rr['t_id'])), ())
            if not t_row_f:
                t_row_f     = ph_formula_d.get(('SYS F', str(rr['t_id'])), ())
            if not t_row_f:continue
            #print '\n========================================='
            #print rr['t_l']
            #val_dict, form_d = self.get_formula_evaluation(t_row_f[1], t_taxo_id_dict, map(lambda x:x['k'], t_f_phs), {}, None, 'Y', {})
            for ii, ph in enumerate(t_f_phs):
                if ph['k'] not in t_data[ti]:continue
                v_d     = t_data[ti][ph['k']]
                rest_ar = v_d.get('rest_ar', [])
                if not rest_ar:continue
                v_d   = v_d['org_d']
                o_v     = v_d['v']
                try:
                    o_v = numbercleanup_obj.get_value_cleanup(o_v)
                except:
                    o_v = ''
                if o_v == '' or float(o_v) in [0.0, -0.0]:continue
                sign    = '+'   
                #print '\tPH', [ph, sign]
                if '-' in o_v:
                    sign    = '-'   
                else:
                    continue
                ks  = []
                sign_ar = []
                for ft in filter(lambda x:x.get('op') != '=', t_row_f[1]):
                    if ft['type'] != 'v' and ft['t_type'] == table_type:
                        tmp_ti  = taxo_value_grp[ft['txid']]
                        v1  = t_data[ti].get(ph['k'] , {}).get('v', '')
                        try:
                            v1 = numbercleanup_obj.get_value_cleanup(v1)
                        except:
                            v1 = ''
                        sign_ar.append('-' if '-' in v1 else '+')
                    
                for key in rest_ar:
                    if key not in t_data[ti]:continue
                    to_v     = t_data[ti][key]['v']
                    try:
                        to_v = numbercleanup_obj.get_value_cleanup(to_v)
                    except:
                        to_v = ''
                    if to_v == '' or float(to_v) in [0.0, -0.0]:continue
                    if '-' in to_v:
                        continue
                    tsign_ar = []
                    for ft in filter(lambda x:x.get('op') != '=', t_row_f[1]):
                        if ft['type'] != 'v' and ft['t_type'] == table_type:
                            tmp_ti  = taxo_value_grp[ft['txid']]
                            v1  = t_data[ti].get(key , {}).get('v', '')
                            try:
                                v1 = numbercleanup_obj.get_value_cleanup(v1)
                            except:
                                v1 = ''
                            tsign_ar.append('-' if '-' in v1 else '+')
                    if sign_ar == tsign_ar:continue
                    ks.append((key, t_data[ti][key], ti, t_data[ti]['t_l']))
                    #flip_d.setdefault(ti, {})[key]  = sign
                    self.flip_sign_foroperands([(sign, filter(lambda x:x.get('op') != '=', t_row_f[1]))], key, sign, ph_formula_d, flip_d, taxo_value_grp, t_data, ks)
                if ks:
                    #print '\n=========================================='
                    #print rr['t_l']
                    #print key
                    #print [v_d['t'], v_d['v'], v_d['phcsv']['pt']+v_d['phcsv']['p']], rest_ar
                    for (key, n_d, tmpti, t_l) in ks:
                        #print '\n\t', key, [t_l]
                        #print '\t', [n_d['t'], n_d['v'], n_d['phcsv']['pt']+n_d['phcsv']['p']]
                        flip_d.setdefault(t_data[tmpti]['t_id'], {})[key]  = sign
                        flip_d.setdefault(t_data[ti]['t_id'], {})[key]  = sign
        return flip_d


    def flip_sign_foroperands(self, toperands, key, tsign, ph_formula_d, flip_d, taxo_value_grp, t_data, ks):
        #t_ops   = []
        #for ft in operands:
        #    if ft['op'] != '=':
        #        t_ops.append(ft)
        done_d  = {}
        tab = 1
        while toperands:
            tmp_operands    = []
            for sign, operands in toperands: 
                for ft in operands:
                        if ft['type'] == 'v':
                            dd  = {}
                            dd['clean_value']   = ft['txid']
                        else:
                            ti  = taxo_value_grp[ft['txid']]
                            if ti in done_d:continue
                            done_d[ti]  =   1
                            #flip_d.setdefault(ti, {})[key]  = sign
                            t_row_f     = ph_formula_d.get(('F', str(ft['txid'])), ())
                            if not t_row_f:
                                t_row_f     = ph_formula_d.get(('SYS F', str(ft['txid'])), ())
                            if key not in t_data[ti]:continue
                            n_d     = t_data[ti][key]
                            if 'org_d' in n_d:
                                n_d = n_d['org_d']
                            n_v     = n_d['v']
                            try:
                                n_v = numbercleanup_obj.get_value_cleanup(n_v)
                            except:
                                n_v = ''
                            if n_v == '' or float(n_v) in [0.0, -0.0]:continue
                            n_sign    = '+'   
                            if '-' in n_v:
                                n_sign    = '-'   
                            f   = 0
                            if sign == '+':
                                if n_sign == '-':
                                    n_sign  = '+'
                            else:
                                if n_sign == '+':
                                    n_sign  = '-'
                                else:
                                    n_sign  = '+'
                            #print '\t'*(tab+1), ( ft['txid'], t_data[ti]['t_l'], key, n_v, n_sign, sign)
                            if 1:#n_sign != sign:
                                ks.append((key, n_d, ti, t_data[ti]['t_l']))
                                if t_row_f:
                                    #print '\t'*(tab+1),'RR', filter(lambda x:x.get('op') != '=', t_row_f[1])
                                    tmp_operands.append((n_sign,  filter(lambda x:x.get('op') != '=', t_row_f[1])))
                                    #tmp_operands    += t_row_f[1]
            toperands    = tmp_operands
            tab += 1


    def update_scale_inrow(self, rr, phs, m_scale, table_type, ijson, scale_info_m={}, actual_v_d={}):
        all_scale_d = {}
        all_vt_d    = {}
        for ph in phs:
            if ph['k'] not in rr:continue
            if not ph['n'] :continue
            if str(rr[ph['k']]['t']) in actual_v_d:continue
            clean_value = rr[ph['k']]['v']
            try:
                clean_value = numbercleanup_obj.get_value_cleanup(clean_value)
            except:
                clean_value = ''
                pass
            phcsv   = rr[ph['k']]['phcsv']
            s, vt   = phcsv['s'], phcsv['vt']
            if s:
                all_scale_d[s] = 1
            if vt:
                all_vt_d[vt] = 1
            if scale_info_m and vt not in scale_info_m:continue
            if str(rr['t_id']) in scale_info_m.get(('IGNORE', vt), {}).get((table_type, ijson.get('grpid', '')), {}):
                continue
            if scale_info_m:
                m_scale = scale_info_m.get(vt, '')
            if clean_value == '':
                pk  = ph['k']
                rr[pk]['phcsv']['s']   = m_scale
                continue
            if not s or (m_scale == s):continue
            c_val, factor   = sconvert_obj.convert_frm_to_1(s, m_scale, clean_value, ijson.get('infotup', ())+(rr['t_id'], rr['t_l'], ph['n'],  rr[ph['k']]['t'], 'Scale Normaize'))
            pk  = ph['k']
            if factor != '':
                rr[pk]['sconv'] = [(s, m_scale, clean_value, c_val)]
                #if ijson.get('err_flg') != 'Y':
                #        return [{'message':str(['Scale Error ', table_type, rr['t_l'], [s, m_scale], rr[ph['k']]['v'], ph]), 'data':[]}]
                #print '\n'
                #print [rr['t_l'], (vl, s), (c_val, m_scale)]
                rr[pk]['phcsv']['s']   = m_scale
                rr[pk]['v']            = c_val
            elif float(clean_value) == 0.0:
                rr[pk]['phcsv']['s']   = m_scale
        rr['m_scale']   = m_scale
        if not m_scale and len(all_scale_d.keys()) == 1:
            rr['m_scale']   = all_scale_d.keys()[0]
        if len(all_vt_d.keys()) == 1:
            rr['m_vt']   = all_vt_d.keys()[0]
            
                
    def update_scale_GChain(self, m_scale, toperands, data, phs, ph_formula_d, taxo_value_grp, table_type, ijson, done_op, scale_info_m={}, actual_v_d={}):
        tab = 1
        while toperands:
            tmp_operands    = []
            for m_scale, operands in toperands: 
                for ft in operands:
                        if ft['type'] != 'v' and table_type == ft['t_type']:
                            ti          = taxo_value_grp[ft['txid']]
                            err = self.update_scale_inrow(data[ti], phs, m_scale, table_type, ijson, scale_info_m, actual_v_d)
                            if err:
                                return err
                            t_row_f     = ph_formula_d.get(('F', str(ft['txid'])), ())
                            if not t_row_f:
                                t_row_f     = ph_formula_d.get(('SYS F', str(ft['txid'])), ())
                            if t_row_f and str(ft['txid']) not in done_op:
                                tmp_operands.append((m_scale,  filter(lambda x:x.get('op') != '=' and x['type'] != 'v', t_row_f[1])))
                            done_op[str(ft['txid'])]    = 1
            toperands    = tmp_operands
            tab += 1

    def update_currency_inrow(self, rr, phs, m_scale, table_type, ijson, actual_v_d):
        if rr.get('m_cy'):
            return
        for ph in phs:
            if ph['k'] not in rr:continue
            if not ph['n'] :continue
            if str(rr[ph['k']]['t']) in actual_v_d:continue
            clean_value = rr[ph['k']]['v']
            try:
                clean_value = numbercleanup_obj.get_value_cleanup(clean_value)
            except:
                clean_value = ''
                pass
            if clean_value == '':continue
            phcsv   = rr[ph['k']]['phcsv']
            s, vt, cy   = phcsv['c'], phcsv['vt'], phcsv['c']
            if not s or (m_scale == s):continue
            if cy not in cconvert_obj.scale_map_d:continue
            c_val, factor   = cconvert_obj.convert_frm_to(s, m_scale, clean_value, ijson.get('infotup', ())+(rr['t_id'], rr['t_l'], ph['n'],  rr[ph['k']]['t'], 'Scale normalize'))
            #if ijson.get('err_flg') != 'Y':
            #        return [{'message':str(['Scale Error ', table_type, rr['t_l'], [s, m_scale], rr[ph['k']]['v'], ph]), 'data':[]}]
            #print '\n'
            #print [rr['t_l'], (vl, s), (c_val, m_scale)]
            pk  = ph['k']
            rr[pk]['phcsv']['c']   = m_scale
            rr[pk]['v']            = c_val
        rr['m_cy']   = m_scale
                
    def update_currency_GChain(self, m_scale, toperands, data, phs, ph_formula_d, taxo_value_grp, table_type, ijson, done_op, actual_v_d):
        tab = 1
        while toperands:
            tmp_operands    = []
            for m_scale, operands in toperands: 
                for ft in operands:
                        if ft['type'] != 'v' and table_type == ft['t_type']:
                            ti          = taxo_value_grp[ft['txid']]
                            err = self.update_currency_inrow(data[ti], phs, m_scale, table_type, ijson, actual_v_d)
                            if err:
                                return err
                            t_row_f     = ph_formula_d.get(('F', str(ft['txid'])), ())
                            if not t_row_f:
                                t_row_f     = ph_formula_d.get(('SYS F', str(ft['txid'])), ())
                            if t_row_f and str(ft['txid']) not in done_op:
                                tmp_operands.append((m_scale,  filter(lambda x:x.get('op') != '=' and x['type'] != 'v', t_row_f[1])))
                            done_op[str(ft['txid'])]    = 1
            toperands    = tmp_operands
            tab += 1

    def get_restated_value(self, res, ph_formula_d, t_ids, txn_m, mat_date, txn, validate_op, actual_v_d, dphs, ijson, ph_f_map, years, ph_gid_map, y, key_map_rev, key_map_rev_t, disp_name, table_type, group_id, doc_d, restated_model, report_pt, report_year, phs_d, ttype_d, display_name_d, dealijson, f_pc_d):
        phs                 = copy.deepcopy(res[0]['phs'])
        data                = copy.deepcopy(res[0]['data'])
        rc_d                = {}
        consider_ks         = {}
        consider_table      = {}
        consider_rows       = {}
        xml_row_map         = {}
        ph_kindex           = {}
        for ii, ph in enumerate(phs):
            ph_kindex[ph['k']]  = ii
        duplicate_taxonomy  = {}
        phk_data            = {}
        phk_ind_data        = {}
        validation_error    = {}
        #print validate_op
        clean_v_d       = {}
        ph_ti               = {}
        taxo_value_grp  = {}
        taxo_id_dict    = {}
        csv_d   = {}
        xml_col_map = {}
        row                 = 1
        row_ind_map         = {}
        taxo_unq_d          = {}
        table_xml_d         = {}
        more_than_one_scale = {}
        rsp_phs             = {}
        pt_order_d          = {}
        pt_arr         = report_year_sort.year_sort(['FY2018', 'Q12018', 'Q22018', 'Q32018', 'Q42018', 'H12018', 'H22018', 'M92018'])
        for ii, ph in  enumerate(pt_arr):
            pt_order_d[ph[:-4]]          = ii
            
        if report_pt and report_year:
            #print pt_arr,[report_pt,report_year]
            for ii, ph in enumerate(phs):
                if len(ph['n']) < 6:continue
                pt  = ph['n'][:-4]
                py  = ph['n'][-4:]
                try:
                    py  = int(py)
                except:
                    continue
                #print [ph['n'], pt, py, pt_order_d.get(pt, -1), pt_order_d[report_pt]]
                if (py < report_year) or  (py == report_year and pt_order_d.get(pt, -1) <= pt_order_d[report_pt]):
                    rsp_phs.setdefault(ph['n'], []).append(ph['k'])
            #for k, v in rsp_phs.items():
            #    print k, v
        for ti, rr in enumerate(data[:]):
            taxo_id_dict[str(rr['t_id'])]    = rr
            taxo_value_grp[str(rr['t_id'])]  = ti
            t_s_f       = ph_formula_d.get(('S', str(rr['t_id'])), {})
            if t_ids and rr['t_id'] not in t_ids:continue
            taxo    = rr.get('taxo', '')
            tmptaxo = taxo.split(' / ')
            f_taxo  = {}
            for tt in tmptaxo:
                for t1 in tt.split('@'):
                    if 'TASTAXO_' in t1 or 'TAS_TAXO' in t1:continue
                    if ' - ' in t1:
                        f_taxo[t1.split(' - ')[1]]  = 1
                    else:
                        f_taxo[t1]  = 1
            f_taxo  = f_taxo.keys()
            f_taxo.sort()
            f_taxo  = ' / '.join(f_taxo[:1])
            tf_taxo    = self.gen_taxonomy_alpha_num(rr['t_l'])
            if not f_taxo:
                f_taxo    = self.gen_taxonomy(rr['t_l'])
            if not f_taxo:
                f_taxo    = rr['t_l']
            if not tf_taxo:
                tf_taxo    = rr['t_l']
            duplicate_taxonomy.setdefault(tf_taxo, {})[rr['t_id']]   = rr['t_l']
            if f_taxo not in taxo_unq_d:
                taxo_unq_d[f_taxo]  = 1
            else:
                ol_cnt  = taxo_unq_d[f_taxo]
                taxo_unq_d[f_taxo]  = ol_cnt+1
                f_taxo  += '-'+str(ol_cnt)
            rr['f_taxo']      = f_taxo
            res[0]['data'][ti]['f_taxo']      = f_taxo
            row += 1
            row_ind_map[ti] = row
            scale_d         = {}
            xml_col_map.setdefault(ti, {})
            for ii, ph in enumerate(phs):
                if ph['k'] not in rr:continue
                v_d         = rr[ph['k']]
                t           = v_d['v']
                clean_value = t
                try:
                    clean_value = numbercleanup_obj.get_value_cleanup(t)
                except:
                    clean_value = ''
                    pass
                if not clean_value and v_d['t'] in actual_v_d:
                    clean_value = v_d['v']
                x_c    = self.clean_xmls(v_d['x'])
                if x_c:
                    table_id    = v_d['t']
                    x_c    = x_c.split(':@:')[0].split('#')[0]
                    table_xml_d.setdefault(table_id, {})[(int(x_c.split('_')[1].split('@')[0].split('$')[0]), int(x_c.split('_')[0].strip('x')))]   = 1
                tk   = self.get_quid(v_d['t']+'_'+v_d['x'])
                c_id        = txn_m.get('XMLID_MAP_'+tk)
                if c_id:
                    ktup    = (rr['t_id'], v_d['t'], int(c_id.split('_')[1]))
                    if ktup in mat_date:
                        v_d['mdate']    = mat_date[ktup][2]
                        res[0]['data'][ti][ph['k']]['mdate']    = mat_date[ktup][2]
                        v_d.setdefault('phcsv', {})['mdate']    =  mat_date[ktup][2]
                        res[0]['data'][ti][ph['k']].setdefault('phcsv', {})['mdate']    = mat_date[ktup][2]
                xml_row_map.setdefault((v_d['t'], v_d['x']), {})[ti]    = 1
                xml_col_map.setdefault(ti, {})[(v_d['t'], v_d['x'])]         = ph['k']
                key     = v_d['t']+'_'+self.get_quid(v_d['x'])
                period_type, period, currency, scale, value_type    = v_d['phcsv']['p'], v_d['phcsv']['pt'], v_d['phcsv']['c'], v_d['phcsv']['s'], v_d['phcsv']['vt']
                #ph_map  = txn.get('PH_MAP_'+str(key))
                #if ph_map:
                #    period_type, period, currency, scale, value_type    = ph_map.split('^')
                if ph['k'] in t_s_f and clean_value:
                    sgrpid, form    = t_s_f[ph['k']]
                    v, factor   = sconvert_obj.convert_frm_to_1(form, scale, clean_value)
                    if v and factor != '':
                        v   =str(v)
                        clean_value = v
                        rr[ph['k']]['expr_str']   = form+'('+str(rr[ph['k']]['v'])+') = '+str(v)
                        rr[ph['k']]['v']    = v
                        rr[ph['k']]['fv']   = 'Y'
                        res[0]['data'][ti][ph['k']]['expr_str']   = form+'('+str(rr[ph['k']]['v'])+') = '+str(v)
                        res[0]['data'][ti][ph['k']]['v']    = v
                        res[0]['data'][ti][ph['k']]['fv']   = 'Y'
                        csv_d[(v_d['t'], v_d['x'])]    = (period_type, period, currency, self.scale_map_rev_d[nscale], value_type)
                if (v_d['t'], v_d['x']) not in csv_d:
                    period_type, period, currency, scale, value_type    = v_d['phcsv']['p'], v_d['phcsv']['pt'], v_d['phcsv']['c'], v_d['phcsv']['s'], v_d['phcsv']['vt']
                    csv_d[(v_d['t'], v_d['x'])]    = (period_type, period, currency, scale, value_type)
                if v_d['v'].strip():
                    scale_d.setdefault(csv_d[(v_d['t'], v_d['x'])][3], {})[v_d['rid']]         = 'N'
                        
                clean_v_d.setdefault(ti, {})[ph['k']]   = clean_value
                ph_ti.setdefault(ph['n'], {})[ti]  = ph['k']
                phk_data.setdefault(ph['k'], []).append((ti, clean_value))
                if 1:#v_d['v']:
                    phk_ind_data.setdefault(ph['k'], {})[ti]    = clean_value
                if clean_value == '':continue
                if not t_ids or rr['t_id'] in t_ids:
                    if validate_op.get(rr['t_id'], {}).get(ph['k'], ''):
                        validation_error.setdefault(rr['t_id'], {})[ph['k']]    = validate_op[str(rr['t_id'])][ph['k']]
            if len(scale_d.keys()) > 1:
                more_than_one_scale[rr['t_id']] = {}
                for sc, rid in scale_d.items():
                    more_than_one_scale[rr['t_id']].update(rid)
        if validation_error:
            res = [{'message':'Error Validation failed', 'data':validation_error}]
            return res
        if 0:#more_than_one_scale:
            res = [{'message':'Error More than one scale', 'data':more_than_one_scale}]
            return res

        if 0:#duplicate_taxonomy:# and gen_type != 'display' :
            tmp_d = {}
            for f_taxo_id, tid_dict in duplicate_taxonomy.items():
                if len(tid_dict.keys()) > 1:
                    tmp_d.update(tid_dict)
            if tmp_d:
                res = [{'message':'Error Duplicate taxonomy', 'data':tmp_d}]
                return res
                    
            
        
        f_phs, dphs_all, col_m, tt_ph_f, done_ph, f_phs_d, ph_map_d, reported_phs, d_new_phs = self.form_ph_cols( ijson, phs, dphs, rsp_phs, res, ph_kindex,phk_ind_data, data, ph_formula_d, taxo_value_grp, y)
        flip_d  = {}
                
        f_phs, f_phs_d, n_phs, f_k_d, new_phs, missing_value, t_op_d, re_stated_cols, done_form_d, latest_year_db  = self.calculate_derivative_values(data, res, ph_formula_d, clean_v_d, col_m, taxo_value_grp, ph_ti, f_phs, f_phs_d, ph_f_map, years, done_ph, ph_gid_map, y, ph_map_d, t_ids, xml_row_map, xml_col_map, txn, taxo_id_dict, key_map_rev, key_map_rev_t, disp_name, table_type, group_id, row_ind_map, csv_d, doc_d, dphs_all, restated_model, phk_ind_data, reported_phs, tt_ph_f, 'Y', flip_d, {}, ijson, {}, {}, phs_d, ttype_d, display_name_d, dealijson, f_pc_d)    

        return (res, data, f_phs, f_phs_d, taxo_value_grp, re_stated_cols, taxo_id_dict)


    def form_ph_cols(self, ijson, phs, dphs, rsp_phs, res,  ph_kindex,phk_ind_data, data, ph_formula_d, taxo_value_grp, y, table_type=None):
        ignore_dphs = ijson.get('ignore_dphs', [])
        report_map_rev_d    = {}
        for k, v_d in self.report_map_d.items():
            for v in v_d.keys():
                report_map_rev_d.setdefault(v, {})[k]   = 1
        ucol_m              = self.read_user_selected_col_map_info(ijson)
        f_phs, reported_phs               = self.get_order_phs(phs, dphs, self.report_map_d, ijson)
        done_ph = {}
        all_years   = {}
        for ph in f_phs:
            done_ph[ph['ph']]   = 1
            try:
                all_years[int(ph['ph'][-4:])]   = 1
            except:pass
        pt_arr         = report_year_sort.year_sort(rsp_phs.keys())
        d_new_phs       = []
        for ph in pt_arr:
            if ph in ignore_dphs:continue
            if ph in done_ph:continue
            dd  = {'ph':ph, 'new':"Y", 'k':rsp_phs[ph][-1], 'g':ph, 'dph':(ph[:-4], int(ph[-4:])), 'reported':"Y"}
            f_phs   = [dd]+f_phs
            reported_phs[ph]    = {}
            for tk in rsp_phs[ph]:
                reported_phs[ph][tk]    = 1
                
            d_new_phs.append(copy.deepcopy(dd))
        dphs_all         = report_year_sort.year_sort(dphs.keys()+pt_arr)
        #for k, v in reported_phs.items():
        #    print k, v
        #sys.exit()
        done_ph             = {}
        f_phs_d             = {}
        ph_map_d    = {}
        for ph in f_phs:
            f_phs_d[ph['ph']]   = ph['k']
            done_ph[ph['ph']]  = 1
            ph_map_d[ph['ph']]    = ph['k']
        exists_ph       = {}
        for ph in f_phs:
            exists_ph[ph['ph']] = 1
        n_ph    = {}
        ph_pkmap    = {}
        tph_map_d    = {}
        f_occ_d     = {}
        if ijson.get('project_name', '').lower() != 'schroders':
            for ph in res[0]['phs']:
                ph_pkmap[ph['k']]    = ph['dph']
                tph_map_d[ph['k']]  = ph
                #print 'PH', ph
                if ph['n'] in exists_ph:continue
                if not ph['n']:continue
                try:
                    ttyear  = int(ph['n'][-4:])
                except:continue
                if ph['n'] in ignore_dphs:continue
                #print '\tYES ',[ph['n']]
                n_ph.setdefault(ph['n'], []).append( ph['k'])
                if 'd' in ph:
                    f_occ_d.setdefault(('D', ph['d'],  ph['n']), []).append(ph['k'])
            for ph in res[0]['phs'][::-1]:
                if ph['n'] not in f_occ_d :
                  if 'd' in ph:
                    f_occ_d[ph['n']]    = ph['d']
    

        if n_ph:
            ph_map_d    = {}
            f_phs_d     = {}
            for ph in f_phs:
                ph_map_d[ph['ph']]   = ph
                f_phs_d[ph['ph']]   = ph['k']
            for tph, pks in n_ph.items():
                ph_kindex[tph]  = ph_kindex[pks[-1]] 
                doc_ph_d        = ph_pkmap[pks[-1]]
                dph             = (tph[:-4], int(tph[-4:]))
                rePflg          = 'Y'
                k1              = tph
                k1          = pks[-1]
                #print tph, pks, [doc_ph_d]
                if tph[:-4] in report_map_rev_d.get(doc_ph_d[0], {}):
                    #print '\t', report_map_rev_d.get(doc_ph_d[0], {})
                    k1          = pks[-1]
                    rePflg      = ''
                    #dph         = doc_ph_d
                
                if ijson.get('table_type', '') not in self.kpi_types:
                    tmp_doc   = f_occ_d[tph]    
                    tmpks    = f_occ_d[('D', tmp_doc, tph)]
                else:
                    tmpks   = []
                if ijson.get('AS_REPORTED_VIEW') == 'Y' or (table_type in self.kpi_types) or (ijson.get('restated_model') != 'Y') or (ijson.get('g_cinfo', {}).get('r_as_r') == 'Y'):
                    ph  = {'ph':tph, 'new':"Y", 'k':k1, 'RS_COL':"Y", 'g':tph, 'dph':dph, 'reported':rePflg, 'frm_d':doc_ph_d} #ph_pkmap[pks[-1]]} #(tph[:-4], int(tph[-4:]))}
                else:
                    ph  = {'ph':tph, 'new':"Y", 'k':k1+'-RS', 'RS_COL':"Y", 'g':tph, 'dph':dph, 'reported':rePflg, 'frm_d':doc_ph_d, 'all_ks':tmpks} #ph_pkmap[pks[-1]]} #(tph[:-4], int(tph[-4:]))}
                ph_map_d[ph['ph']]   = ph
                f_phs_d[ph['ph']]   = ph['k']
                #continue
                #reported_phs[tph]    = {}
                if rePflg == '':
                    for tk in pks:
                        tmpdoc_ph_d        = ph_pkmap[tk]
                        if tmpdoc_ph_d == doc_ph_d:
                            reported_phs.setdefault(tph, {})[tk]    = 1
                else:
                    tmpdocid    = tph_map_d[k1]['g'].split('-')[0]
                    for tk in pks:
                        tmpdocid1    = tph_map_d[tk]['g'].split('-')[0]
                        if tmpdocid != tmpdocid1:continue
                        tmpdoc_ph_d        = ph_pkmap[tk]
                        if tmpdoc_ph_d == doc_ph_d:
                            reported_phs.setdefault(tph, {})[tk]    = 1
                d_new_phs.append(copy.deepcopy(ph))
            dphs_ar             = report_year_sort.year_sort(f_phs_d.keys())
            dphs_all            = copy.deepcopy(dphs_ar)
            dphs_ar.reverse()
            f_phs               = map(lambda x:ph_map_d[x], dphs_ar)
        #for k, v in reported_phs.items():
        #    print k, v
        #sys.exit()
        done_ph             = {}
        f_phs_d             = {}
        ph_map_d    = {}
        for ph in f_phs:
            #print ph
            f_phs_d[ph['ph']]   = ph['k']
            done_ph[ph['ph']]  = 1
            ph_map_d[ph['ph']]    = ph['k']
        col_m               = self.get_column_map(ph_kindex, reported_phs, f_phs, phk_ind_data, data, ph_formula_d, taxo_value_grp, ijson['table_type'], ucol_m)
        tt_ph_f             = self.read_ph_config_formula(ijson)
        if not tt_ph_f:
            tt_ph_f             = self.read_ph_config_formula(ijson, '')
        #print tt_ph_f
        
        n_phs   = {}
        #if 1:
        if ijson.get('print', '') == 'Y':
            print 'col_m ', col_m
        #sys.exit()
        years   = all_years.keys()
        years.sort()
        flip_d  = {}
        r_year  = int(y)+1
        if years and r_year < years[-1]: 
            r_year  = years[-1]+1
        d_ph_ar = []
        for year in range(r_year - ijson.get('year', 20) - 5, r_year+1):
            for  pt in ['FY', 'Q1', 'Q2', 'Q3', 'Q4', 'H1', 'H2', 'M9']:
                d_ph_ar.append(pt+str(year))
        dphs_ar             = report_year_sort.year_sort(d_ph_ar)
        dphs_all            = copy.deepcopy(dphs_ar)
        return f_phs, dphs_all, col_m, tt_ph_f, done_ph, f_phs_d, ph_map_d, reported_phs, d_new_phs
        

    def calculate_derivative_values(self, data, res, ph_formula_d, clean_v_d, col_m, taxo_value_grp, ph_ti, f_phs, f_phs_d, ph_f_map, years, done_ph, ph_gid_map, y, ph_map_d, t_ids, xml_row_map, xml_col_map, txn, taxo_id_dict, key_map_rev, key_map_rev_t, disp_name, table_type, group_id, row_ind_map, csv_d, doc_d, dphs, restated_model, phk_ind_data, reported_phs, tt_ph_f, ret_flg, flip_d, actual_v_d, ijson, t_re_stated_cols, cfstend_d, phs_d, ttype_d, display_name_d, dealijson, f_pc_d):
        remove_not_stated   = ijson.get('remove_not_restated', [])
        #print 'remove_not_stated ',remove_not_stated
        if ijson.get('g_cinfo', {}).get('ex_d', {}).get(("RFR", table_type)):
            remove_not_stated   = remove_not_stated+ijson['g_cinfo']['ex_d'][("RFR", table_type)]
        elif ijson.get('g_cinfo', {}).get('ex_d', {}).get(("RFR", 'DEAL')):
            remove_not_stated   = remove_not_stated+ijson['g_cinfo']['ex_d'][("RFR", 'DEAL')]
        if not remove_not_stated:
            remove_not_stated   = []
        years   = copy.deepcopy(years)
        #for ii, ph in enumerate(res[0]['phs']):
        #    try:
        #        years[ph['n'][-4:]]    = 1
        #    except:pass

        ucol_m              = self.read_user_selected_col_map_info(ijson)
        report_pmap = {}
        for  k, v in self.report_map_d.items():
            #print k, v
            for pt in v.keys():
                report_pmap.setdefault(pt, {})[k]   = 1
            
        deal_id     = str(ijson['deal_id'])
        ignore_dphs = ijson.get('ignore_dphs', [])
        cinfo           = ijson.get('g_cinfo', {})
        if tt_ph_f:
            #remain_phs  = tt_ph_f.keys()
            remain_phs = tt_ph_f.keys()+ph_f_map.keys()
        else:
            remain_phs = ph_f_map.keys()
        n_d     = {}
        new_phs = []
        n_y     = str(int(y)+1)
        if ijson['company_name'].lower() == 'lsrgroup':
            n_y     = str(int(y)) #Aniket - 04-Feb-2020 - This is to say if FY2019 is not available then do not derive H22019
        for ph in remain_phs:
            #print 'PH ', ph
            for year in years.keys():
                tph  = ph+year
                #print '\t', tph, (tph in done_ph,  tph in n_d)
                if tph in done_ph or tph in n_d:continue
                #print '0'
                if table_type not in self.non_financial_tt:
                    if tph in ignore_dphs:continue  
                    #print '1'
                    if 'Q4' == tph[:-4] and int(y) < int(year):continue
                    #print '2'
                    if 'Q4' == ph:
                            if ('FY'+year not in f_phs_d and 'H2'+year not in f_phs_d):continue
                    #print '3'
                    if 'Q2' == ph:
                            if 'H1'+year not in f_phs_d:continue
                    elif 'Q4'+n_y == tph:
                        if 'Q3'+n_y not in f_phs_d:continue
                    elif 'H2'+n_y == tph:
                        if 'Q3'+n_y not in f_phs_d and 'FY'+n_y not in f_phs_d:continue
                    #print '4'
                n_d[tph]             =1
                new_phs.append({'k':tph, 'ph':tph, 'new':'Y', 'dph':(tph[:-4], int(tph[-4:])),'derived':'Y'})
        ph_key_map_d        = {}
        for ti, rr in enumerate(data[:]):
            if t_ids and rr['t_id'] not in t_ids:continue
            t_ph_f      = ph_formula_d.get(str(rr['t_id']), {})
            for ii, ph in enumerate(res[0]['phs']):
                if not ph['n']:continue
                ph_key_map_d[ph['k']]        = ph['n']
                if ph['k'] in t_ph_f:
                    if ph['n'] not in n_d and ph['n'] not in done_ph:
                        n_d[ph['n']]    = 1
                        tph             = ph['n']
                        new_phs.append({'k':tph, 'ph':tph, 'new':'Y', 'dph':(tph[:-4], int(tph[-4:]))})
        pk_map_d        = {}
        pk_map_rev_d    = {}
        latest_year     = {}
        for ph in f_phs:
            pk_map_rev_d[ph['dph']] = ph['k']
            
        if new_phs:
            ph_map_d    = {}
            #for k, v in f_phs_d.items():
            #    print k, v
            for ph in f_phs+new_phs:
                ph_map_d[ph['ph']]   = ph
                f_phs_d[ph['ph']]   = ph['k']
                done_ph[ph['ph']]  = 1
                pk_map_d[ph['k']]    = ph['dph']
                #if ph.get('new', '') != 'Y':
                #    pk_map_rev_d[ph['dph']] = ph['k']
                latest_year[ph['dph'][1]]     = 1
            #print f_phs_d.keys()
            #print n_phs.keys()
            #if 'FY'+y not in n_d and 'Q4'+y in n_d:
            #    del ph_map_d['Q4'+y]
            #    del f_phs_d['Q4'+y]
            dphs_ar             = report_year_sort.year_sort(f_phs_d.keys())
            dphs_ar.reverse()

            f_phs               = map(lambda x:ph_map_d[x], dphs_ar)
        rem_phs  = {}
        for ph in res[0]['phs']:
            pk_map_d[ph['k']]    = ph['dph']
            #pk_map_rev_d[ph['dph']] = ph['k']
            latest_year[ph['dph'][1]]     = 1
                
            if ph['k'] not in phk_ind_data:continue
            if ph['n'] not in f_phs_d:
                rem_phs[ph['n']]    = 1
        for ph in f_phs:
            pk_map_d[ph['k']]    = ph['dph']
        #for ph in f_phs:
        #    print ph
        #sys.exit()
        f_phs.reverse()
        latest_year = latest_year.keys()
        latest_year.sort()
        if latest_year:
            latest_year = latest_year[-1]
        else:
            latest_year = int(y)
        latest_year_db  = str(latest_year)
        latest_year_doc  = str(y)
        latest_pt       = res[0]['phs'][0]['dph'][0]
        if 1:
            latest_pt       = 'NONE'
            latest_year     = None
        n_phs   = {}
        cfgrps, cf_all_grps, grps, all_grps, d_all_grps = {}, {}, {}, {}, {}
        f_k_d   = {}
        missing_value   = {'PKS':{}}
        re_stated_cols  = {}
        for ti, rr in enumerate(data[:]):
            if t_ids and rr['t_id'] not in t_ids:continue
            #print '\n============================'
            #print [ti, rr['t_l']]
            
            key_value           = clean_v_d.get(ti, {})
            for ph in f_phs:
                u_pk    = ucol_m.get(ph['ph'], {}).get('tids', {}).get(str(rr['t_id']))
                if u_pk and u_pk in rr:
                    clean_v_d.setdefault(ti, {})[ph['k']]   = clean_v_d[ti][u_pk]
                    rr[ph['k']] = rr[u_pk]
                    res[0]['data'][ti][ph['k']]    = rr[u_pk]
                    continue
                    
                
                #print ph, ph['k'] not in rr, clean_v_d.get(ti, {}).get(ph['k'], '') == '', col_m.get(ph['k'], {})
                if (ph['k'] not in rr or clean_v_d.get(ti, {}).get(ph['k'], '') == '') and (ti in col_m.get(ph['k'], {})):
                    #print '\tYES', col_m[ph['k']][ti], col_m[ph['k']][ti] in rr
                    if col_m[ph['k']][ti] in rr:
                        clean_v_d.setdefault(ti, {})[ph['k']]   = clean_v_d[ti][col_m[ph['k']][ti]]
                        rr[ph['k']] = rr[col_m[ph['k']][ti]]
                        res[0]['data'][ti][ph['k']]    = rr[col_m[ph['k']][ti]]
                if (ph['k'] not in rr or clean_v_d.get(ti, {}).get(ph['k'], '') == ''):
                    phks    = filter(lambda x:x!=ph['k'], reported_phs.get(ph['ph'], {}).keys())
                    for phk in phks:
                        if (phk in rr or clean_v_d.get(ti, {}).get(phk, '') != ''):
                            missing_value.setdefault(rr['t_id'], {})[phk]   = 1
                            missing_value['PKS'][phk]   = 1
                    

            if restated_model == 'Y':
                restated_year   = ''
                if cinfo.get('restated_period', '') not in ['', 'Latest', None]:
                    t= cinfo.get('restated_period', '').replace('%2B', '+')
                    restated_year   = int(t.split('P+')[1])
                ph_value_d  = {}
                for ii, ph in enumerate(res[0]['phs']):
                    if ph['k'] not in rr:
                        continue
                    clean_value = clean_v_d[ti][ph['k']]
                    sign        = flip_d.get(rr['t_id'], {}).get(ph['k'], '')
                    if sign and str(rr[ph['k']]['t']) not in actual_v_d and data[ti]['th_flg'] not in ['N', 'P']:
                        #print '\n============================'
                        #print [ti, rr['t_l'], ph, clean_value, sign]
                        if clean_value:
                            if float(clean_value) not in [0.0, -0.0]:
                                if sign == '+' and '-' in clean_value:
                                    clean_value = clean_value.strip('-')
                                    clean_v_d[ti][ph['k']]  = clean_value
                                    data[ti][ph['k']]['v']       = clean_value
                                    res[0]['data'][ti][ph['k']]['v'] = clean_value
                                    res[0]['data'][ti][ph['k']]['sign_change'] = 'Y'
                                elif sign == '-' and '-' not in clean_value:
                                    clean_value = '-'+clean_value
                                    clean_v_d[ti][ph['k']]  = clean_value
                                    data[ti][ph['k']]['v']       = clean_value
                                    res[0]['data'][ti][ph['k']]['v'] = clean_value
                                    res[0]['data'][ti][ph['k']]['sign_change'] = 'Y'
                            pass
                    if not clean_value:
                        clean_value= data[ti][ph['k']]['v']
                    ph_value_d.setdefault(ph['n'], {}).setdefault(clean_value, {})[ph['k']]          = 1
                #print '\n============================'
                #print rr['t_l']
                #print ph_value_d
                empty_pk    = {}
                for ii, ph in enumerate(f_phs):
                    if ph['ph'] not in dphs and ('STD' not in ph['ph']):continue
                    #print '\nPH', ph
                    trs_value   = {}
                    c_ph, c_year        = pk_map_d[ph['k']]
                    c_ph                = ph['ph'][:-4]
                    doc_years           = [c_year]
                    d_fph, d_fyear      = [], []
                    tmpres_col          = [] 
                    if ph.get('frm_d'):
                        td_fph, td_fyear      = ph['frm_d']
                        if td_fph and td_fyear:
                            d_fph   = [td_fph]
                            if c_year   != td_fyear and 0:
                                doc_years.append(td_fyear)
                    trs_value_all    = []
                    if ph.get('RS_COL') == 'Y' and (ijson.get('g_cinfo', {}).get('r_as_r') != 'Y'):
                        #key = ph['k'].split('-RS')[0]
                        for key in ph['all_ks']:
                            if rr.get(key, {'v':''})['v'] != '':
                                trs_value.setdefault('Latest', {}).setdefault(c_ph, {})[key]=1
                                break
                        
                    if ph['k'] not in rr or clean_v_d[ti].get(ph['k'], '') == '':
                        other_values            = ph_value_d.get(ph['ph'], {});
                        for v in other_values.keys():
                            if 1:#v and v != clean_v_d[ti].get(ph['k'], ''):
                                for key in other_values[v].keys():
                                    if key == ph['k']:continue
                                    if  pk_map_d[ph['k']] == pk_map_d[key]:continue
                                    if v == '' and rr.get(ph['k'], {'v':''})['v'] == '' and rr[key]['v'] == '':continue
                                    if (table_type in cinfo.get('rfr_tt', []) or rr.get('ttype_flg') == 'BS') and rr[key]['v'] == '':continue
                                    #print '\t',[ph['k'], v, (c_ph, c_year+restated_year),  pk_map_d[key]]
                                    if  rr[key]['t'] == rr.get(ph['k'], {'t':None})['t']: continue
                                    trs_value_all.append(key)

                                    #Ignore RE STATED FROM 0/non-number
                                    if clean_v_d[ti][key] in ['', '0'] and rr[key]['v'] in ['', '0']:continue

                                    if restated_year:# and table_type not in self.non_financial_tt:
                                        doc_cphs    = self.re_state_d.get((str(deal_id), table_type), {}).get(c_ph, [c_ph])
                                        doc_cphs    = list(sets.Set(report_pmap[c_ph].keys()+d_fph))
                                        for tc_ph in doc_cphs:
                                            for tc_year in doc_years:
                                                if (tc_ph, tc_year+restated_year) == pk_map_d[key]:
                                                    trs_value.setdefault('RS_YEAR', {}).setdefault(tc_ph, {})[key]=1
                                                elif table_type in self.non_financial_tt:
                                                    trs_value.setdefault('ALL', {}).setdefault(tc_ph, {})[key]=1
                                                elif ((latest_pt, latest_year) == pk_map_d[key]):
                                                    trs_value.setdefault('Latest', {}).setdefault(tc_ph, {})[key]=1
                            
                                    else:
                                            if cinfo.get('restated_period', '') in ['Latest']:
                                                if ((latest_pt, latest_year) == pk_map_d[key]):
                                                    trs_value.setdefault('Latest', {}).setdefault(tc_ph, {})[key]=1
                                                
                                            trs_value.setdefault('ALL', {}).setdefault(c_ph, {})[key]=1
                    else:
                        v_d = rr[ph['k']]
                        other_values            = ph_value_d.get(ph['ph'], {})
                        clean_value             = clean_v_d[ti][ph['k']]
                        for v in other_values.keys():
                            if 1:#v and v != clean_v_d[ti][ph['k']]:
                                for key in other_values[v].keys():
                                    if key == ph['k']:continue
                                    if  rr[key]['t'] == v_d['t']:continue
                                    if  rr[key]['d'] == v_d['d']:continue
                                    if v == '' and rr[ph['k']]['v'] == '' and rr[key]['v'] == '':continue
                                    if (table_type in cinfo.get('rfr_tt', []) or rr.get('ttype_flg') == 'BS') and rr[key]['v'] == '':continue
                                    if  rr[key]['t'] != v_d['t']:
                                        trs_value_all.append(key)
                                    #Ignore RE STATED FROM 0/non-number
                                    if clean_v_d[ti][key] in ['', '0'] and rr[key]['v'] in ['', '0']:continue
                                    if  rr[key]['t'] == v_d['t']:continue

                                    if restated_year:
                                        doc_cphs    = self.re_state_d.get((str(deal_id), table_type), {}).get(c_ph, [c_ph])
                                        doc_cphs    = list(sets.Set(report_pmap[c_ph].keys()+d_fph))
                                        for tc_ph in doc_cphs:
                                            for tc_year in doc_years:
                                                if (tc_ph, tc_year+restated_year) == pk_map_d[key]:
                                                    trs_value.setdefault('RS_YEAR', {}).setdefault(tc_ph, {})[key]=1
                                                elif table_type in self.non_financial_tt:
                                                    trs_value.setdefault('ALL', {}).setdefault(tc_ph, {})[key]=1
                                                elif ((latest_pt, latest_year) == pk_map_d[key]):
                                                    trs_value.setdefault('Latest', {}).setdefault(tc_ph, {})[key]=1
                                    else:
                                            if cinfo.get('restated_period', '') in ['Latest']:
                                                if ((latest_pt, latest_year) == pk_map_d[key]):
                                                    trs_value.setdefault('Latest', {}).setdefault(tc_ph, {})[key]=1
                                            trs_value.setdefault('ALL', {}).setdefault(c_ph, {})[key]=1
                    tmprs_ar    = []
                    if restated_year and trs_value:
                        if trs_value.get('RS_YEAR'):
                            if c_ph in trs_value['RS_YEAR']:
                                tmprs_ar    = trs_value['RS_YEAR'][c_ph].keys()
                            else:
                                doc_cphs    = report_pmap[c_ph].keys()
                                if d_fph and d_fph[0] not in doc_cphs:
                                    doc_cphs.append(d_fph[0])
                                for tc_ph in doc_cphs:
                                    if tc_ph in trs_value['RS_YEAR']:
                                        tmprs_ar    = trs_value['RS_YEAR'][tc_ph].keys()
                                        break
                        elif table_type in self.non_financial_tt:
                            if c_ph in trs_value['ALL']:
                                tmprs_ar    = trs_value['ALL'][c_ph].keys()
                            else:
                                doc_cphs    = report_pmap[c_ph].keys()
                                if d_fph and d_fph[0] not in doc_cphs:
                                    doc_cphs.append(d_fph[0])
                                for tc_ph in doc_cphs:
                                    if tc_ph in trs_value['ALL']:
                                        tmprs_ar    = trs_value['ALL'][tc_ph].keys()
                                        break
                        elif trs_value.get('Latest'):
                            if c_ph in trs_value['Latest']:
                                tmprs_ar    = trs_value['Latest'][c_ph].keys()
                            else:
                                doc_cphs    = report_pmap[c_ph].keys()
                                if d_fph and d_fph[0] not in doc_cphs:
                                    doc_cphs.append(d_fph[0])
                                for tc_ph in doc_cphs:
                                    if tc_ph in trs_value['Latest']:
                                        tmprs_ar    = trs_value['Latest'][tc_ph].keys()
                                        break
                    elif trs_value.get('ALL'):
                        if c_ph in trs_value['ALL']:
                            tmprs_ar    = trs_value['ALL'][c_ph].keys()
                        else:
                            doc_cphs    = report_pmap[c_ph].keys()
                            if d_fph and d_fph[0] not in doc_cphs:
                                    doc_cphs.append(d_fph[0])
                            for tc_ph in doc_cphs:
                                if tc_ph in trs_value['ALL']:
                                    tmprs_ar    = trs_value['ALL'][tc_ph].keys()
                                    break
                    trs_value   = tmprs_ar
                    #enableprin#t()
                    #print '\n\n',ph
                    #print '\t', [c_ph, report_pmap[c_ph].keys()], map(lambda x:(x, rr[x]['t'], rr[x]['d']), trs_value)    
                    trs_value.sort(key=lambda x:(dphs.index(doc_d[rr[x]['d']][0]) if doc_d[rr[x]['d']][0] in dphs else 0, rr[x]['d']))
                    trs_value_all.sort(key=lambda x:(dphs.index(doc_d[rr[x]['d']][0]) if doc_d[rr[x]['d']][0] in dphs else 0, rr[x]['d']))
                    #print '\t\t', trs_value    
                    if trs_value:
                        re_stated_cols[ph['k']]  = trs_value
                        re_stated_cols.setdefault(('RES_K', ph['k']), [])
                        re_stated_cols[('RES_K', ph['k'])]  += trs_value
                        prev_v      =  copy.deepcopy(rr.get(ph['k'], {'v':'','x':'','bbox':[], 'd':'', 't':'', 'phcsv':{'c':'', 'vt':'','s':'','p':'','pt':''}}))
                        if (ph['k'] not in rr) and (not rr.get(ph['k'], {}).get('v')):
                            tmpdd_v = copy.deepcopy(rr[trs_value[-1]])
                        else:
                            tmpdd_v = copy.deepcopy(rr[ph['k']])
                            if rr[trs_value[-1]]['v'] != rr[ph['k']]['v']:
                                if numbercleanup_obj.get_value_cleanup(rr[trs_value[-1]]['v']) != numbercleanup_obj.get_value_cleanup(rr[ph['k']]['v']):
                                    tmpdd_v = copy.deepcopy(rr[trs_value[-1]])
                            
                        rr[ph['k']] = tmpdd_v
                        rr[ph['k']]['rest_ar']  = trs_value[:]
                        rr[ph['k']]['org_d']    = prev_v
                        rr[ph['k']]['x'] = rr[ph['k']]['x']
                        #csv_d.setdefault(ti, {})[ph['k']]   = csv_d[ti][trs_value[-1]]
                        clean_v_d[ti][ph['k']]   = clean_v_d[ti][trs_value[-1]]
                        #print 'RESTATED_ ', [rr['t_l'], ph, ph['ph'] in dphs, trs_value]
                        if trs_value_all:
                            rr[ph['k']]['rest_ar_all']  = trs_value_all
                        res[0]['data'][ti][ph['k']]    = copy.deepcopy(rr[ph['k']])
                    elif ph['ph'][-4:] != latest_year_db and (ph['ph'] not in remove_not_stated) and (table_type not in cinfo.get('rfr_tt', []) and rr.get('ttype_flg') != 'BS') and ph['k'] in rr and (ph.get('RS_COL') != 'Y' or (ijson.get('g_cinfo', {}).get('r_as_r') != 'Y')):
                        #if ph['k'] not in t_re_stated_cols:continue
                        #print '\tempty_pk'
                        empty_pk[ph['k']]    = 1
                    else:
                        re_stated_cols.setdefault('retain_col', {})[ph['k']]    = 1
                        re_stated_cols.setdefault(('RES_K', ph['k']), [])
                        re_stated_cols[('RES_K', ph['k'])]  += [ph['k']]
                for pk in empty_pk.keys():
                    ph                      = {'k':pk}
                    prev_v      =  copy.deepcopy(rr.get(ph['k'], {'v':'','x':'','bbox':[], 'd':'', 't':'', 'phcsv':{'c':'', 'vt':'','s':'','p':'','pt':''}}))
                    rr[ph['k']]['org_d']    = prev_v
                    #del rr[pk]
                    #del res[0]['data'][ti][pk]
                    rr[ph['k']]['v']        = ''
                    clean_v_d[ti][ph['k']]  = ''
                    res[0]['data'][ti][ph['k']]    = copy.deepcopy(rr[ph['k']])
                    rr[ph['k']]['NO_RESTAT']        = 'Y'
        t_op_d  = {}
        if ret_flg  == 'Y':
            return f_phs, f_phs_d, n_phs, f_k_d, new_phs, missing_value, t_op_d, re_stated_cols, {}, latest_year_db
        
        flip_sign_d = {}
        ph_afftected_tids   = {}
        if ijson.get('project_name', '').lower() != 'schroders':
            if ijson.get('WRITE') == 'Y':
                fout    = open('test_formula.py','wb')
                fout.write(json.dumps({'data':str([data, res, f_phs,ph_formula_d, xml_row_map, xml_col_map, txn, t_ids, taxo_id_dict, taxo_value_grp, clean_v_d, f_k_d, key_map_rev, key_map_rev_t, disp_name, row_ind_map, csv_d, 'V', phs_d, ttype_d, ijson, display_name_d, dealijson, ph_afftected_tids])}))
                fout.close()
            opdd, flip_sign_d, done_form_d, f_pc_d   = self.formula_derivations(data, res, f_phs,ph_formula_d, xml_row_map, xml_col_map, txn, t_ids, taxo_id_dict, taxo_value_grp, clean_v_d, f_k_d, key_map_rev, key_map_rev_t, disp_name, row_ind_map, csv_d, 'V', phs_d, ttype_d, ijson, display_name_d, dealijson, ph_afftected_tids, f_pc_d )    
            if flip_sign_d:
                for ti, rr in enumerate(data[:]):
                    for ii, ph in enumerate(res[0]['phs']):
                        if ph['k'] not in rr:continue
                        if ph['k'] in flip_sign_d.get(ti, {}) and str(rr[ph['k']]['t']) not in actual_v_d and data[ti]['th_flg'] not in ['N', 'P']:
                            pk_ar   = [ph['k']]
                            rest_ar = rr[ph['k']].get('rest_ar', [])
                            if rest_ar:
                                pk_ar.append(rest_ar[-1])
                            #print '\n============================'
                            #print [ti, rr['t_l'], pk_ar]
                            for pk in pk_ar:
                                ph  = {'k':pk}
                                clean_value = clean_v_d[ti][ph['k']]
                                #print [ti, rr['t_l'], ph, clean_value, sign]
                                if clean_value:
                                    if float(clean_value) not in [0.0, -0.0]:
                                        if '-' in clean_value:
                                            clean_value                         = clean_value.strip('-')
                                            clean_v_d[ti][ph['k']]              = clean_value
                                            data[ti][ph['k']]['v']              = clean_value
                                            res[0]['data'][ti][ph['k']]['v']    = clean_value
                                            res[0]['data'][ti][ph['k']]['sign_change'] = 'Y'
                                        elif '-' not in clean_value:
                                            clean_value = '-'+clean_value
                                            clean_v_d[ti][ph['k']]  = clean_value
                                            data[ti][ph['k']]['v']       = clean_value
                                            res[0]['data'][ti][ph['k']]['v'] = clean_value
                                            res[0]['data'][ti][ph['k']]['sign_change'] = 'Y'
                                    pass
        if table_type in cinfo.get('rfr_tt', []):
            l   = len(data) - 1
            for i, rr in enumerate(data[::-1]):
                ti  = l - i
                #print '\n=============================================='
                #print rr['t_id'], rr['t_l']
                for ph in f_phs:
                    c_s = rr.get(ph['k'], {}).get('c_s')
                    if c_s:
                        reported    = 'Y'
                        if rr[ph['k']].get('rest_ar'):
                            reported    = 'N'
                        #print ph, [c_s, reported]
                        v_ar    = []
                        for ft in rr[ph['k']]['f_col_o'][0]:
                            if ft['tt'] != table_type or ijson.get('grpid') != ft['grpid']:continue
                            if ft['operator'] == '=':continue
                            if ft['type'] != 't':continue
                            if reported == 'Y':
                                if not ft.get('rest_ar'):continue
                            else:
                                if ft.get('rest_ar'):continue
                            #print '\tft ', ft
                            if ft.get('clean_value', ''):   
                                clean_value = numbercleanup_obj.get_value_cleanup(ft['clean_value'])
                                if clean_value:
                                    v_ar.append((abs(float(clean_value)), ft))
                        sumv    = sum(map(lambda x:x[0], v_ar))
                        diff    = abs(abs(float(c_s)) - sumv)
                        if diff < 1.0:
                            rr[ph['k']]['c_s']  = ''
                            res[0]['data'][ti][ph['k']]['c_s']  = ''
                            for v, ft in v_ar:
                                #print ft
                                ft['clean_value']    = ''
                                row    = ttype_d.get((ft['to_dealid'], ft['tt'], ft['grpid']), {}).get(ft['taxo_id'], {}) 
                                #print 'REMOVE ', (ft['to_dealid'], ft['tt'], ft['grpid'], ft['taxo_id'], ft['k'] , row.get(ft['k']))
                                if ft['k'] in row:
                                    idx    = ttype_d.get((ft['to_dealid'], ft['tt'], ft['grpid']), {}).get(('IDX', ft['taxo_id'])) 
                                    if idx != None:
                                        res[0]['data'][idx][ft['k']]['v']    = ''
                                    #print 'YES'
                                    row[ft['k']]['v']   = ''
                            
                                
                        #print 'sumv ', sumv, abs(abs(float(c_s)) - sumv)
        if ijson.get('without_derivation') != 'Y' and ijson.get('AS_REPORTED_VIEW') != 'Y':
            tmp_phs = copy.deepcopy(f_phs)
            if tmp_phs and int(tmp_phs[0]['ph'][-4:]) > int(tmp_phs[-1]['ph'][-4:]):
                tmp_phs.reverse()
            for ti, rr in enumerate(data[:]):
                if t_ids and rr['t_id'] not in t_ids:continue
                t_ph_f      = ph_formula_d.get(str(rr['t_id']), {})
                t_cf_f       = ph_formula_d.get(('CF', str(rr['t_id'])), {})
                t_cfs_f     = ph_formula_d.get(('CF_F', str(rr['t_id'])), ())
                if str(rr['t_id']) in cfstend_d:continue
                for pk in t_ph_f.keys():
                    if pk in ph_key_map_d:
                        t_ph_f[ph_key_map_d[pk]]    = copy.deepcopy(t_ph_f[pk])
                        t_ph_f[('rid', ph_key_map_d[pk])]    = t_ph_f[('rid', pk)] 
                        
                if tt_ph_f:
                    #t_ph_f  = {}
                    for ii, ph in enumerate(f_phs):
                        if (ph['k'] not in rr) or (not key_value.get(ph['k'], '')) or not (clean_v_d.get(ti, {}).get(ph['k'], '')):
                            if ph['ph'][:-4] in tt_ph_f and ph['k'] not in t_ph_f:
                                t_ph_f[ph['k']] = tt_ph_f[ph['ph'][:-4]]
                                t_ph_f[('rid', ph['k'])] = 'RID-0'
                #t_s_f       = ph_formula_d.get(('S', str(rr['t_id'])), {})
                #print '\n==============================='
                #print rr['t_l'], t_ph_f
                key_value           = clean_v_d.get(ti, {})
                #print '\toperand_rows ',operand_rows
                for ph in tmp_phs:
                    if ph['ph'] in ignore_dphs:continue
                    if ph['k'] not in rr or (not rr[ph['k']].get('v', '').strip()):#not clean_v_d.get(ti, {}).get(ph['k'], ''):
                        #print '\t',ph
                        f   = self.calculate_user_ph_values(ph, rr, ti, t_cf_f, t_ph_f, t_cfs_f, taxo_value_grp, ph_ti, res, data, f_phs_d, key_value, ph_map_d, cfgrps, cf_all_grps, grps, all_grps, n_phs, ph_f_map, ph_gid_map, d_all_grps, taxo_id_dict, ph_formula_d, deal_id, table_type, group_id, 0 if ijson.get('FROM_PH') != 'Y' else 1, t_re_stated_cols, latest_year_db, latest_year_doc)
                        if f == 1:
                            ph_afftected_tids.setdefault(str(rr['t_id']), {})[ph['k']]  = 1
                            
                if ijson.get('FROM_PH') == 'Y':
                    rr['gids']  = grps
                    rr['cfgids']  = cfgrps


        if cfstend_d and ijson.get('AS_REPORTED_VIEW') != 'Y':
            pk_rev_d    = {}
            for ph in f_phs:
                pk_rev_d[ph['k']]   = ph['ph']
            import pyapi_db
            cf_obj  = pyapi_db.PYAPI()
            cnt = 0
            while cnt < 2:
                cnt += 1
                for pair in cfstend_d['pairs']:
                    t1, t2  = pair
                    if t1 not in taxo_value_grp or t2 not in taxo_value_grp:continue
                    row1 = data[taxo_value_grp[t1]]
                    row2 = data[taxo_value_grp[t2]]
                    map_dict    = cf_obj.get_cash_begin_end_pos(t1, t2, row1, row2, f_phs)
                    if not map_dict:break
                    for tid1, ph_d in map_dict.items():
                        ti1 = taxo_value_grp[tid1]
                        row = data[ti1]
                        for pk, taxotup in ph_d.items():
                            tid2, pk1   = taxotup
                            ti2 = taxo_value_grp[tid2]
                            row2    =  data[ti2]
                            v_d     = row2[pk1]
                            ref_d   = {'x':v_d['x'], 'bbox':v_d['bbox'], 't':v_d['t'], 'd':v_d['d'], 'phcsv':v_d['phcsv'], 'FORM_VALUE':v_d.get('FORM_VALUE', ''), 'v':v_d['v']}
                            row[pk] = ref_d
                            row[pk]['expr_str']   = 'CF ( '+str([row2['t_l'], pk_rev_d[pk1]])+' )'
                            row[pk]['fv']   = 'Y'
                            res[0]['data'][ti1][pk] = copy.deepcopy(row[pk])
                            clean_v_d[ti1][pk]      = clean_v_d[ti2][pk1]
                            #print '\t', [row2['t_l'], pk_rev_d[pk1], ref_d['v']]

                            
        done_form_d = {}
        if ijson.get('AS_REPORTED_VIEW') != 'Y':
            t_op_d, flip_sign_d, done_form_d, f_pc_d   = self.formula_derivations(data, res, f_phs,ph_formula_d, xml_row_map, xml_col_map, txn, t_ids, taxo_id_dict, taxo_value_grp, clean_v_d, f_k_d, key_map_rev, key_map_rev_t, disp_name, row_ind_map, csv_d, 'F', phs_d, ttype_d, ijson, display_name_d, dealijson, ph_afftected_tids, f_pc_d)    
                
        f_k_d['PH_GRP'] = (cfgrps, cf_all_grps, grps, all_grps, d_all_grps)

                        
                    
        return f_phs, f_phs_d, n_phs, f_k_d, new_phs, missing_value, t_op_d, done_form_d, latest_year_db


    def form_fcol_from_sys_formula(self, ti, operand_rows, row_op, row_ind_map, data, ph):
        formula = []
        d_formula   = []
        #if user_f == None:
        row_op[ti]  = {'=':1}
        for ri, rowtup in enumerate(operand_rows):
            tmp_arr = []
            d_tmp_arr = []
            operands    = []
            operand_ids    = []
            rowtup  =(ti, )+rowtup
            for rid in rowtup:
                if rid != ti:
                    row_op[rid]  = {'+':1}
                rv_d    = data[rid].get(ph['k'], {})
                if rv_d:
                    t       = rv_d['v']
                    clean_value = t
                    try:
                        clean_value = float(numbercleanup_obj.get_value_cleanup(t))
                    except:
                        clean_value = 0.00
                        pass
                    if 1:
                        operand_ids.append(rid)
                        operands.append(clean_value)
            if len(operand_ids) > 1 and operand_ids[0] == ti:
                tres = gcom_operator.check_formula_specific(operands[1:], operands[0])
                if not tres:continue
                
                for i, t_r in enumerate(tres):
                    row_op[operand_ids[i+1]]    = {'+':1} if t_r == 0 else {'-':1}
            tmp_arr = []
            for rid in rowtup:
                rv_d    = data[rid]
                op      = row_op[rid].keys()[0]
                dd  = {'txid':str(rv_d['t_id']), 'type':'t', 'op':op}
                tmp_arr.append(dd)
            formula.append(tmp_arr)
        formula.sort(key=lambda x:len(x), reverse=True)
        if formula:
            return formula[0]
        return []

    def formula_derivations(self, data, res, f_phs,ph_formula_d, xml_row_map, xml_col_map, txn, t_ids, taxo_id_dict, taxo_value_grp, clean_v_d, f_k_d, key_map_rev, key_map_rev_t, disp_name, row_ind_map, csv_d, update_form,phs_d, ttype_d, ijson, display_name_d, dealijson, ph_afftected_tids, old_pc_d ):
        #print 'formula_derivation ', [ijson.get('g_cinfo', {}).get('formula_derivation')]
        if 0 and ijson.get('PRINT') == 'Y' or ijson.get('user') == 'captain':
            return {}, {}, {}, {}
        all_phs = {}
        deal_id = ijson['deal_id']
        table_type  = ijson['table_type']
        if update_form != 'DN':
            for ph in f_phs:
                all_phs[ph['ph']]   = ph['k']
                phs_d.setdefault((int(deal_id), ijson['table_type'], ijson.get('grpid', '')), {})[ph['ph']] = ph['k']
        else:
            for ph in f_phs:
                all_phs[ph['k']]   = ph['k']
                phs_d.setdefault((int(deal_id), ijson['table_type'], ijson.get('grpid', '')), {})[ph['k']] = ph['k']
        form_cls_d  = {}
        rev_map = {}
        for ti, rr in enumerate(data[:]):
            if rr.get('ignore_form') == 'Y':continue
            taxonomy    = str(rr['t_id'])
            rev_map[taxonomy] = ti
            t_row_f     = ph_formula_d.get(('USER F', taxonomy), ())
            t_row_f_all = ph_formula_d.get(('ALL_USER F', taxonomy), {})
            fflag       = 'USER F'
            if not t_row_f:
                fflag       = 'F'
                t_row_f     = ph_formula_d.get(('F', taxonomy), ())
                t_row_f_all = ph_formula_d.get(('ALL_F', taxonomy), {})
            if t_row_f and t_row_f[1]:
                if len(t_row_f_all.keys()) > 1:
                    done_rem  = {}
                    for f_taxos, t_row_ft in t_row_f_all.items():
                        found_other = 0
                        ttype_cl    = {}
                        taxo_cl     = {}
                        found_M     = 0
                        for ft in t_row_ft[1]:
                            if ft['type'] != 'v':
                                if ft['op'] != '=':
                                    if ft['op'] == 'M':
                                        found_M = 1
                                    if ft['t_type'] != ijson['table_type']:
                                        ttype_cl[(ft['t_type'], ft['g_id'])]    = 1
                                    taxo_cl[(ft['t_type'], ft['g_id'], ft['txid'])]    = 1
                                if ft['t_type'] != ijson['table_type'] and ft['op'] != '=':
                                    found_other = 1
                        form_cls_d.setdefault(taxonomy, {})['DIRECT']    = t_row_ft
                        #if len(taxo_cl.keys()) == 1 or found_M:
                        #    form_cls_d.setdefault(taxonomy, {})['DIRECT']    = t_row_ft
                        #elif len(ttype_cl.keys()) == 0:
                        #    form_cls_d.setdefault(taxonomy, {})['WITHIN']    = t_row_ft
                        #else:
                        #    form_cls_d.setdefault(taxonomy, {})['ACROSS']    = t_row_ft
                else:
                    form_cls_d.setdefault(taxonomy, {})['DIRECT']    = t_row_f
        pc_d    = old_pc_d
        if not pc_d:## and ijson.get('PRINT') == 'Y':
            level_d = {}
            def form_recursive_formula(resid, opers, done_t_d, level_id):
                c_level_d   = {resid:{}}
                #print '\t'*level_id, [resid]
                c_ar    = []
                for oper in opers:
                    op_txid     = oper['txid']
                    op          = oper['op']
                    op_type     = oper['type']
                    ttype       = oper['t_type']
                    grpid       = oper['g_id']
                    if op_type == 'v':
                        #tmpopers.append(oper)
                        continue
                    to_dealid= oper['to_dealid']
                    if (ttype, grpid) != (ijson['table_type'], ijson.get('grpid', '')):continue
                    #print '\t'*(level_id+1), [op_txid]
                        
                    t_row_f = ()
                    for cnt, ftype in enumerate(['DIRECT']):#, 'ACROSS', 'WITHIN']):
                        t_row_f     = form_cls_d.get(str(op_txid), {}).get(ftype, ()) #ph_formula_d.get(('USER F', rr['t_id']), ())  
                        break
                    if t_row_f:
                        c_level_d[resid][level_id]    = 1
                        c_ar.append(str(op_txid))
                        if ((ttype, grpid, str(op_txid)) not in done_t_d):
                                done_t_d[(ttype, grpid, str(op_txid))]= 1
                                tmplevel_d, tmpc_ar = form_recursive_formula(op_txid, filter(lambda x:x['op'] != '=', t_row_f[1]),  done_t_d, level_id+1)
                                done_t_d[(ttype, grpid, str(op_txid))]= tmpc_ar
                                #print '\t'*(level_id+1), 'N', tmpc_ar 
                                c_ar    += tmpc_ar
                                for k, v in tmplevel_d.items():
                                    c_level_d.setdefault(resid,  {}).update(v)
                                    #c_level_d.setdefault(k,  {}).update(v)
                        else:
                            tmpc_ar = done_t_d[(ttype, grpid, str(op_txid))]
                            #print '\t'*(level_id+1), tmpc_ar 
                            c_ar    += tmpc_ar
                #print '\t'*(level_id), 'F', c_ar 
                return c_level_d, c_ar
            done_t_d    = {}
            for cnt, ftype in enumerate(['DIRECT']):#, 'ACROSS', 'WITHIN']):
                for ti, rr in enumerate(data[:]):
                    t_row_f     = form_cls_d.get(str(rr['t_id']), {}).get(ftype, ()) #ph_formula_d.get(('USER F', rr['t_id']), ())
                    if t_row_f:
                        c_level_d, c_ar   = form_recursive_formula(str(rr['t_id']), filter(lambda x:x['op'] != '=',t_row_f[1]), done_t_d, 1)
                        done_t_d[(ijson['table_type'], ijson.get('grpid', ''), str(rr['t_id']))]    = c_ar
                        pc_d[str(rr['t_id'])]   = c_ar
                        for k, v in c_level_d.items():
                            level_d.setdefault(k,  {}).update(v)
            print 'PC_D'
            #for k, v in level_d.items():
            #    print k, v, pc_d.get(k, [])
            #sys.exit()
        print 'form_cls_d ' #, form_cls_d.keys() #, ph_formula_d.keys()
        done_form_d = {}
        done_taxo_d = {}
        t_ar    = range(len(data))
        #pc_d    = {}
        tmppc_d = {}
        if pc_d and update_form != 'DN':
            t_ar    = []
            for ti, rr in enumerate(data[:]):
                t_ar.append((ti, len(pc_d.get(rr['t_id'], []))))
            t_ar.sort(key=lambda x:x[1], reverse=True)
            t_ar    = map(lambda x:x[0], t_ar)
            tmppc_d = copy.deepcopy(pc_d)
            
        for cnt, ftype in enumerate(['DIRECT']):#, 'ACROSS', 'WITHIN']):
            #for ti, rr in enumerate(data[:]):
            for m_ti in t_ar:
                if m_ti in done_taxo_d:continue
                rr  = data[m_ti]
                #print 'PC ', [update_form, rr['t_id'], ti, rr['t_l'], tmppc_d.get(str(rr['t_id']), [])]
                tmptar  = map(lambda x:rev_map[x], filter(lambda x:x in rev_map, tmppc_d.get(str(rr['t_id']), [])))
                tmptar  = [m_ti]+tmptar
                tmptar.reverse()
                #print tmptar
                for ti in tmptar:
                    #print '\t', ti
                    if ti in done_taxo_d:continue
                    rr  = data[ti]
                    done_taxo_d[ti] = 1
                    t_row_f     = form_cls_d.get(str(rr['t_id']), {}).get(ftype, ()) #ph_formula_d.get(('USER F', rr['t_id']), ())
                    t_row_f_ptype, t_row_f_cell = {}, {}
                    if update_form != 'DN':#ftype == 'WITHIN':
                        t_row_f_ptype   = ph_formula_d.get(('PTYPE F', str(rr['t_id'])), {})
                        t_row_f_cell    = ph_formula_d.get(('CELL F', str(rr['t_id'])), {})
                    if not t_row_f and not t_row_f_ptype and not t_row_f_cell:continue
                    if 0:
                        print '\n=========================================='
                        print [ti, rr['t_id'], rr['t_l'], ftype, all_phs]
                        if t_row_f:
                            for ft in t_row_f[1]:
                                print '\t', ft
                        if t_row_f_ptype:
                            for pk, pv in t_row_f_ptype.items():
                                print '\n\t-----------------------------------'
                                print '\tPTYPE ', pk
                                for ft in pv[1]:
                                    print '\t\t', ft
                        if t_row_f_cell:
                            for pk, pv in t_row_f_cell.items():
                                print '\n\t-----------------------------------'
                                print '\tCELL ', pk
                                for ft in pv[1]:
                                    print '\t\t', ft
                    
                    op_d    = {}
                    if update_form == 'DN':
                        tmoinfo_d   = {(ijson.get('table_type', ''), ijson.get('grpid', '')):form_cls_d}
                        val_dict, form_d, flip_sign, mismatch_scale, mismatch_vt, taxo_grp = self.get_formula_evaluation_across(rr, t_row_f, ttype_d, all_phs.keys(), phs_d, rr.get('m_scale', ''), rr.get('value_type', ''), 'Y', t_row_f_ptype, t_row_f_cell, 'Y', op_d, {},  'Y', tmoinfo_d)
                    else:
                        val_dict, form_d, flip_sign, mismatch_scale, mismatch_vt, taxo_grp = self.get_formula_evaluation_across(rr, t_row_f, ttype_d, all_phs.keys(), phs_d, rr.get('m_scale', ''), rr.get('value_type', ''), 'Y', t_row_f_ptype, t_row_f_cell, 'Y', op_d)
                    update_pk   = {}
                    for k_org, v in val_dict.get(str(rr['t_id']), {}).items():
                        form_d[all_phs[k_org]]   = form_d[k_org]
                        k   = all_phs[k_org]
                        #print '\t', [k, k_org, v['v']]
                        if rr.get(k, {}).get('v', None) != None and rr.get(k, {}).get('v', '') != '' and  rr.get(k, {}).get('FORM_VALUE') != 'Y':# and   rr[k].get('NO_RESTAT') != 'Y':
                            v_d = rr.get(k, {})
                            if v_d.get('FORM_VALUE') != 'Y' or update_form == 'DN':
                                #print '\t', update_form
                                update_pk[k]    = 1
                            #update_pk[k]   = 1
                            try:
                                clean_value = numbercleanup_obj.get_value_cleanup(v_d['v'])
                            except:
                                clean_value = '0'
                                pass
                            if clean_value == '':
                                clean_value = '0'
                            clean_value = float(clean_value)
                            sum_val     = sum(v['v_ar'])
                            if v['ftype'] == 'NG':
                                sum_val  = v['v'].replace(',', '')
                            n_value     = abs(abs(clean_value) - abs(float(sum_val))) #v['v'].replace(',', '')) 
                            n_value     = self.convert_floating_point(n_value).replace(',', '')
                            if n_value not in ['0', '0.00', '0.0']:# and ftype == 'WITHIN':
                                rr[k]['c_s']    = n_value
                                res[0]['data'][ti][k]['c_s']   = n_value
                                #print '\tc_s', [n_value]
                            else:
                                rr[k]['c_s']    = ''
                                res[0]['data'][ti][k]['c_s']   = ''
                            rr[k]['expr_str']    = v['expr_str']
                            res[0]['data'][ti][k]['expr_str']   = v['expr_str']
                            rr[k]['expr_val']    = v['expr_val']
                            res[0]['data'][ti][k]['expr_val']   = v['expr_val']
                            continue
                        #if update_form == 'DN':continue
                        if res[0].get('dont_derive') == 'Y':continue
                        if ijson.get('g_cinfo', {}).get('formula_derivation') == '1' and update_form != 'DN':continue
                        #phs_d.setdefault((int(deal_id), ijson['table_type'], ijson.get('grpid', '')), {})[k] = k
                        if k not in rr or rr[k].get('NO_RESTAT') == 'Y' or  rr or rr[k].get('FORM_VALUE') == 'Y':
                            v['FORM_VALUE'] = 'Y'
                            v['expr_str'] += str(['FORM_VALUE ', 'Y'])
                        update_pk[k]   = 1
                        #v['rid']    = sn
                        rr[k]   = v 
                        #print '\tIN', [k, v.get('v')]
                        res[0]['data'][ti][k]   = v
                        if rr.get('value_type'):
                            v.setdefault('phcsv', {})['vt'] = rr['value_type']
                        if v.get('v'): 
                            rr['da']    = 'Y'
                        clean_v_d.setdefault(ti, {})[k]   = str(v['v'].replace(',',''))
                    #update_pk   = {}
                    ttype_d.setdefault((int(deal_id), table_type, ''), {})[str(rr['t_id'])]    = rr
                    for k, v in all_phs.items():    
                        if k in form_d:
                            form_d[v]   = form_d[k]
                            update_pk[v]    = 1
                    #print '\tupdate_pk',update_pk
                    res_row = self.read_sys_form_across(t_row_f, rr, map(lambda x:{'k':x['k'], 'n':x.get('ph', x.get('n'))}, f_phs), ttype_d, table_type, {}, phs_d, ijson, display_name_d, update_pk, form_d, dealijson)
                    for pk in update_pk.keys():
                        if not rr.get(pk,  {}).get('f_col'):continue
                        done_form_d.setdefault(rr['t_id'], {'ph':{},'rids':{}})[rr[pk]['f_col'][0][0]['rid']]   = form_d
                        done_form_d[rr['t_id']]['rids'][rr[pk]['f_col'][0][0]['rid']] = 1
                        #print '\t', [rr['t_id'], pk]
                        rr[pk]['f_col_o']   = copy.deepcopy(rr[pk]['f_col'])
                        res[0]['data'][ti][pk]['f_col_o']   = copy.deepcopy(rr[pk]['f_col'])
                        res[0]['data'][ti][pk]['fv']   = 'Y'
                        res[0]['data'][ti][pk]['f']   = 'Y'
                        res[0]['data'][ti][pk]['g_f'] = 'Y'
                        res[0]['data'][ti]['fv']   = 'Y'
                        res[0]['data'][ti]['f']   = 'Y'
                        res[0]['data'][ti]['g_f'] = 'Y'

        #if update_form == 'F' and ijson.get('MULTI_FORM') == 'Y':
                

            
        t_op_d      = {}
        flip_sign_d = {}
        return t_op_d, flip_sign_d, done_form_d, pc_d


        for ti, rr in enumerate(data[:]):
            if t_ids and rr['t_id'] not in t_ids:continue
            t_row_f     = ph_formula_d.get(('SYS F', str(rr['t_id'])), ())
            ttype   = ''
            op_d        = {}
            if t_row_f:
                op_d    = ph_formula_d.get(("OP", t_row_f[0]), {})
                ttype   = 'SYS'
            if not t_row_f:
                t_row_f     = ph_formula_d.get(('F', str(rr['t_id'])), ())
            if t_row_f:
                ttype   = 'USER'
            r_key, rs_value, operand_rows, row_form, row_op = self.read_formula(ti, rr, f_phs, txn, xml_row_map, xml_col_map[ti])
            if update_form == 'V' and 0:
                if not t_row_f:
                    for ph in f_phs:
                        if ph['k'] in rr:
                            formula_ar  = self.form_fcol_from_sys_formula(ti, operand_rows, row_op, row_ind_map, data, ph)
                            if formula_ar:
                                t_row_f = ('', formula_ar)
                                ph_formula_d[('F', str(rr['t_id']))] = t_row_f
                                break
            if t_row_f:
                #print 't_id', rr['t_id'], rr['t_l'], [ttype]
                if update_form == 'F':
                    for ft in t_row_f[1]:#t_row_f[1]:
                        if ft['type'] == 'v':
                            pass
                        else:
                            tmp_ti  = taxo_value_grp[ft['txid']]
                            t_op_d.setdefault(tmp_ti, {})[ti]   = 1
                ph_key_map  = {}
                for tr in f_phs:
                    ph_key_map[tr['k']]  = tr['ph']
                #print 't_id', rr['t_id'], rr['t_l']
                #for tr in t_row_f[1]:
                #    print tr
                tmpform_lst = [t_row_f[1]]
                #print '\nt_id', rr['t_id'], rr['t_l'], [ttype, t_row_f[0]]
                for ftype in ['F', 'SYS F'][:0]:
                    for form_str, tmpt_row_f in ph_formula_d.get(('ALL_'+ftype, str(rr['t_id'])), {}).items():
                        #print '\t',[ftype, tmpt_row_f[0], form_str]
                        if tmpt_row_f[0] == t_row_f[0]:continue
                        tmpform_lst.append(tmpt_row_f[1])
                final_form_str  = []
                for findex, fop_ar in enumerate(tmpform_lst):
                    trow_op  = {}
                    val_dict, form_d, flip_sign = self.get_formula_evaluation(t_row_f[1], taxo_id_dict, map(lambda x:x['k'], f_phs), {}, None, 'Y', op_d)
                    toperand_rows    = []
                    l   = len(t_row_f[1])
                    er_op   = 1
                    for ft in t_row_f[1]:
                        if ft['type'] != 'v' and ft['op'] != '=':
                            if ft['txid'] not in taxo_value_grp:
                                er_op = 0
                                break
                            toperand_rows.append(taxo_value_grp[ft['txid']])
                            trow_op[taxo_value_grp[ft['txid']]]  = {ft['op']:1}
                    if er_op == 0:continue
                    toperand_rows    = [tuple(toperand_rows)]
                    final_form_str.append((form_d, toperand_rows, trow_op))
                    if findex != 0:continue
                    if update_form == 'V' and 0:
                        for txid, ph_kd in flip_sign.items():
                            flip_sign_d.setdefault(taxo_value_grp[txid], {}).update(ph_kd)
                             
                    if val_dict:
                        for k, v in val_dict.get(str(rr['t_id']), {}).items():
                            if rr.get(k, {}).get('v', None) != None and rr.get(k, {}).get('v', '') != '' and rr.get(k, {}).get('FORM_VALUE') != 'Y':
                                v_d = rr.get(k, {})
                                try:
                                    clean_value = numbercleanup_obj.get_value_cleanup(v_d['v'])
                                except:
                                    clean_value = '0'
                                    pass
                                if clean_value == '':
                                    clean_value = '0'
                                clean_value = float(clean_value)
                                sum_val     = sum(v['v_ar'])
                                if 1:#v['ftype'] == 'NG':
                                    sum_val  = v['v'].replace(',', '')
                                n_value     = abs(abs(clean_value) - abs(float(sum_val))) #v['v'].replace(',', '')) 
                                n_value     = self.convert_floating_point(n_value).replace(',', '')
                                if n_value not in ['0', '0.00', '0.0']:
                                    rr[k]['c_s']    = n_value
                                    res[0]['data'][ti][k]['c_s']   = n_value
                                    #if deal_id == '216':
                                    #    print [ph_key_map[k], clean_value, ' - ',float(v['v'].replace(',', '')), ' = ',n_value]
                                else:
                                    rr[k]['c_s']    = ''
                                    res[0]['data'][ti][k]['c_s']   = ''
                                continue
                            if rr.get(k, {}).get('v', '') != '' and rr.get(k, {}).get('FORM_VALUE') != 'Y': continue
                            if 1:#update_form == 'V' or rr.get(k, {}).get('FORM_VALUE') == 'Y':
                                clean_v_d.setdefault(ti, {})[k]   = str(v['v'])
                                if k not in rr or rr[k].get('NO_RESTAT') == 'Y':
                                    v['FORM_VALUE'] = 'Y'
                                rr[k]   = v
                                res[0]['data'][ti][k]   = copy.deepcopy(v)
                                #taxo_id_dict[str(rr['t_id'])][k]   = copy.deepcopy(v) 
                if update_form == 'F':
                    #print '\n============================================'
                    #print rr['t_l']
                    for fidx,(form_d, toperand_rows, trow_op) in enumerate(final_form_str):
                        #print '\t', toperand_rows
                        for ph in f_phs:
                            if ph['k'] in rr:
                                #print '\t',[ph, rr[ph['k']]['v'], rr[ph['k']].get('PH_D'), toperand_rows]
                                f_k_d.setdefault(ti, {})[ph['k']]   = {}
                                self.add_row_formula(ti, f_k_d[ti][ph['k']], toperand_rows, res, data, ph, key_map_rev, key_map_rev_t, trow_op, txn, disp_name, row_ind_map, csv_d, 'Y', form_d, taxo_value_grp, fidx)
                                #self.add_column_formula(ti, f_k_d[ti][ph['k']], row_form, res, rr, ph, data, key_map_rev, key_map_rev_t, txn, disp_name, csv_d)
            elif 0:
                r_formula   = []
                for ph in f_phs:
                    if ph['k'] in rr:
                        f_k_d.setdefault(ti, {})[ph['k']]   = {}
                        self.add_row_formula(ti, f_k_d[ti][ph['k']], operand_rows, res, data, ph, key_map_rev, key_map_rev_t, row_op, txn, disp_name, row_ind_map, csv_d, '', {}, {})
                        self.add_column_formula(ti, f_k_d[ti][ph['k']], row_form, res, rr, ph, data, key_map_rev, key_map_rev_t, txn, disp_name, csv_d)
                        if rr[ph['k']].get('f_col', []):
                            r_formula   = rr[ph['k']]['f_col'][0]
                if r_formula:
                    tmpar   = []
                    for ii, tr in enumerate(r_formula):
                        tmpar.append({'txid':str(tr['taxo_id']), 'type':'t', 't_type':table_type, 'g_id':group_id, 'op':tr['operator']})
                    t_row_f = ('NEW F', tmpar)
                    trow_op  = {}
                    ph_key_map  = {}
                    for tr in f_phs:
                        ph_key_map[tr['k']]  = tr['ph']
                    val_dict, form_d, flip_sign = self.get_formula_evaluation(t_row_f[1], taxo_id_dict, map(lambda x:x['k'], res[0]['phs']))
                    toperand_rows    = []
                    l   = len(t_row_f[1])
                    for ft in t_row_f[1]:
                        if ft['type'] != 'v' and ft['op'] != '=':
                            toperand_rows.append(taxo_value_grp[ft['txid']])
                            trow_op[taxo_value_grp[ft['txid']]]  = {ft['op']:1}
                    toperand_rows    = [tuple(toperand_rows)]
                    if val_dict:
                        val_dict    = val_dict.get(str(rr['t_id']), {})
                        for k, v in val_dict.items():
                            clean_v_d.setdefault(ti, {})[k]   = str(v['v'])
                            rr[k]   = v
                            res[0]['data'][ti][k]   = copy.deepcopy(v)
                        for ph in f_phs:
                            if ph['k'] in rr and (ph['k'] in val_dict or (not rr[ph['k'] ].get('f_col', []))):
                                f_k_d.setdefault(ti, {})[ph['k']]   = {}
                                self.add_row_formula(ti, f_k_d[ti][ph['k']], toperand_rows, res, data, ph, key_map_rev, key_map_rev_t, trow_op, txn, disp_name, row_ind_map, csv_d, 'Y', {}, {})
        return t_op_d, flip_sign_d

    def get_column_map(self, ph_kindex, reported_phs, f_phs, phk_ind_data, data, ph_formula_d, taxo_value_grp, table_type, ucol_m={}):
        taxo_ind_d  = {}
        for ii, r in enumerate(data):
            taxo_ind_d[str(r['t_id'])]  = ii
            
        #print 'ucol_m ',ucol_m
        col_m   = {}
        num_obj = {'One': 1, 'Dozen': 12, 'Hundred': 100, 'Thousand': 1000, 'Million': 1000000, 'Billion': 1000000000, 'Trillion': 1000000000000}
        for ph in f_phs:
            if ph['k'] not in ph_kindex:continue
            phi         = ph_kindex[ph['k']]+1
            #if ph['k'] not in phk_ind_data:continue
            #remain_phs  = phs[:phi]
            c_col       = sets.Set(phk_ind_data.get(ph['k'], {}).keys())
            c_col_no_val= sets.Set(filter(lambda x:phk_ind_data[ph['k']][x] ,phk_ind_data.get(ph['k'], {}).keys()))
            col_map     = {}
            #print '\n-----------------------------------------------'
            #print ph, reported_phs.get(ph['ph'], {}).keys()
            phks    = reported_phs.get(ph['ph'], {}).keys()
            phks.sort(key=lambda x:ph_kindex[x])
            done_r  = {}
            for phk in phks:
                if phk == ph['k']:continue
                if phk not in phk_ind_data:continue
                d_col   = sets.Set(phk_ind_data[phk].keys())
                d_col_no_val   = sets.Set(filter(lambda x:phk_ind_data[phk][x] , phk_ind_data[phk].keys()))
                common  = c_col.intersection(d_col)
                c_f = 1
                tmpcommon   = filter(lambda x:phk_ind_data[ph['k']][x] and phk_ind_data[phk][x], list(common))
                for c in tmpcommon:
                    if ucol_m.get(ph['ph'], {}).get('tids', {}).get(str(data[c]['t_id'])):
                        continue
                    t_row_f     = ph_formula_d.get(('F', str(data[c]['t_id'])), ())
                    if not t_row_f:
                        t_row_f     = ph_formula_d.get(('SYS F', str(data[c]['t_id'])), ())
                    if not t_row_f:continue
                    formula = t_row_f[1]
                    res     = {} #formula[0]
                    opers   = [] #formula[1:]
                    for rr in formula:
                        if rr['op'] == '=':
                            res = rr
                        else:
                            opers.append(rr)
                    op_overlap  = 'N'
                    for oper in opers:
                        op_txid    = oper['txid']
                        op_type    = oper['type']
                        if op_type == 'v':
                            pass
                        elif phk_ind_data[ph['k']][c]:
                            if oper['t_type'] != table_type:continue
                            tc  = taxo_value_grp[oper['txid']]
                            if ph['k'] not in data[tc] or phk not in data[tc]:continue
                            ph_csv1  = data[tc][ph['k']]['phcsv']['s']
                            ph_csv2  = data[tc][phk]['phcsv']['s']
                            if ph_csv1 == ph_csv2:
                                if phk_ind_data[ph['k']][c] != phk_ind_data[phk][c]:
                                    op_overlap  = 'Y'
                                    break
                            elif ph_csv1 and ph_csv2 and phk_ind_data[phk][c]:
                                if sconvert_obj.num_obj.get(sconvert_obj.scale_map_d.get(ph_csv2)) > sconvert_obj.num_obj.get(sconvert_obj.scale_map_d.get(ph_csv1)):
                                        tmp2    = ph_csv1
                                        ph_csv1 = ph_csv2
                                        ph_csv2 = tmp2
                                v2          = phk_ind_data[phk][c]
                                #print 'ELSE'
                                v2, factor   = sconvert_obj.convert_frm_to_1(ph_csv2, ph_csv1, v2)
                                v2          = self.convert_floating_point(v2).replace(',','')
                                #print '\tELSE',[ph_csv2, ph_csv1, (float(v2), float(phk_ind_data[ph['k']][c])), float(phk_ind_data[ph['k']][c]) != float(v2)]
                                if float(phk_ind_data[ph['k']][c]) != float(v2):
                                    op_overlap  = 'Y'
                                    break
                    if op_overlap  == 'Y':
                        c_f = 0
                        break
                        
                #print '\t', [phk, c_f, list(common)]
                if c_f == 1:
                    rem     =  list(d_col - c_col_no_val)
                    #print 'Remaining ', rem
                    for r in rem:
                        if r in done_r and col_map.get(r, {}) and phk_ind_data[col_map[r]][r]:continue
                        if not col_map.get(r, {}) or  not phk_ind_data[col_map[r]][r]:
                            col_map[r]  = phk
                            done_r[r]   = 1
                
                phi -= 1
            #print 'FINAL ', col_map
            if col_map:
                #print ph['ph']
                #print '\t',ph['k'], col_map
                col_m[ph['k']]               = col_map
            for tid, k in ucol_m.get(ph['ph'], {}).get('tids', {}).items():
                #print [tid, k, ph['k']]
                if k != ph['k'] and tid in taxo_ind_d:
                    col_m.setdefault(ph['k'], {})[taxo_ind_d[tid]]  = k
        return col_m

    def read_mat_date(self, ijson, mat_date, txn_m):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        if mat_date.get('vids', {}) and isinstance(mat_date.get('vids', {}), list):
            mat_date['vids']   = mat_date['vids'][0]
        else:
            mat_date['vids']   = {}
        table_type    = ijson['table_type']
        mat_d   = {}
        if 1:
            db_file         = self.get_db_path(ijson)
            conn, cur   = conn_obj.sqlite_connection(db_file)
            if not mat_date.get('vids', {}):
                sql         = "select row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, doc_id, m_rows, vgh_text, vgh_group, xml_id, period, period_type, scale, currency, value_type from mt_data_builder where table_type='%s' and isvisible='Y'"%(table_type)
            else:
                sql         = "select row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, doc_id, m_rows, vgh_text, vgh_group, xml_id, period, period_type, scale, currency, value_type from mt_data_builder where table_type='%s' and isvisible='Y' and vgh_text in (%s)"%(table_type, ', '.join(mat_date['vids'].keys()))
            try:
                cur.execute(sql)
                res         = cur.fetchall()
            except:
                res = []
            g_vids  = mat_date.get('vids', {})
            for rr in res:
                row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, gv_xml, ph, ph_label,gcom, ngcom, doc_id,m_rows, vgh_text, vgh_group, xml_id, period, period_type, scale, currency, value_type    = rr
                tk   = self.get_quid(table_id+'_'+xml_id)
                c_id        = txn_m.get('XMLID_MAP_'+tk)
                if not c_id:continue
                c   = int(c_id.split('_')[2])
                r   = int(c_id.split('_')[1])
                if g_vids.get(vgh_text, {}) and table_id+'-'+str(c) not in g_vids.get(vgh_text, {}):continue
                t   = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                if t:
                    mat_d[(taxo_id, table_id, r)]   = (c_id, xml_id, t)
    
        return mat_d

                

    def get_bds_taxo_map(self, company_name):
        fname   = '/home/avinash/MappingInfoBds.sh'
        data    = self.read_from_shelve(fname, {})
        tmp_d   = {}
        #company_name    = ''.join(company_name.split())
        for k, v in data.items():
            k   = tuple(map(lambda x:''.join(self.remove_index_num(self.convert_html_entity(x.lower())).split()), k))
            if k[0] != company_name:continue
            tmp_d[k]    = v
        return tmp_d

    def read_cell_dict_HGH_data(self, company_name, model_number, table_ids):
        db_file     = os.path.join('/mnt/eMB_db/', company_name, model_number, 'tas_company.db')
        conn, cur   = conn_obj.sqlite_connection(db_file)
        stmt = "select doc_id, page_no, row_id, table_dict, table_type from table_dict_phcsv_info where row_id in (%s)"%(', '.join(table_ids))
        cur.execute(stmt)
        res = cur.fetchall()
        cur.close()
        conn.close()
        hgh_map_d   = {}
        for row in res:
            doc_id, page_no, row_id, cell_dict, table_type = map(str, row[:])
            celldata = eval(cell_dict)
            for key, cell_info in celldata.items():
                section_type = cell_info.get('section_type', '')
                if section_type not in ['HGH', 'VGH']:continue
                txml_ids = cell_info.get('text_ids', [])
                for ii, txid in enumerate(txml_ids):
                    txml_ids[ii] = txid.replace('$', '#')
                xml_id = '#'.join(txml_ids)
                hgh_map_d[str(row_id)+':$$:'+xml_id]   = ' '.join(cell_info['text_lst'])
        return hgh_map_d
                




    def update_relation_formula_value(self, ijson, res, ph_formula_d):
        company_name    = ijson['company_name']
        model_number    = ijson['project_id']
        deal_id         = int(ijson['deal_id'])
        reported        = ijson['reported']
        import data_builder.db_data as db_data
        obj = db_data.PYAPI()
        all_table_types = {}
        table_type  = ijson['table_type']
        grpid       = ijson.get('grpid')
        ftype_d = {}
        index_d = {}
        for ti, rr in enumerate(res[0]['data']):
            index_d[str(rr['t_id'])]    = ti
            t_row_f_all = ph_formula_d.get(('ALL_REL F', str(rr['t_id'])), {})
            if not t_row_f_all:continue
            #print t_row_f_all.keys()
            for f_taxos, t_row_f in t_row_f_all.items():
                print f_taxos
                if not t_row_f:continue
                for ft in t_row_f[1]:
                    print ft
                    if ft['type'] != 'v':
                        if ft['op'] != '=':
                            if ft['op'] == 'M':
                                found_M = 1
                            if ft['t_type'] != ijson['table_type'] or grpid != ft['g_id']:
                                print 'YES'
                                ftype_d.setdefault(rr['t_id'], {})[(ft['to_dealid'], ft['t_type'], ft['g_id'], ft['txid'])] = 1
                                all_table_types.setdefault(ft['to_dealid'], {}).setdefault(ft['t_type'], {}).setdefault(ft['g_id'], {})[ft['txid']]  = 1
        m_tables, rev_m_tables, doc_m_d, table_type_m = self.get_main_table_info(company_name, model_number)
        dealijson   = self.read_company_info({"cids":map(lambda x:str(x), all_table_types.keys())+[str(deal_id)]})
        ttype_d     = {}
        display_name_d  = {}
        phs_d           = {}
        all_phs         = {}
    
        for to_dealid, tinfo in all_table_types.items():
            tmpjson    = copy.deepcopy(dealijson['%s_%s'%(ijson['project_id'], to_dealid)])
            tmpjson['project_name']    = ijson['project_name']
            tmpjson['model_number']    = ijson['model_number']
            tmpjson['store_flg']        = ijson.get('store_flg', '')
            tmpjson['norm_scale']        = ijson.get('norm_scale', 'Y')
            tmpjson                    = self.get_year(tmpjson)
            tmpjson['reported'] =  reported
            #if table_type in tinfo:
            #    del tinfo[table_type]
            self.get_table_type_data(tinfo, tmpjson, company_name, model_number, doc_m_d, obj, tmpjson, to_dealid, display_name_d, ttype_d, phs_d, all_phs)
        for k, v_dict in ftype_d.items():
            rr  = res[0]['data'][index_d[str(k)]]
            for v in v_dict.keys():
                #print k, v
                if v[:3] not in ttype_d:continue
                row = ttype_d[v[:3]][str(v[3])]
                #print row
                for ph in res[0]['phs']:
                    if ph['k'] not in rr:continue
                    pk  = phs_d[v[:3]].get(ph['n'], '')
                    if pk not in row:continue
                    val   = numbercleanup_obj.get_value_cleanup(row[pk]['v'])
                    val1   = numbercleanup_obj.get_value_cleanup(rr[ph['k']]['v'])
                    cval    = val
                    cval1    = val1
                    if val and val1 and val != val1:
                        scal1   = row[pk]['phcsv']['s']
                        scal2   = rr[ph['k']]['phcsv']['s']
                        #print [ph['n'], val, val1, scal1, scal2]
                        if scal1 != scal2:
                            scales  = map(lambda x:(sconvert_obj.num_obj.get(sconvert_obj.scale_map_d.get(x, ''), ''), x), [scal1, scal2])
                            scales.sort(key=lambda x:x[0])
                            if scales[-1] == scal2:
                                c_val, factor   = sconvert_obj.convert_frm_to_1(scal1, scal2, val, ())
                                if factor:
                                    val = c_val.replace(',','')
                            else:
                                c_val, factor   = sconvert_obj.convert_frm_to_1(scal2, scal1, val1, ())
                                if factor:
                                    val1 = c_val.replace(',', '')
                            
                        val = round(float(val))
                        val1 = round(float(val1))
                        #print [ph['n'], val, val1]
                        if abs(val) == abs(val1):
                            if val < 0 and val1 > 0:
                                rr[ph['k']]['v']    = '-'+str(cval1)
                            
                            elif val > 0 and val1 < 0:
                                rr[ph['k']]['v']    = str(cval1).strip('-')
                                
                            #rr[ph['k']]['v']    = row[pk]['v']
                            #print 'UPDATE ', [row[pk]['v']]
                    
            
            pass

    def create_final_output_with_ph(self, ijson,ph_filter='P', restated_model='Y', DB_DATA_D={}):
        ijson['restated_model'] = restated_model
        cinfo           = ijson.get('g_cinfo', {})
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        consider_pts     = {}
        if cinfo.get('derived_ph', ''):
            for ph in cinfo['derived_ph'].split('##'):
                consider_pts[ph] = 1
        #print 'consider_pts', consider_pts
        if restated_model == 'Y':
            ijson['reported']   = 'N'
        else:
            ijson['reported']   = 'Y'
        norm_scale  = str(ijson.get('norm_scale'))
        #if str(deal_id) in ['37', '39']:
        #    norm_scale  = 'Y'
        print '\t',(ph_filter, restated_model, norm_scale)
        #print cinfo
        #print consider_pts
        #sys.exit()

        if ijson.get('project_name') == 'Financial Model for Financial Institutions':
            self.non_financial_tt   = {}
            


        ph_d        = {}
        path    = "%s/%s/%s/1_1/21/sdata/doc_map.txt"%(self.doc_path, project_id, deal_id)
        if os.path.exists(path):
            fin = open(path, 'r')
            lines   = fin.readlines()
            fin.close()
        else:
            lines   = []
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number)
        doc_d       = {}
        dphs        = {}
        years       = {}
        c_year      = self.get_cyear(lines)
        #start_year  = c_year - int(ijson['year'])
        start_year  = c_year - int(ijson.get('year', 5))
        all_docd    = {}    
        if ph_filter != 'P' and  ijson['table_type'] in rev_m_tables:
            for k in rev_m_tables[ijson['table_type']].keys():
                all_docd[doc_m_d[k]]    = 1
        dphs_t  = {}
        l_ys    = {}
        for line in lines[1:]:
            line    = line.split('\t')
            if len(line) < 8:continue
            line    = map(lambda x:x.strip(), line)
            ph      = line[3]+line[7]
            l_ys[int(line[7])]    = 1
            if ijson.get('consider_docs', []):
                if line[0] in ijson.get('consider_docs', []):
                    doc_id  = line[0]
                    doc_d[doc_id]   = (ph, line[2])
                    dphs.setdefault(ph, {})[doc_id]        = 1
                    years[ph[-4:]]       = {}
                continue

            print 'PH', [ph]
            if ph and start_year<int(ph[-4:]):
                doc_id  = line[0]
                doc_d[doc_id]   = (ph, line[2])
                dphs.setdefault(ph, {})[doc_id]        = 1
                years[ph[-4:]]       = {}
                if doc_id in all_docd:
                    dphs_t.setdefault(ph, {})[doc_id]        = 1
        l_ys    = l_ys.keys()
        l_ys.sort()
        l_y   = str(l_ys[-1])
        y   = str(c_year)
        #print 'FY'+y not in dphs, 'Q4'+y in dphs
        #sys.exit()
        ''' This will happen while forming derived cols
        if 'FY'+y not in dphs and 'Q4'+y in dphs:
            del dphs['Q4'+y]
        if 'H1'+y not in dphs and 'Q2'+y in dphs:
            del dphs['Q2'+y]
        '''
            
        dphs_ar         = report_year_sort.year_sort(dphs.keys())
        dphs_ar.reverse()
        if ph_filter != 'P':
            tdphs_ar         = report_year_sort.year_sort(dphs_t.keys())
            tdphs_ar.reverse()
        else:
            tdphs_ar    = dphs_ar
        ignore_doc_ids  = {}
        if ph_filter != 'P':
            rem = int(ph_filter.split('-')[1])
            new_phs = tdphs_ar[rem:]
            for ph in tdphs_ar[:rem]:
                ignore_doc_ids.update(dphs_t[ph])
                for doc_id in dphs_t[ph].keys():
                    del doc_d[doc_id]
                del dphs_t[ph]

        ph_f_map, ph_grp_ar, ph_gid_map         = self.read_ph_formula(ijson, 'Y')

        group_id    = ijson.get('grpid', '')
        report_pt   = ijson.get('report_pt', '')
        report_year = ijson.get('report_year', '')
        es_pt   = ijson.get('es_p', '')
        es_yr = ijson.get('es_yr', '')
        #if '-' in group_id:
        #    group_id, doc_grpid = group_id.split('-')
        #    #ijson['grpid']  = group_id

        if ijson.get('custom') != 'Y' and ijson['table_type'] not in rev_m_tables and (ijson['table_type'] not in self.kpi_types and not ijson.get('template_id')):
            return [{"message":'done',"ID":0}]
        final_d         = {}
        table_mapping_d = {
                    }
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn         = env.begin()
        
        #for table_type in rev_m_tables.keys():
        #    if ijson.get('table_types', []) and table_type not in ijson.get('table_types', []):continue
        key_map = self.key_map
        key_map_rev     = {}
        key_map_rev_t   = {}
        for k, v in key_map.items():
            key_map_rev[v]  = k
            key_map_rev_t[v]  = v
        t_ids       = ijson.get('t_ids', [])
        report_map_d    = {
                            'Q1'    : {'Q1':1},
                            'FY'    : {'FY':1},
                            'Q2'    : {'Q2':1},
                            'Q3'    : {'Q3':1},
                            'Q4'    : {'Q4':1},
                            'H1'    : {'H1':1, 'Q2':1},
                            'H2'    : {'H2':1, 'Q4':1, 'FY':1},
                            'M9'    : {'M9':1, 'Q3':1},
                            }
        order_d     = self.order_d
        table_name_mp_d = {
                            '216' :{
                                        'STD'   : 'Debt coming due in the next two years',
                                        'LTD'   : 'Debt',
                                        'OLSE'  : 'Rent Expense',
                                        'COGS'  : 'Cash and Cash equivalent',
                                        'COSS' : 'Stock based compensation', 
                                        'RNTL'    : 'Revolver Availability',
                                    },
                              '218':      {
                                        'RNTL':'Revolver Availability',
                                        'COGSS':'Plan Benefit Obligation',
                                        'STD':'Debt coming due in the next two years',
                                        'LTD':'Debt',
                                        'OLSE':'Rent Expense'
                                    },

                              '221':      {
                                        'STD':'Debt coming due in the next two years',
                                        'LTD':'Debt',
                                        'OLSE':'Rent Expense'
                                    },
                               '220':{
                                        'COGS':'Employee Benefit Plans',
                                        'STD': 'Debt coming due in the next two years', 
                                        'LTD':'Debt',
                                        'OLSE':'Rent Expense',
                                        'CLSE':'Capital Lease Obligation'
                                    },
                            '217' :{
                                        'STD'   : 'Debt coming due in the next two years',
                                        'LTD'   : 'Debt',
                                        'OLSE'  : 'Rent Expense',
                                    },
                            '219' :{
                                        'COGSS' : 'Plan Benefit Obligation',
                                        'LTD'   : 'Debt', 
                                        'STD' : 'Debt coming due in the next two years', 
                                        'RNTL'   : 'Revolver Availability', 
                                    },
                            '83'    :{
                                        'FE'    : 'Finance Income/Expense',
                                    }
                                

                            }
        gen_type    = ijson.get('type','')
        grp_mad_d   = {}
        table_type  = ijson['table_type']
        db_file     = self.config['cinfo']
        conn, cur   = conn_obj.sqlite_connection(db_file)
        
        sql = "CREATE TABLE IF NOT EXISTS model_id_map(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_type TEXT, gen_id INTEGER, description TEXT, user_name VARCHAR(100), datetime TEXT);"
        cur.execute(sql)
        conn.commit()
        sql = "select gen_id, description from model_id_map where table_type='%s'"%(table_type)
        cur.execute(sql)
        res = cur.fetchone()
        gen_id, description = '', ''
        if res:
            try:
                gen_id  = int(res[0])
            except:pass
            if res[1]:
                description = res[1]
        disp_name   = table_name_mp_d.get(deal_id, {}).get(table_type, table_type_m.get(table_type, table_type))
        if gen_id == '':
            if table_type in self.order_d:
                gen_id  = self.order_d[table_type]
            else:
                with conn:
                    sql = "select seq from sqlite_sequence WHERE name = 'model_id_map'"
                    cur.execute(sql)
                    r       = cur.fetchone()
                    if r:
                        g_id    = int(r[0])
                        sql     = "select max(gen_id) from model_id_map" 
                        cur.execute(sql)
                        r       = cur.fetchone()
                        tg_id   = int(r[0])
                        g_id    = max(g_id, tg_id)
                        g_id    += 1
                    else:
                        g_id    = 1
                    gen_id  = g_id
            cur.executemany('insert into model_id_map(gen_id, description, table_type)values(?,?,?)', [(gen_id, disp_name, table_type)])
            conn.commit()
        conn.close()
        #if description:
        #    disp_name   = description
            

        if 1:#gen_type == 'group':
            db_file         = self.get_db_path(ijson)
            conn, cur   = conn_obj.sqlite_connection(db_file)
            crt_qry = 'CREATE TABLE IF NOT EXISTS tt_group_display_name_info(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_type TEXT, group_id TEXT, group_name TEXT, display_name TEXT)'
            cur.execute(crt_qry)
            conn.commit()
            read_qry = 'SELECT table_type, group_id, display_name FROM tt_group_display_name_info;'
            cur.execute(read_qry)
            table_data = cur.fetchall()

            tt_group_disp_nm_info = {}
            for row in table_data:
                ttable_type, tgroup_id, display_name = row[:]
                if display_name:
                    if tgroup_id:
                        tt_group_disp_nm_info.setdefault(ttable_type, {})[str(tgroup_id)] = display_name
                    else:
                        tt_group_disp_nm_info.setdefault(ttable_type, {})[''] = display_name
            if tt_group_disp_nm_info.get(table_type, {}).get(group_id):
                tdisp_name   = tt_group_disp_nm_info.get(table_type, {}).get(group_id)
                ijson['m_disp_name']    = tdisp_name
            if tt_group_disp_nm_info.get(table_type, {}).get(''):
                disp_name   = tt_group_disp_nm_info.get(table_type, {}).get('')
            sql         = "select table_id  from clean_actual_value_status where table_type='%s'"%(table_type)
            try:
                cur.execute(sql)
                res = cur.fetchall()
            except:
                res = []

            actual_v_d  = {}
            for rr in res:
                table_id    = str(rr[0])
                actual_v_d[table_id]    = 1     
            sql = "select vgh_group_id, doc_group_id, group_txt from vgh_doc_map where table_type='%s'"%(table_type)
            vgh_doc_map = {}
            try:
                cur.execute(sql)
                res = cur.fetchall()
            except:
                res = []
            for rr in res:
                vgh_group_id, doc_group_id, group_txt   = rr
                vgh_doc_map[(vgh_group_id, doc_group_id)]   = group_txt
            sql = "select row_id, table_type, group_txt from f_vgh_group_info where table_type='%s'"%(table_type)
            try:
                cur.execute(sql)
                res = cur.fetchall()
            except:
                res = []
            grp_info    = {}
            for rr in res:
                tgroup_id, table_type, group_txt = rr
                group_txt = group_txt.replace('\t', '')
                grp_info[str(tgroup_id)]   = group_txt
            sql = "select row_id, table_type, group_txt from f_vgh_group_info where table_type like '%s'"%('HGHGROUP-'+table_type+'%')
            try:
                cur.execute(sql)
                res = cur.fetchall()
            except:
                res = []
            for rr in res:
                #print rr
                tgroup_id, table_type, group_txt = rr
                group_txt = group_txt.replace('\t', '')
                grp_info[str(table_type)]   = group_txt
            #sql         = "select row_id, group_txt from vgh_group_map where table_type='%s'"%(table_type)
            #cur.execute(sql)
            #res = cur.fetchall()
            #for rr in res:
            #    if rr[1] not in grp_mad_d:
            #        grp_mad_d[rr[1]]    = len(grp_mad_d.keys())
            conn.close()
            tmp_map = {}    
            #grpids  = grp_info.keys()
            #grpids.sort(key=lambda x:int(x))
            #for ii, grp in enumerate(grpids):
            #    tmp_map[grp_info[grp]]  = ii
            #grp_mad_d   = tmp_map
        def filter_ph_by_year(ph, start_year):
            #print [ph, start_year]
            try:
                year    = int(ph['n'][-4:])
            except:
                return True
            if start_year < year:
                return True
            #print 'FALSE'
            return False
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        stend_ar    = self.read_start_end(ijson, 'Y')[0]['data']
        cfstend_d = {}
        for rr in stend_ar:
            cfstend_d[rr['b']] = ('E', rr['b'])
            cfstend_d[rr['e']] = ('S', rr['e'])
            cfstend_d.setdefault('pairs', []).append((rr['b'], rr['e']))
            
        for table_type in [ijson['table_type']]:
            #disp_name   = table_name_mp_d.get(deal_id, {}).get(table_type, table_type_m.get(table_type, table_type))
            ijson['gen_output'] = 'Y'
            ijson['ignore_doc_ids'] = ignore_doc_ids
            tmpkey  = (table_type, ijson.get('grpid', ''), ph_filter)
            if ijson.get('grp_doc_types'):
                tmpkey  = (table_type, ijson.get('grpid', ''), ph_filter, '_'.join(ijson['grp_doc_types']))
            if tmpkey in DB_DATA_D:
                res = copy.deepcopy(DB_DATA_D[tmpkey])
            elif ijson.get('DB_DATA', []):
                res = ijson['DB_DATA']
            else:
                res                 = self.create_seq_across(ijson)
                #print res[0]['phs']
                DB_DATA_D[tmpkey]    = copy.deepcopy(res)
            INPUT_DB_DATA   = ()
            if 'INPUT_DB_DATA' in res[0]:
                INPUT_DB_DATA   = res[0]['INPUT_DB_DATA']
                del res[0]['INPUT_DB_DATA']
            if res[0]['message'] != 'done':
                return res
            if not res[0]['data'] or not res[0]['phs']:
                return res
            for th_flg, tids in res[0].get('th_flg_d', {}).items():
                for ti in tids.keys():
                    rr  = res[0]['data'][ti]
                    for ph in res[0]['phs']:
                        if ph['k'] in rr:
                            clean_value = rr[ph['k']]['v']
                            try:
                                clean_value = numbercleanup_obj.get_value_cleanup(clean_value)
                            except:
                                clean_value = ''
                                pass
                            if clean_value == '':continue
                            sign        = '+'   
                            if '-' in clean_value:
                                sign    = '-'   
                            if sign == '-':
                                if rr.get('th_flg') == 'P':
                                    clean_value = clean_value.strip('-') 
                                    rr[ph['k']]['v'] = clean_value
                                pass
                            else:
                                if rr.get('th_flg') == 'N':
                                    clean_value = '-'+clean_value 
                                    rr[ph['k']]['v'] = clean_value
                
            if ijson.get('t_ids', []) and 0:
                tmp_ar  = []
                i_ids   = ijson.get('t_ids', [])
                for rr in res[0]['data']:
                    if rr['t_id'] < 0:
                        f   = 0
                        for tid in rr['org_t_id'].keys():
                            if tid in i_ids:
                                f   =1
                                break
                        if f == 0:continue
                    elif  rr['t_id'] not in i_ids:continue
                    tmp_ar.append(rr)
                res[0]['data']  = tmp_ar
            all_tids    = {}
            for rr in res[0]['data']:
                all_tids[str(rr['t_id'])]    = 1
            ph_formula_d                            = self.read_ph_user_formula(ijson, '', {}, all_tids, ['Ignore_taxo'] if group_id else  [])
            if group_id:
                ph_formula_d                            = self.read_ph_user_formula(ijson, group_id, ph_formula_d, all_tids)
            #if ijson.get('PRINT') == 'Y':
            self.update_relation_formula_value(ijson, res, ph_formula_d)
            #sys.exit()
            all_table_types = {}
            years_d = {}
            form_d  = {}
            #print 'RES', res
            for ti, rr in enumerate(res[0]['data']):
                if t_ids and rr['t_id'] not in t_ids:continue
                t_row_f     = ph_formula_d.get(('SYS F', str(rr['t_id'])), ())
                ttype   = ''
                op_d        = {}
                if t_row_f:
                    op_d    = ph_formula_d.get(("OP", t_row_f[0]), {})
                    ttype   = 'SYS'
                if not t_row_f:
                    t_row_f     = ph_formula_d.get(('F', str(rr['t_id'])), ())
                form_d[str(rr['t_id'])]  = 1
                if t_row_f:
                    taxonomy    = str(rr['t_id'])
                    t_row_f_all = ph_formula_d.get(('ALL_F', taxonomy), {})
                    if len(t_row_f_all.keys()) > 1:
                        for f_taxos, t_row_ft in t_row_f_all.items():
                            for ft in t_row_ft[1]:
                                if ft['type'] != 'v':
                                    if ft['t_type'] != ijson['table_type'] or (ijson.get('grpid') and ft['g_id'] != ijson.get('grpid')):
                                        all_table_types.setdefault(ft['to_dealid'], {}).setdefault(ft['t_type'], {}).setdefault(ft['g_id'], {})[ft['txid']]  = 1
                                    elif ft['op'] != '='  and ft['type'] != 'v':
                                        form_d.setdefault(('OP', str(ft['txid'])), {})[str(rr['t_id'])]  = 1
                            
                    else:
                        for ft in t_row_f[1]:
                            if ft['type'] != 'v':
                                if ft['t_type'] != ijson['table_type'] or (ijson.get('grpid') and ft['g_id'] != ijson.get('grpid')):
                                    all_table_types.setdefault(ft['to_dealid'], {}).setdefault(ft['t_type'], {}).setdefault(ft['g_id'], {})[ft['txid']]  = 1
                                elif ft['op'] != '='  and ft['type'] != 'v':
                                        form_d.setdefault(('OP', str(ft['txid'])), {})[str(rr['t_id'])]  = 1
                for flg in  ['CELL F', 'PTYPE F']:
                    for cell_id, t_row_f in ph_formula_d.get((flg, str(rr['t_id'])), {}).items():
                        t_kpi_taxo_f_d  = {}
                        for ft in t_row_f[1]:
                            if ft['type'] != 'v' and (ft['t_type'] != ijson['table_type'] or (ijson.get('grpid') and ft['g_id'] != ijson.get('grpid'))):
                                all_table_types.setdefault(ft['to_dealid'], {}).setdefault(ft['t_type'], {}).setdefault(ft['g_id'], {})[ft['txid']]  = 1
                            elif ft['op'] != '=' and ft['type'] != 'v':
                                        form_d.setdefault(('OP', str(ft['txid'])), {})[str(rr['t_id'])]  = 1
            tmpt_ids    = []
            ig_tids = {}
            for t_id in t_ids:
                ig_tids[str(t_id)] = 1
            #print form_d
            #print ijson
            if not ijson.get('offset_year'):
                for k in form_d.keys():
                    #print
                    #print k
                    if isinstance(k, tuple) and k[0] == 'OP' and k[1] not in ig_tids:
                        tmpt_id = k[1]
                        if not tmpt_id:continue
                        ig  = 0
                        for tmptid in form_d[('OP', tmpt_id)].keys():
                            #print '\t', [tmpt_id]
                            if tmptid in ig_tids:
                                ig = 1
                                break
                        if ig == 0:continue
                        tmpt_ids.append(int(tmpt_id))
            #sys.exit()
            t_ids   = t_ids+tmpt_ids
            ijson['t_ids']  = t_ids
            
            phs_d           = {}
            display_name_d  = {}
            ttype_d         = {}
            dealijson   = self.read_company_info({"cids":map(lambda x:str(x), all_table_types.keys())+[str(deal_id)]})
            #print 'dealijson', dealijson
            display_name_d[(int(deal_id), table_type, ijson.get('grpid', ''))]    = disp_name
            #print '\t all_table_types ', all_table_types
            if all_table_types and table_type not in self.kpi_types and ijson.get('AS_REPORTED_VIEW') != 'Y':
                import data_builder.db_data as db_data
                obj = db_data.PYAPI()
                for to_dealid, tinfo in all_table_types.items():
                 tmpjson    = copy.deepcopy(dealijson['%s_%s'%(ijson['project_id'],to_dealid)])
                 tmpjson['project_name']    = ijson['project_name']
                 tmpjson['model_number']    = ijson['model_number']
                 tmpjson                    = self.get_year(tmpjson)
                 for ttable_type, gid in tinfo.items():
                    #g_ar    = self.create_group_ar(ijson_c, txn_m)
                    g_ar    = obj.read_all_vgh_groups(ttable_type, company_name, model_number, doc_m_d, {})
                    g_d     = {}
                    for rr in g_ar:
                        g_d[rr['grpid']] =  rr
                    for grpid, tids in gid.items():
                        print (to_dealid, ttable_type, grpid)
                        ijson_c = copy.deepcopy(tmpjson)
                        ijson_c['table_type'] = ttable_type
                        ijson_c['template_id'] = ''
                        tres = []
                        if grpid == '':
                            ijson_c['table_type']   = ttable_type
                            if ijson.get('gen_output') != 'Y':
                                ijson_c['c_tids']       = tids.keys()
                            ijson_c['type']         = 'display'
                            if ijson_c.get('grpid'):
                                del ijson_c['grpid']
                            #print json.dumps(ijson_c)
                            if (ttable_type, grpid, restated_model, ph_filter) in DB_DATA_D:
                                tres    = DB_DATA_D[(ttable_type, grpid, restated_model, ph_filter)]
                            else:
                                tres = self.create_final_output(ijson_c, {})
                            if ijson.get("PRINT") != "Y":
                                disableprint()
                        elif grpid in g_d:
                            ijson_c['table_type']   = ttable_type
                            if ijson.get('gen_output') != 'Y':
                                ijson_c['c_tids']       = tids.keys()
                            ijson_c['grpid']        = grpid
                            ijson_c['vids']         = g_d[grpid]['vids']
                            ijson_c['type']         = 'display'
                            if (ttable_type, grpid, restated_model, ph_filter) in DB_DATA_D:
                                tres    = DB_DATA_D[(ttable_type, grpid, restated_model, ph_filter)]
                            else:
                                tres = self.create_final_output(ijson_c, {})
                            if ijson.get("PRINT") != "Y":
                                disableprint()
                        if tres:
                            if tres[0]['message'] != 'done':
                                #print '\t', (ttable_type, grpid, res[0]['message'] )
                                if tres[0]['message'] == 'Error PHS not available':continue
                                if 'res' not in tres:
                                    return tres
                                tres = tres[0]['res']
                            if tres  and 'data' in tres[0]:
                                display_name_d[(to_dealid, ttable_type, grpid)]    = tres[0]['disp_name']
                                for rr in tres[0]['data']:
                                    ttype_d.setdefault((to_dealid, ttable_type, grpid), {})[str(rr['t_id'])]   = rr
                                for ph in tres[0]['phs']:
                                    phs_d.setdefault((to_dealid, ttable_type, grpid), {})[ph['n']] = ph['k']
                                    phs_d.setdefault((to_dealid, ttable_type, grpid), {})[ph['k']] = ph['k']
                
            if norm_scale == 'Y' and table_type not in self.kpi_types:
                import ignore_scale_conv
                self.ignore_scale_conv  = ignore_scale_conv.ignore_scale_conv
                iktup   = (str(deal_id), table_type)
                scale_info_m    = ijson.get('scale_info_m', {})
                if 0:#not scale_info_m:
                    v_scale_info    = res[0].get('v_scale_info', {})
                    scale_info_m    = {}
                    for k, v in v_scale_info.items():
                        scales  = map(lambda x:(sconvert_obj.num_obj[sconvert_obj.scale_map_d[x]], x), v.keys())
                        scales.sort(key=lambda x:x[0])
                        scale_info_m[k] = scales[0][1]
                #print 'scale_info_m ',scale_info_m
                    
                taxo_value_grp  = {}
                for ti, rr in enumerate(res[0]['data']):
                    taxo_value_grp[str(rr['t_id'])] = ti
                #print 'normalize '
                for ti, rr in enumerate(res[0]['data'][::-1]):
                    #print '\n', [rr['t_id'], rr['t_l']]
                    #if self.ignore_scale_conv.get(iktup, {}).get(str(rr['t_id'])):continue
                    if rr.get('m_scale') and rr.get('m_cy'):continue
                    t_row_f     = ph_formula_d.get(('F', str(rr['t_id'])), ())
                    op_d    = {}
                    ttype   = ''
                    if not t_row_f:
                        t_row_f     = ph_formula_d.get(('SYS F', str(rr['t_id'])), ())
                    form_ar = []
                    if t_row_f:
                        form_ar = filter(lambda x:x.get('op') != '=' and x['type'] != 'v', t_row_f[1])
                    if not form_ar:continue
                    if filter(lambda x:x.get('op') in {'/':1, '*':1} and x['type'] != 'v', form_ar):continue
                    scale_d     = {}
                    vt_types    = {}
                    f_val       = 0
                    currency_d  = {}
                    valid_scale = {}
                    for ph in res[0]['phs']:
                        if ph['k'] not in rr:continue
                        if not ph['n'] :continue
                        if str(rr[ph['k']]['t']) in actual_v_d:continue
                        clean_value = rr[ph['k']]['v']
                        phcsv   = rr[ph['k']]['phcsv']
                        try:
                            clean_value = numbercleanup_obj.get_value_cleanup(clean_value)
                        except:
                            clean_value = ''
                            pass
                        if clean_value == '':
                            s, vt,cy   = phcsv['s'], phcsv['vt'], phcsv['c']
                            scale_d.setdefault(s, {})[ph['k']]  = '0'
                            continue
                        if clean_value == '':
                            clean_value = '0'
                        f_val      = 1
                        s, vt,cy   = phcsv['s'], phcsv['vt'], phcsv['c']
                        if cy in cconvert_obj.scale_map_d:
                            currency_d[cy]  = 1
                        vt_types[vt]    = 1
                        if not s:# and (phcsv['vt'] != 'Other'):
                            #print rr['t_l'], ph,[clean_value], rr[ph['k']]['phcsv']
                            #sys.exit()
                            if ijson.get('err_flg') != 'Y':
                                return [{"message":"Empty Scale %s "%(rr[ph['k']]['t']), 'data':[]}]
                            continue
                        valid_scale[s] = 1
                        scale_d.setdefault(s, {})[ph['k']]  = float(clean_value)
                    #if f_val == 0:continue
                    m_scale = ''
                    if scale_info_m and not self.ignore_scale_conv.get(iktup, {}).get(str(rr['t_id'])):
                        err = self.update_scale_inrow(rr, res[0]['phs'], m_scale, table_type, ijson, scale_info_m, actual_v_d)
                    if not scale_info_m and (not self.ignore_scale_conv.get(iktup, {}).get(str(rr['t_id']))):
                        if len(scale_d.keys()) > 1 and not rr.get('m_scale'):
                            scales  = map(lambda x:(sconvert_obj.num_obj.get(sconvert_obj.scale_map_d.get(x, ''), ''), x), scale_d.keys())
                            vmscale = filter(lambda x:x[1] in valid_scale, scales)
                            if vmscale:
                                scales  = vmscale
                            scales.sort(key=lambda x:x[0])
                            m_scale = scales[0][1]
                            #print scales
                            #print m_scale
                            err = self.update_scale_inrow(rr, res[0]['phs'], m_scale, table_type, ijson, {}, actual_v_d)
                            if err:
                                return err
                        else:
                            m_scale = scale_d.keys()[0]
                    if m_scale and t_row_f and not scale_info_m and (not self.ignore_scale_conv.get(iktup, {}).get(str(rr['t_id']))):# and not rr.get('m_scale'):
                        err = self.update_scale_GChain(m_scale, [(m_scale, form_ar)], res[0]['data'], res[0]['phs'], ph_formula_d, taxo_value_grp, table_type, ijson, {str(rr['t_id']):1}, scale_info_m, actual_v_d)
                        if err:
                            return err
                    if currency_d:
                        if len(currency_d.keys()) > 1:
                            cys  = map(lambda x:(cconvert_obj.num_obj[cconvert_obj.scale_map_d[x]], x), currency_d.keys())
                            cys.sort(key=lambda x:x[0])
                            m_cy = cys[0][1]
                            err = self.update_currency_inrow(rr, res[0]['phs'], m_cy, table_type, ijson, actual_v_d)
                            if err:
                                return err
                        else:
                            m_cy = currency_d.keys()[0]
                        if t_row_f and m_cy:
                            err = self.update_currency_GChain(m_cy, [(m_cy, form_ar)], res[0]['data'], res[0]['phs'], ph_formula_d, taxo_value_grp, table_type, ijson, {str(rr['t_id']):1}, actual_v_d)
                            if err:
                                return err
                for ti, rr in enumerate(res[0]['data']):
                    #if self.ignore_scale_conv.get(iktup, {}).get(str(rr['t_id'])):continue
                    scale_d     = {}
                    vt_types    = {}
                    if rr.get('m_scale') and rr.get('m_cy'):continue
                    currency_d  = {}
                    for ph in res[0]['phs']:
                        if ph['k'] not in rr:continue
                        if not ph['n'] :continue
                        clean_value = rr[ph['k']]['v']
                        try:
                            clean_value = numbercleanup_obj.get_value_cleanup(clean_value)
                        except:
                            clean_value = ''
                            pass
                        if clean_value == '':continue
                        phcsv   = rr[ph['k']]['phcsv']
                        s, vt, cy   = phcsv['s'], phcsv['vt'], phcsv['c']
                        if cy in cconvert_obj.scale_map_d:
                            currency_d[cy]  = 1
                        if vt:
                            vt_types[vt]    = 1
                        if not s:# and phcsv['vt'] != 'Other':
                            if ijson.get('err_flg') != 'Y':
                                return [{"message":"Empty Scale %s "%(rr[ph['k']]['t']), 'data':[]}]
                            continue
                            #print rr['t_l'], [clean_value],ph, rr[ph['k']]['phcsv']
                            #sys.exit()
                        scale_d.setdefault(s, {})[ph['k']]  = float(clean_value)
                    if scale_info_m and (not self.ignore_scale_conv.get(iktup, {}).get(str(rr['t_id']))):
                        m_scale = ''
                        err = self.update_scale_inrow(rr, res[0]['phs'], m_scale, table_type, ijson, scale_info_m, actual_v_d)
                    #print 'RRRRRRRRRRR', rr['t_id'], rr['t_l']   , [rr['m_scale']]
                    if not scale_info_m and (not self.ignore_scale_conv.get(iktup, {}).get(str(rr['t_id']))):
                        if len(scale_d.keys()) > 1 and not  rr.get('m_scale'):
                            #print scale_d
                            #print sconvert_obj.scale_map_d
                            #print sconvert_obj.num_obj
                            scales  = map(lambda x:(sconvert_obj.num_obj.get(sconvert_obj.scale_map_d.get(x, ''), ''), x), scale_d.keys())
                            scales.sort(key=lambda x:x[0])
                            m_scale = scales[0][1]
                            err = self.update_scale_inrow(rr, res[0]['phs'], m_scale, table_type, ijson, {}, actual_v_d)
                            if err:
                                return err
                            pass
                        elif len(scale_d.keys()) == 1:
                            rr['m_scale']   = scale_d.keys()[0]
                    if vt_types:
                        rr['value_type']   = vt_types.keys()[0]
                    #print 'CY ', [rr['t_l'], currency_d]
                        
                    if len(currency_d.keys()) > 1 and not rr.get('m_cy'):
                        cys  = map(lambda x:(cconvert_obj.num_obj[cconvert_obj.scale_map_d[x]], x), currency_d.keys())
                        cys.sort(key=lambda x:x[0])
                        m_cy = cys[0][1]
                        err = self.update_currency_inrow(rr, res[0]['phs'], m_cy, table_type, ijson, actual_v_d)
                        if err:
                            return err
                    elif len(currency_d.keys()) == 1 and currency_d.keys()[0] in cconvert_obj.scale_map_d:
                        rr['m_cy']  = currency_d.keys()[0]
            std_num = ''
            years   = {}
            for ph in res[0]['phs']:
                #print '\tph', ph
                if ph['n']:
                    try:
                        years_d[int(ph['n'][-4:])]  = 1
                        years[ph['n'][-4:]]   = 1
                        if 'STD' in ph['n'][:-4]:
                            std_num = 'Y'
                            
                    except:pass
            #print 'std_num', std_num
            if std_num == 'Y':
                new_d   = {}
                for k, v in self.report_map_d.items():
                    #print k, v
                    if 'STD' in k:continue
                    k = 'STD'+k
                    new_d[k]   = {}
                    for k1 in v.keys():
                        new_d[k]['STD'+k1]   = 1
                self.report_map_d.update(new_d)
                #print self.report_map_d
                        
                    
            #tyears   = years_d.keys()
            #tyears.sort()
            #y   = str(tyears[-1])
            '''
            if 'FY'+y not in dphs and 'Q4'+y in dphs:
                del dphs['Q4'+y]
            if 'H1'+y not in dphs and 'Q2'+y in dphs:
                del dphs['Q2'+y]
            '''
            if ijson.get('NO_FORM', '') == 'Y':
                ph_f_map        = {}
                if not ijson.get('consider_docs', []):
                    ph_formula_d    = {}
            t_ids   = []
            if not ijson.get('consider_docs', []):
                res[0]['phs']       = filter(lambda x:filter_ph_by_year(x,start_year), res[0]['phs'] )
            if ijson.get('RETURN_TXT', []) == 'Y' and not res[0]['phs']:
                return {}
            if not res[0]['phs']:
                return [{'message':'Error PHS not available'}]
            
            mdata               = self.create_maturity_date(ijson, txn_m)
            mat_date            = {}
            if mdata:
                if '-' in group_id:
                    group_id_sp = group_id.split('-')
                    if group_id_sp[1] in mdata:
                        mat_date            = mdata[group_id_sp[1]]
                elif '' in mdata:
                    mat_date            = mdata['']
            if mat_date:
                mat_date    = self.read_mat_date(ijson, mat_date, txn_m)
            f_pc_d  = {}
            if 1:#(ijson.get('project_name', '').lower() != 'schroders'): # FLIP Sign
                ph_afftected_tids   = {}
                xml_row_map         = {}
                xml_col_map         = {}
                taxo_id_dict    = {}
                clean_v_d       = {}
                row_ind_map     = {}
                csv_d   = {}
                taxo_value_grp  = {}
                #print '###########################', res[0]['data']
                for ti, rr in enumerate(res[0]['data']):
                    taxo_id_dict[str(rr['t_id'])]    = rr
                    taxo_value_grp[str(rr['t_id'])]  = ti
                    ttype_d.setdefault((int(deal_id), ijson['table_type'], ijson.get('grpid', '')), {})[str(rr['t_id'])]    = rr
                    for ph in res[0]['phs']:
                        if ph['k'] in rr:
                            clean_value = numbercleanup_obj.get_value_cleanup(rr[ph['k']]['v'])
                            clean_v_d.setdefault(ti, {})[ph['k']]   = clean_value
                tmpres  = copy.deepcopy(res)
                f_k_d   = {}
                t_op_d, flip_sign_d, done_form_d, f_pc_d   = self.formula_derivations(tmpres[0]['data'], tmpres, tmpres[0]['phs'],ph_formula_d, xml_row_map, xml_col_map, txn, t_ids, taxo_id_dict, taxo_value_grp, clean_v_d, f_k_d, key_map_rev, key_map_rev_t, disp_name, row_ind_map, csv_d, 'DN', phs_d, ttype_d, ijson, display_name_d, dealijson, ph_afftected_tids, f_pc_d)    
                #print 'INN ', (ijson['table_type'], ijson.get('grpid'))
                tmpflip_sign_d  = self.flip_formula_sign(tmpres[0]['data'], tmpres, tmpres[0]['phs'], clean_v_d, taxo_value_grp, ijson)
                for ti, pkd in tmpflip_sign_d.items():
                    for pk, (clean_value, flip_sign) in pkd.items():
                        #print (ti, pk, flip_sign,  clean_value)
                        if res[0]['data'][ti].get(pk, {}).get('v'):
                            res[0]['data'][ti][pk]['v'] = clean_value
                        
                            if flip_sign == 'Y':
                                    res[0]['data'][ti][pk]['flip_sign'] = flip_sign
                #if ijson.get('EXIT') == 'Y':
                #    sys.exit()
                #for ti, rr in enumerate(res[0]['data']):
                #    for ph in res[0]['phs']:
                #       if rr.get(ph['k'], {}).get('f_col'):
                #            del rr[ph['k']]['f_col']
                #sys.exit()
            flip_d  = {}
            ignore_taxo = {}
            t_re_stated_cols  = {}
            remove_not_stated   = ijson.get('remove_not_restated', [])
            if restated_model == 'Y' and table_type not in self.kpi_types:# and (ijson.get('FV', '') == 'Y' or (table_type == 'FE' and deal_id=='36')):
                t_res   = self.get_restated_value(copy.deepcopy(res), ph_formula_d, t_ids, txn_m, mat_date, txn, {}, actual_v_d, dphs, ijson, ph_f_map, years, ph_gid_map, y, key_map_rev, key_map_rev_t, disp_name, table_type, group_id, doc_d, restated_model, report_pt, report_year, phs_d, ttype_d, display_name_d, dealijson, f_pc_d)
                tt_res, tt_data, tt_f_phs, tt_f_phs_d, tttaxo_value_grp, t_re_stated_cols,t_taxo_id_dict    = t_res
                if table_type not in self.kpi_types:
                    for rr in tt_data:
                        f   = 0
                        for ph in tt_f_phs:
                            if ph['k'] in rr and rr[ph['k']].get('NO_RESTAT', '') != 'Y' or  ph['ph']  in remove_not_stated:
                                f   = 1
                                break
                        if f == 0 and rr['t_id'] >=0:
                            ignore_taxo[rr['t_id']] = 1
                    
                
                if isinstance(t_res, list) and t_res[0]['message'] != 'done':
                    return t_res
                #flip_d  = self.flip_sign(t_res, ph_formula_d, table_type)
            #else:
            #ignore_taxo = {}
            #print ignore_taxo
            #print 'all_tids', all_tids
            #sys.exit()
            #print 'AAAAAAAAAAAAAAAAAAAAAAAAAAA', [res[0]['data']]
            #print 'BBBBBBBBBBBBBBBBBBBBBBBBBBB', [res[0]['phs']]
            if ignore_taxo:
                res[0]['data']  = filter(lambda x:x['t_id'] not in ignore_taxo, res[0]['data'])
                all_tids    = {}
                for rr in res[0]['data']:
                    all_tids[str(rr['t_id'])]    = 1
                ph_formula_d                            = self.read_ph_user_formula(ijson, '', {}, all_tids)
                if group_id:
                    ph_formula_d                            = self.read_ph_user_formula(ijson, group_id, ph_formula_d, all_tids)
                if ijson.get('NO_FORM', '') == 'Y':
                    ph_f_map        = {}
                    ph_formula_d    = {}
                    
            
                    
            org_res             = copy.deepcopy(res)
            phs                 = copy.deepcopy(res[0]['phs'])
            data                = copy.deepcopy(res[0]['data'])
            #print 'DATA', data
            #print 'PHS', phs
                    
            validate_op         = {}
            rc_d                = {}
            consider_ks         = {}
            consider_table      = {}
            consider_rows       = {}
            xml_row_map         = {}
            ph_kindex           = {}
            for ii, ph in enumerate(phs):
                ph_kindex[ph['k']]  = ii
            duplicate_taxonomy  = {}
            phk_data            = {}
            phk_ind_data        = {}
            validation_error    = {}
            #print validate_op
            clean_v_d       = {}
            ph_ti               = {}
            taxo_value_grp  = {}
            taxo_id_dict    = {}
            csv_d   = {}
            xml_col_map = {}
            row                 = 1
            row_ind_map         = {}
            taxo_unq_d          = {}
            table_xml_d         = {}
            more_than_one_scale = {}
            rsp_phs             = {}
            pt_order_d          = {}
            pt_arr         = report_year_sort.year_sort(['FY2018', 'Q12018', 'Q22018', 'Q32018', 'Q42018', 'H12018', 'H22018', 'M92018'])
            for ii, ph in  enumerate(pt_arr):
                pt_order_d[ph[:-4]]          = ii
                
            cinfo   = self.read_company_info({"cids":[str(deal_id)]})[int(deal_id)]
            org_cname   = self.convert_html_entity(cinfo['org_company_name'])
            org_cname   = org_cname.lower()
            map_d   = {'kiz gmbh schlu00fcsselfertiges bauen bautru00e4ger':'kiz gmbh schl\xfcsselfertiges bauen bautr\xe4ger'}
            org_cname   = map_d.get(org_cname, org_cname)
            org_cname   = ''.join(org_cname.split())
            bds_taxo_map    = {}
            if 0 and  'bds' in ijson['project_name'].lower():# and ijson.get('type') != 'display':
                bds_taxo_map    = self.get_bds_taxo_map(org_cname)
            if report_pt and report_year:
                #print pt_arr,[report_pt,report_year]
                for ii, ph in enumerate(phs):
                    if len(ph['n']) < 6:continue
                    pt  = ph['n'][:-4]
                    py  = ph['n'][-4:]
                    try:
                        py  = int(py)
                    except:
                        continue
                    #print [ph['n'], pt, py, pt_order_d.get(pt, -1), pt_order_d[report_pt]]
                    if (py < report_year) or  (py == report_year and pt_order_d.get(pt, -1) <= pt_order_d[report_pt]):
                        #print 'YES'
                        rsp_phs.setdefault(ph['n'], []).append(ph['k'])
                #for k, v in rsp_phs.items():
                #    print k, v
            rep_docids  = {}
            for ti, rr in enumerate(data[:]):
                taxo_id_dict[str(rr['t_id'])]    = rr
                taxo_value_grp[str(rr['t_id'])]  = ti
                ttype_d.setdefault((int(deal_id), ijson['table_type'], ijson.get('grpid', '')), {})[str(rr['t_id'])]    = rr
                ttype_d.setdefault((int(deal_id), ijson['table_type'], ijson.get('grpid', '')), {})[('IDX', str(rr['t_id']))]    = ti
                t_s_f       = ph_formula_d.get(('S', str(rr['t_id'])), {})
                #if t_ids and rr['t_id'] not in t_ids:continue
                taxo    = rr.get('taxo', '')
                tmptaxo = taxo.split(' / ')
                f_taxo  = {}
                for tt in tmptaxo:
                    for t1 in tt.split('@'):
                        if 'TASTAXO_' in t1 or 'TAS_TAXO' in t1:continue
                        if ' - ' in t1:
                            f_taxo[t1.split(' - ')[1]]  = 1
                        else:
                            f_taxo[t1]  = 1
                f_taxo  = f_taxo.keys()
                f_taxo.sort()
                f_taxo  = ' / '.join(f_taxo[:1])
                tf_taxo    = self.gen_taxonomy_alpha_num(rr['t_l'])
                if not f_taxo:
                    f_taxo    = self.gen_taxonomy(rr['t_l'])
                if not f_taxo:
                    f_taxo    = rr['t_l']
                if not tf_taxo:
                    tf_taxo    = rr['t_l']
                duplicate_taxonomy.setdefault(tf_taxo, {})[rr['t_id']]   = rr['t_l']
                if f_taxo not in taxo_unq_d:
                    taxo_unq_d[f_taxo]  = 1
                else:
                    ol_cnt  = taxo_unq_d[f_taxo]
                    taxo_unq_d[f_taxo]  = ol_cnt+1
                    f_taxo  += '-'+str(ol_cnt)
                tktup   = (org_cname, ''.join(self.remove_index_num(rr['t_l']).lower().split()))
                if bds_taxo_map:
                        #print tktup
                        if tktup in bds_taxo_map:
                            f_taxo  = bds_taxo_map[tktup]
                rr['f_taxo']      = f_taxo
                rr['taxo']      = [f_taxo, bds_taxo_map.get(tktup)]
                res[0]['data'][ti]['f_taxo']      = f_taxo
                res[0]['data'][ti]['taxo']      = bds_taxo_map.get(tktup)
                row += 1
                row_ind_map[ti] = row
                scale_d         = {}
                xml_col_map.setdefault(ti, {})
                crncy_d             = {}
                for ii, ph in enumerate(phs):
                    if ph['k'] not in rr:continue
                    v_d         = rr[ph['k']]
                    rep_docids[str(v_d['d'])]  = 1
                    t           = v_d['v']
                    clean_value = t
                    try:
                        clean_value = numbercleanup_obj.get_value_cleanup(t)
                        if clean_value:
                            clean_value = self.convert_floating_point(clean_value).replace(',', '')
                    except:
                        clean_value = ''
                        pass
                    if not clean_value and v_d['t'] in actual_v_d:
                        clean_value = v_d['v']
                    x_c    = self.clean_xmls(v_d['x'])
                    if x_c:
                        table_id    = v_d['t']
                        x_c    = x_c.split(':@:')[0].split('#')[0]
                        table_xml_d.setdefault(table_id, {})[(int(x_c.split('_')[1].split('@')[0].split('$')[0]), int(x_c.split('_')[0].strip('x').strip('DUMM')))]   = 1
                    if mat_date:
                        tk   = self.get_quid(v_d['t']+'_'+v_d['x'])
                        c_id        = txn_m.get('XMLID_MAP_'+tk)
                        if c_id:
                            ktup    = (rr['t_id'], v_d['t'], int(c_id.split('_')[1]))
                            if ktup in mat_date:
                                v_d['mdate']    = mat_date[ktup][2]
                                res[0]['data'][ti][ph['k']]['mdate']    = mat_date[ktup][2]
                                v_d.setdefault('phcsv', {})['mdate']    =  mat_date[ktup][2]
                                res[0]['data'][ti][ph['k']].setdefault('phcsv', {})['mdate']    = mat_date[ktup][2]
                    xml_row_map.setdefault((v_d['t'], v_d['x']), {})[ti]    = 1
                    xml_col_map.setdefault(ti, {})[(v_d['t'], v_d['x'])]         = ph['k']
                    key     = v_d['t']+'_'+self.get_quid(v_d['x'])
                    period_type, period, currency, scale, value_type    = v_d['phcsv']['p'], v_d['phcsv']['pt'], v_d['phcsv']['c'], v_d['phcsv']['s'], v_d['phcsv']['vt']
                    #ph_map  = txn.get('PH_MAP_'+str(key))
                    #if ph_map:
                    #    period_type, period, currency, scale, value_type    = ph_map.split('^')
                    crncy_d.setdefault(currency, {})[ph['k']]   = 1
                    if ph['k'] in t_s_f and clean_value:
                        sgrpid, form    = t_s_f[ph['k']]
                        v, factor   = sconvert_obj.convert_frm_to_1(form, scale, clean_value)
                        if v and factor != '':
                            v   =str(v)
                            clean_value = v
                            rr[ph['k']]['expr_str']   = form+'('+str(rr[ph['k']]['v'])+') = '+str(v)
                            rr[ph['k']]['v']    = v
                            rr[ph['k']]['fv']   = 'Y'
                            res[0]['data'][ti][ph['k']]['expr_str']   = form+'('+str(rr[ph['k']]['v'])+') = '+str(v)
                            res[0]['data'][ti][ph['k']]['v']    = v
                            res[0]['data'][ti][ph['k']]['fv']   = 'Y'
                            csv_d[(v_d['t'], v_d['x'])]    = (period_type, period, currency, self.scale_map_rev_d[nscale], value_type)
                    if (v_d['t'], v_d['x']) not in csv_d:
                        period_type, period, currency, scale, value_type    = v_d['phcsv']['p'], v_d['phcsv']['pt'], v_d['phcsv']['c'], v_d['phcsv']['s'], v_d['phcsv']['vt']
                        csv_d[(v_d['t'], v_d['x'])]    = (period_type, period, currency, scale, value_type)
                    if v_d['v'].strip():
                        scale_d.setdefault(csv_d[(v_d['t'], v_d['x'])][3], {})[v_d['rid']]         = 'N'
                            
                    clean_v_d.setdefault(ti, {})[ph['k']]   = clean_value
                    ph_ti.setdefault(ph['n'], {})[ti]  = ph['k']
                    phk_data.setdefault(ph['k'], []).append((ti, clean_value))
                    if 1:#v_d['v']:
                        phk_ind_data.setdefault(ph['k'], {})[ti]    = clean_value
                    if clean_value == '':continue
                    rr['data_exists']   = 'Y'
                    #if not t_ids or rr['t_id'] in t_ids:
                    #    if validate_op.get(rr['t_id'], {}).get(ph['k'], ''):
                    #        validation_error.setdefault(rr['t_id'], {})[ph['k']]    = 'N'
                if len(scale_d.keys()) > 1:
                    more_than_one_scale[rr['t_id']] = {}
                    for sc, rid in scale_d.items():
                        more_than_one_scale[rr['t_id']].update(rid)
                if len(crncy_d.keys()) == 2 and 0:
                    cks     = crncy_d.keys()
                    cks.sort(key=lambda x:len(crncy_d[x].keys()), reverse=True)
                    ctup    = (cks[0], cks[1])
                    final_cur   = ctup[0]
                    m_cur   = ''
                    if ctup in self.within_crncy_d:
                        if self.within_crncy_d[ctup][0] == ctup[1]:
                            m_cur       =  ctup[0]
                            final_cur   = ctup[1]
                        else:
                            m_cur       =  ctup[1]
                            final_cur   = ctup[0]
                    if m_cur:
                        v_c = self.within_crncy_d[(final_cur, m_cur)][1]
                        for pk in crncy_d[m_cur]:
                            v_d     = rr[pk]
                            clean_value = clean_v_d[ti][pk]
                            if clean_value:
                                tv      = self.convert_floating_point(float(clean_value)*v_c)
                                v_d['v']= tv
                                res[0]['data'][ti][pk]['v']= tv
                                clean_v_d[ti][pk]  = tv.replace(',', '')
                                csv_tup = csv_d[(v_d['t'], v_d['x'])]
                                csv_tup = csv_tup[:2]+(final_cur, )+csv_tup[3:]
                                csv_d[(v_d['t'], v_d['x'])] = csv_tup
                                v_d['phcsv']['c']           = final_cur
                                v_d['phcsv']['cinfo']   = map(lambda x:(x, len(crncy_d[x].keys())), cks)
                                res[0]['data'][ti][pk]['phcsv']['cinfo']  = [map(lambda x:(x, len(crncy_d[x].keys())), cks), final_cur, m_cur]
                                res[0]['data'][ti][pk]['phcsv']['c']  = final_cur
            if validation_error and 0:
                res = [{'message':'Error Validation failed', 'data':validation_error}]
                return res
            if 0:#more_than_one_scale:
                res = [{'message':'Error More than one scale', 'data':more_than_one_scale}]
                return res

            if 0:#duplicate_taxonomy:# and gen_type != 'display' :
                tmp_d = {}
                for f_taxo_id, tid_dict in duplicate_taxonomy.items():
                    if len(tid_dict.keys()) > 1:
                        tmp_d.update(tid_dict)
                if tmp_d:
                    res = [{'message':'Error Duplicate taxonomy', 'data':tmp_d}]
                    return res
                        
            f_phs, dphs_all, col_m, tt_ph_f, done_ph, f_phs_d, ph_map_d, reported_phs, d_new_phs = self.form_ph_cols( ijson, phs, dphs, rsp_phs, res,  ph_kindex,phk_ind_data, data, ph_formula_d, taxo_value_grp, y, table_type)
            #print 'FINAL'
            #for ph in f_phs:
            #    print ph
            #sys.exit()
            done_form_d = {}
            if table_type not in self.kpi_types and ijson['project_name'] != 'Direct Owners And Executive Officers':
                f_phs, f_phs_d, n_phs, f_k_d, new_phs, missing_value, t_op_d, done_form_d, latest_year_db  = self.calculate_derivative_values(data, res, ph_formula_d, clean_v_d, col_m, taxo_value_grp, ph_ti, f_phs, f_phs_d, ph_f_map, years, done_ph, ph_gid_map, y, ph_map_d, t_ids, xml_row_map, xml_col_map, txn, taxo_id_dict, key_map_rev, key_map_rev_t, disp_name, table_type, group_id, row_ind_map, csv_d, doc_d, dphs_all, restated_model, phk_ind_data, reported_phs, tt_ph_f, 'N', flip_d, actual_v_d, ijson, t_re_stated_cols, cfstend_d, phs_d, ttype_d, display_name_d, dealijson, f_pc_d)    
            else:
                f_k_d   = {}
                new_phs = []
                missing_value   = {'PKS':{}}
                t_op_d      = {}

            remove_not_stated   = ijson.get('remove_not_restated', [])
            #print 'remove_not_stated ',remove_not_stated
            if ijson.get('g_cinfo', {}).get('ex_d', {}).get(("RFR", table_type)):
                remove_not_stated   = remove_not_stated+ijson['g_cinfo']['ex_d'][("RFR", table_type)]
            elif ijson.get('g_cinfo', {}).get('ex_d', {}).get(("RFR", 'DEAL')):
                remove_not_stated   = remove_not_stated+ijson['g_cinfo']['ex_d'][("RFR", 'DEAL')]
            if not remove_not_stated:
                remove_not_stated   = []
            #print 'remove_not_stated ',remove_not_stated
            #print
            ignored_phs = []
            #print (restated_model == 'Y' and table_type not in self.kpi_types) and (ijson.get('g_cinfo', {}).get('r_as_r') != 'Y')
            if (restated_model == 'Y' and table_type not in self.kpi_types) and (ijson.get('g_cinfo', {}).get('r_as_r') != 'Y'):
                tmpf_phs    = []
                for ph in f_phs:
                    #if ph.get('derived') != 'Y' and ph['ph'][-4:] != l_y and ph['ph'][-4:] != y and (ph['k'] not in t_re_stated_cols and ph['ph'] not in remove_not_stated):
                    if ph.get('derived') != 'Y' and ph['ph'][-4:] != latest_year_db and (ph['k'] not in t_re_stated_cols and ph['ph'] not in remove_not_stated):
                        ignored_phs.append(ph)
                        continue
                    tmpf_phs.append(ph)
                f_phs   = tmpf_phs
            #print 'f_phs', f_phs
            if table_type in self.kpi_types:
                csv_d   = {}

            if  1:#ijson.get('MULTI_FORM') == 'Y':
                top_d    = {}
                for ti, rr in enumerate(data[:]):
                    if rr['t_id'] not in done_form_d:continue
                    #print '\n', [rr['t_id'], rr['t_l']]
                    done_rids   = done_form_d[rr['t_id']]['rids']
                    #print '1 done_rids ', [table_type, done_rids]
                    taxonomy    = str(rr['t_id'])
                    t_row_f     = ph_formula_d.get(('USER F', taxonomy), ())
                    t_row_f_all = ph_formula_d.get(('ALL_USER F', taxonomy), {})
                    fflag       = 'USER F'
                    if not t_row_f:
                        fflag       = 'F'
                        t_row_f     = ph_formula_d.get(('F', taxonomy), ())
                        t_row_f_all = ph_formula_d.get(('ALL_F', taxonomy), {})
                    if t_row_f and t_row_f[1]:
                        for ft in t_row_f[1]:
                            if ft['type'] == 't' and ft['op'] != '=':
                                top_d.setdefault((ft['t_type'], ft['g_id'], ft['txid']), {})[(table_type, group_id, str(rr['t_id']))]   = 1
                        if len(t_row_f_all.keys()) > 1:
                            done_rem  = {}
                            row_form_d  = {}
                            for f_taxos, t_row_ft in t_row_f_all.items():
                                found_other = 0
                                ttype_cl    = {}
                                taxo_cl     = {}
                                found_M     = 0
                                row_form_d[t_row_ft[0].split('RID-')[1]]    = t_row_ft
                                for ft in t_row_ft[1]:
                                    if ft['type'] == 't' and ft['op'] != '=':
                                        top_d.setdefault((ft['t_type'], ft['g_id'], ft['txid']), {})[(table_type, group_id, str(rr['t_id']))]   = 1
                            ph_fd   = {}
                            for ph in f_phs:
                                if ph['k'] not in rr:continue
                                #print ph
                                f_rids   = {}
                                for ft in rr[ph['k']]['f_col']:
                                    f_rids[str(ft[0]['rid'])] = 1
                                #print f_rids, len(rr[ph['k']]['f_col'])
                                for rid, t_row_ft in row_form_d.items():
                                    #print '\trid ', [rid]
                                    if rid in f_rids:continue
                                    ph_fd.setdefault(rid, {})[ph['k']]  = 1
                            for rid, pks in ph_fd.items():
                                tmprr  = copy.deepcopy(rr)
                                res_row = self.read_sys_form_across(row_form_d[rid], tmprr, map(lambda x:{'k':x['k'], 'n':x['ph']}, filter(lambda x:x['k'] in pks, f_phs)), ttype_d, table_type, {}, phs_d, ijson, display_name_d, pks, {}, dealijson, 'Y')
                                #print '\n=========================================='
                                #print [ti, rr['t_id'], rr['t_l'], rid]
                                #print
                                for pk in pks.keys():
                                    #print 'PKKKKKKKKK', pk
                                    rr[pk]['f_col_o'].append(tmprr[pk]['f_col'][0])
                                    #print '\t', pk, len(rr[pk]['f_col_o'])
                                    #for formula in tmprr[pk]['f_col']:
                                    #   #print
                                    #    for ft in formula:
                                    #        print '\t\t', ft
                                    #print '************************************************************************'

            ignore_heaer    = {}
            tignore_taxo = {}
            if not (restated_model == 'Y' and table_type not in self.kpi_types) and  ijson['project_name'] != 'Direct Owners And Executive Officers':# and (ijson.get('FV', '') == 'Y' or (table_type == 'FE' and deal_id=='36')):
                for rr in data:
                    if rr.get('data_exists') == 'Y':
                        f   = 0
                        for ph in f_phs:
                            if rr.get(ph['k'], {}).get('v') or rr.get('th_flg') == 'H':
                                f   = 1
                                break
                        if f == 0 and rr['t_id'] >=0:
                            tignore_taxo[rr['t_id']] = 1
                if tignore_taxo:
                    f_data  = []
                    for ti, rr in enumerate(data):
                        if rr['t_id'] in tignore_taxo:continue
                        if rr['t_id'] < 0:
                            f   = 0
                            for tid in rr['org_t_id'].keys():
                                if tid not in tignore_taxo:
                                    f   =1
                                    break
                            if f == 0:
                                ignore_heaer[ti]    = 1
                                continue
                        f_data.append(rr)
                    #data    = f_data

            tks = tignore_taxo.keys()
            for k in tks:
                #print (table_type, group_id, str(k)), (table_type, group_id, str(k)) in top_d
                if (table_type, group_id, str(k)) in top_d:
                    f_al    = 1
                    for rtup in top_d[(table_type, group_id, str(k))].keys():
                        if table_type not in self.kpi_types:
                            if int(rtup[2]) not in tignore_taxo:
                                f_al    = rtup
                                break
                        else:
                            if rtup[2] not in tignore_taxo:
                                f_al    = rtup
                                break
                    #print 'f_al ', f_al
                    if f_al != 1:
                        del tignore_taxo[k]
                    

                        
            t_ids   = ijson.get('t_ids', [])
            tmpt_ids    = []
            if t_ids:
                for ti, rr in enumerate(res[0]['data'][:]):
                    if rr['t_id'] in t_ids:
                        tmpt_ids.append(rr['t_id'] )
                t_ids   = tmpt_ids
            if ijson.get('t_ids', []):
                tmp_ar  = []
                i_ids   = ijson.get('t_ids', [])
                for ti, rr in enumerate(res[0]['data']):
                    if rr['t_id'] < 0:
                        f   = 0
                        for tid in rr['org_t_id'].keys():
                            if tid in i_ids:
                                f   =1
                                break
                        if f == 0:
                            ignore_heaer[ti]    = 1
                            continue
            #self.calculate_missing_values_by_fx(t_ids, res, data, ph_formula_d, taxo_id_dict, table_type, group_id, row_formula, f_phs)
            run_date_diff   = 0
            if ijson.get('offset_year'): #(table_type, str(deal_id)) in self.run_date_diff:
                run_date_diff   = 1
            #print 'run_date_diff ', run_date_diff, (table_type, str(deal_id), (table_type, str(deal_id)) in self.run_date_diff)
            if run_date_diff == 1:
                data, clean_v_d, f_k_d    = self.add_date_diff_row(ijson, data, f_phs, t_ids, company_name, model_number, clean_v_d, f_k_d, f_phs_d, taxo_unq_d, ijson['offset_year'], form_d)
                res[0]['data']  = data
                #print 'DD ', map(lambda x:x['t_id'], data)
            #if ph['ph'] in self.ignore_phs.get(str(deal_id), {}).get(ijson.get('grpid', ''), {}):continue
            if self.ignore_phs.get(str(deal_id), {}).get(ijson.get('grpid', ''), {}):
                f_phs   = filter(lambda x:x['ph'] not in self.ignore_phs[str(deal_id)][ijson['grpid']], f_phs)
            ignore_dphs = ijson.get('ignore_dphs', [])
            if ignore_dphs:
                f_phs   = filter(lambda x:x['ph'] not in ignore_dphs, f_phs)
                     

            n_y                 = str(int(y)+1)
            row                 = 1
            update_sign         = ''
            if 0:#ijson.get('project_name', '').lower() == 'schroders' and restated_model !='Y':# and ijson.get('FV', '') == 'Y':
                update_sign         = 'Y'
                self.flip_formula_sign(data, res, f_phs, clean_v_d, taxo_value_grp)
                if ijson.get('EXIT') == 'Y':
                    xxxxxx
            f_col_k = {}
            all_table_ids   = {}
            for ti, rr in enumerate(data[:]):
                for ii, ph in enumerate(f_phs):
                    if ph['k'] in rr:
                        f_col_k[ ph['k']]   = 1
                        if t_ids and rr['t_id'] not in t_ids:continue
                        if  rr['t_id'] in tignore_taxo:continue
                        all_table_ids[rr[ph['k']]['t']] = 1
            hgh_text_map    = {}
            if 'bds' in ijson['project_name'].lower() and 0:
                hgh_text_map    = self.read_cell_dict_HGH_data(company_name, model_number, all_table_ids)
                
            #print
            #for ph in f_phs:
            #    print ph
                    
            if ijson.get('derivation_cal') != 'Y' and table_type not in self.kpi_types:
                #for x in f_phs:
                #    print x
                #    print '\t', (x['k'] in f_col_k or x.get('reported') == 'Y' or x.get('RS_COL') == 'Y'), (not consider_pts or x['ph'][:-4] in consider_pts)
                f_phs   = filter(lambda x:(x['k'] in f_col_k or x.get('reported') == 'Y' or x.get('RS_COL') == 'Y') and (not consider_pts or x['ph'][:-4] in consider_pts), f_phs)
            #print
            #for ph in f_phs:
            #    print '\tPH ', ph
            #print 'AFTER'
            #print
            #for ph in f_phs:
            #    print ph
        
            pt_value_e          = {}
            row_ind_map = {}
            tmprowid    = 2
            for ti, rr in enumerate(data[:]):
                if t_ids and rr['t_id'] not in t_ids:continue
                #if (t_ids and rr['t_id'] not in t_ids) and (not ijson.get('template_ids') or str(rr['t_id']) not in ijson.get('template_ids')):continue
                if  rr['t_id'] in tignore_taxo:continue
                row_ind_map[ti] = tmprowid
                tmprowid    += 1
            tmpdisp_name    = disp_name
            if gen_type == 'all':
                tmpdisp_name   = ijson.get('m_disp_name', tmpdisp_name)
            elif gen_type == 'group':
                if 'HGHGROUP' in ijson['grpid']:
                    grpid_sp    = ijson['grpid']
                    grp_name    = grp_info[grpid_sp]
                    tmpdisp_name   = tmpdisp_name+' - '+grp_name
                elif '-' in ijson['grpid']:
                    grpid_sp    = ijson['grpid'].split('-')
                    grp_name    = grp_info[grpid_sp[0]]+' - '+grp_info[grpid_sp[1]]
                    if (grpid_sp[0], grpid_sp[1]) in vgh_doc_map:
                        grp_name    = vgh_doc_map[(grpid_sp[0], grpid_sp[1])]
                    tmpdisp_name   = tmpdisp_name+' - '+grp_name
                else:
                    tmpdisp_name   = tmpdisp_name+' - '+grp_info[ijson['grpid']] #ijson['data'][0]
            
                #final_d[str(gen_id*100)+ijson['grpid']] = (binascii.b2a_hex(disp_name)+':@@:'+ph_filter, {'data':rc_d, 'key_map':key_map_rev, 'table_type':main_header}, res, disp_name)
                disp_name   = ijson.get('m_disp_name', disp_name)
                tmpdisp_name   = ijson.get('m_disp_name', tmpdisp_name)
            display_name_d[(int(deal_id), table_type, ijson.get('grpid', ''))]    = tmpdisp_name
            #print display_name_d
            #if ijson.get('type') != 'display':sys.exit()

            if es_pt and es_yr:
                li = map(lambda x:x+'2018', report_year_sort.phs) #['FY2018', 'Q12018', 'Q22018', 'Q32018', 'Q42018', 'H12018', 'H22018', 'M92018']
                li_order    = report_year_sort.year_sort(li)
                li_order    = map(lambda x:x[:-4], li_order)
                cidx        = li_order.index(es_pt)
                idx = -1
                #print li_order, [es_pt, es_yr], [cidx]
                tmpf_phs    = copy.deepcopy(f_phs)
                t_phs       = map(lambda x:x['ph'], tmpf_phs)
                t_phs       = report_year_sort.year_sort(t_phs)
                tmpf_phs.sort(key=lambda x:t_phs.index(x['ph']))
                for i, ph in enumerate(tmpf_phs):
                    #print '\t', [i, ph['ph'], li_order.index(ph['ph'][:-4]), cidx]
                    if li_order.index(ph['ph'][:-4]) >= cidx and int(ph['ph'][-4:]) >= int(es_yr):
                        idx = i
                        break
                if idx != -1:
                        f_phs   = tmpf_phs[idx:]
                        f_phs.reverse()
                    
            
            #print 't_ids ', t_ids
            #sys.exit()
            if ijson['project_name'] == 'Direct Owners And Executive Officers':
                tmphs   = []
                for i, ph in enumerate(res[0]['phs']):
                    tph = copy.deepcopy(ph)
                    num_p   = str(1000+i+1)
                    #num_p   = '0'*(4-len(num_p))+num_p
                    tph['ph']   = 'Group-%s'%(num_p) #ph['n']
                    ph['n']     = tph['ph']
                    tmphs.append(tph)
                f_phs   = tmphs[:]
            for ti, rr in enumerate(data[:]):
                #print '\n============================'
                #print rr['t_id'], rr['t_l']
                #if t_ids and rr['t_id'] not in t_ids:continue
                #if (t_ids and rr['t_id'] not in t_ids) and (not ijson.get('template_ids') or str(rr['t_id']) not in ijson.get('template_ids')):continue
                if  rr['t_id'] in tignore_taxo:continue
                #print '1'
                row += 1
                col = 0
                f_taxo  = rr['f_taxo']
                dd  = {
                        key_map_rev['actual_value']     : rr['t_l'],
                        key_map_rev['taxo']             : f_taxo,
                        key_map_rev['clean_value']      : '',
                        key_map_rev['d']                : rr.get('d', ''),
                        key_map_rev['table_id']         : rr.get('t', ''),
                        key_map_rev['xml_id']           : rr.get('x', ''),
                        key_map_rev['pno']              : rr.get('x', '_1').split(':@:')[0].split('#')[0].split('_')[1].split('@')[0],
                        key_map_rev['bbox']             : rr['bbox'],
                        key_map_rev['h_or_t']           : rr.get('th_flg', ''),
                        key_map_rev['taxo_id']          : rr['t_id'],
                        key_map_rev['data_available']   : rr.get('da', 'N'),
                        key_map_rev['mnemonic_id']      : rr.get('mnemonic_id', ''),
                        key_map_rev['TASNODE']      : rr.get('n_node', ''),
                        }
                if str(rr['t_id']) in ijson.get('template_ids', {}):
                    dd[key_map_rev['available_in_template']]    = 'Y'
                x_key   = rr.get('t', '')+':$$:'+rr.get('x', '')
                if x_key in hgh_text_map:
                    hgh_txt_d   = hgh_text_map[x_key]
                    dd[key_map_rev['translated_text']] = dd[key_map_rev['actual_value']]
                    dd[key_map_rev['actual_value']] = hgh_txt_d #['v']
                if 'level_id' in rr:
                    dd[key_map_rev['level_id']]    = rr['level_id']
                lchange = []
                for lc in rr.get('ldata', []):
                    if lc['s'] == 'Y' and lc['x']:
                        periods = report_year_sort.year_sort(lc['v'].keys())
                        periods.reverse()
                        ydd  = {key_map_rev['actual_value']:lc['txt'], key_map_rev['d']:lc['d'],  key_map_rev['bbox']:lc['bbox'], key_map_rev['pno']:lc['x'].split(':@:')[0].split('#')[0].split('_')[1].split('@')[0], key_map_rev['period']:periods[0], key_map_rev['ph']:periods}
                        x_key   =lc['t']+':$$:'+lc['x']
                        if x_key in hgh_text_map:
                            hgh_txt_d   = hgh_text_map[x_key]
                            ydd[key_map_rev['translated_text']] = ydd[key_map_rev['actual_value']]
                            ydd[key_map_rev['actual_value']] = hgh_txt_d #['v']
                        lchange.append(ydd)
                dd[key_map_rev['label change']]  = lchange
                if ((not t_ids or rr['t_id'] in t_ids) and (rr['t_id'] not in tignore_taxo)) or (ijson.get('template_ids') and str(rr['t_id']) in ijson.get('template_ids')):
                    rc_d[(row, col)]    = dd
                tmpphs              = [] 
                key_value           = clean_v_d.get(ti, {})
                ph_value_d          = {}
                indx_d              = {}
                #xml_col_map         = {}
                ph_map              = {}
                for ii, ph in enumerate(phs):
                    if ph['k'] not in rr:continue
                    #ph_map[ph['k']]              = ph['ph']
                    v_d = rr[ph['k']]
                    t       = v_d['v']
                    clean_value = clean_v_d.get(ti, {}).get(ph['k'], '')
                    tmpphs.append(ph)
                    #key_value[ph['k']]       = clean_value
                    #print [ph['n'], clean_value, ph['k']]
                    ph_value_d.setdefault(ph['n'], {}).setdefault(clean_value, {})[ph['k']]          = 1
                    indx_d[ph['k']]              = ii
                #f_phs       = self.get_order_phs(tmpphs, dphs, report_map_d)
                #r_key, rs_value, operand_rows, row_form, row_op = self.read_formula(ti, rr, f_phs, txn, xml_row_map, xml_col_map)
                #print '\n============================='
                #print rr['t_l']
                rs_value    = {}
                f_ph_val    = 0
                for ii, ph in enumerate(f_phs):
                    col += 1
                    if not rr.get(ph['k'], {}).get('v') and  rr.get('th_flg', '') == 'H':continue
                    if ph['k'] in rr:
                        #if rr[ph['k']].get('NO_RESTAT') == 'Y':
                        #    del res[0]['data'][ti][ph['k']]
                        #    continue
                        consider_ks[ph['k']]         = 1
                    #print '\t', [ph, rr.get(ph['k'], {}).get('v'), rr.get(ph['k'], {}).get('kpi_re_stated_all')]
                    if ph['k'] not in rr:
                        if ph['ph'] in dphs and key_value.get(ph['k'], '') == '' and restated_model=='Y':
                            trs_value       = rr.get(ph['k'], {}).get('rest_ar', [])
                            trs_value_all   = rr.get(ph['k'], {}).get('rest_ar_all', [])
                            #other_values            = ph_value_d.get(ph['ph'], {});
                            #trs_value    = []
                            #for v in other_values.keys():
                            #    if v != key_value.get(ph['k'], ''):
                            #        for key in other_values[v].keys():
                            #                if 1:#indx_d[key] > c_index:
                            #                    trs_value.append(key)
                            
                            if trs_value:
                                dd      = self.get_gv_dict({'v':'','x':'','bbox':[], 'd':'', 't':''}, txn, key_map_rev, ph, csv_d)
                                trs_value.sort(key=lambda x:(dphs_all.index(doc_d[rr[x]['d']][0]), rr[x]['d']))
                                self.add_restated_values(ti, rr, {'v':'','x':'','bbox':[], 'd':'', 't':''}, dd, res, trs_value, key_map_rev, key_map_rev_t, ph, txn, dphs_ar, doc_d, csv_d, actual_v_d, dphs, ijson, ph['dph'], table_type, rep_docids, 'Y')
                                rc_d[(row, col)]    = dd
                            if trs_value_all:
                                dd1     = self.get_gv_dict({'v':'','x':'','bbox':[], 'd':'', 't':''}, txn, key_map_rev, ph, csv_d)
                                dd1     = copy.deepcopy(dd1)
                                trs_value_all.sort(key=lambda x:(dphs_all.index(doc_d[rr[x]['d']][0]), rr[x]['d']))
                                self.add_restated_values(ti, rr, {'v':'','x':'','bbox':[], 'd':'', 't':''}, dd1, res, trs_value_all, key_map_rev, key_map_rev_t, ph, txn, dphs_ar, doc_d, csv_d, actual_v_d, dphs, ijson, ph['dph'], table_type, rep_docids, 'N')
                                if dd1.get(key_map_rev['re_stated']):
                                    dd[key_map_rev['re_stated_all']]    = dd1[key_map_rev['re_stated']]
                                    rr[ph['k']]['re_stated_cell']   =  'Y'
                                    rr[ph['k']]['re_stated_all']   = dd1[key_map_rev['re_stated']]
                                    res[0]['data'][ti][ph['k']]['re_stated_all']   = dd1[key_map_rev['re_stated']]
                        continue
                    v_d                     = rr[ph['k']]
                    if v_d.get('pos_formula'):
                        dd      = self.get_gv_dict({'v':'','x':'','bbox':[], 'd':'', 't':''}, txn, key_map_rev, ph, csv_d)
                        dd[key_map_rev['available_formula']]   = {}
                        if 0:
                            tmpfrm_ar   = []
                            map_d           = {'DIRECT':"Available", "WITHIN":"Derived", "ACROSS":"Base Number"}
                            for findex, frm_d in enumerate(v_d['pos_formula']):
                                tmp_f_ar    = []
                                f_exists    = 0
                                not_found_op    = 0
                                for ft in frm_d['data']:
                                    if ft['type'] == 't':
                                        val_d   = {}
                                        if INPUT_DB_DATA:
                                            pk = INPUT_DB_DATA[2].get((to_dealid, ft['tt'], ft['grpid']), {}).get(ph['ph'])   
                                            if pk and pk in INPUT_DB_DATA[1][(to_dealid, ft['tt'], ft['grpid'])][ft['txid']]:
                                                for k in ['v', 'd', 'x', 'phcsv', 'bbox', 't']:
                                                  if k in INPUT_DB_DATA[1][(to_dealid, ft['tt'], ft['grpid'])][ft['txid']][pk]: #error for deal 28
                                                    val_d[k]    = INPUT_DB_DATA[1][(to_dealid, ft['tt'], ft['grpid'])][ft['txid']][pk][k]
                                        new_ft  = val_d
                                        for kkk in ['i_f_type', 'type', 'description', 'to_dealid', 'taxo_id', 'tt', 'grpid', 'operator', 'disp_name']:
                                            new_ft[kkk] = ft.get(kkk, '')
                                        if not val_d.get('v'):
                                            if new_ft['operator'] != '=':
                                                new_ft['not_exists']    = 'Y'
                                                not_found_op    = 1
                                        else:
                                            f_exists    = 1
                                        new_ft['clean_value']   = val_d.get('v', '')
                                        if ft.get('i_f_type') != 'PTYPEFORMULA':
                                            new_ft['ph']            = ph['ph']
                                        else:
                                            new_ft['ph']            = ft.get('ph', '')
                                        tmp_f_ar.append(new_ft)
                                    else:
                                        tmp_f_ar.append(ft)
                                if not_found_op == 0:
                                    print 'GREEN FORMULA NOT Tagged '
                                    for ft in tmp_f_ar:
                                        print '\t', ft
                                    xxxxxxxxxxxxxxxxxxx
        
                                tmprdd  = {'txid':'1', 'type':'t', 't_type':'', 'g_id':'', 'op':'', 'c':'', 's':'','vt':'', 'period':'', 'fyear':'', 'yd':'', 'rid':'', 'to_dealid':'', 'i_f_type':'', 'description':'Formula-%s'%(findex+1), 'c':frm_d['c'], 'f_col':[tmp_f_ar], 'operator':'', 'ph':ph['ph'], 'op_exists':frm_d['mp'], 'availability':'Y'}
                                tmpfrm_ar.append(tmprdd)
                        tmpfinal_d  = {}
                        fdone_d = {}
                        self.get_availability_label(INPUT_DB_DATA, v_d, table_type, ph, int(deal_id), tmpfinal_d, fdone_d)
                        tmpf_ar    = []
                        for ftype in ['Direct Line Items', 'Contributing Line Items']:
                            if ftype not in tmpfinal_d:continue
                            tmprdd  = {'txid':'1', 'type':'t', 't_type':'', 'g_id':'', 'op':'', 'c':'', 's':'','vt':'', 'period':'', 'fyear':'', 'yd':'', 'rid':'', 'to_dealid':'', 'i_f_type':'', 'description':ftype, 'c':'', 'f_col':[tmpfinal_d[ftype].values()], 'operator':'', 'ph':ph['ph'], 'op_exists':'', 'availability':'Y'}
                            tmpf_ar.append(tmprdd)
                        done_resultant  = {}
                        self.add_row_formula_only(ijson, display_name_d, rr, ti, res, [tmpf_ar], ph, dd[key_map_rev['available_formula']], key_map_rev, row_ind_map, taxo_value_grp, done_resultant, 'POS')
                        rc_d[(row, col)]    = dd
                        continue
                    if v_d.get('NO_RESTAT') == 'Y' and clean_v_d[ti][ph['k']] in ['', '0'] and ph['k'] in res[0]['data'][ti]:
                        del res[0]['data'][ti][ph['k']]
                        continue
                    pt, pyear   = ph['ph'][:-4], ph['ph'][-4:]
                    if 0:#v_d.get('phcsv', {}).get('vt') not in ['Percentage'] and (pyear != n_y or rr[ph['k']].get('PH_D', '') == 'Y'):
                        pt_flg, expr_str  = self.validate_pt_value(pt, pyear, key_value, f_phs_d, rr, rr[ph['k']].get('PH_D', ''))
                        if pt_flg:
                            cs_f    = 0
                            cs_ar   = []
                            for p_id in t_op_d.get(ti, {}).keys():
                                cs_ar.append(data[p_id].get(ph['k'], {}).get('cs', ''))
                                if data[p_id].get(ph['k'], {}).get('cs', '') != '':
                                    cs_f == 1
                                    break
                            #pt_value_e.setdefault(rr['t_id'], {})[ph['k']]  = 'Y'
                            #res[0]['data'][ti][ph['k']]['expr_str'] =  res[0]['data'][ti][ph['k']].get('expr_str', '')+' -- '+expr_str+str([t_op_d.get(ti, {}).keys(), cs_ar])
                            if (not t_op_d.get(ti, {}).keys() or cs_f == 1):
                                pt_value_e.setdefault(rr['t_id'], {})[ph['k']]  = 'N'
                                res[0]['data'][ti][ph['k']]['expr_str'] =  res[0]['data'][ti][ph['k']].get('expr_str', '')+' -- '+expr_str+str([t_op_d.get(ti, {}).keys(), cs_ar])
                                pass
                    #if table_type in self.kpi_types and v_d.get('f_col_o', []):
                    f_k_d.setdefault(ti, {})[ph['k']]   = {}
                    if v_d.get('f_col_o', []):
                        #print ph, v_d.get('f_col_o', [])
                        done_resultant  = {}
                        self.add_row_formula_only(ijson, display_name_d, rr, ti, res, v_d.get('f_col_o', []), ph, f_k_d[ti][ph['k']], key_map_rev, row_ind_map, taxo_value_grp, done_resultant, None)
                        if 'f_col_o' in res[0]['data'][ti][ph['k']]:
                            del res[0]['data'][ti][ph['k']]['f_col_o']
                        if 'f_col_o' in v_d:
                            del v_d['f_col_o']
                        res[0]['data'][ti][ph['k']]['g_f']  = 'Y'
                        res[0]['data'][ti][ph['k']]['f']  = 'Y'
                    elif 'f_col' in res[0]['data'][ti].get(ph['k'], {}):
                        del res[0]['data'][ti][ph['k']]['f_col']

                        
                    #r_key[ph['k']]   = 1
                    consider_ks[ph['k']]         = 1
                    rs_value[ph['k']]  = []
                    #other_values            = ph_value_d.get(ph['ph'], {})
                    #c_index                 = indx_d[ph['k']]
                    t                       = v_d['v']
                    consider_table[v_d['t']]       =  1
                    clean_value             = key_value[ph['k']]
                    #if clean_value == '':continue
                    #for v in other_values.keys():
                    #    if v != key_value[ph['k']]:
                    #        for key in other_values[v].keys():
                    #            if 1:#indx_d[key] > c_index:
                    #                if  rr[key]['t'] != v_d['t']:
                    #                    rs_value[ph['k']].append(key)
                    
                    dd      = self.get_gv_dict(v_d, txn, key_map_rev, ph, csv_d, actual_v_d)
                    if 0:# not v_d.get('PH_FORM', []) and v_d.get('FORM_VALUE') != 'Y':
                        x_key   = v_d['t']+':$$:'+v_d['x']
                        if x_key in rr:
                            hgh_txt_d   = rr[x_key]
                            dd[key_map_rev['celldict_text']] = hgh_txt_d['v']
                    trs_value_all   = v_d.get('rest_ar_all', [])
                    if trs_value_all:
                        #print '\t',[ph['k'], trs_value_all, v_d.keys()]
                        #del v_d['rest_ar']
                        dd1 = copy.deepcopy(dd)
                        self.add_restated_values(ti, rr, copy.deepcopy(v_d['org_d']),dd1, res, trs_value_all, key_map_rev, key_map_rev_t, ph, txn, dphs_ar, doc_d, csv_d, actual_v_d, dphs, ijson, ph['dph'], table_type, rep_docids, 'N')
                        #del v_d['org_d']
                        if dd1.get(key_map_rev['re_stated']):
                            dd[key_map_rev['re_stated_all']]    = dd1[key_map_rev['re_stated']]
                            rr[ph['k']]['re_stated_all']   = dd1[key_map_rev['re_stated']]
                            res[0]['data'][ti][ph['k']]['re_stated_all']   = dd1[key_map_rev['re_stated']]
                    #rs_value[ph['k']].sort(key=lambda x:(dphs_ar.index(doc_d[rr[x]['d']][0]), rr[x]['d']))
                    trs_value   = v_d.get('rest_ar', [])
                    if trs_value:
                        #print '\t',[ph['k'], trs_value]
                        del v_d['rest_ar']
                        self.add_restated_values(ti, rr, v_d['org_d'],dd, res, trs_value, key_map_rev, key_map_rev_t, ph, txn, dphs_ar, doc_d, csv_d, actual_v_d, dphs, ijson, ph['dph'], table_type, rep_docids, 'Y')
                        if self.convert_floating_point(numbercleanup_obj.get_value_cleanup(v_d['org_d']['v'])) != self.convert_floating_point(numbercleanup_obj.get_value_cleanup(v_d['v'])):
                            rr[ph['k']]['re_stated_cell']   =  'Y'
                            res[0]['data'][ti][ph['k']]['re_stated_cell']   =  'Y'
                        del v_d['org_d']
                        #res[0]['data'][ti][ph['k']]['re_stated_cell']    ='Y'
                    elif v_d.get('NO_RESTAT') == 'Y' and clean_v_d[ti][ph['k']] not in ['', '0']:
                        self.add_restated_values(ti, rr,  v_d['org_d'],dd, res, [''], key_map_rev, key_map_rev_t, ph, txn, dphs_ar, doc_d, csv_d, actual_v_d, dphs, ijson, ph['dph'], table_type, rep_docids, 'Y')
                        
                    #self.add_row_formula(ti, dd, operand_rows, res, data, ph, key_map_rev, key_map_rev_t, row_op, txn, disp_name, row_ind_map, csv_d)
                    #self.add_column_formula(ti, dd, row_form, res, rr, ph, data, key_map_rev, key_map_rev_t, txn, disp_name, csv_d)
                    #print (v_d['v'], v_d['t'], v_d.get('expr_str', ''), clean_value, (row, col))
                    tmp_fd  = f_k_d.get(ti, {}).get(ph['k'], {})
                    #print 'tmp_fd ',tmp_fd
                    if key_map_rev['f_col'] in tmp_fd:
                        dd[key_map_rev['f_col']] = tmp_fd[key_map_rev['f_col']]
                        if update_sign == 'Y':
                            dd[key_map_rev['f_col']]    = self.update_form_sign(data, taxo_value_grp, dd[key_map_rev['f_col']])
                        res[0]['data'][ti]['f']         = 'Y'
                    if key_map_rev['f_row'] in tmp_fd:
                        dd[key_map_rev['f_row']] = tmp_fd[key_map_rev['f_row']]
                    if v_d.get('PH_FORM', []) and v_d.get('PH_FORM', []):
                        dd[key_map_rev['calc_value_db']]    = 'Y'
                        rr[ph['k']]['calc_value_db']   =  'Y'
                        res[0]['data'][ti][ph['k']]['calc_value_db']    ='Y'
                        done_resultant  = {}
                        #print '\n',[rr['t_l'], ph]
                        for ft in v_d['PH_FORM']:
                            #print '\t',ft
                            ft['grpid'] = ijson.get('grpid','')
                            ft['disp_name'] = tmpdisp_name
                            ft['clean_value']    = ft.get('v', '')
                        rr[ph['k']]['FORM_VALUE']   = 'Y'
                        res[0]['data'][ti][ph['k']]['FORM_VALUE']   = 'Y'
                        self.add_row_formula_only(ijson, display_name_d, rr, ti, res, [v_d['PH_FORM']], ph, dd, key_map_rev, row_ind_map, taxo_value_grp, done_resultant, None, 'Y')
                        rr[ph['k']]['f_col'] = [v_d['PH_FORM']]
                        dd[key_map_rev['f_ph_str']]    = v_d.get('PH_FORM_STR', '')
                        rr[ph['k']]['f_ph_str']    = v_d.get('PH_FORM_STR', '')
                        res[0]['data'][ti][ph['k']]['f_ph_str']    = v_d.get('PH_FORM_STR', '')
                        if 0:
                            tmp_f_ar, d_tmp_arr = self.add_ph_formula(v_d.get('PH_FORM', []), data, taxo_value_grp, disp_name, csv_d, key_map_rev_t, txn)
                            if tmp_f_ar:
                                dd[key_map_rev['f_ph_col']]    = [tmp_f_ar]
                                dd[key_map_rev['f_ph_str']]    = v_d.get('PH_FORM_STR', '')
                                rr[ph['k']]['f_ph_str']    = v_d.get('PH_FORM_STR', '')
                                res[0]['data'][ti][ph['k']]['f_ph_str']    = v_d.get('PH_FORM_STR', '')
                                rr[ph['k']]['f_col_org']    = copy.deepcopy(rr[ph['k']].get('f_col', []))
                                res[0]['data'][ti][ph['k']]['f_col_org']    = copy.deepcopy(res[0]['data'][ti][ph['k']].get('f_col', []))
                                
                                rr[ph['k']]['f_col']    = [d_tmp_arr]
                                res[0]['data'][ti][ph['k']]['f_col']    = [d_tmp_arr]
                    if v_d.get('FORM_VALUE') == 'Y' or v_d.get('calc_value_db') == 'Y':
                        dd[key_map_rev['calc_value']]    = 'Y'
                        dd[key_map_rev['calc_value_db']]    = 'Y'
                        rr[ph['k']]['calc_value_db']   =  'Y'
                        res[0]['data'][ti][ph['k']]['calc_value_db']    ='Y'
                    #if t_ids and rr['t_id'] not in t_ids:continue
                    if (t_ids and rr['t_id'] not in t_ids) and (not ijson.get('template_ids') or str(rr['t_id']) not in ijson.get('template_ids')):continue
                    if  rr['t_id'] in tignore_taxo:continue
                    f_ph_val    = 1
                    rc_d[(row, col)]    = dd
                #print 'END '
                if ti in ignore_heaer:continue
                #if t_ids and rr['t_id'] not in t_ids:continue
                if (t_ids and rr['t_id'] not in t_ids) and (not ijson.get('template_ids') or str(rr['t_id']) not in ijson.get('template_ids')):continue
                if  rr['t_id'] in tignore_taxo:continue
                consider_rows[rr['t_id']]       = 1
                if f_ph_val == 1 and rc_d.get((row, 0), {}).get(key_map_rev['h_or_t'], '') == 'H':
                    rc_d[(row, 0)][key_map_rev['h_or_t']]   = ''
                            
                        

            #self.flip_formula_sign(data, res, f_phs)
            table_type          = table_mapping_d.get(table_type, table_type)
            #print 'Result ', len(rc_d.keys())
            #print consider_ks
            #print f_phs
            #for ph in res[0]['phs']:
            #    if ph['k'] in consider_ks:
            #        print ph
            #sys.exit()
            ph_orgd    = {}
            for ph in res[0]['phs']:
                ph_orgd[ph['k']]   = 1
            if ijson.get('store_flg') == 'Y' and ph_filter == 'P':
                tmpres      = copy.deepcopy(res)
                #print 'consider_rows ', map(lambda x:x['t_id'], tmpres[0]['data'])
                #print t_ids
                tmpconsider_rows    = {x['t_id']:1 for x in tmpres[0]['data']}
                tmpconsider_ks      = {x['k']:1 for x in f_phs}
                #print 'tmpconsider_ks ', tmpconsider_ks
                #print 'B phs'
                #for ph in res[0]['phs']:
                #    print '\t', ph
                
                self.filter_phs_taxo(tmpres, tmpconsider_rows, tmpconsider_ks, consider_table, new_phs, d_new_phs, ijson)
                #print 'phs'
                #for ph in tmpres[0]['phs']:
                #    print '\t', ph
            else:
                tmpres  = res
            #print 'consider_rows ',consider_rows
            self.filter_phs_taxo(res, consider_rows, consider_ks, consider_table, new_phs, d_new_phs, ijson)
            main_header = 'Main Table'
            ctmp_d   = {}
            if ijson.get('derivation_cal') == 'Y':
                for ti, ph_d in clean_v_d.items():
                    ctmp_d[data[ti]['t_id']]    = {}
                    for ph in res[0]['phs']:
                        if ph['k'] in data[ti]:
                            ctmp_d[data[ti]['t_id']][ph['k']]   = (ph_d[ph['k']], data[ti][ph['k']]['v'])
            
            if len(res[0]['main_header'])   == 1 and res[0]['main_header'][0]:
                main_header = {'Schroders':'Main Table'}.get(res[0]['main_header'][0], res[0]['main_header'][0])

            if gen_type == 'all':
                disp_name   = ijson.get('m_disp_name', disp_name)
                final_d[gen_id] = (binascii.b2a_hex(disp_name)+':@@:'+ph_filter, {'data':rc_d, 'key_map':key_map_rev, 'table_type':main_header, 'non_template_db':ijson.get('non_template_db', '')}, res, disp_name,)
                if ijson.get('store_flg') == 'Y' and ph_filter == 'P':
                    res[0]['disp_name'] = disp_name
                    tmpres[0]['disp_name'] = disp_name
                    self.clean_preview_data(res, ph_orgd, ijson, restated_model, tmpres)
            elif gen_type == 'group':
                if 'HGHGROUP' in ijson['grpid']:
                    grpid_sp    = ijson['grpid']
                    grp_name    = grp_info[grpid_sp]
                    disp_name   = disp_name+' - '+grp_name
                elif '-' in ijson['grpid']:
                    grpid_sp    = ijson['grpid'].split('-')
                    grp_name    = grp_info[grpid_sp[0]]+' - '+grp_info[grpid_sp[1]]
                    if (grpid_sp[0], grpid_sp[1]) in vgh_doc_map:
                        grp_name    = vgh_doc_map[(grpid_sp[0], grpid_sp[1])]
                    disp_name   = disp_name+' - '+grp_name
                else:
                    disp_name   = disp_name+' - '+grp_info[ijson['grpid']] #ijson['data'][0]
            
                #final_d[str(gen_id*100)+ijson['grpid']] = (binascii.b2a_hex(disp_name)+':@@:'+ph_filter, {'data':rc_d, 'key_map':key_map_rev, 'table_type':main_header}, res, disp_name)
                disp_name   = ijson.get('m_disp_name', disp_name)
                final_d[str(gen_id)+'-'+ijson['grpid']] = (binascii.b2a_hex(disp_name)+':@@:'+ph_filter, {'data':rc_d, 'key_map':key_map_rev, 'table_type':main_header, 'non_template_db':ijson.get('non_template_db', '')}, res, disp_name)
                if ijson.get('store_flg') == 'Y' and ph_filter == 'P':
                    res[0]['disp_name'] = disp_name
                    tmpres[0]['disp_name'] = disp_name
                    self.clean_preview_data(res, ph_orgd, ijson, restated_model, tmpres)
            elif gen_type == 'display':
                #ignored_phs = []
                #if (restated_model == 'Y' and table_type not in self.kpi_types) and (ijson.get('g_cinfo', {}).get('r_as_r') != 'Y'):
                #    for ph in f_phs:
                #        if ph.get('derived') != 'Y' and ph['ph'][-4:] != l_y and ((ph['k'] not in t_re_stated_cols and ph['ph'] not in remove_not_stated) or ph['k'] in  t_re_stated_cols.get('retain_col', {})):
                #            ignored_phs.append(ph)
                #            continue
                if ijson.get('grpid', ''):
                    if 'HGHGROUP' in ijson['grpid']:
                        grpid_sp    = ijson['grpid']
                        grp_name    = grp_info[grpid_sp]
                        disp_name   = disp_name+' - '+grp_name
                    elif '-' in ijson['grpid']:
                        grpid_sp    = ijson['grpid'].split('-')
                        grp_name    = grp_info[grpid_sp[0]]+' - '+grp_info[grpid_sp[1]]
                        if (grpid_sp[0], grpid_sp[1]) in vgh_doc_map:
                            grp_name    = vgh_doc_map[(grpid_sp[0], grpid_sp[1])]
                        disp_name   = disp_name+' - '+grp_name
                    else:
                        disp_name   = disp_name+' - '+grp_info[ijson['grpid']] #ijson['data'][0]
                disp_name   = ijson.get('m_disp_name', disp_name)
                res[0]['disp_name'] = disp_name
                tmpres[0]['disp_name'] = disp_name
                if ijson.get('NO_FORM', '') != 'Y':
                    if gen_type == 'display':
                        if pt_value_e:
                            res = [{'message':'Error Peroid type validation Error', 'data':pt_value_e, 'res':res}]
                            return res
                    else:
                        if pt_value_e:
                            res = [{'message':'Error Peroid type validation Error', 'data':pt_value_e}]
                            return res
                if ijson.get('RETURN_TXT', '') == 'Y':
                    if ijson.get('grpid'):
                        return {disp_name:{'data':rc_d, 'key_map':key_map_rev, 'table_type':main_header, 'id':str(gen_id)+'-'+ijson['grpid'], 'phs':map(lambda x:x['n'], res[0]['phs']), 'non_template_db':ijson.get('non_template_db', '')}}
                    else:
                        return {disp_name:{'data':rc_d, 'key_map':key_map_rev, 'table_type':main_header, 'id':str(gen_id), 'phs':map(lambda x:x['n'], res[0]['phs']), 'non_template_db':ijson.get('non_template_db', '')}}
                res[0]['disp_name'] = disp_name
                ph_d    = {}
                for ph in res[0]['phs']:
                    ph_d[ph['k']]   = 1
                DB_DATA_D[(table_type, group_id, restated_model, ph_filter)]  = copy.deepcopy(tmpres)
                self.clean_preview_data(res, ph_orgd, ijson, restated_model, tmpres)
                            
                   
                if ijson.get('err_flg') == 'Y':
                    res = self.clean_response(res)
                if ctmp_d:
                    res[0]['clean_value_dict']  = ctmp_d
                sgrps   = sconvert_obj.scale_factor_map
                res_lst_na, not_available_scale_lst = self.get_scale_factor_info(sgrps) 
                if res_lst_na:
                    self.update_db_data(res_lst_na, res[0]['data'])
                res[0]['s_grps']  = res_lst_na
                res[0]['c_grps']  = not_available_scale_lst
                res[0]['ignored_rphs']  = ignored_phs
                res_ks  = {}
                rev_ph_map  = {}
                for ph in org_res[0]['phs']:
                    rev_ph_map[ph['k']] = ph
                for ph in res[0]['phs']:
                    if ('RES_K', ph['k']) in t_re_stated_cols and ijson.get('REP_RES_DOC', '') == 'Y':
                        if ph.get('derived') == 'Y':
                            res_ks[ph['k']]  = [('', ph['n'])]
                        else:
                            ks  = sets.Set(t_re_stated_cols[('RES_K', ph['k'])])
                            tmp_ar  = []
                            for k in ks:
                                tph = rev_ph_map[k.split('-RS')[0]]
                                doc_id, table_id = tph['g'].split()[0].split('-')
                                tmp_ar.append((doc_id, k))
                            res_ks[ph['k']]  = tmp_ar
                        
                        
                    if ph['k'] not in t_re_stated_cols:
                        ph['not_restated']  = 'Y'
                if ijson.get('REP_RES_DOC', '') == 'Y':
                    print 'REP_RES_DOC'
                    res[0]['res_info'] = res_ks
                
                return res
                
        #print key_map
        #print final_d.keys()
        i_project_name  = ''.join(ijson['project_name'].split())
        doctype_path    = ''
        if ijson.get('grp_doc_types'):
            doctype_path    = '%s/'%('_'.join(ijson.get('grp_doc_types')))
        if restated_model == 'Y':
            txtpath      = '/var/www/html/DB_Model/%s/%s/%s'%(company_name, i_project_name,doctype_path)
        else:
            txtpath      = '/var/www/html/DB_Model_Reported/%s/%s/%s'%(company_name, i_project_name,doctype_path)
        if norm_scale == 'Y':
            txtpath      = '%s/Norm_Scale/'%(txtpath)
        os.system("mkdir -p '%s'"%txtpath)
        path         = '/mnt/eMB_db/%s/%s/FINAL_OUTPUT/'%(company_name, model_number)
        os.system("mkdir -p "+path)
        for k, v in final_d.items():
            iD  = k
            k   = str(k) +'-'+ph_filter
            fname   = txtpath+'/'+k+'.txt'
            #print fname
            fout    = open(fname, 'w')
            fout.write(str({v[3]:v[1]}))
            fout.close()
        if restated_model == 'Y':
            txtpath      = '/var/www/html/DB_Model_Missing/%s/%s/'%(company_name,i_project_name)
        else:
            txtpath      = '/var/www/html/DB_Model_Reported_Missing/%s/%s/'%(company_name, i_project_name)
        os.system("mkdir -p '%s'"%txtpath)
        for k, v in final_d.items():
            k   = str(k) +'-'+ph_filter
            fname   = txtpath+'/'+k+'.html'
            org_res[0]['phs']   = filter(lambda x:x['k'] in missing_value['PKS'], org_res[0]['phs'])
            org_res[0]['data']  = filter(lambda x:x['t_id'] in missing_value, org_res[0]['data']) 
            if not org_res[0]['phs']:continue
            table_str   = '<table border=1>'
            tr          = '<tr><th rowspan=2>Description</th>'
            tr1         = '<tr>'
            for r in org_res[0]['phs']:
                th          = '<th>%s</th>'%(r['g'])
                th1          = '<th>%s</th>'%(r['ph'])
                tr          += th
                tr1          += th1
            tr          += '</tr>'
            tr1          += '</tr>'
            table_str   += tr+tr1    
            for rr in org_res[0]['data']:
                tr  = "<tr><td>%s</td>"%(rr['t_l'])
                for ph in org_res[0]['phs']:
                    td  = '<td>%s</td>'%(rr.get(ph['k'], {}).get('v', ''))
                    tr  += td
                tr  += '<tr>'
                table_str   += tr
            table_str   += '</table>'
            fout    = open(fname, 'w')
            fout.write(table_str)
            fout.close()
        res = [{'message':'done', 'ID':iD}]
        if ijson.get('NO_FORM', '') != 'Y':
            if gen_type == 'display':
                if pt_value_e:
                    res = [{'message':'Error Peroid type validation Error', 'data':pt_value_e, 'res':res}]
                    return res
            else:
                if pt_value_e:
                    res = [{'message':'Error Peroid type validation Error', 'data':pt_value_e}]
                    return res
        return res
        env1        = lmdb.open(path, map_size=2**39)
        iD          = ''
        with env1.begin(write=True) as txn1:
            for k, v in final_d.items():
                iD  = k
                k   = str(k) +'-'+ph_filter
                fname   = txtpath+'/'+k+'.txt'
                print fname
                fout    = open(fname, 'w')
                fout.write(str({v[3]:v[1]}))
                fout.close()
                txn1.put('OUTPUT_'+str(k), str(v[1]))
                txn1.put('DISPLAYOUTPUT_'+str(k), str(v[2]))
                txn1.put('TEXT_VALUE_'+str(k), str(v[0]))
        #print path
        #sys.exit()
        res = [{'message':'done', 'ID':iD}]
        return res


    def filter_phs_taxo(self, res, consider_rows, consider_ks, consider_table, new_phs, d_new_phs, ijson):
        if ijson.get('derivation_cal') != 'Y':
            res[0]['data']      = filter(lambda x:x['t_id'] in consider_rows, res[0]['data'])
            res[0]['phs']       = filter(lambda x:x['k'] in consider_ks, res[0]['phs'])
                
        new_phs = new_phs + d_new_phs
        if new_phs:
            ph_map_d    = {}
            for ph in filter(lambda x:x['k'] in consider_ks, res[0]['phs']):
                ph_map_d[ph['n']]   = ph
                #print '\t', [ph['n'], ph]
            for ph in new_phs:
                ph['n'] = ph['ph']
                if not ph.get('g'):
                    ph['g'] = ph['n']
                ph_map_d[ph['n']]   = ph
            dphs_ar             = report_year_sort.year_sort(ph_map_d.keys())
            dphs_ar.reverse()
            #print d_new_phs
            #print ph_map_d.keys()
            #print dphs_ar, consider_ks
            res[0]['phs']       = map(lambda x:ph_map_d[x], dphs_ar)
        else:
            if ijson.get('derivation_cal') != 'Y':
                res[0]['phs']       = filter(lambda x:x['k'] in consider_ks, res[0]['phs'])
        if ijson.get('derivation_cal') != 'Y':
            res[0]['phs']       = filter(lambda x:x['k'] in consider_ks, res[0]['phs'])
        #print res[0]['phs']
        #sys.exit()
        if ijson.get('derivation_cal') != 'Y':
            res[0]['table_ar']  = filter(lambda x:x['t'] in consider_table, res[0]['table_ar'])
        if not res[0]['table_ar']:
            res[0]['table_ar']    = [{'dt':[], 't':''}]


    def clean_preview_data(self, res, ph_orgd, ijson, restated_model, tmpres, rm_keys={}):
        print 'rm_keys ', rm_keys
        def clean_preview_data(res, cellks):
            ph_d    = {}
            for ph in res[0]['phs']:
                ph_d[ph['k']]   = 1
            for rr in res[0]['data']:
                ks  = rr.keys()
                for k in ks:
                    if ':$$:' in k:
                        del rr[k]
                for k in ['f_taxo', 'tids', 'order', 'da', 'dbf','fd']:
                    if k in rr:
                        del rr[k]
                for k in ['th_flg']:
                    if k in rr and not rr.get(k):
                        del rr[k]
                for pk in ph_orgd.keys():
                    if pk in rr and (pk not in ph_d):
                        del rr[pk]
                for pk in ph_d.keys():
                    if pk in rr:
                        if 'sconv' in rr[pk]:
                            rr[pk]['expr_str']  = rr[pk].get('expr_str', '')+str(rr[pk]['sconv']) 
                        for k in cellks+rm_keys.get('CELL', []):
                            if k in rr[pk]:
                                del rr[pk][k]
                        for k in ['f_col', 'f_col_org']:
                            if k not in rr[pk]:continue
                            tcl_ar  = []
                            #{"xml_id": "x1949_93", "description": "DILUTED EARNINGS PER SHARE (CENTS)", "currency": "CENTS", "clean_value": "27.9", "value_type": "MNUM", "bbox": [[1529.0, 1992.0, 51.0, 24.0]], "pno": "93", "x": "x1949_93", "operator": "+", "taxo_id": 56, "scale": "1", "d": "5", "actual_value": "27.9", "t": "636", "table_id": "636", "ph": "FY2012", "gv_ph": "2012FY"}
                            for ft_ar in rr[pk][k]:
                                tar = []
                                for ft in ft_ar:
                                    dd  = {}
                                    for k1 in rm_keys.get('F', ft.keys()): #["description", "operator", "taxo_id", "ph","pno","clean_value", "actual_value","tt", 'f_col', 'type', 'conv_value']:
                                        if ft.get(k1):
                                            dd[k1]  = ft[k1]
                                    dd['operator']  = ft.get('operator', '')
                                    tar.append(dd)
                                tcl_ar.append(tar)
                            rr[pk][k]  = tcl_ar
        cellks  = ['rest_ar', 'org_d', 'PH_FORM', 'PH_FORM_STR', 'PH_D', 're_stated_all', 'rest_ar_all']
        clean_preview_data(res, cellks)
        if ijson.get('store_flg') == 'Y':
            cellks  = ['rest_ar', 'org_d', 'PH_FORM', 'PH_FORM_STR', 'PH_D', 'rest_ar_all']
            clean_preview_data(tmpres, cellks)
            sgrps   = sconvert_obj.scale_factor_map
            res_lst_na, not_available_scale_lst = self.get_scale_factor_info(sgrps) 
            if res_lst_na:
                self.update_db_data(res_lst_na, tmpres[0]['data'])
            tmpres[0]['s_grps']  = res_lst_na
            tmpres[0]['c_grps']  = not_available_scale_lst
            company_name    = ijson['company_name']
            mnumber         = ijson['model_number']
            model_number    = mnumber
            deal_id         = ijson['deal_id']
            project_id      = ijson['project_id']
            company_id      = "%s_%s"%(project_id, deal_id)
            table_type  = ijson['table_type']
            grpid       = ijson.get('grpid', '')
            if not grpid:
                grpid   = ''
            reported    = 'Y'
            if restated_model == 'Y':
                reported    = 'N'
            key             = ''.join(table_type)+'_'+''.join(str(grpid).split())+'_'+''.join(ijson.get('project_name', '').split())+'_'+reported
            path            = '/mnt/eMB_db/%s/%s/PREVIEW_OUTPUT/%s/'%(company_name, model_number, key)
            os.system("rm -rf '%s'"%(path))
            os.system("mkdir -p '%s'"%(path))
            env1        = lmdb.open(path, map_size=2**39)
            print '\tSTORE ', path
            with env1.begin(write=True) as txn1:
                txn1.put('res', str(tmpres))

    def update_form_sign(self, data, taxo_value_grp, f_col):
        for formula in f_col:
            for ft in formula:
                if ft['type'] == 't':
                    if ft['op']   == '=':continue
                    tid         = ft['t_id']
                    if str(tid) not in taxo_value_grp:continue
                    ti          = taxo_value_grp[str(tid)]
                    ft['op']    =  '+'
        return f_col
            
                

    def flip_formula_sign(self, data, res, f_phs, clean_v_d, taxo_value_grp, ijson, direction='B-T'):
        flip_sign_d = {}
        tmpar   = data[::-1]
        if direction == 'T-B':
            tmpar   = data
        for tmp_ti, rr in enumerate(data[::-1]):
            sign_d  = {}
            #print '\n==============================================='
            #print [rr['t_id'], rr['t_l']]
            for ph in f_phs:
                #print '\n\t', ph
                f_col   = rr.get(ph['k'], {}).get('f_col', [])
                if not f_col:
                    f_col   = rr.get(ph['k'], {}).get('f_col_o', [])
                if not f_col:continue
                formula = f_col[0]
                op_sign = {}
                for ft in formula:
                    if ft['type'] != 't':continue
                    #if 'operator' not in ft:
                    #    ft['operator']   = ft['op']
                    #if 'taxo_id' not in ft:
                    #    ft['taxo_id']   = ft.get('txid', '')
                    if ft['operator'] == '=':continue
                    if ft['tt'] != ijson['table_type']:continue
                    if ft['grpid'] != ijson.get('grpid', ''):continue
                    op_sign[ft['taxo_id']]  = ft['operator']
                if str(rr['t_id']) not in taxo_value_grp:
                    continue

                tmp_ti          = taxo_value_grp[str(rr['t_id'])]
                res_sign    = ''
                if res[0]['data'][tmp_ti].get(ph['k'], {}).get('flip_sign', '') == 'Y':
                    clean_value = clean_v_d.get(tmp_ti, {}).get(ph['k'], '')
                    tmpres_sign        = '+'   
                    if '-' in clean_value:
                        tmpres_sign    = '-'   
                    if data[tmp_ti][ph['k']].get('org_sign') != tmpres_sign:
                        res_sign    = tmpres_sign
                    
                #print 'res_sign ', [res_sign]
                for tid, op in op_sign.items():
                    if str(tid) not in taxo_value_grp:continue
                    ti          = taxo_value_grp[str(tid)]
                    #if data[ti]['th_flg'] in ['N', 'P']: continue
                    #print '\t\t',[tid,op, data[ti]['t_l'], data[ti].get(ph['k'], {}).get('v')]
                    if ph['k'] in data[ti]:
                        clean_value = clean_v_d[ti][ph['k']]
                            
                        #print '\t\t', [clean_value]
                        sign        = '+'   
                        if '-' in clean_value:
                            sign    = '-'   
                        sign_org    = sign
                        if data[ti]['th_flg'] == 'N' and 0:
                            if sign == '-':
                                clean_value = clean_value.strip('-') 
                            else:
                                clean_value = '-'+clean_value 
                            clean_v_d[ti][ph['k']]  = clean_value
                            data[ti][ph['k']]['v']       = clean_value
                            data[ti][ph['k']]['flip_sign']       = 'Y'
                            res[0]['data'][ti][ph['k']]['v'] = clean_value
                            res[0]['data'][ti][ph['k']]['flip_sign'] = 'Y'
                            flip_sign_d.setdefault(ti, {})[ph['k']]    = (clean_value, 'Y')
                            continue
                        if res_sign:
                            if res_sign == '-':
                                if sign == '-':
                                    sign    = '+'
                                elif sign == '+':
                                    sign    = '-'
                            elif res_sign == '+':
                                if sign == '-':
                                    sign    = '+'
                                elif sign == '+':
                                    sign    = '-'
                            if sign == '-':
                                #print 'CHANGE  1'
                                clean_value = '-'+clean_value
                            else:
                                #print 'CHANGE  1'
                                clean_value = clean_value.strip('-')
                            if not data[ti][ph['k']].get('org_sign'):
                                data[ti][ph['k']]['org_sign']   = sign
                                
                            clean_v_d[ti][ph['k']]  = clean_value
                            data[ti][ph['k']]['v']       = clean_value
                            data[ti][ph['k']]['flip_sign']       = 'Y'
                            res[0]['data'][ti][ph['k']]['v'] = clean_value
                            res[0]['data'][ti][ph['k']]['flip_sign'] = 'Y'
                            flip_sign_d.setdefault(ti, {})[ph['k']]    = (clean_value, 'Y')
                        #print '\t\t', [sign, clean_value]
                        
                            
                        if op == '-':
                            #print 'CHANGE  2'
                            if sign == '-':
                                clean_value = clean_value.strip('-') 
                            else:
                                clean_value = '-'+clean_value 
                            clean_v_d[ti][ph['k']]  = clean_value
                            data[ti][ph['k']]['v']       = clean_value
                            data[ti][ph['k']]['flip_sign']       = 'Y'
                            if not data[ti][ph['k']].get('org_sign'):
                                data[ti][ph['k']]['org_sign']   = sign
                            res[0]['data'][ti][ph['k']]['v'] = clean_value
                            res[0]['data'][ti][ph['k']]['flip_sign'] = 'Y'
                            if (sign_org == '+' and sign == '-') or (sign_org == '-' and sign == '+'):
                                del res[0]['data'][ti][ph['k']]['flip_sign']
                                if 'flip_sign' in data[ti][ph['k']]: #Aniket - Error as key is not available '{"cmd_id":-8946,"company_name":"OldNationalBancorp","deal_id":"154","from_merge":"N","grp_doc_ids":{},"industry_type":"Financial%20Services","model_number":"20","norm_scale":"Y","pid":"1","project_id":"20","project_name":"Key Banking Capital and Profitability Figures","store_flg":"Y","taxo_flg":0,"user":"sinchana","year":15, "PRINT":"Y"}'
                                    del data[ti][ph['k']]['flip_sign']
                                
                            flip_sign_d.setdefault(ti, {})[ph['k']]    = (clean_value, res[0]['data'][ti][ph['k']].get('flip_sign', 'N'))
                        #print '\t\tFINAL', [clean_value]
        return flip_sign_d

    def detect_date(self, txt_d, fye):
        for txt in txt_d.keys():
            cmd = 'cd /root/ParaAPI>/dev/null;python Para_Comp_API_Back_New.py %s "%s";cd - >/dev/null'%(fye, txt)
            #print cmd
            res = os.popen(cmd).read()
            txt_d[txt]  = eval(res)
            if not txt_d[txt].get('p'):
                try:
                    tmpdate = pydateutils.parse(txt, fuzzy_with_tokens=True)
                except:
                    tmpdate = ('','')
                if tmpdate and tmpdate[0]:
                    txt_d[txt]['p'] = tmpdate[0].strftime('%Y')
        
        return txt_d
        Para_Comp_API_Back_New.disableprint()
        for txt in txt_d.keys():
            year, periodtype, month1 = map(str, date_obj.Process_String(txt, fye))
            if year.strip():
                txt_d[txt]  = {'p':year, 'pt':periodtype, 'm':str(month1)}
            else:
                year1, periodtype1, month2 = map(str, date_obj.Process_String_Func(txt, fye))
                txt_d[txt]  = {'p':year1, 'pt':periodtype1, 'm':str(month2)}
        Para_Comp_API_Back_New.enableprint()
        return txt_d
    


    def add_date_diff_row(self, ijson, data, f_phs, t_ids, company_name, model_number, clean_v_d, f_k_d, f_phs_d, taxo_unq_d, run_date_diff_type, form_d):
        table_type  = ijson['table_type']
        grpid       = ijson.get('grpid', '')
        lmdb_path2      =  "/mnt/eMB_db/%s/%s/table_phcsv_data"%(company_name, model_number)
        env1             = lmdb.open(lmdb_path2, readonly=True)
        txn_trip             = env1.begin()
        #print lmdb_path2

        lmdb_path2      =  "/mnt/eMB_db/%s/%s/default_table_phcsv_data"%(company_name, model_number)
        #print lmdb_path2
        env1             = lmdb.open(lmdb_path2, readonly=True)
        txn_trip_default             = env1.begin()
        txt_d           = {}
        ph_map          = {
                                'Q1'    : {'m':3, "mstr":"March 31"},
                                'Q2'    : {'m':6, "mstr":"June 30"},
                                'H1'    : {'m':6, "mstr":"June 30"},
                                'Q3'    : {'m':9, "mstr":"September 30"},
                                'M9'    : {'m':9, "mstr":"September 30"},
                                'Q4'    : {'m':12, "mstr":"Dec 31"},
                                'FY'    : {'m':12, "mstr":"Dec 31"},
                                'H2'    : {'m':12, "mstr":"Dec 31"},
                            }
            
        #print t_ids
        #sys.exit()
        t_ids_org   = copy.deepcopy(t_ids)
        header_grp  = []
        tmplist = []
        ti_d    = {}
        f_header    = 0
        for ti, rr in enumerate(data[:]):
            #if t_ids and rr['t_id'] not in t_ids:continue
            txt_d[rr['t_l']]    = {}
            if rr.get('th_flg') == 'H':
                f_header    = 1
                if tmplist:
                    #if t_ids and rr['t_id'] in t_ids:
                    header_grp.append((tmplist, ti_d))
                tmplist = []
                ti_d    = {}
            ti_d[len(tmplist)]  = ti
            tmplist.append(rr)
        if tmplist:
            header_grp.append((tmplist, ti_d))
            
        final_ar    = []
        iclean_v_d  = copy.deepcopy(clean_v_d)
        old_new_taxomap = {}
        for data, ti_d in header_grp:
            txt_d   = {}
            for ti, rr in enumerate(data[:]):
                txt_d[rr['t_l']]    = {}
            self.detect_date(txt_d, 13)
            ph_ind  = {}
            for ii, ph in enumerate(f_phs):
                year    = ph['ph'][-4:]
                ph_ind[ph['ph']]  = ii
                txt_d[ph['ph']] = {}
                txt_d[ph['ph']]['p'] = year
                txt_d[ph['ph']]['m'] = ph_map[ph['ph'][:-4]]['m']
            import calendar
            ph_value_d  = {}
            if run_date_diff_type == 'format_1':
                cond_ar     = [
                                (1.0, 'Less than one year'),
                                ]
                for i in range(2, 10):
                    cond_ar.append((float(i), 'Less than '+self.numToWords(i)+' years'))
            else:
                cond_ar     = [
                                (0.0, 'Offset Year 0 ', 1.0),
                                ]
                prev_i  = 1
                for i in range(2, 7):
                    cond_ar.append((float(prev_i), 'Offset Year %s '%(prev_i), i))
                    prev_i  = i
                cond_ar.append((6, 'Thereafter', 9999999))
            break_loop  = 0
            tfinal_ar   = []
            tclean_v_d  = {}
            r_map       = {}
            #print '\n-------------------------------------------------------------------------------------------------------------------'
            for ti_org, rr in enumerate(data[:]):
                #if t_ids and rr['t_id'] not in t_ids:continue
                ti  = ti_d[ti_org]
                n_row   = 0
                #print ti, rr['t_l'], iclean_v_d.get(ti, {})
                tmphs   = f_phs[:]
                if break_loop == 1 or (f_header == 1 and ti_org == 0 and  t_ids and rr['t_id'] not in t_ids):
                    tmphs   = []
                    break_loop  = 1
                #print 'tmphs ', len(tmphs)
                for ii, ph in enumerate(tmphs):
                    if ph['k'] not in rr or (not rr.get(ph['k'], {}).get('v')):continue # or (iclean_v_d.get(ti, {})[ph['k']] == ''):continue
                    #print '\t',[ph, rr.get(ph['k'], {}).get('v'), iclean_v_d.get(ti, {}).get(ph['k'], '')]
                    v_d = rr[ph['k']]
                    t   = v_d['t']          
                    x   = v_d['x']          
                    VGH = ph['ph']
                    HGH = txt_d[rr['t_l']]
                    if HGH:
                        VGH     = txt_d[VGH]
                        for tmptid in form_d.get(('OP', str(rr['t_id'])), {}).keys():
                            r_map.setdefault(tmptid, {})[str(rr['t_id'])]   = ii
                        #print '\t\n==============================================================================='
                        #print '\tHGH ', rr['t_l']
                        #print '\tVGH', VGH
                        #print HGH
                        #print VGH
                        if VGH:
                            year_h  = HGH['p']
                            month_h = HGH['m']

                            year_v  = VGH['p']
                            month_v = VGH['m']
                            if year_h and year_v:
                                if month_h:
                                    d2  = datetime.datetime.strptime('%s-%s'%(year_h, month_h), '%Y-%m')
                                else:
                                    d2  = datetime.datetime.strptime('%s'%(year_h), '%Y')
                                if month_v:
                                    d1  = datetime.datetime.strptime('%s-%s'%(year_v, month_v), '%Y-%m')
                                else:
                                    d1  = datetime.datetime.strptime('%s'%(year_v), '%Y')
                                #print '\n==============================================================================='
                                #print 'HGH ', rr['t_l']
                                #print 'VGH', ph['ph']
                                #print HGH
                                #print VGH
                                #print d1
                                #print d2
                                diffy   = d2.year - d1.year
                                diff    = d2 - d1.replace(d2.year)
                                d_i_y   = calendar.isleap(d2.year) and 366 or 365
                                d_y     = abs(diffy + (diff.days + diff.seconds/86400.0)/d_i_y)
                                #print '\t\td_y ',[ph['ph'], iclean_v_d[ti][ph['k']]],d_y
                                f_less  = 0
                                if run_date_diff_type == 'format_2':
                                    d_y = diffy
                                    for c_r in cond_ar:
                                        #print '\n',c_r, d_y
                                        if d_y >= c_r[0] and d_y < c_r[2]:
                                            #print 'YES'
                                            val = iclean_v_d[ti][ph['k']]
                                            if val == '':
                                                val = '0'
                                            ph_value_d.setdefault(c_r, {}).setdefault(ph['ph'], []).append((val, copy.deepcopy(rr[ph['k']]), rr))
                                            #break
                                            f_less  = 1
                                        #elif f_less == 1:break
                                else:
                                    for c_r in cond_ar:
                                        if d_y <= c_r[0]:
                                            val = iclean_v_d[ti][ph['k']]
                                            if val == '':
                                                val = '0'
                                            ph_value_d.setdefault(c_r, {}).setdefault(ph['ph'], []).append((val, copy.deepcopy(rr[ph['k']]), rr))
                                            #break
                                            f_less  = 1
                                        elif f_less == 1:break
                if (t_ids_org and rr['t_id'] in t_ids_org):
                    tclean_v_d[len(tfinal_ar)]    = iclean_v_d.get(ti, {})
                    tfinal_ar.append((ti_org, rr))
            #final_ar    += data
            done_idx_d  = {}
            st  = -1
            for tmpi, tmpr in enumerate(tfinal_ar):
                if (tmpr[0] - st) > 1:
                    break
                done_idx_d[tmpr[0]] = 1
                st      = tmpr[0]
                tmpr    = tmpr[1]
                #old_new_taxomap[str(tmpr['t_id'])] = tmpr
                clean_v_d[len(final_ar)]    = tclean_v_d.get(tmpi, {})
                final_ar.append(tmpr)
            pks = ph_value_d.keys()
            pks.sort(key=lambda x:x[0])
            rev_map = {}
            tmpids  = []
            for pk in pks:
                rr  = {'t_id':"N-"+str(len(final_ar)), 't_l':pk[1], 'n_node':'Y'}
                t_ids.append(rr['t_id'])
                tmpids.append(rr)
                #print '\n============================================='
                #print pk, rr
                for k in ph_value_d[pk]:
                    #print k
                    v   = ph_value_d[pk][k]
                    f_val   = []
                    for tmpv in v:
                        f_val.append(tmpv[0])
                    f_val   = eval('+'.join(f_val))
                        
                    f_val   = self.convert_floating_point(f_val)
                    if f_val == '0':
                        f_val   = '-'
                    #print '\tf_val ',[ rr['t_l'], k, f_val]
                    dd      = copy.deepcopy(v[0][1] )
                    if 'PH_FORM' in dd:
                        del dd['PH_FORM']
                    if 'rest_ar_all' in dd:
                        del dd['rest_ar_all']
                    if 'rest_ar' in dd:
                        del dd['rest_ar']
                    if 'org_d' in dd:
                        del dd['org_d']
                    dd['v'] = f_val
                    rr[f_phs_d[k]]   = dd
                    rr.update(dd)
                    clean_v_d.setdefault(len(final_ar), {})[f_phs_d[k]] = f_val
                    f_col   = []
                    #print
                    for tmpv in [('', {'v':f_val}, rr, '=')]+map(lambda x:x+('+', ), v):
                        old_new_taxomap.setdefault(str(tmpv[2]['t_id']), {})[rr['t_id']] = rr
                        if tmpv[3] != '=':
                            rev_map.setdefault(rr['t_id'], {})[tmpv[2]['t_id']] = 1
                        tmpdd    = tmpv[1]
                        rd  = copy.deepcopy(tmpdd)
                        rd.update({'txid':tmpv[2]['t_id'], 'tt':table_type, 'grpid':grpid, 'clean_value':tmpdd['v'], 'v':tmpdd['v'], 'type':'t', 'operator':tmpv[3], 'ph':k, 'k':f_phs_d[k], 'description':tmpv[2]['t_l']})
                        #print '\t\t',rd['clean_value'], rd['txid'], rd['description'], rd['operator']
                        f_col.append(rd)
                    dd['f_col_o']   = [f_col]
                f_taxo      = self.gen_taxonomy(rr['t_l'])
                if f_taxo not in taxo_unq_d:
                    taxo_unq_d[f_taxo]  = 1
                else:
                    ol_cnt  = taxo_unq_d[f_taxo]
                    taxo_unq_d[f_taxo]  = ol_cnt+1
                    f_taxo  += '-'+str(ol_cnt)
                rr['f_taxo']    = f_taxo
                final_ar.append(rr)
            for tmpi, tmpr in enumerate(tfinal_ar):
                if tmpr[0] in done_idx_d:continue
                tmpr    = tmpr[1]
                operands    = [] #filter(lambda x:x in rev_map, r_map.get(str(tmpr['t_id']), {}).keys())
                for op_r in tmpids[:0]:
                    tmpid   =op_r['t_id']
                    f   = 0
                    for mid in rev_map.get(tmpid, {}).keys():
                        if str(mid) in r_map.get(str(tmpr['t_id']), {}):
                            f = 1
                            break
                    if f == 1:
                        operands.append(op_r)
                #print '\n============================='
                #print 'TT ', tmpr['t_l'], map(lambda x:x['t_l'], operands)
                f   = 0
                if operands :
                    for ph in f_phs:
                        if ph['k'] not in tmpr:continue
                        f_col   = []
                        #print ph['k']
                        for tmpv in [('', tmpr[ph['k']], tmpr, '=')]+map(lambda x:('', x.get(ph['k'], {'v':''}), x, '+'), operands):
                            tmpdd    = tmpv[1]
                            rd  = copy.deepcopy(tmpdd)
                            f   = 1
                            rd.update({'txid':tmpv[2]['t_id'], 'tt':table_type, 'grpid':grpid, 'clean_value':tmpdd['v'], 'v':tmpdd['v'], 'type':'t', 'operator':tmpv[3], 'ph':ph['ph'], 'k':ph['k'], 'description':tmpv[2]['t_l']})
                            #print '\t\t',rd['clean_value'], rd['txid'], rd['description'], rd['operator']
                            f_col.append(rd)
                        tmpr[ph['k']]['f_col_o']   = [f_col]
                        
                        
                clean_v_d[len(final_ar)]    = tclean_v_d.get(tmpi, {})
                final_ar.append(tmpr)
        self.redefine_formula(final_ar, f_phs, old_new_taxomap, table_type, grpid)
        #for ti, r in enumerate(final_ar):
        #    print r['t_l'], clean_v_d.get(ti)
        return final_ar, clean_v_d, f_k_d

    def redefine_formula(self, res, phs, old_new_taxomap, table_type, grpid):
        order_d = {}
        for i, rr in enumerate(res):
            order_d[str(rr['t_id'])]     =(i , rr)
        for rr in res:
            if rr.get('n_node') == 'Y':continue
            all_taxo    = {}
            for ph in phs:
                if ph['k'] not in rr:continue
                tmpform = []
                for fs in rr[ph['k']].get('f_col_o', []):
                    tmpar   = []
                    done_d  = {}
                    for ft in fs:
                        if ft['operator'] == '=' or ft['type'] == 'v':
                            tmpar.append(ft)
                        else:
                            if 'txid' not in ft:
                                ft['txid']  = ft['taxo_id']
                            if str(ft['txid']) not in old_new_taxomap:
                                all_taxo[ str(ft['txid'])] = 1
                                tmpar.append(ft)
                                continue
                            #print ft['txid']
                            tmprr_d   = old_new_taxomap[str(ft['txid'])]

                            for t_id, tmprr in tmprr_d.items():
                                if str(t_id) in done_d:continue
                                if str(t_id) != str(ft['txid']):
                                    dd  = tmprr.get(ph['k'], {})
                                    if dd:
                                        done_d[str(t_id)]   = 1
                                        all_taxo[ str(tmprr['t_id'])] = 1
            for ph in phs:
                if ph['k'] not in rr:continue
                tmpform = []
                for fs in rr[ph['k']].get('f_col_o', []):
                    tmpar   = []
                    done_d  = {}
                    for ft in fs:
                        if ft['operator'] == '=' or ft['type'] == 'v':
                            tmpar.append(ft)
                    all_taxos   =  all_taxo.keys()
                    all_taxos.sort(key=lambda x:order_d[x][0])
                    for t_id in all_taxos:
                        tmprr   = order_d[t_id][1]
                        dd  = tmprr.get(ph['k'], {})
                        if dd:
                            #dd['i_f_type']          = ft['i_f_type']
                            dd['type']          = 't'
                            dd['clean_value']   = dd.get('v', '')
                            dd['description']   = tmprr['t_l']
                            dd['ph']            = ph['ph']
                            dd['taxo_id']       = str(tmprr['t_id'] )
                            dd['txid']       = str(tmprr['t_id'] )
                            dd['tt']            = ft['tt']
                            dd['grpid']            = ft.get('grpid', '')
                            dd['k']             = ph['k']
                            dd['operator']          = ft['operator']
                            tmpar.append(dd)
                        else:
                            dd  = {}
                            dd['type']          = 't'
                            dd['clean_value']   = ''
                            dd['description']   = 'Not Exists'
                            dd['ph']            = ph['ph']
                            dd['taxo_id']       = t_id
                            dd['tt']            = table_type
                            dd['grpid']            = grpid
                            dd['operator']          = '+'
                            tmpar.append(dd)
                    if tmpar:
                        tmpform.append(tmpar)
                if tmpform:
                    rr[ph['k']]['f_col_o'] = tmpform 
                                

    def read_triplet_ar(self, table_id, txn1, xml, txn1_default):
        triplet = {}
        gv_tree = txn1.get(self.get_quid(table_id+'^!!^'+xml))
        if not gv_tree:
            gv_tree = txn1_default.get(self.get_quid(table_id+'^!!^'+xml))
        if gv_tree:
            gv_tree = eval(gv_tree)
            triplet = gv_tree['triplet']
        dd  = {}
        for tk, tv in triplet.items():
            if tk not in ['HRP']:continue
            for ii, tv1 in enumerate(tv):  
                for tv2 in tv1:
                    tflg    = ''
                    if tv2:
                        tflg    = tv2[0][2]
                        dd.setdefault(tflg, []).append(tv2)
        return dd

    def read_formula(self, ti, rr, f_phs, txn, xml_row_map, xml_col_map):
        r_key           = {}
        rs_value        = {}
        operand_rows    = []
        row_form        = {}
        row_op          = {}
        if 1:#ijson.get('form', '') == 'Y':
            g_ng, f_d, all_xml= self.check_formula(rr, f_phs, txn)
            if len(g_ng.keys())> 1:
                #print 'Error ', ti, rr['t_l'], g_ng
                #sys.exit()
                pass
            if f_d:
                #print '\n*******************************************************************'
                #print ti, rr['t_l']
                operand_rows    = {}
                for formula, xml_ids in f_d.items():
                    #print formula, len(xml_ids.keys()), len(list(sets.Set(all_xml.keys())-sets.Set(xml_ids.keys())))
                    if formula[1] == 'ROW':
                        for (table_id, pxml), c_xml in xml_ids.items():
                            tmp_rows    = {}
                            #print
                            p_k         = xml_col_map[(table_id, pxml)]
                            child_k     = []
                            for cx in c_xml[1:]:
                                cxml, op = cx.split('!!')
                                if cxml:
                                    if (table_id, cxml) in xml_col_map:
                                        child_k.append((ti,xml_col_map[(table_id, cxml)], op))
                                    else:
                                        child_k = []
                                        break
                            if child_k:
                                child_k.sort()
                                row_form.setdefault(p_k, {})[tuple(child_k)]    = 1
                                
                        
                    else:    
                        for (table_id, pxml), c_xml in xml_ids.items():
                            tmp_rows    = {}
                            #print
                            for cx in c_xml[1:]:
                                cxml, op = cx.split('!!')
                                if cxml:
                                    #print (table_id, cxml), (table_id, cxml) in xml_row_map
                                    if (table_id, cxml) in xml_row_map:
                                        trows   = xml_row_map[(table_id, cxml)]
                                        for tr in trows.keys():
                                            row_op.setdefault(tr, {})[op]   = 1
                                        tmp_rows.update(trows)
                                    else:
                                        tmp_rows    = {}
                                        break
                            trows   = tmp_rows.keys()
                            trows.sort()
                            if trows:
                                operand_rows[tuple(trows)]  = 1
                operand_rows    = self.get_overlap_rows(operand_rows)
                operand_rows.sort()
                #print len(operand_rows), operand_rows
                #print 'operand_rows   ', map(lambda x:(x, row_op[x].keys()), operand_rows)
        return r_key, rs_value, operand_rows, row_form, row_op

    def add_restated_values(self, ti, rr, v_d, dd, res, rs_value, key_map_rev, key_map_rev_t, ph, txn, dphs_ar, doc_d, csv_d, actual_v_d, dphs, ijson,dph, table_type, rep_docids, add_main):
        #print rr['t_id'], rr['t_l'], v_d
        #sys.exit()
        c_ph, c_year      = dph
        restated_year   = ''
        cinfo           = ijson.get('g_cinfo', {})
        n_doc           = ''
        if cinfo.get('restated_period', '') not in ['', 'Latest', None]:
            t= cinfo.get('restated_period', '').replace('%2B', '+')
            restated_year   = int(t.split('P+')[1])
        if restated_year and table_type not in self.non_financial_tt:   
            n_year  = c_ph+str(c_year+restated_year)
            if n_year in dphs:
                n_docs   = filter(lambda x:str(x) in rep_docids, dphs[n_year].keys())
                if n_docs:
                    n_doc   = sorted(n_docs)[0]
        odd      = self.get_gv_dict(v_d, txn, key_map_rev, ph, csv_d)
        dd1     = self.get_gv_dict(v_d, txn, key_map_rev_t, ph, csv_d, {}, 'N')
        odd[key_map_rev['tn']]= doc_d.get(v_d['d'], ('', ''))[1]+'-'+doc_d.get(v_d['d'], ('',''))[0]
        #row[table_id+':$$:'+xml_id] = {'v':txts, 'bbox':bbox1, 'x':xml_ar, 'd':doc_id, 't':table_id}
        table_id, xml_id    =  v_d['t'],  v_d['x']
        lbl_d   = rr.get(table_id+':$$:'+xml_id, {})
        if not lbl_d.get('txt', ''):
            lbl_d['txt']   = rr['t_l']
        odd[key_map_rev['description']] = lbl_d
        dd1['t'] = v_d['t']
        dd1['tn']= v_d['t']+'-'+v_d['d']+' ('+doc_d.get(v_d['d'], ('', ''))[1]+'-'+doc_d.get(v_d['d'], ('',''))[0]+')'
        dd1['x']= v_d['x']
        dd1['R']= 'Y'
        re_stated   = []
        re_stated1   = []
        #print '\n====================================='
        #print ph['ph'], [v_d['v'], v_d['t'], v_d['d']], other_values
        for rs_k in rs_value:#[ph['k']]:
            rv_d     = rr.get(rs_k, {'v':'','x':'','bbox':[], 'd':'', 't':'', 'phcsv':{'c':'', 'vt':'','s':'','p':'','pt':''}})
            #print '\t',[rv_d['v'], rv_d['t'], rv_d['d']]
            tdd     = self.get_gv_dict(rv_d, txn, key_map_rev, ph, csv_d, actual_v_d)
            table_id, xml_id    =  rv_d['t'],  rv_d['x']
            lbl_d   = rr.get(table_id+':$$:'+xml_id, {})
            if not lbl_d.get('txt', ''):
                lbl_d['txt']   = rr['t_l']
            tdd[key_map_rev['description']] = lbl_d

            if rs_k != '':
                tdd[key_map_rev['ph']]   = doc_d[rv_d['d']][0]
            elif n_doc:
                tdd[key_map_rev['ph']]   = doc_d[n_doc][0]
            tdd1    = self.get_gv_dict(rv_d, txn, key_map_rev_t, ph, csv_d, actual_v_d, 'N')
            if rs_k != '':
                tdd1['ph']   = doc_d[rv_d['d']][0]
            elif n_doc:
                tdd1['ph']   = doc_d[n_doc][0]
            if rs_k != '':
                tdd[key_map_rev['tn']]   = doc_d[rv_d['d']][1]+'-'+doc_d[rv_d['d']][0]
            elif n_doc:
                tdd[key_map_rev['tn']]   =  doc_d[n_doc][1]+'-'+doc_d[n_doc][0]
            re_stated.append(tdd)
            tdd1['t'] = rv_d['t']
            if rs_k != '':
                tdd1['tn']= rv_d['t']+'-'+rv_d['d']+' ('+doc_d[rv_d['d']][1]+'-'+doc_d[rv_d['d']][0]+')'
            elif n_doc:
                tdd1['tn']= rv_d['t']+'-'+n_doc+' ('+doc_d[n_doc][1]+'-'+doc_d[n_doc][0]+')'
            tdd1['x']= rv_d['x']
            tdd1['k']   = rs_k
            re_stated1.append(tdd1)
        if re_stated:
            if add_main == 'Y':
                res[0]['data'][ti].setdefault(ph['k'], v_d)['re_stated'] = [copy.deepcopy(dd1)]+re_stated1
            re_stated   = [copy.deepcopy(odd)]+re_stated
            dd[key_map_rev['re_stated']]    = re_stated
            for tk in ['actual_value', 'clean_value', 'd', 'pno', 'bbox', 'gv_ph', 'currency', 'scale', 'value_type'][:0]:
                if key_map_rev[tk] in re_stated[-1]:
                    dd[key_map_rev[tk]] = re_stated[-1][key_map_rev[tk]]
                elif key_map_rev[tk] in dd:
                    dd[key_map_rev[tk]] = ''

    def add_row_formula_only(self, ijson, display_name_d, rr, ti, res, f_col, ph, dd, key_map_rev, row_ind_map, taxo_value_grp, done_resultant, first=None, ph_form=None):
        deal_id = int(ijson['deal_id'])
        if first == None:
            if ijson.get('type') == 'display':
                tmpfcols    = copy.deepcopy(f_col)
                for tmpfcol in tmpfcols:
                    for ft in tmpfcol:
                        for k in ['f_col', 'f_col_o', 'company_info', 'expr_str', 'expr_val', 'sconv', 'phcsv']:
                            if k in ft:
                                del ft[k]
                res[0]['data'][ti][ph['k']]['f_col'] = tmpfcols
            else:
                res[0]['data'][ti][ph['k']]['f_col'] = copy.deepcopy(f_col)
        if ijson.get('type') == 'display':return
        f_ar    = []
        #print '\n============================================'
        #print 'first ', first
        for fi, fs in enumerate(f_col):
            tmp_ar  = []
            #print '\n=========================================='
            #print 'FORMULA ', fi+1
            ftypes  = {}
            for ft in fs:
                #print '\t',ft
                ftypes[ft.get('i_f_type', '')]  = 1
                rv_d    = ft
                if 'txid' not in rv_d and 'taxo_id' in rv_d:
                    rv_d['txid']    = rv_d['taxo_id']
                if not rv_d.get('v') and rv_d.get('clean_value'):
                    rv_d['v']    = rv_d['clean_value']
                if ft.get('type', '') == 'v':
                    tmp_ar.append(ft)
                    tmp_ar[-1]['row_id']    = 1
                    tmp_ar[-1]['t']         = ''
                    tmp_ar[-1]['doc_id']    = ''
                    tmp_ar[-1]['bbox']      = []
                    tmp_ar[-1]['x']         = ''
                    tmp_ar[-1]['page_no']   = ''
                    tmp_ar[-1]['t_id']      = ''
                    tmp_ar[-1]['op']        = ft['operator']
                else:
                    clean_value = rv_d.get('v', '')
                    try:
                        clean_value = numbercleanup_obj.get_value_cleanup(clean_value)
                    except:
                        clean_value = ''
                        pass
                    rid = taxo_value_grp.get(rv_d['txid'], -1) #28
                    c_d = {'doc_id':rv_d.get('d', ''), 'op':ft['operator'], 'ph':ft['ph'], 't':'', 'ty':ft.get('disp_name', ''), 'page_no':rv_d.get('x', '_1').split(':@:')[0].split('#')[0].split('_')[1].split('@')[0], 'bbox':rv_d.get('bbox', []), 'row_id':row_ind_map.get(rid, -1), 'v':clean_value, 'label':rv_d.get('description', ''), 't_id':rv_d['txid'], 'conv_value':ft.get('conv_value', {}), 'type':'t', 'phcsv':ft.get('phcsv', {}), 'not_exists':ft.get('not_exists', ''), 'op_exists':ft.get('op_exists', ''), 'FORM_VALUE':ft.get('FORM_VALUE', ''), 'FORM_VALUE_KPI':ft.get('FORM_VALUE_KPI', ''), 'PH_D':rv_d.get('PH_D', ''), 'rid':ft.get('rid', ''), 'class_type':display_name_d.get((deal_id, ft['tt'], ft.get('grpid', '')), ''), 'factors':ft.get('factors', []), 'tt':ft['tt'], 'g_id':ft.get('grpid', ''), 'grpid':ft.get('grpid', ''), 'k':ft.get('k', ''), 'x':rv_d.get('x', ''), 'table_id':rv_d.get('t', '')}
                    ktup    = (rv_d['txid'], ft.get('disp_name', ''), row_ind_map.get(rid, -1))
                    if ft['operator'] == '=':
                        done_resultant[ktup]   = 1
                    elif ktup not in done_resultant and ft.get('f_col') and (ft.get('availability') == 'Y' or (rv_d.get('FORM_VALUE') == 'Y' or rv_d.get('PH_D') == 'Y' or rv_d.get('FORM_VALUE_KPI') == 'Y')):
                        self.add_row_formula_only(ijson, display_name_d, rr, ti, res, ft.get('f_col', []), ph, c_d, key_map_rev, row_ind_map, taxo_value_grp, done_resultant, 'Remaining')
                    tmp_ar.append(c_d)
            #if 'CELLFORMULA' in ftypes:continue
            f_ar.append(tmp_ar)
        #print f_ar
        if f_ar:
            if ph_form == 'Y':
                dd[key_map_rev['f_ph_col']]  = f_ar
            else:
                dd[key_map_rev['f_col']]  = f_ar
        
            
    def add_row_formula(self, ti, dd, operand_rows, res, data, ph, key_map_rev, key_map_rev_t, row_op, txn, disp_name, row_ind_map, csv_d, user_f, form_d, taxo_value_grp, append_form=None):
        formula = []
        d_formula   = []
        #if user_f == None:
        row_op[ti]  = {'=':1}
        for ri, rowtup in enumerate(operand_rows):
            tmp_arr = []
            d_tmp_arr = []
            operands    = []
            operand_ids    = []
            rowtup  =(ti, )+rowtup
            for rid in rowtup:
                if user_f == None:
                    if rid != ti:
                        row_op[rid]  = {'+':1}
                rv_d    = data[rid].get(ph['k'], {})
                if rv_d:
                    t       = rv_d['v']
                    clean_value = t
                    try:
                        clean_value = float(numbercleanup_obj.get_value_cleanup(t))
                    except:
                        clean_value = 0.00
                        pass
                    if 1:
                        operand_ids.append(rid)
                        operands.append(clean_value)
            if 0:#user_f == None:
                if len(operand_ids) > 1 and operand_ids[0] == ti:
                    tres = gcom_operator.check_formula_specific(operands[1:], operands[0])
                    if not tres:continue
                    
                    for i, t_r in enumerate(tres):
                        row_op[operand_ids[i+1]]    = {'+':1} if t_r == 0 else {'-':1}
            if form_d.get(ph['k'], []):
                for ft in form_d[ph['k']]:
                    if ft['type'] != 'v':
                        row_op[taxo_value_grp[ft['txid']]]      = {ft['op']:1}
            
                    
            for rid in rowtup:
                rv_d    = data[rid].get(ph['k'], {})
                if rv_d:
                    d_tmp_d   = self.get_gv_dict(rv_d, txn, key_map_rev_t, ph, csv_d)
                    d_tmp_d['t']= rv_d['t']
                    d_tmp_d['x']= rv_d['x']
                    t       = rv_d['v']
                    clean_value = t
                    try:
                        clean_value = numbercleanup_obj.get_value_cleanup(t)
                    except:
                        clean_value = ''
                        pass

                    c_d = {'doc_id':rv_d['d'], 'op':row_op[rid].keys()[0], 'ph':ph['ph'], 't':data[rid]['f_taxo'], 'ty':disp_name, 'page_no':rv_d['x'].split(':@:')[0].split('#')[0].split('_')[1], 'bbox':rv_d['bbox'], 'row_id':row_ind_map[rid], 'v':clean_value, 'label':data[rid]['t_l'], 't_id':data[rid]['t_id']}
                    table_id, xml_id    =  rv_d['t'],  rv_d['x']
                    #lbl_d   = data[rid].get(table_id+':$$:'+xml_id, {})
                    #if not lbl_d.get('txt', ''):
                    #    lbl_d['txt']   = data[rid]['t_l']
                    #c_d['tn'] = lbl_d
                else:
                    d_tmp_d   = {}
                    c_d = {'doc_id':'', 'op':row_op[rid].keys()[0], 'ph':ph['ph'], 't':data[rid]['f_taxo'], 'ty':disp_name, 'page_no':'', 'bbox':'', 'row_id':row_ind_map[rid], 'v':'', 'label':data[rid]['t_l'], 't_id':data[rid]['t_id']}
                    #lbl_d   = {}
                    #if not lbl_d.get('txt', ''):
                    #    lbl_d['txt']   = data[rid]['t_l']
                    #c_d['tn'] = lbl_d
                tmp_arr.append(c_d)

                d_tmp_d[key_map_rev_t['description']]   = data[rid]['t_l']
                d_tmp_d[key_map_rev_t['taxo_id']]       = data[rid]['t_id']
                d_tmp_d[key_map_rev_t['operator']]      = row_op[rid].keys()[0]
                if ti   == rid:
                    d_tmp_d['R']    = 'Y'
                d_tmp_arr.append(d_tmp_d)
            formula.append(tmp_arr)
            d_formula.append(d_tmp_arr)
        if formula:
            if append_form:
                dd.setdefault(key_map_rev['f_col'], []).append(formula[0])
            else:
                dd[key_map_rev['f_col']]  = formula
        if d_formula:
            if append_form:
                res[0]['data'][ti][ph['k']].setdefault('f_col', []).append(d_formula[0])
            else:
                res[0]['data'][ti][ph['k']]['f_col'] = d_formula
            res[0]['data'][ti][ph['k']]['g_f'] = 'Y'
            res[0]['data'][ti][ph['k']]['f'] = 'Y'
            res[0]['data'][ti][ph['k']]['fv'] = 'Y'
            if append_form:
                data[ti][ph['k']].setdefault('f_col', []).append(d_formula[0])
            else:
                data[ti][ph['k']]['f_col'] = d_formula
            data[ti][ph['k']]['g_f'] = 'Y'
            data[ti][ph['k']]['f'] = 'Y'
            data[ti][ph['k']]['fv'] = 'Y'

    def add_column_formula(self, ti, dd, row_form, res, rr, ph, data, key_map_rev, key_map_rev_t, txn, disp_name, csv_d):
        formula = []
        rid = ti
        d_formula = []
        for col_k in row_form.get(ph['k'], {}).keys():
            tmp_arr     = []
            d_tmp_arr   = []
            col_k       = ((ti, ph['k'], '='), )+col_k
            for c_k in col_k:
                rid, op, c_k = c_k
                rv_d    = data[rid].get(c_k, {})
                t       = rv_d['v']
                clean_value = t
                try:
                    clean_value = numbercleanup_obj.get_value_cleanup(t)
                except:
                    clean_value = ''
                    pass
                if rv_d:
                    c_d = {'doc_id':rv_d['d'], 'op':op, 'ph':ph['ph'], 't':data[rid]['f_taxo'], 'ty':disp_name, 'page_no':rv_d['x'].split(':@:')[0].split('#')[0].split('_')[1], 'bbox':rv_d['bbox'], 'v':clean_value, 'label':data[rid]['t_l']}
                else:
                    c_d = {'doc_id':'', 'op':op, 'ph':ph['ph'], 't':data[rid]['f_taxo'], 'ty':disp_name, 'page_no':'', 'bbox':'', 'row_id':row_ind_map[rid], 'v':'', 'label':data[rid]['t_l']}
                tmp_arr.append(c_d)

                if rv_d:
                    d_tmp_d   = self.get_gv_dict(rv_d, txn, key_map_rev_t, ph_map[c_k], csv_d)
                    d_tmp_d['t']= rv_d['t']
                    d_tmp_d['x']= rv_d['x']
                else:
                    d_tmp_d   = {}
                d_tmp_d[key_map_rev_t['description']]   = res[0]['data'][rid]['t_l']
                d_tmp_d[key_map_rev_t['taxo_id']]   = res[0]['data'][rid]['t_id']
                d_tmp_d[key_map_rev_t['operator']]      = op
                if op == '=':
                    d_tmp_d['R']    = 'Y'
                d_tmp_arr.append(d_tmp_d)
        if formula:
            dd[key_map_rev['f_row']]  = formula
        if d_formula:
            res[0]['data'][ti][ph['k']]['f_row'] = d_formula


    def check_formula(self, rr, f_phs, txn):
        f_d = {}
        all_xml = {}
        g_ng    = {}
        for ii, ph in enumerate(f_phs):
            if ph['k'] not in rr:continue
            v_d = rr[ph['k']]
            if v_d.get('rest_ar', []):continue
            table_id    = v_d['t']
            key         = table_id+'_'+self.get_quid(v_d['x'])
            all_xml[v_d['x']] = 1
            for gflg in ['G', 'NG']:
                for rc in ['COL', 'ROW']:
                    gcom        = txn.get('COM_OP_'+gflg+'_'+rc+'MAP_'+key)
                    if gcom:
                        for eq in gcom.split('|'):
                            formula, eq_str = eq.split(':$$:')
                            g_ng[gflg]=1
                            #f_taxo_arr[ii].setdefault('gcom', {})[c_id] = formula
                            f_d.setdefault((gflg, rc, formula), {})[(table_id, v_d['x'])]   = eq_str.split('^')
        return g_ng,f_d, all_xml


    def read_final_output(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        path         = '/mnt/eMB_db/%s/%s/FINAL_OUTPUT/'%(company_name, model_number)
        env1        = lmdb.open(path, readonly=True)
        text        = ijson.get('text', '')
        res = [{'message':'Error'}]
        with env1.begin() as txn1:
            op  = txn1.get('DISPLAYOUTPUT_'+str(text))
            if op:
                res = eval(op)
        return res

    def read_final_output_demo(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        path        = '/mnt/eMB_db/%s/%s/FINAL_OUTPUT/'%(company_name, model_number)
        env1        = lmdb.open(path, readonly=True)
        text        = ijson.get('ph', 'P')
        res         = [{'message':'Error'}]
        k           = ijson['table_type']
        with env1.begin() as txn1:
            op  = txn1.get('OUTPUT_'+str(k))
            if op:
                res = eval(op)
                
                pass
        return []

    def read_final_output_ks(self, ijson):
        res = [{'message':'done', 'data':[]}]
        return res
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        path         = '/mnt/eMB_db/%s/%s/FINAL_OUTPUT/'%(company_name, model_number)
        env1        = lmdb.open(path, readonly=True)
        text        = ijson.get('text', '')
        table_type  = ijson['table_type']
        arr = [{'k':'null', 'n':''}]
        order_d     = self.order_d
        txtpath      = '/var/www/html/DB_Model/%s/'%(company_name)
        os.system("mkdir -p "+txtpath)
        with env1.begin() as txn1:
            ks  = []
            for k, v in txn1.cursor():
                if 'DISPLAYOUTPUT_' in k:
                    try:
                        k_sp    = k.split('DISPLAYOUTPUT_')[1].split('-')[0][0]
                    except:continue
                    if str(order_d[table_type]) != k_sp :continue
                    fname   = txtpath+'/'+k.split('DISPLAYOUTPUT_')[1]+'.txt'
                    if not os.path.exists(fname):continue
                    ks.append(k.split('DISPLAYOUTPUT_')[1])
            for k in ks:
                #try:
                txt_sp =txn1.get('TEXT_VALUE_'+k).split(':@@:')
                txt = binascii.a2b_hex(txt_sp[0])
                arr.append({'k':k, 'n':txt, 'ph':''.join(txt_sp[1:])})
                #if table_type   == arr[-1]['n']:
                #    arr[-1]['n']    = 'ALL'
                #except:pass
        res = [{'message':'done', 'data':arr}]
        return res


    def get_gv_dict(self, v_d, txn, key_map_rev, ph, csv_d, actual_v_d={}, add_phcsv=None):
        t       = v_d['v']
        clean_value = t
        try:
            clean_value = numbercleanup_obj.get_value_cleanup(t)
        except:
            clean_value = ''
            pass
        dd  = {
                key_map_rev['actual_value']     : v_d['v'],
                #key_map_rev['clean_value']      : clean_value if str(v_d['t']) not in actual_v_d else v_d['v'],
                key_map_rev['d']                : v_d['d'],
                key_map_rev['pno']              : v_d['x'].split(':@:')[0].split('#')[0].split('_')[1].split('@')[0] if v_d['x'] else '',
                key_map_rev['bbox']             : v_d['bbox'],
                key_map_rev['ph']               : ph['ph'],
                key_map_rev['table_id']         : v_d['t'],
                key_map_rev['xml_id']           : v_d['x'],
                key_map_rev['edited_value']     : v_d.get('edited', ''),
                }
        if str(v_d['t']) not in actual_v_d:
            dd[key_map_rev['clean_value']]  = clean_value
        else:
            dd[key_map_rev['clean_value']]  =  v_d['v']
            dd[key_map_rev['actual_value_flg']]  =  'Y'
        #print [v_d.get('kpi_re_stated_all')]
        if v_d.get('kpi_re_stated_all'):
            dd[key_map_rev['re_stated_all']]    = v_d['kpi_re_stated_all']
        if v_d.get('org_d'):
                dd[key_map_rev['as_reported_value']]     = v_d.get('org_d', {}).get('org_v', '')
                dd[key_map_rev['as_reported_phcsv']]     = v_d.get('org_d', {}).get('org_phcsv', '')
        else:
                dd[key_map_rev['as_reported_value']]     = v_d.get('org_v', '')
                dd[key_map_rev['as_reported_phcsv']]     = copy.deepcopy(v_d.get('org_phcsv', ''))
        dd[key_map_rev['re_stated_cell']]     = v_d.get('re_stated_cell', '')
        #print (dd[key_map_rev['as_reported_value']], dd[key_map_rev['as_reported_phcsv']])
        #sys.exit()
            
        
        if v_d.get('c_s', ''):
            dd[key_map_rev['checksum']] = v_d.get('c_s', '')
        
        key     = v_d['t']+'_'+self.get_quid(v_d['x'].strip('RESTATED_'))
        period_type, period, currency, scale, value_type    = '', '', '', '', ''
        if v_d.get('phcsv', {}):
            period_type, period, currency, scale, value_type    = v_d['phcsv']['p'], v_d['phcsv']['pt'], v_d['phcsv']['c'], v_d['phcsv']['s'], v_d['phcsv']['vt']
        #period_type, period, currency, scale, value_type    = ph_map.split('^')
        #ph_map  = txn.get('PH_MAP_'+str(key))
        if add_phcsv != 'N':
            #period_type, period, currency, scale, value_type    = ph_map.split('^')
            if(v_d['t'], v_d['x']) in csv_d    :
                period_type, period, currency, scale, value_type    = csv_d[(v_d['t'], v_d['x'])]
            dd[key_map_rev['gv_ph']]        = period_type+period
            if v_d.get('fv', '') == 'Y':
                dd[key_map_rev['gv_ph']]        = ph['ph']
            dd[key_map_rev['currency']]  = currency
            dd[key_map_rev['scale']]     = scale
            dd[key_map_rev['value_type']]= value_type
        return dd

    def clean_value(self, txt):
        return txt

    def insert_new_taxonomy(self, ijson):
        import create_table_seq
        obj = create_table_seq.TableSeq()
        return obj.insert_new_taxonomy(ijson)

    def re_order(self, ijson):
        return self.re_order_seq(ijson)
        return
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        #m_tables, rev_m_tables, doc_m_d = self.get_main_table_info(company_name, model_number)
        ph_d        = {}
        path    = "%s/%s/%s/1_1/21/sdata/doc_map.txt"%(self.doc_path, project_id, deal_id)
        if os.path.exists(path):
            fin = open(path, 'r')
            lines   = fin.readlines()
            fin.close()
        else:
            lines   = []
        doc_d       = {}
        dphs        = {}
        #c_year      = int(datetime.datetime.now().strftime('%Y'))
        c_year      = self.get_cyear(lines)
        #start_year  = c_year - int(ijson['year'])
        start_year  = c_year - int(ijson.get('year', 5))
        for line in lines[1:]:
            line    = line.split('\t')
            if len(line) < 8:continue
            line    = map(lambda x:x.strip(), line)
            ph      = line[3]+line[7]
            if ph and start_year<int(ph[2:]):
                doc_id  = line[0]
                if ijson.get('ignore_doc_ids', {}) and doc_id in  ijson.get('ignore_doc_ids', {}):continue
                doc_d[doc_id]   = (ph, line[2])
                dphs[ph]        = 1

                    

        i_table_type    = ijson['table_type']
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, [i_table_type])
        #m_path          = self.taxo_path%(company_name, model_number)
        #path            = m_path+'/TAXO_RESULT/'
        #env1        = lmdb.open(path, max_dbs=27)
        #db_name1    = env1.open_db('other')
        #txn         = env1.begin(db=db_name1)

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn         = env.begin()

        #lmdb_path   = '/var/www/html/Rajeev/BBOX/'+str(project_id)+'_'+str(deal_id)
        #lmdb_path    = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        #env1    = lmdb.open(lmdb_path)
        #txn1    = env1.begin()
        table_ph_d  = {}
        all_ph_d    = {}
        table_type  = str(i_table_type)
        f_taxo_arr  = [] #json.loads(txn.get('TABLE_RESULTS_'+table_type, "[]"))
        taxo_d      = {} #json.loads(txn.get('TABLE_RESULTS_'+table_type, "[]"))
        table_ids   = {}
        g_ar        = []
        table_col_phs   = {}
        ph_d        = {}
        phcsv_d     = {}
        if not taxo_d:
            db_file         = self.get_db_path(ijson)
            conn, cur   = conn_obj.sqlite_connection(db_file)
            if not ijson.get('vids', {}):
                sql = "select group_id, table_type, group_txt from vgh_group_info where table_type='%s'"%(table_type)
                try:
                    cur.execute(sql)
                    res = cur.fetchall()
                except:
                    res = []
                grp_info    = {}
                for rr in res:
                    group_id, table_type, group_txt = rr
                    group_txt = group_txt.replace('\t', '')
                    grp_info[str(group_id)]   = group_txt
                sql         = "select vgh_id, group_txt from vgh_group_map where table_type='%s'"%(table_type)
                cur.execute(sql)
                res         = cur.fetchall()
                group_d     = {}
                for rr in res:
                    vgh_id, group_txt   = rr
                    group_d.setdefault(group_txt, {})[vgh_id]   = 1
                g_ar    = []
                for k, v in group_d.items():
                    dd  = {'n':grp_info.get(k, k), 'vids':v.keys(), 'grpid':k}
                    g_ar.append(dd)
            if not ijson.get('vids', {}):
                sql         = "select row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, doc_id, m_rows, vgh_text, vgh_group, xml_id, isvisible from mt_data_builder where table_type='%s' and isvisible='Y'"%(table_type)
            else:
                sql         = "select row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, doc_id, m_rows, vgh_text, vgh_group, xml_id, isvisible from mt_data_builder where table_type='%s' and isvisible='Y' and vgh_text in (%s)"%(table_type, ', '.join(ijson['vids']))
            cur.execute(sql)
            res         = cur.fetchall()
            for rr in res:
                row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, gv_xml, ph, ph_label,gcom, ngcom, doc_id,m_rows, vgh_text, vgh_group, xml_id, isvisible    = rr
                doc_id      = str(doc_id)
                if doc_id not in doc_d:continue
                table_id    = str(table_id)
                if table_id not in m_tables:continue
                tk   = self.get_quid(table_id+'_'+xml_id)
                c_id        = txn_m.get('XMLID_MAP_'+tk)
                if not c_id:continue
                gv_xml  = c_id
                c   = int(c_id.split('_')[2])
                table_col_phs.setdefault((table_id, c), {})[ph]   = table_col_phs.setdefault((table_id, c), {}).get(ph, 0) +1
                table_ids[table_id]   = 1
                comp    = ''
                #if gcom == 'Y' or ngcom == 'Y':
                #    comp    = 'Y'
                taxo_d.setdefault(taxo_id, {'l_change':{},'order_id':order_id, 'rid':row_id, 'u_label':user_taxonomy, 't_l':taxonomy, 'missing':missing_taxo, 'comp':comp, 'ks':[], 'm_rows':m_rows})['ks'].append((table_id, gv_xml, ph, ph_label,gcom, ngcom, doc_id, xml_id))
                if vgh_group == 'N':
                    taxo_d[taxo_id]['l_change'][xml_id]  = 1
            conn.close()
        r_ld    = {}
        for table_id in table_ids.keys():
            k       = 'HGH_'+str(table_id)
            ids     = txn_m.get(k)
            if ids:
                ids     = ids.split('#')
                for c_id in ids:
                    r       = int(c_id.split('_')[1])
                    c       = int(c_id.split('_')[2])
                    x       = txn_m.get('XMLID_'+c_id)
                    key     = table_id+'_'+self.get_quid(x)
                    t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                    t       = ' '.join(t.split())
                    rs      = int(txn_m.get('rowspan_'+c_id))
                    for tr in range(rs): 
                        r_ld.setdefault(table_id, {}).setdefault(r+tr, []).append((c, t))
        f_ar        = []
        done_d      = {}
        #for dd in f_taxo_arr:
        tmptable_col_phs    = {}
        for k, v in table_col_phs.items():
            phs = v.keys()
            phs.sort(key=lambda x:v[x], reverse=True)
            tmptable_col_phs[k] = phs[0]
        row_ids = taxo_d.keys()
        row_ids.sort(key=lambda x:len(taxo_d[x]['ks']), reverse=True)
        f_taxo_arr   = []
        for row_id in row_ids:
            dd  = {'t_l':[str(row_id)], 'ks':map(lambda x:(str(x[0]), str(x[1])), taxo_d[row_id]['ks'])}
            f_taxo_arr.append(dd)
        import create_table_seq
        obj = create_table_seq.TableSeq()
        ph_d    = {}
        phcsv_d = {}
        f_taxo_ar  = obj.resolve_conflict_by_gcom(f_taxo_arr, txn_m, txn, ph_d, phcsv_d, doc_m_d)
        new_order   = []
        new_order_d = {}
        for ii, rr in enumerate(f_taxo_ar):
            new_order_d[rr['t_l'][0]] = ii+1
            new_order.append((ii+1, rr['t_l'][0], table_type))
        db_file         = self.get_db_path(ijson)
        conn, cur   = conn_obj.sqlite_connection(db_file)
        cur.executemany("update mt_data_builder set order_id=? where taxo_id=? and table_type=?", new_order)
        conn.commit()
        conn.close()
        res = [{'message':'done', 'order_d':new_order_d}]
        return res
            
            



        

        

        
    


        
    

            
            


        
        
        


    def find_missing_table(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        m_tables, rev_m_tables, doc_m_d, table_type_m = self.get_main_table_info(company_name, model_number)
        db_file         = self.get_db_path(ijson)
        print db_file
        conn, cur       = conn_obj.sqlite_connection(db_file)
        sql             = "select table_type, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, vgh_text, vgh_group, doc_id, xml_id,m_rows from mt_data_builder"
        cur.execute(sql)
        res             = cur.fetchall()
        table_ds        = {}
        for rr in res:
            table_type, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, vgh_text, vgh_group, doc_id, xml_id, m_rows = rr
            if xml_id:
                table_ds[table_id]        = (table_type, doc_id)
        conn.close()
        missing_table   = list(sets.Set(m_tables.keys()) - sets.Set(table_ds.keys()))
        print len(missing_table), missing_table
        ijson['table_ids']  = missing_table
        #self.add_new_table(ijson)


    def re_order_seq(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        order_tid       = ijson.get('table_id')
        #m_tables, rev_m_tables, doc_m_d = self.get_main_table_info(company_name, model_number)
        ph_d        = {}
        path    = "%s/%s/%s/1_1/21/sdata/doc_map.txt"%(self.doc_path, project_id, deal_id)
        if os.path.exists(path):
            fin = open(path, 'r')
            lines   = fin.readlines()
            fin.close()
        else:
            lines   = []
        doc_d       = {}
        dphs        = {}
        #c_year      = int(datetime.datetime.now().strftime('%Y'))
        c_year      = self.get_cyear(lines)
        #start_year  = c_year - int(ijson['year'])
        start_year  = c_year - int(ijson.get('year', 5))
        for line in lines[1:]:
            line    = line.split('\t')
            if len(line) < 8:continue
            line    = map(lambda x:x.strip(), line)
            ph      = line[3]+line[7]
            print [ph]
            if ph and (start_year<int(ph[-4:]) or ijson.get('taxo_flg') == 1):
                doc_id  = line[0]
                if ijson.get('ignore_doc_ids', {}) and doc_id in  ijson.get('ignore_doc_ids', {}):continue
                doc_d[doc_id]   = (ph, line[2])
                dphs[ph]        = 1

                    

        i_table_type    = ijson['table_type']
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, [i_table_type])
        #m_path          = self.taxo_path%(company_name, model_number)
        #path            = m_path+'/TAXO_RESULT/'
        #env1        = lmdb.open(path, max_dbs=27)
        #db_name1    = env1.open_db('other')
        #txn         = env1.begin(db=db_name1)

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn         = env.begin()

        #lmdb_path   = '/var/www/html/Rajeev/BBOX/'+str(project_id)+'_'+str(deal_id)
        #lmdb_path    = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        #env1    = lmdb.open(lmdb_path)
        #txn1    = env1.begin()
        table_ph_d  = {}
        all_ph_d    = {}
        table_type  = str(i_table_type)
        f_taxo_arr  = [] #json.loads(txn.get('TABLE_RESULTS_'+table_type, "[]"))
        taxo_d      = {} #json.loads(txn.get('TABLE_RESULTS_'+table_type, "[]"))
        table_ids   = {}
        g_ar        = []
        table_col_phs   = {}
        ph_d        = {}
        phcsv_d     = {}
        table_xml_d = {}
        if not taxo_d:
            db_file         = self.get_db_path(ijson)
            #print ">>>>>>>>>>>>>>"
            #print db_file
            
            conn, cur   = conn_obj.sqlite_connection(db_file)
            if not ijson.get('vids', {}):
                sql         = "select row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, doc_id, m_rows, vgh_text, vgh_group, xml_id, isvisible from mt_data_builder where table_type='%s' and isvisible='Y'"%(table_type)
            else:
                sql         = "select row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, doc_id, m_rows, vgh_text, vgh_group, xml_id, isvisible from mt_data_builder where table_type='%s' and isvisible='Y' and vgh_text in (%s)"%(table_type, ', '.join(ijson['vids']))
            cur.execute(sql)
            res         = cur.fetchall()
            for rr in res:
                row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, gv_xml, ph, ph_label,gcom, ngcom, doc_id,m_rows, vgh_text, vgh_group, xml_id, isvisible    = rr
                doc_id      = str(doc_id)
                if doc_id not in doc_d:continue
                table_id    = str(table_id)
                if table_id not in m_tables:continue
                tk   = self.get_quid(table_id+'_'+xml_id)
                c_id        = txn_m.get('XMLID_MAP_'+tk)
                if not c_id:continue
                gv_xml  = c_id
                c   = int(c_id.split('_')[2])
                table_col_phs.setdefault((table_id, c), {})[ph]   = table_col_phs.setdefault((table_id, c), {}).get(ph, 0) +1
                table_ids[table_id]   = 1
                comp    = ''
                #if gcom == 'Y' or ngcom == 'Y':
                #    comp    = 'Y'
                taxo_d.setdefault(taxo_id, {'l_change':{},'order_id':order_id, 'rid':row_id, 'u_label':user_taxonomy, 't_l':taxonomy, 'missing':missing_taxo, 'comp':comp, 'ks':[], 'm_rows':m_rows})['ks'].append((table_id, gv_xml, ph, ph_label,gcom, ngcom, doc_id, xml_id))
                if (deal_id, table_type) in self.ph_grp_d :
                    c_ph        = ph
                    c_tlabel    = '' #tlabel
                else:
                    c_ph        = str(c)
                    c_tlabel    = ''
                if (deal_id, table_type) in self.ph_grp_d :
                    table_ph_d.setdefault((doc_id, table_id), {})[(c_ph, c_tlabel)]   = (c, ph)
                else:
                    table_ph_d.setdefault((doc_id, table_id), {})[(c_ph, c_tlabel)]   = (c, ph)
                if vgh_group == 'N':
                    taxo_d[taxo_id]['l_change'][xml_id]  = 1
                
                x    = self.clean_xmls(xml_id)
                if x:
                    x    = x.split(':@:')[0].split('#')[0]
                    table_xml_d.setdefault(table_id, {})[(int(x.split('_')[1].split('@')[0]), int(x.split('_')[0].strip('x').strip('DUMM')))]   = 1
            conn.close()
        r_ld    = {}
        for table_id in table_ids.keys():
            k       = 'HGH_'+str(table_id)
            ids     = txn_m.get(k)
            if ids:
                ids     = ids.split('#')
                for c_id in ids:
                    r       = int(c_id.split('_')[1])
                    c       = int(c_id.split('_')[2])
                    x       = txn_m.get('XMLID_'+c_id)
                    key     = table_id+'_'+self.get_quid(x)
                    t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                    t       = ' '.join(t.split())
                    rs      = int(txn_m.get('rowspan_'+c_id))
                    for tr in range(rs): 
                        r_ld.setdefault(table_id, {}).setdefault(r+tr, []).append((c, t))
        f_ar        = []
        done_d      = {}
        #for dd in f_taxo_arr:
        tmptable_col_phs    = {}
        for k, v in table_col_phs.items():
            phs = v.keys()
            phs.sort(key=lambda x:v[x], reverse=True)
            tmptable_col_phs[k] = phs[0]
        row_ids = taxo_d.keys()
        row_ids.sort(key=lambda x:len(taxo_d[x]['ks']), reverse=True)
        f_taxo_arr   = []
        for row_id in row_ids:
            dd  = {'t_l':[str(row_id)], 'ks':map(lambda x:(str(x[0]), str(x[1])), taxo_d[row_id]['ks'])}
            f_taxo_arr.append(dd)
        import create_table_seq
        obj = create_table_seq.TableSeq()
        ph_d    = {}
        phcsv_d = {}
        dphs = report_year_sort.year_sort(dphs.keys())
        dphs.reverse()
        table_ids   = table_ph_d.keys()
        #try:
        #table_ids.sort(key=lambda x:(dphs.index(doc_d[x[0]][0]), x[1]))
        table_ids.sort(key=lambda x:(dphs.index(doc_d[x[0]][0]), sorted(table_xml_d[x[1]].keys())[0]))
        #except:
        #    table_ids.sort(key=lambda x:(x[0], x[1]))
        #    pass
        if order_tid:
            order_tid   = str(order_tid)
        f_taxo_ar  = self.order_by_table_structure(f_taxo_arr, txn_m, txn, ph_d, phcsv_d, doc_m_d,table_ids, order_tid)
        new_order   = []
        new_order_d = {}
        for ii, rr in enumerate(f_taxo_ar):
            #print ii,'. ', rr['t_l'][0]
            new_order_d[rr['t_l'][0]] = ii+1
            new_order.append((ii+1, rr['t_l'][0], table_type))
        if ijson.get('update_db', '') == 'Y':# and str(deal_id) in ['219', '214', '221']:
            db_file         = self.get_db_path(ijson)
            conn, cur   = conn_obj.sqlite_connection(db_file)
            cur.executemany("update mt_data_builder set order_id=? where taxo_id=? and table_type=?", new_order)
            conn.commit()
            conn.close()
            pass
        res = [{'message':'done', 'order_d':new_order_d}]
        return res

    def clean_xmls(self, xml):
        xml     = xml.replace('@@@', ':@:') 
        fxml    = []
        for x in xml.split(':@:'):
            xml_lst     = []
            for tx in x.split('#'):
                if tx.strip():
                    xml_lst.append(tx)
            x   = str('#'.join(xml_lst))
            fxml.append(x)
        return ':@:'.join(fxml)

    def order_by_table_structure(self, f_taxo_arr, txn_m, txn, ph_d, phcsv_d, doc_m_d, table_ids, order_tid=None):
        tmptable_ids   = map(lambda x:x[1], table_ids)
        if order_tid and order_tid in tmptable_ids:
            tmptable_ids.remove(order_tid)
            tmptable_ids    = [order_tid]+tmptable_ids
        table_match_d   = {}
        for ti, dd in enumerate(f_taxo_arr):
            ks      = dd['ks']
            for table_id, c_id in ks:
                table_match_d.setdefault(table_id, {}).setdefault(ti, {})[c_id] = 1
        tabl_ids    = table_match_d.keys()
        tabl_ids.sort(key=lambda x:tmptable_ids.index(x))
        #tabl_ids.sort(key=lambda x:len(table_match_d[x].keys()), reverse=True)
        final_arr   = []
        for table_id in tabl_ids:
            inds    = table_match_d[table_id].keys()
            inds.sort()
            #print '\n==========================================================='
            #print table_id, sorted(inds)
            #inds.sort(key=lambda x:int(table_match_d[table_id][x].keys()[0].split('_')[1]))
            inds.sort(key=lambda x:int(sorted(map(lambda x1:int(x1.split('_')[1]), table_match_d[table_id][x].keys()))[0]))
            #print 'Ordered ', inds
            if not final_arr:
                final_arr   = inds
            else:
                m_d = list(sets.Set(final_arr).intersection(sets.Set(inds)))
                deletion    = {}
                tmp_arr     = []
                ftmp_arr     = []
                for t in inds:
                    if t in m_d:
                        ftmp_arr    = []
                        if tmp_arr:
                            deletion[t] = copy.deepcopy(tmp_arr[:])
                            #tmp_arr = []    
                        continue
                    tmp_arr.append(t)
                    ftmp_arr.append(t)
                done_d  = {}
                m_d.sort(key=lambda x:final_arr.index(x))
                
                for t in m_d:
                    if t in deletion:
                        tmp_arr = []
                        for t1 in deletion[t]:
                            if t1 not in done_d:
                                tmp_arr.append(t1)
                                done_d[t1]  = 1
                        index       = final_arr.index(t)
                        final_arr   = final_arr[:index]+tmp_arr+final_arr[index:]    
                
                if ftmp_arr:
                    final_arr   = final_arr+ftmp_arr
            
            #print 'FINAL ', final_arr
        missing = sets.Set(range(len(f_taxo_arr))) - sets.Set(final_arr)
        if len(final_arr) > len(f_taxo_arr):
            print 'Duplicate ', final_arr
            sys.exit()
        if len(missing):
            print 'missing ', list(missing)
            sys.exit()
        f_taxo_arr  = map(lambda x:f_taxo_arr[x], final_arr)
        return f_taxo_arr

    def auto_validate_order(self, ijson):
        deal_ids         = ijson['deal_ids']
        cinfo   = self.read_company_info({"cids":deal_ids})
        order_missmatch = {}
        done            = {}
        all_t           = {}
        for deal_id in deal_ids:
            ijson   = cinfo[int(deal_id)]
            company_name    = ijson['company_name']
            mnumber         = ijson['model_number']
            model_number    = mnumber
            m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number)
            for ii, table_type in enumerate(rev_m_tables.keys()):
                print 'Running ', ii, ' / ', len(rev_m_tables.keys()), [table_type, ijson['company_name']]
                ijson['table_type'] = table_type
                res                 = self.create_seq_across(ijson)
                phs                 = copy.deepcopy(res[0]['phs'])
                data                = copy.deepcopy(res[0]['data'])
                order               = self.re_order_seq(ijson)[0]['order_d']
                key                 = (table_type,  ijson['company_name'])
                all_t[key]           = len(order.keys())
                tmp_md              = {}
                for ii, rr in enumerate(data):
                    if order[str(rr['t_id'])] != ii+1:
                        tmp_md[rr['t_id']]  = (rr['t_l'], ii+1, order[str(rr['t_id'])])
                if tmp_md:
                    order_missmatch[key]    = tmp_md
                else:
                    done[key]               = 1
        t   = len(all_t.keys())
        ks  = order_missmatch.keys()
        ks.sort(key=lambda x:len(order_missmatch[x].keys()))
        for k in ks:
            v   = order_missmatch[k]
            print k, len(v.keys()), ' / ', all_t[k]
            for k1, v1 in v.items():
                print '\t', k1, v1
        print 'Total ', t
        print 'Matched  ', len(done.keys())
        print 'Not Matched  ', len(order_missmatch.keys())
        
    def validate_cell_data(self, data, phs, actual_v_d):
        dmap = {'0':1, '1':1, '2':1, '3':1, '4':1, '5':1, '6':1, '7':1, '8':1, '9':1}
        invalid_cell_dict = {}
        for ph_dict in phs:
            mkey = ph_dict['k']
            for ddict in data:
                val_map = ddict.get(mkey, {})
                taxo_id = ddict.get('t_id', '')
                if not taxo_id:continue
                if val_map and str(val_map.get('t', '')) not in actual_v_d:
                    cell_txt    = self.convert_html_entity(val_map.get('v', ''))
                    final_cell_txt = ''
                    for t in cell_txt:
                        if t in dmap:
                            final_cell_txt += t
                    
                    if final_cell_txt.strip():
                        clean_value = cell_txt
                        try:
                            clean_value = numbercleanup_obj.get_value_cleanup(cell_txt)
                        except:
                            clean_value = ''
                        if (not clean_value.strip()):
                            invalid_cell_dict.setdefault(taxo_id, {})[mkey] = 1

                    cell_bbox   = val_map.get('bbox', [])
                    if not cell_bbox:
                        if not invalid_cell_dict.get(taxo_id, {}).get(mkey, ''):
                            invalid_cell_dict.setdefault(taxo_id, {})[mkey] = 2
                        else:
                            invalid_cell_dict.setdefault(taxo_id, {})[mkey] = 3
        #print invalid_cell_dict
        return invalid_cell_dict

    def disable_duplicate(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type      = ijson['table_type']
        ijson['taxo_flg']   = 1
        #if ijson.get('custom') == 'Y':
        #    ijson['taxo_flg']   = 0
        db_file         = self.get_db_path(ijson)
        #print db_file
        conn, cur       = conn_obj.sqlite_connection(db_file)
        t_ids            = map(lambda x:str(x), ijson['t_ids'])
        dtime   = str(datetime.datetime.now()).split('.')[0]
        if ijson.get('custom') == 'Y':
            sql             = "delete from mt_data_builder  where table_type='%s' and taxo_id in (%s)"%(table_type, ','.join(t_ids))
        else:
            sql             = "update mt_data_builder set isvisible='N', user_name='%s', datetime='%s' where table_type='%s' and taxo_id in (%s)"%(ijson['user'], dtime, table_type, ','.join(t_ids))
        #print sql
        cur.execute(sql)
        conn.commit()
        conn.close()
        res = [{'message':'done', 'd_ids':t_ids}]
        return res

    def restore_taxo(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type      = ijson['table_type']
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        t_ids            = map(lambda x:str(x), ijson['t_ids'])
        sql             = "update mt_data_builder set isvisible='Y' where table_type='%s' and taxo_id in (%s)"%(table_type, ','.join(t_ids))
        cur.execute(sql)
        conn.commit()
        conn.close()
        res = [{'message':'done', 'd_ids':t_ids}]
        return res

    def insert_ph_formula_frm_data(self, ijson):    
        dd  = self.ph_deivation_ar
        dd  = list(sets.Set(dd))
        ijson['data']   = dd
        ijson['user']   = 'SYSTEM'
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        db_file         = self.get_db_path(ijson)
        conn, cur   = conn_obj.sqlite_connection(db_file)
        sql = 'drop table if exists ph_formula'
        cur.execute(sql)
        conn.commit()
        conn.close()
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number)
        for table_type in rev_m_tables.keys():
            ijson['table_type'] = table_type
            self.insert_ph_formula(ijson)
        return self.read_ph_formula(ijson)

    def insert_ph_formula(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type  = ijson['table_type']
        db_file         = self.get_db_path(ijson)
        conn, cur   = conn_obj.sqlite_connection(db_file)
        sql         = 'CREATE TABLE IF NOT EXISTS ph_formula(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_type TEXT, ph TEXT, formula TEXT, operands TEXT, user_name TEXT, datetime TEXT)'
        cur.execute(sql)
        sql         = "select ph, formula, operands from ph_formula where table_type='%s'"%(table_type)
        cur.execute(sql)
        res         = cur.fetchall()
        exists      = {}
        for rr in res:
            ph, formula, operands   = rr
            operands    = operands.split(',')
            operands.sort()
            exists[(ph, formula, tuple(operands))]  = 1
        dtime   = str(datetime.datetime.now()).split('.')[0]
        user    = ijson['user']
        i_ar    = []
        opr     = {
                    '=' : 1,
                    '+' : 1,
                    '-' : 1,
                    '*' : 1,
                    '/' : 1,
                    '(' : 1,
                    ')' : 1,
                    }
        for rr in ijson['data']:
            if not rr.strip():continue
            ph, op_str  = rr.split('=')
            formula = rr
            operands    = []
            tmp_ar  = []
            for c in op_str:
                if c in opr:
                    if tmp_ar:
                        operands.append(''.join(tmp_ar))
                        tmp_ar  = []
                elif c:
                    tmp_ar.append(c)
            if tmp_ar:
                operands.append(''.join(tmp_ar))
            operands.sort()
            operands    = tuple(operands)
            if (ph, formula, operands) not in exists:
                i_ar.append((table_type, ph, formula, ','.join(operands), user, dtime))
        cur.executemany('insert into ph_formula(table_type, ph, formula, operands, user_name, datetime)values(?,?,?,?,?,?)', i_ar)
        conn.commit()
        conn.close()
        return self.read_ph_formula(ijson)
    
    def read_ph_formula(self, ijson, ret_flg=None):
        opr     = {
                    '=' : 1,
                    '+' : 1,
                    '-' : 1,
                    '*' : 1,
                    '/' : 1,
                    '(' : 1,
                    ')' : 1,
                    }
        dd_ar  = self.ph_deivation_ar
        deriv_phs   = self.get_deriv_phs(ijson)
        ph_f_map    = {}
        map_d       = {}
        for row_id, rr in enumerate(dd_ar):
            ph, op_str  = rr.split('=')
            if deriv_phs and ph not in deriv_phs:continue
            formula = rr
            operands    = []
            tmp_ar  = []
            for c in op_str:
                if c in opr:
                    if tmp_ar:
                        operands.append(''.join(tmp_ar))
                        tmp_ar  = []
                elif c:
                    tmp_ar.append(c)
            if tmp_ar:
                operands.append(''.join(tmp_ar))
            operands.sort()
            formula = rr
            map_d[formula]       = row_id
            dd  = {'f':formula, 'op':operands, 'g_id':row_id, 'ph':ph}
            ph_f_map.setdefault(ph, []).append(dd) 
        if 0:
            company_name    = ijson['company_name']
            mnumber         = ijson['model_number']
            model_number    = mnumber
            deal_id         = ijson['deal_id']
            project_id      = ijson['project_id']
            company_id      = "%s_%s"%(project_id, deal_id)
            table_type  = ijson['table_type']
            db_file         = self.get_db_path(ijson)
            conn, cur   = conn_obj.sqlite_connection(db_file)
            sql         = 'CREATE TABLE IF NOT EXISTS ph_formula(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_type TEXT, ph TEXT, formula TEXT, operands TEXT, user_name TEXT, datetime TEXT)'
            cur.execute(sql)
            sql         = "select row_id, ph, formula, operands from ph_formula where table_type='%s'"%(table_type)
            cur.execute(sql)
            res         = cur.fetchall()
            conn.close()
            ph_f_map    = {}
            map_d       = {}
            for rr in res:
                row_id, ph, formula, operands   = rr
                map_d[formula]       = row_id
                dd  = {'f':formula, 'op':operands.split(','), 'g_id':row_id, 'ph':ph}
                ph_f_map.setdefault(ph, []).append(dd) 
        phs = ph_f_map.keys()
        phs.sort()
        f_ar    = []
        for ph in phs:
            f_ar    += ph_f_map[ph]
        if ret_flg == 'Y':
            return ph_f_map, f_ar, map_d
        res = [{'message':'done', 'data':f_ar}]
        return res

    def read_ph_config_formula(self, ijson, mgrp_id=None):
        if ijson.get('grpid', '') and mgrp_id==None:
            mgrp_id = ijson['grpid']
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type  = ijson['table_type']
        opr     = {
                    '=' : 1,
                    '+' : 1,
                    '-' : 1,
                    '*' : 1,
                    '/' : 1,
                    '(' : 1,
                    ')' : 1,
                    }
        deriv_phs   = self.get_deriv_phs(ijson)
        db_file         = self.get_db_path(ijson) #'/mnt/eMB_db/node_mapping.db' #self.get_db_path(ijson)
        conn, cur   = conn_obj.sqlite_connection(db_file)
        self.alter_table_coldef(conn, cur, 'table_type_wise_formula', ['null_type', 'not_null', 'main_header', 'group_id', 'pt_operator'])
        if mgrp_id:
            sql         = 'select period, period_expression, null_type,not_null, pt_operator, group_id from table_type_wise_formula where (table_type="%s" or main_header="%s") and group_id="%s" and project_name="%s"'%(table_type, table_type, mgrp_id, ijson.get('project_name', ''))
        else:
            sql         = 'select period, period_expression, null_type, not_null, pt_operator, group_id from table_type_wise_formula where (table_type="%s" or main_header="%s") and project_name="%s"'%(table_type, table_type, ijson.get('project_name', ''))
        try:
            cur.execute(sql)
            res         = cur.fetchall()
        except:
            res  = []
        ph_formula_d    = {}
        for rr in res:
            period, period_expression, null_type,not_null, ph_expression, group_id    = rr
            if group_id and str(mgrp_id) != str(group_id):continue
            if period not in deriv_phs:continue
            if period and period_expression:
                cond        = []
                cond_null   = []
                ph_expression_ar    = []
                if ph_expression:
                    for nph in ph_expression.split('##'):
                        nph = nph.strip()
                        if not nph:continue
                        nph = nph.split('_')
                        if len(nph) != 2:continue
                        if nph[0]:
                            ph_expression_ar.append(nph[0])
                        if nph[1]:
                            ph_expression_ar.append(nph[1])
                if null_type:
                    for nph in null_type.split('##'):
                        nph = nph.strip()
                        if not nph:continue
                        cond_null.append(' (not '+nph+' )')
                if not_null:
                    for nph in not_null.split('##'):
                        nph = nph.strip()
                        if not nph:continue
                        cond.append(nph)
                fcond   = ph_expression_ar
                if not ph_expression_ar:
                    fcond   = []
                    if cond:
                        fcond.append('( '+' and '.join(cond)+' )')
                    if cond_null:
                        fcond.append('( '+' and '.join(cond_null)+' )')
                formula = period_expression
                if '=' not in formula:
                    formula =period+'='+formula
                #print formula
                operands    = []
                tmp_ar      = []
                tph, op_str  = formula.split('=')
                for c in op_str:
                    #print '\t',[c, c in opr]
                    if c in opr:
                        if tmp_ar:
                            operands.append({'t':'op', 'v':''.join(tmp_ar)})
                            tmp_ar  = []
                        operands.append({'t':'opr', 'v':c})
                    elif c:
                        tmp_ar.append(c)
                #print [tmp_ar]
                if tmp_ar:
                    operands.append({'t':'op', 'v':''.join(tmp_ar)})
                #print operands
                if operands:
                    if ph_expression_ar:
                        operands[0]['cond'] = ' '.join(fcond)
                    else:
                        operands[0]['cond'] = ' and '.join(fcond)
                    ph_formula_d.setdefault(period, []).append((formula, operands, 'RID-0'))
        return ph_formula_d

    def get_formula_unit_scale(self, formula, ph, row, taxo_id_dict, phMap, tscale, tvtype, ph_map_d, op_inds, res_val, update_sign, sys_op_inds):
        conditiona_op   = {
                            'and':1,
                            'or':1,
                        }
        n_form              = []
        val_li, expr_str    = [], []
        cys_d               = {}
        re_ar               = []
        op_ind              = 0
        conver_arr          = []
        val_ar              = []
        ftype   = 'G'
        sv_ar             = []
        sv_ar_org             = []
        op_scals            = {}
        for oper in formula:
            oper        = copy.deepcopy(oper)
            tph         = ph
            s_ph        = ph 
            try:
                year        = int(ph[-4:])
            except:
                year        = ''
            if oper.get('k', ''):
                tph  = oper['k']
                s_ph    = oper['ph']
            elif oper.get('period', ''):
                s_ph    = oper['period']+str(int(year)-int(oper['yd']))
                tph  = ph_map_d.get(s_ph , s_ph)
            if oper['type'] != 'v' and oper['txid'] == 'PHConstant' and s_ph[:-4] in self.ph_constant:
                oper['type']    = 'v'
                oper['txid']    = self.ph_constant[s_ph[:-4]]
                
            op_txid    = oper['txid']
            op         = oper['op']
            op_type    = oper['type']
            ttype    = oper['t_type']
            grpid    = oper['g_id']
            to_dealid= oper['to_dealid']
            tph         = phMap.get((to_dealid, ttype, grpid), {}).get(tph, tph)
            #print '\n==================================='
            #print oper
            #print tph
            desc        = ''
            v_f         = ()
            conv_value  = 0
            clean_value = ''
            if op_type == 'v':
                op_val  = op_txid
                desc    = op_txid
                clean_value = op_val
                if op not in {'+':1, '-':1}:
                    ftype   = 'NG'
                oper['scale']           = '1'
                oper['value_type']      = 'MNUM'
            else:
                if op != '=':
                    if op_inds:
                        op  = op_inds[op_ind]
                    op_ind  += 1
                    if op not in {'+':1, '-':1}:
                        ftype   = 'NG'
                data_row    = taxo_id_dict.get((to_dealid, ttype, grpid), {}).get(op_txid, {}) 
                cell_d      = data_row.get(tph, {})
                op_val      = cell_d.get('v', '')
                tmp_rinfo = data_row
                    
                desc    = data_row.get('t_l', '')
                op_val  = numbercleanup_obj.get_value_cleanup(op_val)
                clean_value = op_val
                clean_value_org = op_val
                scale_org           = cell_d.get('phcsv', {}).get('s', '')
                #print [scale_org, clean_value_org]
                if clean_value_org and scale_org and scale_org.lower() not in ['1', 'one']:
                    tv, factor  = sconvert_obj.convert_frm_to_1(scale_org, '1', clean_value_org)
                    #print [tv, factor]
                    if factor != '':
                        clean_value_org = tv.replace(',', '')
                    pass
                if clean_value_org == '':
                    clean_value_org = '0'
                clean_value_org = float(clean_value_org)
                factors = []
                if op == '=':
                    oper['scale']           = tscale
                    if not tscale:
                        oper['scale']           = cell_d.get('phcsv', {}).get('s', '')
                    oper['value_type']      = tvtype
                    if not tvtype:
                        oper['value_type']      = cell_d.get('phcsv', {}).get('vt', '')
                else:
                    oper['scale']           = cell_d.get('phcsv', {}).get('s', '')
                    oper['value_type']      = cell_d.get('phcsv', {}).get('vt', '')
                if cell_d.get('x', None) != None:
                    v_d = cell_d
                    if v_d.get('phcsv', {}).get('s'):
                        op_scals[v_d['phcsv']['s']] = 1
                    if v_d.get('phcsv', {}).get('c') and not oper['c']:# in cconvert_obj.scale_map_d:
                        cys_d[v_d['phcsv']['c']]       = 1
                    if op != '=' or v_d.get('NO_RESTAT') != 'Y':
                        re_ar.append({'i':len(re_ar), 'x':v_d['x'], 'bbox':v_d.get('bbox', []), 't':v_d['t'], 'd':v_d['d'], 'phcsv':copy.deepcopy(v_d.get('phcsv', {})), 'org_v':v_d.get('org_v', ''), 'org_phcsv':copy.deepcopy(v_d.get('org_phcsv', {})), 're_stated_cell':v_d.get('re_stated_cell', ''), 'calc_value_db':v_d.get('calc_value_db', ''), 'kpi_re_stated_all':v_d.get('re_stated_all', [])})
                    p_cy        = cell_d.get('phcsv', {}).get('c')
                    if clean_value and oper['c'] and p_cy and oper['c'] != p_cy:
                        tv, factor          = cconvert_obj.convert_frm_to(p_cy, oper['c'], clean_value)
                        if factor != '':
                            factors.append(factor)
                            conver_arr.append((clean_value, tv, p_cy+' - '+oper['c'] ))
                            oper.setdefault('conv_value', {}).update({'v':tv, 'to_scale':oper['c'], 'from_scale':p_cy, 'factor':factor})
                            clean_value = tv.replace(',', '')
                            op_val      = clean_value
                            cys_d[oper['c']]    =  1
                            p_cy    = oper['c']
                    t_cy    = row.get('target_currency')
                    if t_cy and p_cy and t_cy != p_cy:
                        tv, factor          = cconvert_obj.convert_frm_to(p_cy, t_cy, clean_value)
                        if factor != '':
                            factors.append(factor)
                            conver_arr.append((clean_value, tv, p_cy+' - '+t_cy ))
                            oper.setdefault('conv_value', {}).update({'v':tv, 'to_scale':t_cy, 'from_scale':p_cy, 'factor':factor})
                            clean_value = tv.replace(',', '')
                            op_val      = clean_value
                            cys_d[t_cy]    =  1
                            p_cy    = t_cy
                if op != '=':
                    val_ar.append(clean_value)
                if clean_value == '':
                    clean_value = '0.0'
                clean_value = str(float(clean_value))
                if op != '=':
                    sv_ar.append(float(clean_value))
                    sv_ar_org.append(clean_value_org)
            oper['v']   = op_val
            oper['op']  = op
            oper['ph']  = s_ph
            oper['pk']  = tph
            oper['factors']  = factors
            if op == '=':
                n_form.append(copy.deepcopy(oper))
                continue
            expr_str += [str(op), desc] 
            if op in conditiona_op:
                op  = ' '+op+' '
            if op == 'M':
                tmpcell_val = ''
                try:
                    tmpcell_val = eval(''.join(val_li))
                except:pass
                if tmpcell_val in ['', 0]:
                    n_form      = n_form[:1]
                    val_li      = []
                    expr_str    = []
                    conver_arr  = []
                else:
                    break
                op  = ''
            
            #print '\t', [str(op), str(clean_value)]
            val_li += [str(op), str(clean_value)]
            if oper['op'] in conditiona_op:
                continue
            n_form.append(copy.deepcopy(oper))
            if oper['op'] == 'M':
                n_form[-1]['op']    = ''
        op_inds = []
        if not res_val and update_sign == 'Y' and ftype == 'G':
            if sys_op_inds:
                op_inds = sys_op_inds
        
        if res_val and update_sign == 'Y' and ftype == 'G':
            res_val_org = res_val
            if tscale and tscale.lower() not in ['1', 'one']:
                tv, factor  = sconvert_obj.convert_frm_to_1(tscale, '1', res_val_org)
                if factor != '':
                    res_val_org = tv.replace(',', '')
                pass
            if res_val_org == '':
                res_val_org = '0'
            sv_ar.append(float(res_val))
            sv_ar_org.append(float(res_val_org))
            #print 'input ar ', sv_ar
            #print 'input ar org ', sv_ar_org
            tmp_ar  = sv_ar
            #if filter(lambda x:x < 1.0, tmp_ar):
            #    tmp_ar  = sv_ar_org
                
            if tuple(tmp_ar) in self.sign_compute_d:
                f_sig   = self.sign_compute_d[tuple(tmp_ar)]
            else:
                f_sig   = tavinash.get_sig(tmp_ar)
                self.sign_compute_d[tuple(tmp_ar)]    = f_sig
            #print 'output ', f_sig
            if f_sig:
                op_inds = map(lambda x:'+' if x==0 else '-', f_sig[0])
            elif sys_op_inds:
                op_inds = sys_op_inds
            if op_inds:
                tmp_v_ar    = []
                for i, v1 in enumerate(sv_ar[:-1]):
                    op  = op_inds[i]
                    if op == '-':
                        #if v1 < 0:
                        #    op_inds[i]  = '+'
                        v1  = -1*v1
                    #elif op == '+':
                        #if v1 < 0:
                        #    op_inds[i]  = '-'
                    tmp_v_ar.append(v1)
                #print '\n'
                #print v_ar, tmp_v_ar
                #print v_ar[-1], sum(tmp_v_ar), abs(v_ar[-1]-sum(tmp_v_ar)) > 0
                sv_ar    = tmp_v_ar[:] #+[float(res_val)]
            else:
                sv_ar   = sv_ar[:-1]
        #print 'MMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMM', op_inds
        if op_inds:
            cind    = 0
            i       = 0
            #print 'DDDDDDDDDDDDDDDDDDDDDD', op_inds
            while i < len(val_li):
                #print 'CHNAGE ', val_li[i:i+2], op_inds[cind]
                val_li[i]    = op_inds[cind]
                n_form[cind+1]['op']  = op_inds[cind]
                i   += 2
                cind    += 1

        #for ft in n_form:
        #    print '\t', ft
        flg, exp_opr, exp_res, ind_ar   = frm_parse_obj.evaluate_unit_r(n_form)
        if not ind_ar:
            return exp_opr, exp_res, val_li, expr_str, n_form, re_ar, cys_d, conver_arr, val_ar, sv_ar, ftype, op_inds, op_scals
            #print exp_opr, ' vs: ', exp_res   # exp_opr has ':@:'
        else:
            return '', '',  val_li, expr_str, n_form, re_ar, cys_d, conver_arr, val_ar, sv_ar, ftype, op_inds, op_scals
            #print 'Error in ind_ar:', ind_ar 
        #sys.exit()      
        pass
            



    ## EVALUATION 
    def get_formula_evaluation_across(self, row, formula, taxo_id_dict, phs, phMap, tscale, tvtype, run_all=None, t_row_f_ptype={}, t_row_f_cell={}, update_sign=None, cmeta_info={}, f_op_d={}, raw_builder='N', form_info_d={}):
        ph_map_d={}
        year=None
        sys_op_d={}
        mres     = {} #formula[0]
        mopers   = [] #formula[1:]
        if formula:
            for rr in formula[1]:
                if rr['op'] == '=':
                    mres = rr
                else:
                    mopers.append(rr)
        opers   = [] 
        res_taxo_id = str(row['t_id'])
        flip_sign   = {}
        mismatch_scale  = {}
        mismatch_vt     = {}

        one_taxo        = {}
        conditiona_op   = {
                            'and':1,
                            'or':1,
                        }
        def form_recursive_formula(opers, tmpopers, form_cls_d, done_t_d):
            for oper in opers:
                op_txid     = oper['txid']
                op          = oper['op']
                op_type     = oper['type']
                ttype       = oper['t_type']
                grpid       = oper['g_id']
                if op_type == 'v':
                    tmpopers.append(oper)
                    continue
                to_dealid= oper['to_dealid']
                tph         = ph
                if oper.get('k', ''):
                    tph  = oper['k']
                elif oper.get('period', ''):
                    s_ph    = oper['period']+str(int(year)-int(oper['yd']))
                    tph  = ph_map_d.get(s_ph , s_ph)
                tph         = phMap.get((to_dealid, ttype, grpid), {}).get(tph, tph)
                data_row    = taxo_id_dict.get((to_dealid, ttype, grpid), {}).get(op_txid, {}) 
                cell_d      = data_row.get(tph, {})
                if ((ttype, grpid, str(op_txid)) not in done_t_d) and cell_d.get('v', '') == '' and form_cls_d.get((ttype, grpid), {}).get(str(op_txid)):
                    t_row_f = ()
                    for cnt, ftype in enumerate(['DIRECT', 'ACROSS', 'WITHIN']):
                        t_row_f     = form_cls_d[(ttype, grpid)].get(str(op_txid), {}).get(ftype, ()) #ph_formula_d.get(('USER F', rr['t_id']), ())  
                        break
                    if t_row_f:
                        done_t_d[(ttype, grpid, str(op_txid))]= 1
                        form_recursive_formula(filter(lambda x:x['op'] != '=', t_row_f[1]), tmpopers, form_cls_d, done_t_d)
                    else:
                        tmpopers.append(oper)
                else:
                    tmpopers.append(oper)
        def get_eval(taxo_id, ph, res_val, sys_op_inds, re_scale, ftype_r, cell_scale):
            val_li      = []
            res_txid    = str(row['t_id'])
            #res_tt      = res['t_type']
            re_ar       = []
            expr_str    = []
            #print '\n========================'
            op_inds = []
            scale_d = {}
            v_ar        = []
            try:
                year        = int(ph[-4:])
            except:
                year        = ''
            m_scale = tscale
            if cell_scale and not m_scale:
                m_scale = cell_scale
            v_ar        = []
            ftype   = 'G'
            n_form  = [copy.deepcopy(res)]
            op_ind  = 0
            scale_d = {}
            conver_arr  = []
            ftype       = 'G'
            cys_d       = {}
            #print 'sys_op_inds ', sys_op_inds
            exp_opr, exp_res, val_li, expr_str, n_form, re_ar, cys_d, conver_arr, val_ar, v_ar, ftype, op_inds, op_scals    = self.get_formula_unit_scale([copy.deepcopy(res)]+opers, ph, row, taxo_id_dict, phMap, tscale, tvtype, ph_map_d, op_inds, res_val, update_sign, sys_op_inds)
            #print re_ar
            tv_ar   = copy.deepcopy(v_ar)
            #print 'exp_opr : ', [exp_opr, exp_res]
            #print 'val_li  : ', val_li
            #print 'expr_str: ', expr_str
            if 0:#not exp_opr or not exp_opr:
                val_li      = []
                expr_str    =[ ]
            fp  = 1
            if '' in val_ar:
                fp = 0
            
            cell_val = ''
            #print '\t\t',[ph, ''.join(val_li), len(n_form)]
            try:
                cell_val = eval(''.join(val_li))
            except:pass
            conv_d = {}
            if cell_val and exp_opr and exp_res and exp_opr != exp_res:
                tv, factor          = sconvert_obj.convert_frm_to_1(exp_opr, exp_res, cell_val, (res_txid, ph))
                if factor != '':
                    conver_arr.append((clean_value, tv, exp_opr+' - '+exp_res+' ('+str(factor)+' )'))
                    conv_d.setdefault('conv_value', {}).update({'v':tv, 'to_scale':exp_res, 'from_scale':exp_opr, 'factor':factor})
                    cell_val    =  float(tv.replace(',', ''))
                    
            #print '\tcell_val ', [cell_val, len(re_ar)]
            re_ar.sort(key=lambda v_d: (v_d['i'], 0 if v_d['x'] and v_d['bbox'] and v_d['t'] and v_d['d'] else 1 ))
            if not re_ar or (cell_val == '' or (not cell_val and fp == 0)):
                if 0:#ftype_r != 'PTYPE':
                    return '', {}, [], [], [],  [], op_inds
                else:
                    return '', {}, [], [], n_form, [], op_inds
            if re_ar and (m_scale in scale_d):
                re_ar   = [scale_d[m_scale]]
            if row.get('kpi_row_sign') == 'P':
                if cell_val < 0:
                    cell_val    = cell_val * -1
            elif row.get('kpi_row_sign') == 'N':
                if cell_val > 0:
                    cell_val    = cell_val * -1
            cell_val    = str(cell_val)
            #print '\t\t 1 ',[cell_val, ''.join(val_li)]
            cell_val    = self.convert_floating_point(cell_val)
            #print '\t\t 2 ',[cell_val, ''.join(val_li)]
            if cell_val == '':# or cell_val == '0':
                if 0:#ftype_r != 'PTYPE':
                    return '', {}, [], [], [], [], op_inds
                else:
                    return '', {}, [], [], n_form, [], op_inds
            cell_val    = cell_val.replace(',', '')
            dd          = copy.deepcopy(re_ar[0])
            #if len(re_ar) > 1:
            #    dd['FORM_VALUE']    = ''
                
                
            dd['v']     = str(cell_val)
            dd['v_ar']  = tv_ar
            dd['ftype']  = ftype
            if not exp_res and row.get('m_scale'):
                exp_res = row['m_scale']
            tmptvtype   = tvtype
            if not tmptvtype and row.get('m_vt'):
                tmptvtype  = row.get('m_vt')
            if not exp_res:
                for  tmpdd in re_ar: 
                    if tmpdd.get('phcsv', {}).get('s'):
                        exp_res = tmpdd.get('phcsv', {}).get('s')
                        break
            dd.setdefault('phcsv', {})['s']  = exp_res

            if cys_d:
                dd.setdefault('phcsv', {})['c']  = cys_d.keys()[0]
            if row.get('m_currency'):
                dd.setdefault('phcsv', {})['c']  = row.get('m_currency')
            if tmptvtype:
                dd.setdefault('phcsv', {})['vt']  = tmptvtype
            if dd.get('phcsv', {}).get('s') == '1' and dd['phcsv'].get('vt') == 'Percentage':
                dd.setdefault('phcsv', {})['c']  = ''
                    
                
            #print 'END', [str(cell_val), dd, val_li, expr_str]
            conver_arr.append(''.join(val_li))
            n_form[0]['conver_arr'] = conver_arr
            n_form[0].update(conv_d)
            dd.update(conv_d)
            #for ft in n_form:
            #    print '\t', ft
            return str(cell_val), dd, val_li, expr_str, n_form, conver_arr, op_inds
           
        val_dict = {}
        form_d  = {}
        #print '\n===================================='
        #print taxo_id_dict[res_taxo_id]['t_l']
        #phs.sort()
        try:
            phs = report_year_sort.year_sort(phs)
        except:pass
        non_res_ph  = []
        op_inds_d= {}
        #print 'raw_builder ', raw_builder
        for ph in phs:
            #print '\n========================='
            #print 'PH ', [res_taxo_id, ph]
            cell_d  = {}
            if mres:
                oper    = mres
                op_txid    = oper['txid']
                op         = oper['op']
                op_type    = oper['type']
                ttype    = oper['t_type']
                grpid    = oper['g_id']
                to_dealid= oper['to_dealid']
                tph         = ph
                if oper.get('k', ''):
                    tph  = oper['k']
                elif oper.get('period', ''):
                    s_ph    = oper['period']+str(int(year)-int(oper['yd']))
                    tph  = ph_map_d.get(s_ph , s_ph)
                tph         = phMap.get((to_dealid, ttype, grpid), {}).get(tph, tph)
                data_row    = taxo_id_dict.get((to_dealid, ttype, grpid), {}).get(op_txid, {}) 
                cell_d      = data_row.get(tph, {})
                #print '\t TOH ',[tph, cell_d.get('v')]
            if raw_builder == 'Y' and cell_d.get('v', '') == '': 
                non_res_ph.append((ph, cell_d))
                continue
            


            #if taxo_id_dict.get(res_taxo_id, {}).get(ph, {}).get('v', '') == '' or run_all=='Y':
            #    clean_value = taxo_id_dict.get(res_taxo_id, {}).get(ph, {}).get('v', '')
            if cell_d.get('v', '') == '' or run_all=='Y':
                clean_value = cell_d.get('v', '')
                try:
                    clean_value = numbercleanup_obj.get_value_cleanup(clean_value)
                except:clean_value = ''
                #print '\t', [ph, clean_value, taxo_id_dict[res_taxo_id].get(ph, {}).get('phcsv', {}).get('s', '')]
                opers   = []
                ftype_r = ''
                if mopers:
                    res = mres
                    opers   = copy.deepcopy(mopers)
                    ftype_r = 'ROW' 
                    tmpopers    = []
                    if raw_builder == 'Y':# and 'RR' in form_info_d:
                        done_t_d    = {}
                        form_recursive_formula(opers, tmpopers, form_info_d, done_t_d)
                        opers   = tmpopers
                    
                elif t_row_f_ptype and t_row_f_ptype.get(ph[:-4]):
                    tmp_form    = t_row_f_ptype[ph[:-4]][1]
                    opers       = []
                    for x in tmp_form:
                        if x['op'] == '=':
                            res = x
                        else:
                            opers.append(x)
                    ftype_r = 'PTYPE'
                elif t_row_f_cell and t_row_f_cell.get(ph):
                    tmp_form    = t_row_f_cell[ph][1]
                    opers       = []
                    for x in tmp_form:
                        if x['op'] == '=':
                            res = x
                        else:
                            opers.append(x)
                    ftype_r = 'PTYPE'

                if opers:
                    cell_val, ref, expr_val, expr_str, n_form, conver_arr, op_inds = get_eval(res_taxo_id, ph, clean_value, sys_op_d.get((str(res_taxo_id), ph), []), taxo_id_dict.get(res_taxo_id, {}).get(ph, {}).get('phcsv', {}).get('s', ''), ftype_r, cell_d.get('phcsv', {}).get('s', ''))
                    if op_inds:
                        op_inds_d[ph]= op_inds
                    #print '\tFINAL ', [cell_val]
                    if cell_val not in ['']:
                        ref['v'] = cell_val
                        ref['fv'] = 'Y'
                        ref['expr_str'] = ''.join(expr_str)+' '+str(conver_arr)
                        ref['expr_val'] = ''.join(expr_val)
                        val_dict.setdefault(res_taxo_id, {})[ph] = ref
                        form_d[ph]  = n_form
                    elif n_form:
                        form_d[ph]  = n_form
        for ph, cell_d in non_res_ph[:0]:
            index = phs.index(ph)
            op_inds = []
            for tph in phs[index+1:]:
                if tph in op_inds_d:
                    op_inds = op_inds_d[tph]
                    break
            if not op_inds:
                for tph in phs[:index][::-1]:
                    if tph in op_inds_d:
                        op_inds = op_inds_d[tph]
                        break
            #print 'NON-RES', ph, op_inds
            if not op_inds:continue
            #if taxo_id_dict.get(res_taxo_id, {}).get(ph, {}).get('v', '') == '' or run_all=='Y':
            #    clean_value = taxo_id_dict.get(res_taxo_id, {}).get(ph, {}).get('v', '')
            if cell_d.get('v', '') == '' or run_all=='Y':
                clean_value = cell_d.get('v', '')
                try:
                    clean_value = numbercleanup_obj.get_value_cleanup(clean_value)
                except:clean_value = ''
                #print '\t', [ph, clean_value, taxo_id_dict[res_taxo_id].get(ph, {}).get('phcsv', {}).get('s', '')]
                opers   = []
                ftype_r = ''
                if mopers:
                    res = mres
                    opers   = copy.deepcopy(mopers)
                    ftype_r = 'ROW'
                elif t_row_f_ptype and t_row_f_ptype.get(ph[:-4]):
                    tmp_form    = t_row_f_ptype[ph[:-4]][1]
                    opers       = []
                    for x in tmp_form:
                        if x['op'] == '=':
                            res = x
                        else:
                            opers.append(x)
                    ftype_r = 'PTYPE'
                elif t_row_f_cell and t_row_f_cell.get(ph):
                    tmp_form    = t_row_f_cell[ph][1]
                    opers       = []
                    for x in tmp_form:
                        if x['op'] == '=':
                            res = x
                        else:
                            opers.append(x)
                    ftype_r = 'PTYPE'

                if opers:
                    cell_val, ref, expr_val, expr_str, n_form, conver_arr, op_inds = get_eval(res_taxo_id, ph, clean_value, op_inds, taxo_id_dict.get(res_taxo_id, {}).get(ph, {}).get('phcsv', {}).get('s', ''), ftype_r, cell_d.get('phcsv', {}).get('s', ''))
                    #if op_inds:
                    #    op_inds_d[ph]= op_inds
                    #print '\tFINAL ', [cell_val]
                    if cell_val not in ['']:
                        ref['v'] = cell_val
                        ref['fv'] = 'Y'
                        ref['expr_str'] = ''.join(expr_str)+' '+str(conver_arr)
                        ref['expr_val'] = ''.join(expr_val)
                        val_dict.setdefault(res_taxo_id, {})[ph] = ref
                        form_d[ph]  = n_form
                    elif n_form:
                        form_d[ph]  = n_form
                    
        return val_dict, form_d, flip_sign, mismatch_scale, mismatch_vt, one_taxo

    ## EVALUATION 
    def get_formula_evaluation(self, formula, taxo_id_dict, phs, ph_map_d={}, year=None, run_all=None, sys_op_d={}):
        res     = {} #formula[0]
        opers   = [] #formula[1:]
        for rr in formula:
            if rr['op'] == '=':
                res = rr
            else:
                opers.append(rr)
        res_taxo_id = str(res['txid'])
        flip_sign   = {}

        def get_eval(taxo_id, ph, res_val, sys_op_inds, re_scale, r_cy):
            val_li      = []
            res_txid    = res['txid']
            #res_tt      = res['t_type']
            re_ar       = []
            expr_str    = []
            #print '\n========================'
            op_inds = []
            scale_d = {}
            v_ar        = []
            for oper in opers:
                op_txid    = oper['txid']
                op         = oper['op']
                op_type    = oper['type']
                tph         = ph
                if oper.get('k', ''):
                    tph  = oper['k']
                elif oper.get('period', ''):
                    tph  = ph_map_d.get(oper['period']+str(int(year)+int(oper['yd'])), '')
                if op_type == 'v':
                    pass
                else:
                    op_val = taxo_id_dict.get(op_txid, {}).get(tph, {'v':0})['v']
                    clean_value = op_val
                    try:
                        clean_value = numbercleanup_obj.get_value_cleanup(clean_value)
                    except:clean_value = ''
                    if clean_value == '':
                        clean_value = '0'
                    clean_value = float(clean_value)
                    v_ar.append(clean_value)
                    phcsv   = taxo_id_dict.get(op_txid, {}).get(tph, {}).get('phcsv', {}).get('s', '')
                    print op_txid, phcsv
                    if phcsv:
                        scale_d[phcsv]  = 1
            m_scale = ''
            if len(scale_d.keys()) > 1:
                if not re_scale:
                    num_obj = {'One': 1, 'Dozen': 12, 'Hundred': 100, 'Thousand': 1000, 'Million': 1000000, 'Billion': 1000000000, 'Trillion': 1000000000000}
                    print scale_d
                    scales  = map(lambda x:(sconvert_obj.num_obj[sconvert_obj.scale_map_d[x]], x), scale_d.keys())
                    scales.sort(key=lambda x:x[0])
                    m_scale = scales[-1][1]
                else:
                    m_scale = re_scale
            elif re_scale:
                m_scale = re_scale

            
            v_ar        = []
            v_ar_org        = []
            for oper in opers:
                op_txid    = oper['txid']
                op         = oper['op']
                op_type    = oper['type']
                tph         = ph
                if oper.get('k', ''):
                    tph  = oper['k']
                elif oper.get('period', ''):
                    tph  = ph_map_d.get(oper['period']+str(int(year)+int(oper['yd'])), '')
                if op_type == 'v':
                    pass
                else:
                    op_val = taxo_id_dict.get(op_txid, {}).get(tph, {'v':0})['v']
                    clean_value = op_val
                    try:
                        clean_value = numbercleanup_obj.get_value_cleanup(clean_value)
                    except:clean_value = ''
                    clean_value_org = clean_value
                    if clean_value and m_scale:
                        phcsv   = taxo_id_dict.get(op_txid, {}).get(tph, {}).get('phcsv', {}).get('s', '')
                        if phcsv and  m_scale != phcsv:
                            tv, factor  = sconvert_obj.convert_frm_to_1(phcsv, m_scale, clean_value)
                            if factor != '':
                                clean_value = tv.replace(',', '')
                    if clean_value and r_cy in cconvert_obj.scale_map_d and clean_value and clean_value not in ['0', '0.00', 0]:
                        phcsv   = taxo_id_dict.get(op_txid, {}).get(tph, {}).get('phcsv', {}).get('c', '')
                        if phcsv in cconvert_obj.scale_map_d and  r_cy != phcsv:
                            tv, factor  = cconvert_obj.convert_frm_to(phcsv, r_cy, clean_value)
                            if factor != '':
                                clean_value = tv.replace(',', '')
                    if clean_value == '':
                        clean_value = '0'
                    clean_value = float(clean_value)
                    scale   = taxo_id_dict.get(op_txid, {}).get(tph, {}).get('phcsv', {}).get('s', '')
                    if scale and scale.lower() not in ['1', 'one']:
                        tv, factor  = sconvert_obj.convert_frm_to_1(scale, '1', clean_value_org)
                        if factor != '':
                            clean_value_org = tv.replace(',', '')
                        pass
                    if clean_value_org == '':
                        clean_value_org = '0'
                    clean_value_org = float(clean_value_org)
                    v_ar_org.append(clean_value_org)
                    v_ar.append(clean_value)

            if res_val :
                res_val_org = float(res_val)
                #if r_scale and r_scale.lower() not in ['1', 'one']:
                #        tv, factor  = sconvert_obj.convert_frm_to_1(r_scale, '1', res_val_org)
                #        if factor != '':
                #            res_val_org = float(tv.replace(',', ''))
                #        pass
                v_ar.append(float(res_val))
                #v_ar_org.append(float(res_val_org))
                print 'v_ar', v_ar
                print 'v_ar_org ', v_ar_org
                f_sig   = tavinash.get_sig(v_ar)
                if f_sig:
                    op_inds = map(lambda x:'+' if x==0 else '-', f_sig[0])
                elif sys_op_inds:
                    op_inds = sys_op_inds
                if op_inds:
                    tmp_v_ar    = []
                    for i, v1 in enumerate(v_ar[:-1]):
                        op  = op_inds[i]
                        if op == '-':
                            #if v1 < 0:
                            #    op_inds[i]  = '+'
                            v1  = -1*v1
                        #elif op == '+':
                            #if v1 < 0:
                            #    op_inds[i]  = '-'
                        tmp_v_ar.append(v1)
                    #print '\n'
                    #print v_ar, tmp_v_ar
                    #print v_ar[-1], sum(tmp_v_ar), abs(v_ar[-1]-sum(tmp_v_ar)) > 0
                    v_ar    = tmp_v_ar[:] #+[float(res_val)]
                else:
                    v_ar    = v_ar[:-1]
            tv_ar   = copy.deepcopy(v_ar)
            #print '\t\t',[m_scale, scale_d.keys()]
            

            n_form  = [copy.deepcopy(res)]
            op_ind  = 0
            scale_d = {}
            conver_arr  = []
            ftype   = 'G'
            for oper in opers:
                #print oper, op_inds, sys_op_inds
                op_txid    = oper['txid']
                op         = oper['op']
                #op_tt      = oper['t_type']
                op_type    = oper['type']
                tph         = ph
                s_ph        = ph 
                if oper.get('k', ''):
                    tph  = oper['k']
                elif oper.get('period', ''):
                    #print '\tTT',[oper['period'],year,  oper['yd']]
                    s_ph    = oper['period']+str(int(year)+int(oper['yd']))
                    tph  = ph_map_d.get(oper['period']+str(int(year)+int(oper['yd'])), '')
                #print [tph]
                desc        = ''
                v_f         = ()
                if op_type == 'v':
                    op_val  = op_txid
                    desc    = op_txid
                    if op not in {'+':1, '-':1}:
                        ftype   = 'NG'
                else:
                    if op_inds:
                        op  = op_inds[op_ind]
                    if op not in {'+':1, '-':1}:
                        ftype   = 'NG'
                    op_ind  += 1
                    op_val = taxo_id_dict.get(op_txid, {}).get(tph, {'v':0})['v']
                        
                    desc    = taxo_id_dict.get(op_txid, {}).get('t_l', '')
                    #print [desc, op_val]
                    if taxo_id_dict.get(op_txid, {}).get(tph, {}).get('x', None) != None:
                        v_d = taxo_id_dict.get(op_txid, {}).get(tph, {})
                        re_ar.append({'i':len(re_ar), 'x':v_d['x'], 'bbox':v_d['bbox'], 't':v_d['t'], 'd':v_d['d'], 'phcsv':copy.deepcopy(v_d.get('phcsv', {}))})
                        phcsv   = taxo_id_dict.get(op_txid, {}).get(tph, {}).get('phcsv', {}).get('s', '')
                        cy      = taxo_id_dict.get(op_txid, {}).get(tph, {}).get('phcsv', {}).get('c', '')
                        clean_value = op_val
                        try:
                            clean_value = numbercleanup_obj.get_value_cleanup(clean_value)
                        except:clean_value = ''
                        if clean_value:
                            v_f         = (op_txid, clean_value, tph)

                        if phcsv and m_scale:
                            if clean_value == '':
                                clean_value = '0'
                            if phcsv != m_scale:
                                tv, factor          = sconvert_obj.convert_frm_to_1(phcsv, m_scale, clean_value)
                                if factor != '':
                                    conver_arr.append((clean_value, tv, sconvert_obj.scale_map_d[phcsv]+' - '+sconvert_obj.scale_map_d[m_scale]))
                                    op_val      = str(tv)
                                    clean_value = op_val
                                
                            else:
                                scale_d[m_scale]    = re_ar[-1]
                        if cy in cconvert_obj.scale_map_d and r_cy in cconvert_obj.scale_map_d and clean_value and clean_value not in ['0', '0.00', 0]:
                            if cy != r_cy:
                                tv, factor          = cconvert_obj.convert_frm_to(cy, r_cy, clean_value)
                                if factor != '':
                                    conver_arr.append((clean_value, tv, cconvert_obj.scale_map_d[cy]+' - '+cconvert_obj.scale_map_d[r_cy]))
                                    clean_value  = str(tv)
                                    op_val      = str(tv)
                            
                oper['op']  = op
                oper['ph']  = s_ph
                oper['pk']  = tph
                n_form.append(copy.deepcopy(oper))
                #print [op_val]
                clean_value = op_val
                try:
                    clean_value = numbercleanup_obj.get_value_cleanup(clean_value)
                except:clean_value = ''
                if op == '-':
                    if v_f:
                        if '-' in v_f[1]:
                            flip_sign.setdefault(v_f[0], {})[v_f[2]]    = 1
                        elif v_f[1]:
                            flip_sign.setdefault(v_f[0], {})[v_f[2]]    = 1

                if clean_value == '' and op_type != 'v':
                    clean_value = '0'
                elif clean_value and  op_type != 'v':
                    clean_value = str(float(clean_value))
                expr_str += [str(op), desc] 
                val_li += [str(op), str(clean_value)]
            cell_val = 0
            #print [''.join(val_li)]
            try:
                cell_val = eval(''.join(val_li))
            except:pass
            re_ar.sort(key=lambda v_d: (v_d['i'], 0 if v_d['x'] and v_d['bbox'] and v_d['t'] and v_d['d'] else 1 ))
            if not re_ar or cell_val == 0:
                return '', {}, [], [], n_form, []
            if re_ar and (m_scale in scale_d):
                re_ar   = [scale_d[m_scale]]
            cell_val    = str(cell_val)
            #print '\n==================='
            #print [cell_val, ''.join(val_li)]
            cell_val    = self.convert_floating_point(cell_val)
            if cell_val == '' or cell_val == '0':
                return '', {}, [], [], n_form, []
            cell_val    = cell_val.replace(',', '')
            dd          = copy.deepcopy(re_ar[0])
            dd['v']     = str(cell_val)
            dd['v_ar']  = tv_ar
            dd['ftype']  = ftype
            if r_cy:
                dd.setdefault('phcsv', {})['c'] = r_cy
            #print 'END', [str(cell_val), dd, val_li, expr_str]
            n_form[0]['conver_arr'] = conver_arr
            return str(cell_val), dd, val_li, expr_str, n_form, conver_arr
           
        val_dict = {}
        form_d  = {}
        #print '\n===================================='
        #print taxo_id_dict[res_taxo_id]['t_l']
        for ph in phs:
            if taxo_id_dict[res_taxo_id].get(ph, {}).get('v', '') == '' or run_all=='Y':
                clean_value = taxo_id_dict[res_taxo_id].get(ph, {}).get('v', '')
                try:
                    clean_value = numbercleanup_obj.get_value_cleanup(clean_value)
                except:clean_value = ''
                #print '\t', [ph, clean_value, taxo_id_dict[res_taxo_id].get(ph, {}).get('phcsv', {}).get('s', '')]
                cell_val, ref, expr_val, expr_str, n_form, conver_arr = get_eval(res_taxo_id, ph, clean_value, sys_op_d.get((str(res_taxo_id), ph), []), taxo_id_dict[res_taxo_id].get(ph, {}).get('phcsv', {}).get('s', taxo_id_dict[res_taxo_id].get('m_scale')), taxo_id_dict[res_taxo_id].get(ph, {}).get('phcsv', {}).get('c', taxo_id_dict[res_taxo_id].get('m_cy')))
                form_d[ph]  = n_form
                if cell_val not in ['0', '']:
                    ref['v'] = cell_val
                    ref['fv'] = 'Y'
                    ref['expr_str'] = ''.join(expr_str)+' '+str(conver_arr)
                    ref['expr_val'] = ''.join(expr_val)
                    val_dict.setdefault(res_taxo_id, {})[ph] = ref
        return val_dict, form_d, flip_sign

    def compute_formula(self, f_lst, consider_ph, taxo_d):
        final_d = {}
        for resultant, formula, s_ph in f_lst:
            l   = len(formula) - 1
            new_d   = {}
            for ph in consider_ph.keys():
                expr_lst        = []
                expr_label_lst  = []
                expr_label_lst  = []
                doc_d           = {}
                i   = 0
                fn_all  = 1
                for tt in formula:
                    #'t_l':val_d[taxo_id]['t_l'] ,'t_id':taxo_id, 'ph':ph, 'v':value, 'symbol':sign, 'g_t':r_group_taxo
                    t_id    = tt['t_id']
                    t_l     = taxo_d[t_id]['t_l']
                    sign    = tt['symbol']
                    expr_label_lst.append(t_l)
                    if tt.get('break', '') in ['(']:
                        expr_label_lst[-1]  = ' ( '+expr_label_lst[-1]
                    if tt.get('break', '') in [')']:
                        expr_label_lst[-1]  = expr_label_lst[-1]+' ) '

                    dd  = taxo_d.get(t_id, {}).get(ph, {})
                    if dd.get('v', '') == '' and s_ph != 'Y':
                        fn_all  = 0
                        break
                    v   = self.normalized_txt(dd.get('v', '0'))
                    if not v:
                        v   = 0
                    d   = dd.get('d', '')
                    x   = dd.get('x', '')
                    if d and x:
                        x_sp    = x.split('@')
                        for ti, d1 in enumerate(d.split('@')):
                            doc_d.setdefault(d1, []).append(x_sp[ti])
                    expr_lst.append(str(v))
                    if tt.get('break', '') in ['(']:
                        expr_lst[-1]  = ' ( '+expr_lst
                    if tt.get('break', '') in [')']:
                        expr_lst[-1]  = expr_lst+' )'

                    #print i, l, tt
                    if sign == '%':
                        if i == l:
                            expr_lst        = ['(']+expr_lst+[')']
                            expr_label_lst  = ['(']+expr_label_lst+[')']
                            expr_lst.append('*100')
                            expr_label_lst.append('*100')
                        else:
                            expr_lst.append('*100')
                            expr_label_lst.append('*100')
                    elif sign == '/100':
                        if i == l:
                            expr_lst        = ['(']+expr_lst+[')']
                            expr_label_lst  = ['(']+expr_label_lst+[')']
                            expr_lst.append('/100')
                            expr_label_lst.append('/100')
                        else:
                            expr_lst.append('/100')
                            expr_label_lst.append('/100')
                    elif sign and i != l:
                        expr_lst.append(sign)
                        expr_label_lst.append(sign)
                    i   += 1
                if fn_all  ==0 and s_ph != 'Y':continue
                expr    = ' '.join(expr_lst)
                expr_str= ' '.join(expr_label_lst) +' = '+taxo_d[resultant]['t_l']
                try:
                    r   = self.convert_floating_point(eval(expr))
                except:
                    r   = 0
                d_arr   = []
                x_arr   = []
                for k, v in doc_d.items():
                    d_arr.append(k)
                    x_arr.append('#'.join(v))
                new_d[ph]   = {'v':r, 'd':'@'.join(d_arr), 'x':'@'.join(x_arr), 'expr':expr+' = '+str(r), 'expr_str':expr_str}
            final_d[resultant] = new_d
        return final_d

    def convert_floating_point(self, string, r_num=2):
         if string == '':
             return ''
         try:
             string = float(string)
         except:
             return string
         f_num  = '{0:.10f}'.format(string).split('.')
         #print [string, f_num]
         r_num  = 0
         if f_num[0] in ['0', '-0']:
             if len(f_num) ==2:
                  r_num     = 0 #len(f_num[1])
                  f_value   = 0
                  #print '\t','f_num ', f_num[1]  
                  for num in f_num[1]:
                      #print '\t\t', [num, r_num]
                      r_num += 1
                      if f_value and r_num > 2:break
                      if num != '0':
                        f_value = 1
                  if f_value == 0:
                        r_num = 0
         else:
             if len(f_num) ==2:
                  r_num     = 0 #len(f_num[1])
                  #print '\t',[r_num]
                  f_val     = []
                  for i, num in enumerate(f_num[1]):
                      #print '\t\tNUM',[num, r_num]
                      if num != '0':
                        f_val.append(i)
                  f_val.sort()
                  if f_val:
                    if f_val[0] == 0:
                        r_num   = 2
                    else:
                        r_num   = f_val[0]+1

             else:
                r_num   = 2
             if r_num < 0:
                 r_num = 0
             if r_num > 2:
                r_num = 2
         tr_num = 2
         form_str   = "{0:,."+str(tr_num)+"f}"
         #print [string, form_str, r_num, f_num]
         if (form_str.format(string) in ['0.00', '-0.00'] and abs(string) > 0.00) or f_num[0] == '0':
            form_str   = "{0:,."+str(r_num)+"f}"
         #print form_str
         return form_str.format(string)



    def eval_ph_col_formula(self, row, col_phk, ops_ar, ph_map_d):
        for ops in ops_ar:
            expr_str    = []
            expr_val    = []
            f           = 1
            re_ar       = []
            #print '\n===================================================='
            for rr in ops:
                typ = rr['t']
                v   = rr['v']
                if typ == 'opr':
                    expr_val.append(v)
                    expr_str.append(v)
                elif typ == 'cons':
                    expr_val.append(v)
                    expr_str.append(v)
                elif typ == 'op':
                    v   = v+year
                    #print [v, ph_map_d.get(v), clean_v_d.get(ph_map_d.get(v))]
                    if v not in ph_map_d:
                        f           = 0
                        break
                    if ph_map_d[v] not in clean_v_d:
                        f           = 0
                        break
                    v_d     = row[ph_map_d[v]]
                    if v_d['x'] and v_d['bbox'] and v_d['t'] and v_d['d']:
                        re_ar.append({'x':v_d['x'], 'bbox':v_d['bbox'], 't':v_d['t'], 'd':v_d['d']})
                    expr_val.append(clean_v_d[ph_map_d[v]])
                    expr_str.append(rr['v'])
            #print f, ''.join(expr_val)
            if f == 0:
                continue
            v   = ''
            try:
                v   = str(eval(''.join(expr_val)))
            except:
                continue
            #print [v]
            if re_ar:
                ref_d   = re_ar[0]
            else:
                ref_d   = {}
            ref_d['expr_str']   = ''.join(expr_str)
            ref_d['expr_val']   = ''.join(expr_val)
            ref_d['fv']   = 'Y'
            ref_d['v']          = v
            return v, ref_d, ph_gid_map.get(formula, '')
        return '', {}, ''



        
    

    def eval_ph_formula(self, row, ops_ar, ph_map_d,clean_v_d, year, ph_gid_map, ptype, deal_id, table_type, group_id, t_re_stated_cols, r_ph, latest_year, latest_year_doc):
        r_report    = 'N'
        if r_ph.get('derived') != 'Y' and (r_ph['k'] not in t_re_stated_cols and r_ph.get('RS_COL') != 'Y') and table_type not in self.non_financial_tt:# DO NOT CALCULATE IF OPERAND COL IS RESTATED and RESULTANT PH IS REPORTED
            r_report    = 'Y'
        #print 'r_report ', r_report
        for formula, ops, tmp_rowid in ops_ar:
            expr_str    = []
            expr_val    = []
            f           = 0
            re_ar       = []
            #print '\n===================================================='
            scale_d     = {}
            fy          = 0
            fy_val      = 0
            pt_d        = {}
            nt_f        = 0
            opnds       = filter(lambda x:x['t'] == 'op', ops)
            #if table_type in self.non_financial_tt and len(opnds) == 1:
            if len(opnds) == 1:
                f_cond  = self.ph_f_cond.get(deal_id, {}).get(table_type, {}).get(formula, '')
                if not f_cond and ops and ops[0].get('cond'):
                    f_cond  = ops[0].get('cond')
                    
                #print '\n==========================='
                #print row['t_l'], [deal_id, table_type, formula, self.ph_f_cond.get(deal_id, {}), self.ph_f_cond.get(deal_id, {}).get(table_type, {})]
                #print [formula, f_cond]
                if f_cond:
                    texpr_str, texpr, tconver_arr    = self.form_expr_str(f_cond, year, clean_v_d, ph_map_d, row, '')
                    try:
                        o_t = eval(texpr_str)
                    except:
                        o_t = False
                    #print [texpr_str, o_t]
                    if not o_t:continue
                rr  = ops[0]
                typ = rr['t']
                v   = rr['v']
                v   = v+year
                if v not in ph_map_d or ph_map_d[v] not in row:
                    continue
                if r_report == 'Y':
                    if str(year) != latest_year_doc: #Wells Forago Profitability Ratio Derive Latest year, Do't check Restated Check 08-Nov_2019 BY Sinchana
                        if ph_map_d[v] in t_re_stated_cols:continue
                #elif t_re_stated_cols and ph_map_d[v] not in t_re_stated_cols:
                #    if str(latest_year  ) != str(year):
                #        continue
                form_ar = []
                v_d     = row[ph_map_d[v]]
                ppk = ph_map_d[v]
                expr_str    = [v]
                v       = v_d.get('v')
                tmpddd  = copy.deepcopy(v_d)
                tmpddd.update({'txid':str(row['t_id']), 'taxo_id':str(row['t_id']), 'type':'t','t_type':table_type, 'operator':'', 'ph':rr['v']+year, 'pk':ppk, 'period':r_ph['ph'][:-4], 'fyear':r_ph['ph'][-4:], 'yd':0, 'i_f_type':'PTYPEFORMULA', 'tt':table_type, 'grpid':group_id, "description":row['t_l']})
                form_ar.append(tmpddd)
                if not v_d.get('v'):continue
                expr_val    = [v]
                conver_arr  = []
                ref_d   = {'x':v_d['x'], 'bbox':v_d['bbox'], 't':v_d['t'], 'd':v_d['d'], 'phcsv':v_d['phcsv'], 'FORM_VALUE':v_d.get('FORM_VALUE', ''), 'v':v_d['v']}
                ref_d['expr_str']   = ''.join(expr_str)+' -- '+str(conver_arr)
                ref_d['expr_val']   = ''.join(expr_val)
                ref_d['fv']         = 'Y'
                ref_d['PH_D']       = 'Y'
                ref_d['v']          = v
                tmpdd   = copy.deepcopy(ref_d)
                tmpdd.update({'txid':str(row['t_id']), 'taxo_id':str(row['t_id']), 'operator':'=', 'ph':r_ph['ph'], 'pk':r_ph['ph'], 'period':r_ph['ph'][:-4], 'fyear':r_ph['ph'][-4:], 'yd':0, 'i_f_type':'PTYPEFORMULA',"R":"Y", 'tt':table_type, 'grpid':group_id, "description":row['t_l']})
                form_ar = [tmpdd]+form_ar
                for tr in form_ar:
                    tr['description']   = row['t_l']
                ref_d['PH_FORM']    = form_ar
                ref_d['PH_FORM_STR']    = formula
                if tmp_rowid == 'RID-0':
                    return v, ref_d, str(ph_gid_map.get(formula, ''))+'~'+tmp_rowid
                else:
                    return v, ref_d, ph_gid_map.get(formula, '')
            else:
                for rr in ops:
                    typ = rr['t']
                    v   = rr['v']
                    if typ == 'op':
                        pt_d[v] = 0
                        v   = v+year
                        if v not in ph_map_d:
                            expr_val.append('0')
                            continue
                        if ph_map_d[v] not in clean_v_d:
                            expr_val.append('0')
                            continue
                        v_d     = row[ph_map_d[v]]
                        phcsv   = v_d['phcsv']['s']
                        pt_d[rr['v']] = 1
                        #if clean_v_d[ph_map_d[v]]:
                        #    pt_d[rr['v']] = 1
                        if phcsv:
                            scale_d[phcsv]  = 1
                #if 0 in pt_d.values():
                #    return '', {}, ''
                #if 'FY' in pt_d and pt_d['FY'] != 1:
                #    return '', {}, ''
                #if ptype == 'Q4' and 'Q1' in pt_d and pt_d['Q1'] != 1:
                #    return '', {}, ''
                f_cond  = self.ph_f_cond.get(deal_id, {}).get(table_type, {}).get(formula, '')
                if not f_cond and ops and ops[0].get('cond'):
                    f_cond  = ops[0].get('cond')
                    
                #print '\n==========================='
                #print row['t_l'], [deal_id, table_type, formula, self.ph_f_cond.get(deal_id, {}), self.ph_f_cond.get(deal_id, {}).get(table_type, {})]
                #print [formula, f_cond]
                if f_cond:
                    texpr_str, texpr, tconver_arr    = self.form_expr_str(f_cond, year, clean_v_d, ph_map_d, row, '')
                    try:
                        o_t = eval(texpr_str)
                    except:
                        o_t = False
                    #print [texpr_str, o_t]
                    if not o_t:continue
                    
                    
                
                    
                m_scale = ''
                if len(scale_d.keys()) > 1:
                    num_obj = {'One': 1, 'Dozen': 12, 'Hundred': 100, 'Thousand': 1000, 'Million': 1000000, 'Billion': 1000000000, 'Trillion': 1000000000000}
                    scales  = map(lambda x:(sconvert_obj.num_obj.get(sconvert_obj.scale_map_d.get(x, ''), 0), x), scale_d.keys())
                    scales.sort(key=lambda x:x[0])
                    m_scale = scales[-1][1]
                     
                expr_str    = []
                expr_val    = []
                f           = 0
                re_ar       = []
                    
                conver_arr  = []
                scale_d     = {}
                form_ar     = []
                opr_f       = '+'
                for rr in ops:
                    typ = rr['t']
                    v   = rr['v']
                    expr_str.append(v)
                    if typ == 'opr':
                        opr_f       = v
                        expr_val.append(v)
                    elif typ == 'cons':
                        expr_val.append(v)
                    elif typ == 'op':
                        v   = v+year
                        #print [v, ph_map_d.get(v), clean_v_d.get(ph_map_d.get(v))]
                        if v not in ph_map_d:
                            expr_val.append('0')
                            #f           = 0
                            #break
                            form_ar.append({'txid':str(row['t_id']), 'taxo_id':str(row['t_id']), 'operator':opr_f, 'ph':rr['v']+year, 'pk':'', 'type':'t','t_type':table_type, 'period':r_ph['ph'][:-4], 'fyear':r_ph['ph'][-4:], 'yd':0, 'i_f_type':'PTYPEFORMULA', 'tt':table_type, 'grpid':group_id, "description":row['t_l']})
                            continue
                        if r_report == 'Y':
                            if str(year) != latest_year_doc: #Wells Forago Profitability Ratio Derive Latest year, Do't check Restated Check 08-Nov_2019 BY Sinchana
                                if ph_map_d[v] in t_re_stated_cols:
                                    f == 0
                                    break
                        #elif t_re_stated_cols and ph_map_d[v] not in t_re_stated_cols:
                        #        if str(latest_year  ) != str(year):
                        #            f == 0
                        #            break
                        if ph_map_d[v] not in clean_v_d:
                            form_ar.append({'txid':str(row['t_id']), 'taxo_id':str(row['t_id']), 'operator':opr_f, 'ph':rr['v']+year, 'pk':ph_map_d[v], 'type':'t','t_type':table_type, 'period':r_ph['ph'][:-4], 'fyear':r_ph['ph'][-4:], 'yd':0, 'i_f_type':'PTYPEFORMULA', 'tt':table_type, 'grpid':group_id, "description":row['t_l']})
                            expr_val.append('0')
                            #f           = 0
                            #break
                            continue
                        #print rr, ph_map_d[v]
                        v_d     = row[ph_map_d[v]]
                        #if v_d['x'] and v_d['bbox'] and v_d['t'] and v_d['d']:
                        re_ar.append({'i':len(re_ar), 'x':v_d['x'], 'bbox':v_d['bbox'], 't':v_d['t'], 'd':v_d['d'], 'phcsv':v_d['phcsv'], 'FORM_VALUE':v_d.get('FORM_VALUE', '')})
                        tmpdd   = copy.deepcopy(v_d)
                        tmpdd.update({'txid':str(row['t_id']), 'taxo_id':str(row['t_id']), 'operator':opr_f, 'ph':rr['v']+year, 'pk':ph_map_d[v], 'period':r_ph['ph'][:-4], 'fyear':r_ph['ph'][-4:], 'yd':0, 'i_f_type':'PTYPEFORMULA', 'tt':table_type, 'grpid':group_id, "description":row['t_l']})
                        form_ar.append(tmpdd)
                        if clean_v_d[ph_map_d[v]] == '':
                            expr_val.append('0')
                        else:
                            f = 1
                            #print v_d
                            if m_scale:
                                phcsv   = v_d['phcsv']['s']
                                if phcsv != m_scale:
                                    tv, factor  = sconvert_obj.convert_frm_to_1(phcsv, m_scale, clean_v_d[ph_map_d[v]])
                                    if factor != '':
                                        conver_arr.append((clean_v_d[ph_map_d[v]], tv, sconvert_obj.scale_map_d.get(phcsv, '')+' - '+sconvert_obj.scale_map_d.get(m_scale, '')))
                                        tv      = tv.replace(',', '')
                                        expr_val.append(str(tv))
                                    else:
                                        expr_val.append(clean_v_d[ph_map_d[v]])
                                else:
                                    scale_d[m_scale]    = re_ar[-1]
                                    expr_val.append(clean_v_d[ph_map_d[v]])
                            else:
                                expr_val.append(clean_v_d[ph_map_d[v]])
            #print f, ''.join(expr_val)
            if f == 0:
                continue
            v   = ''
            try:
                v   = str(eval(''.join(expr_val)))
            except:
                continue
            if v == '' or float(v) == 0.0:continue
            v    = self.convert_floating_point(v)
            #print [v]
            re_ar.sort(key=lambda v_d: (v_d['i'], 0 if v_d['x'] and v_d['bbox'] and v_d['t'] and v_d['d'] else 1 ))
            if re_ar and (m_scale in scale_d):
                ref_d   = scale_d[m_scale]
            elif re_ar:
                ref_d   = re_ar[0]
            else:
                ref_d   = {}
            ref_d['expr_str']   = ''.join(expr_str)+' -- '+str(conver_arr)
            ref_d['expr_val']   = ''.join(expr_val)
            ref_d['fv']         = 'Y'
            ref_d['PH_D']       = 'Y'
            ref_d['v']          = v
            tmpdd   = copy.deepcopy(ref_d)
            tmpdd.update({'txid':str(row['t_id']), 'taxo_id':str(row['t_id']), 'operator':'=', 'ph':r_ph['ph'], 'pk':r_ph['ph'], 'period':r_ph['ph'][:-4], 'fyear':r_ph['ph'][-4:], 'yd':0, 'i_f_type':'PTYPEFORMULA',"R":"Y", 'tt':table_type, 'grpid':group_id, "description":row['t_l']})
            form_ar = [tmpdd]+form_ar
            ref_d['PH_FORM']    = form_ar
            ref_d['PH_FORM_STR']    = formula
            for tr in form_ar:
                tr['description']   = row['t_l']
            if tmp_rowid == 'RID-0':
                return v, ref_d, str(ph_gid_map.get(formula, ''))+'~'+tmp_rowid
            else:
                return v, ref_d, ph_gid_map.get(formula, '')
        return '', {}, ''

    def add_ph_formula(self, form_ar, data, taxo_value_grp, disp_name, csv_d, key_map_rev_t, txn):
        tmp_arr = []
        d_tmp_arr   = []
        for ft in form_ar:
            rid = taxo_value_grp[ft['txid']]
            rv_d    = data[rid].get(ft['pk'], {})
            if rv_d:
                d_tmp_d   = self.get_gv_dict(rv_d, txn, key_map_rev_t, {'k':ft['pk'], 'ph':ft['ph']}, csv_d)
                d_tmp_d['t']= rv_d['t']
                d_tmp_d['x']= rv_d['x']
                t       = rv_d['v']
                clean_value = t
                try:
                    clean_value = numbercleanup_obj.get_value_cleanup(t)
                except:
                    clean_value = ''
                    pass

                c_d = {'doc_id':rv_d['d'], 'op':ft['op'], 'ph':ft['ph'], 't':data[rid]['f_taxo'], 'ty':disp_name, 'page_no':rv_d['x'].split(':@:')[0].split('#')[0].split('_')[1], 'bbox':rv_d['bbox'], 'v':clean_value, 'label':data[rid]['t_l'], 't_id':data[rid]['t_id']}
                table_id, xml_id    =  rv_d['t'],  rv_d['x']
                #lbl_d   = data[rid].get(table_id+':$$:'+xml_id, {})
                #if not lbl_d.get('txt', ''):
                #    lbl_d['txt']   = data[rid]['t_l']
                #c_d['tn'] = lbl_d
            else:
                d_tmp_d   = ft
                c_d = {'doc_id':'', 'op':ft['op'], 'ph':ft['ph'], 't':data[rid]['f_taxo'], 'ty':disp_name, 'page_no':'', 'bbox':'',  'v':'', 'label':data[rid]['t_l'], 't_id':data[rid]['t_id']}
                #lbl_d   = {}
                #if not lbl_d.get('txt', ''):
                #    lbl_d['txt']   = data[rid]['t_l']
                #c_d['tn'] = lbl_d
            tmp_arr.append(c_d)
            d_tmp_d[key_map_rev_t['description']]   = data[rid]['t_l']
            d_tmp_d[key_map_rev_t['taxo_id']]       = data[rid]['t_id']
            d_tmp_d[key_map_rev_t['operator']]      = ft['op']
            d_tmp_arr.append(d_tmp_d)
        return tmp_arr, d_tmp_arr


    def convert_frm_to(self, value, frm_to, scale):
            
        num     = float(value)
        frm, to = frm_to.split(' - ')
        mscale  = ''
        if scale:
            mscale  = sconvert_obj.scale_map_d[scale]
        if frm  != mscale:
            return '', ''
        num_obj = {'One': 1, 'Dozen': 12, 'Hundred': 100, 'Thousand': 1000, 'Million': 1000000, 'Billion': 1000000000, 'Trillion': 1000000000000, "PerMiles":1.6,"PerKilometer":1,"PerKilometre":1, "Miles":1.6,"Kilometer":1,"Kilometre":1}
        frm_val = sconvert_obj.num_obj[frm]
        to_val  = sconvert_obj.num_obj[to]
        if frm == "PerMiles" and to in ['PerKilometre', 'PerKilometer']:
            return self.cal_mile_to_kilo(frm_val, to_val, num), to
        div_val = float(frm_val)/float(to_val)
        final_val = float(num * div_val)
        return final_val, to

    def cal_mile_to_kilo(self, frm_val, to_val, num):
        div_val = float(frm_val)/float(to_val)
        final_val = float(num / div_val)
        final_val   = self.convert_floating_point(final_val)
        return final_val #.replace(",", "")


    def calculate_user_cf_ph_values(self, ti, ph, t_cfs_f, rr, key_value, cfgrps, n_phs, cf_all_grps, res, ph_map_d, taxo_id_dict):
        f   = 0
        if ph['k'] in t_cfs_f:
            t_row_f     = t_cfs_f[ph['k']]
            year    = ph['ph'][-4:]
            #print '\n===================================='
            #print rr['t_l']
            #print t_row_f[2]
            val_dict, form_d, flip_sign = self.get_formula_evaluation(t_row_f[2], taxo_id_dict, [ph['k']], ph_map_d)
            #print  ph['ph'], val_dict
            if val_dict.get(str(rr['t_id']), {}):
                ref_d   = copy.deepcopy(val_dict[str(rr['t_id'])][ph['k']])
                rr[ph['k']] = ref_d
                try:
                    clean_value = numbercleanup_obj.get_value_cleanup(ref_d['v'])
                except:
                    clean_value = ''
                    pass
                key_value[ph['k']]       = clean_value
                f_ar    = []
                for ft in t_row_f[2]:
                    if ft['type'] == 'v':
                        dd  = {}
                        dd['clean_value']   = ft['txid']
                    else:
                        tph         = ''
                        if ft.get('k', ''):
                            tph  = ft['k']
                        elif ft.get('period', ''):
                            #print '\tTT',[oper['period'],year,  oper['yd']]
                            tph  = ft['period']+str(int(year)+int(ft['yd']))
                        if ft['op'] == '=':
                            dd  = copy.deepcopy(ref_d)
                        else:
                            dd  = copy.deepcopy(taxo_id_dict[ft['txid']].get(ph_map_d.get(tph, ''), {}))
                        dd['clean_value']   = dd.get('v', '')
                        dd['description']   = taxo_id_dict[ft['txid']]['t_l']
                        dd['ph']            = tph
                        dd['taxo_id']       = ft['txid']
                    dd['operator']      = ft['op']
                    dd['k']      = ph['k']
                    f_ar.append(dd)
                f_ar[0]['R']    = 'Y'
                f_ar[0]['rid']    = t_row_f[0].split('-')[-1]
                ref_d['f_col_CF']  = [f_ar]
                f   = 1
                n_phs[ph['ph']] = 1
                rr[ph['k']]['expr_str']   = 'CF  '+ref_d['expr_str']
                rr[ph['k']]['fv']   = 'Y'
                rr[ph['k']]['PH_D']   = 'Y'
                rr[ph['k']]['PH_FORM']   = form_d[ph['k']][1:]
                res[0]['data'][ti][ph['k']]    = copy.deepcopy(rr[ph['k']])
        if f == 0 and ph['ph'][:-4] in t_cfs_f:
            t_row_f     = t_cfs_f[ph['ph'][:-4]]
            year    = ph['ph'][-4:]
            #print '\n===================================='
            #print rr['t_l'], t_row_f[2], ph_map_d
            val_dict, form_d, flip_sign = self.get_formula_evaluation(t_row_f[2], taxo_id_dict, [ph['k']], ph_map_d, ph['ph'][-4:] )
            #print  ph['ph'], val_dict
            if val_dict.get(str(rr['t_id']), {}):
                ref_d   = copy.deepcopy(val_dict[str(rr['t_id'])][ph['k']])
                #print '\n===================================='
                #print rr['t_l']
                #print  ph['ph'], ref_d
                rr[ph['k']] = ref_d
                try:
                    clean_value = numbercleanup_obj.get_value_cleanup(ref_d['v'])
                except:
                    clean_value = ''
                    pass
                key_value[ph['k']]       = clean_value
                f_ar    = []
                for ft in t_row_f[2]:
                    if ft['type'] == 'v':
                        dd  = {}
                        dd['clean_value']   = ft['txid']
                    else:
                        tph         = ''
                        if ft.get('k', ''):
                            tph  = oper['k']
                        elif ft.get('period', ''):
                            #print '\tTT',[oper['period'],year,  oper['yd']]
                            tph  = ft['period']+str(int(year)+int(ft['yd']))
                        if ft['op'] == '=':
                            dd  = copy.deepcopy(ref_d)
                        else:
                            dd  = copy.deepcopy(taxo_id_dict[ft['txid']].get(ph_map_d.get(tph, ''), {}))
                        dd['clean_value']   = dd.get('v', '')
                        dd['description']   = taxo_id_dict[ft['txid']]['t_l']
                        dd['ph']            = tph
                        dd['taxo_id']       = ft['txid']
                    dd['operator']      = ft['op']
                    dd['k']      = ph['k']
                    f_ar.append(dd)
                f_ar[0]['R']    = 'Y'
                f_ar[0]['rid']    = t_row_f[0].split('-')[-1]
                ref_d['f_col_CF']  = [f_ar]
                f   = 1
                n_phs[ph['ph']] = 1
                rr[ph['k']]['expr_str']   = 'CF  '+ref_d['expr_str']
                rr[ph['k']]['fv']   = 'Y'
                res[0]['data'][ti][ph['k']]    = copy.deepcopy(rr[ph['k']])
        return f
        

    def calculate_user_ph_values(self, ph, rr, ti, t_cf_f, t_ph_f, t_cfs_f, taxo_value_grp, ph_ti, res, data, f_phs_d, key_value, ph_map_d, cfgrps, cf_all_grps, grps, all_grps, n_phs, ph_f_map, ph_gid_map, d_all_grps, taxo_id_dict, ph_formula_d, deal_id, table_type, group_id, do_null, t_re_stated_cols, latest_year, latest_year_doc):
        f   = 0
        if ph['k'] in t_cf_f:
            g_id, cf_taxo_id, cf_phk, cf_ph, year_diff = t_cf_f[ph['k']]
            if cf_taxo_id in taxo_value_grp and cf_phk in data[taxo_value_grp[cf_taxo_id]]:
                ref_d   = copy.deepcopy(data[taxo_value_grp[cf_taxo_id]][cf_phk])
                #print '\n===================================='
                #print rr['t_l']
                #print  ph['ph'], t_ph_f[ph['ph'][:-4]]
                rr[ph['k']] = ref_d
                try:
                    clean_value = numbercleanup_obj.get_value_cleanup(ref_d['v'])
                except:
                    clean_value = ''
                    pass
                key_value[ph['k']]       = clean_value
                f   = 1
                cfgrps.setdefault(g_id, {})[ph['k']]  = 'Y'
                n_phs[ph['ph']] = 1
                cf_all_grps[g_id]   = 1
                rr[ph['k']]['expr_str']   = 'CF ( '+str([cf_taxo_id, cf_phk, cf_ph])+' )'
                rr[ph['k']]['fv']   = 'Y'
                rr[ph['k']]['PH_D']   = 'Y'
                res[0]['data'][ti][ph['k']]    = copy.deepcopy(rr[ph['k']])
        if f == 0 and ph['ph'][:-4] in t_cf_f:
            g_id, cf_taxo_id, cf_phk, cf_ph, year_diff = t_cf_f[ph['ph'][:-4]]
            if cf_taxo_id in taxo_value_grp:
                cf_ph   = cf_ph+str(int(ph['ph'][-4:])+year_diff)
                cf_phk  = ph_ti.get(cf_ph, {}).get(taxo_value_grp[cf_taxo_id], '')
                #print '\n===================================='
                #print rr['t_l']
                #print  ph['ph'], cf_phk
                if cf_phk:
                    ref_d   = copy.deepcopy(data[taxo_value_grp[cf_taxo_id]][cf_phk])
                    #print '\tGot...',ref_d['v']
                    rr[ph['k']] = ref_d
                    try:
                        clean_value = numbercleanup_obj.get_value_cleanup(ref_d['v'])
                    except:
                        clean_value = ''
                        pass
                    key_value[ph['k']]       = clean_value
                    f   = 1
                    cfgrps.setdefault(g_id, {})[ph['k']]  = 'Y'
                    n_phs[ph['ph']] = 1
                    cf_all_grps[g_id]   = 1
                    rr[ph['k']]['expr_str']   = 'CF ( '+str([cf_taxo_id, cf_phk, cf_ph])+' )'
                    rr[ph['k']]['fv']   = 'Y'
                    rr[ph['k']]['PH_D']   = 'Y'
                    res[0]['data'][ti][ph['k']]    = copy.deepcopy(rr[ph['k']])
        if f == 0:
            f   = self.calculate_user_cf_ph_values(ti, ph, t_cfs_f, rr, key_value, cfgrps, n_phs, cf_all_grps, res, f_phs_d, taxo_id_dict)
            
        if f == 0 and (ph['k'] in t_ph_f or ph['ph'] in t_ph_f):
            ph_fs   = []
            tmpph_ar    = t_ph_f.get(ph['k'], [])
            keytt       = ph['k']
            if not tmpph_ar:
                keytt       = ph['ph']
                tmpph_ar    = t_ph_f.get(ph['ph'], [])
            #print '\n===================================='
            #print rr['t_l']
            for ftup in  tmpph_ar:
                #if ftup[2] != 'RID-0' or  (do_null == 1 or rr['t_id'] not in ph_formula_d.get(('Ignore_taxo', ftup[0]), {})):
                if ftup[2] != 'RID-0' or  (do_null == 1 or rr['t_id'] not in ph_formula_d.get(('Ignore_taxo', ftup[0]), {})):
                    ph_fs.append(ftup)
            #print  ph['ph'], ph_fs
            v, ref_d, g_id    = self.eval_ph_formula(rr, ph_fs, f_phs_d, key_value, ph['ph'][-4:], ph_gid_map, ph['ph'][:-4], deal_id, table_type, group_id, t_re_stated_cols, ph, latest_year, latest_year_doc)
            if v:
                #print  '\t', ref_d
                rr[ph['k']] = ref_d
                try:
                    clean_value = numbercleanup_obj.get_value_cleanup(ref_d['v'])
                except:
                    clean_value = ''
                    pass
                key_value[ph['k']]       = clean_value
                f   = 1
                grps.setdefault(g_id, {})[ph['k']]  = 'Y'
                n_phs[ph['ph']] = 1
                d_all_grps.setdefault(g_id, {})[t_ph_f[('rid', keytt)]]  = 1
                res[0]['data'][ti][ph['k']]    = copy.deepcopy(rr[ph['k']])
        if f == 0 and do_null:
            year    = ph['ph'][-4:]
            #print '\n===================================='
            #print rr['t_l'], ph
            for ph_f in ph_f_map.get(ph['ph'][:-4], []):
                #if rr['t_id'] in ph_formula_d.get(('Ignore_taxo', ph_f['f']), {}):continue
                f_all   = 1
                #print '\n\t---------------------------------------------'
                #print '\t',ph_f
                f_fy    = 0
                f_one   = 0
                for op in ph_f['op']:
                    if  op == 'FY' and f_fy == 0:
                        f_fy    = 1
                    op_ph   = op+year
                    #print '\t', [f_phs_d.get(op_ph, 'NOPH'), f_phs_d.get(op_ph, 'NOPH') not in rr]
                    if key_value.get(f_phs_d.get(op_ph, 'NOPH'), None) == None:
                        f_all   = 0
                        continue
                    f_one   = 1
                    if  op == 'FY':
                        f_fy    = 2
                #print 'f ', f, (f_all, f_one)
                #f_fy    = 0
                if f_all == 1 or f_one == 1:# and f_fy != 1):
                    n_phs[ph['ph']] = 1
                    grps.setdefault(ph_f['g_id'], {})[ph['k']]  = 'N'
                    all_grps.setdefault(ph_f['g_id'], {})[rr['t_id']]  = 1
        return f


    def create_group_ar(self, ijson, txn_m):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        ijson['vids']   = {}
        table_type    = ijson['table_type']
        vgh_id_d, vgh_id_d_all, docinfo_d   = {}, {}, {}
        if 1:
            db_file         = self.get_db_path(ijson)
            conn, cur   = conn_obj.sqlite_connection(db_file)
            sql         = "select row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, doc_id, m_rows, vgh_text, vgh_group, xml_id, period, period_type, scale, currency, value_type from mt_data_builder where table_type='%s' and isvisible='Y'"%(table_type)
            #print sql
            try:
                cur.execute(sql)
                res         = cur.fetchall()
            except:
                res = []
            for rr in res:
                row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, gv_xml, ph, ph_label,gcom, ngcom, doc_id,m_rows, vgh_text, vgh_group, xml_id, period, period_type, scale, currency, value_type    = rr
                tk   = self.get_quid(table_id+'_'+xml_id)
                c_id        = txn_m.get('XMLID_MAP_'+tk)
                if not c_id:continue
                vgh_id_d.setdefault(vgh_text, {})[(table_id, c_id, row_id, doc_id)]        = 1
                vgh_id_d_all[vgh_text]  = 1
                doc_id      = str(doc_id)
                table_id    = str(table_id)
                docinfo_d.setdefault(doc_id, {})[(table_id, c_id, vgh_text)]   = 1
            conn.close()
        g_ar  = self.read_all_vgh_groups(table_type, company_name, model_number,vgh_id_d, vgh_id_d_all, docinfo_d)
        return g_ar

    def create_maturity_date(self, ijson, txn_m):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        ijson['vids']   = {}
        table_type    = ijson['table_type']
        vgh_id_d, vgh_id_d_all, docinfo_d   = {}, {}, {}
        if 1:
            db_file         = self.get_db_path(ijson)
            conn, cur   = conn_obj.sqlite_connection(db_file)
            sql         = "select row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, doc_id, m_rows, vgh_text, vgh_group, xml_id, period, period_type, scale, currency, value_type from mt_data_builder where table_type='%s' and isvisible='Y'"%(table_type)
            #print sql
            try:
                cur.execute(sql)
                res         = cur.fetchall()
            except:
                res = []
            for rr in res:
                row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, gv_xml, ph, ph_label,gcom, ngcom, doc_id,m_rows, vgh_text, vgh_group, xml_id, period, period_type, scale, currency, value_type    = rr
                tk   = self.get_quid(table_id+'_'+xml_id)
                c_id        = txn_m.get('XMLID_MAP_'+tk)
                if not c_id:continue
                vgh_id_d.setdefault(vgh_text, {})[(table_id, c_id, row_id, doc_id)]        = 1
                vgh_id_d_all[vgh_text]  = 1
                doc_id      = str(doc_id)
                table_id    = str(table_id)
                docinfo_d.setdefault(doc_id, {})[(table_id, c_id, vgh_text)]   = 1
            conn.close()
        g_ar  = self.read_all_vgh_groups(table_type, company_name, model_number,vgh_id_d, vgh_id_d_all, docinfo_d)
        mdate           = filter(lambda x:x['n'].lower().split(' - ')[0] == 'maturity date', g_ar)
        if mdate:
            m_d = {}
            for mr in mdate:
                if '-' in mr['grpid']:
                    m_d[mr['grpid'].split('-')[1]]  = mr
                else:
                    m_d['']  = mr
            return m_d
        return {}

            
        

    def create_ph_group(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        if ijson.get("PRINT") != 'Y':
                disableprint()


        ph_d        = {}
        path    = "%s/%s/%s/1_1/21/sdata/doc_map.txt"%(self.doc_path, project_id, deal_id)
        if os.path.exists(path):
            fin = open(path, 'r')
            lines   = fin.readlines()
            fin.close()
        else:
            lines   = []
        doc_d       = {}
        dphs        = {}
        years       = {}
        #c_year      = int(datetime.datetime.now().strftime('%Y'))
        c_year      = self.get_cyear(lines)
        #start_year  = c_year - int(ijson['year'])
        start_year  = c_year - int(ijson.get('year', 5))
        for line in lines[1:]:
            line    = line.split('\t')
            if len(line) < 8:continue
            line    = map(lambda x:x.strip(), line)
            ph      = line[3]+line[7]
            if ph and start_year<int(ph[-4:]):
                doc_id  = line[0]
                doc_d[doc_id]   = (ph, line[2])
                dphs.setdefault(ph, {})[doc_id]        = 1
                years[ph[-4:]]       = {}
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn         = env.begin()

        stend_ar    = self.read_start_end(ijson, 'Y')[0]['data']
        cfstend_d = {}
        for rr in stend_ar:
            cfstend_d[rr['b']] = ('E', rr['b'])
            cfstend_d[rr['e']] = ('S', rr['e'])
            cfstend_d.setdefault('pairs', []).append((rr['b'], rr['e']))

        dphs_ar         = report_year_sort.year_sort(dphs.keys())
        dphs_ar.reverse()
        ignore_doc_ids  = {}
        key_map = self.key_map
        key_map_rev     = {}
        key_map_rev_t   = {}
        for k, v in key_map.items():
            key_map_rev[v]  = k
            key_map_rev_t[v]  = v
        ph_f_map, ph_grp_ar, ph_gid_map         = self.read_ph_formula(ijson, 'Y')
        group_id    = ijson.get('grpid', '')
        #if '-' in group_id:
        #    group_id, doc_grpid = group_id.split('-')
        #ph_formula_d                            = self.read_ph_user_formula(ijson, '')
        #if group_id:
        #    ph_formula_d                            = self.read_ph_user_formula(ijson, group_id, ph_formula_d)
        ph_filter       = 'P'
        if ph_filter != 'P':
            rem = int(ph_filter.split('-')[1])
            new_phs = dphs_ar[rem:]
            for ph in dphs_ar[:rem]:
                ignore_doc_ids.update(dphs[ph])
                del dphs[ph]

        t_ids       = ijson.get('t_ids', [])
        gen_type    = ijson.get('type','')
        grp_mad_d   = {}
        table_type  = ijson['table_type']
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()
        print 'Running ', table_type
        ijson['gen_output']     = 'Y'
        ijson['ignore_doc_ids'] = ignore_doc_ids
        res                 = self.create_seq_across(ijson)
        phs                 = res[0]['phs']
        data                = res[0]['data']
        all_tids    = {}
        for rr in res[0]['data']:
            all_tids[str(rr['t_id'])]    = 1
        ph_formula_d    = {}
        if group_id:
            ph_formula_d                            = self.read_ph_user_formula(ijson, group_id, ph_formula_d, all_tids)
            #print 'ph_formula_d ', ph_formula_d.get(('Ignore_taxo', 'Q2=H1'), {})
        else:
            ph_formula_d                            = self.read_ph_user_formula(ijson, '', {}, all_tids)
        #sys.exit()
        
        mdata               = self.create_maturity_date(ijson, txn_m)
        mat_date            = {}
        if mdata:
            if '-' in group_id:
                group_id_sp = group_id.split('-')
                if group_id_sp[1] in mdata:
                    mat_date            = mdata[group_id_sp[1]]
            elif '' in mdata:
                mat_date            = mdata['']
        #print mat_date
        if mat_date:
            mat_date    = self.read_mat_date(ijson, mat_date, txn_m)
        validate_op         = {}
        rc_d                = {}
        consider_ks         = {}
        consider_table      = {}
        consider_rows       = {}
        xml_row_map         = {}
        ph_kindex           = {}
        for ii, ph in enumerate(phs):
            ph_kindex[ph['k']]  = ii
        duplicate_taxonomy  = {}
        phk_data            = {}
        validation_error    = {}
        #print validate_op
        ph_ti               = {}
        taxo_value_grp  = {}
        phk_ind_data        = {}
        clean_v_d   = {}
        row_ind_map = {}
        row                 = 1
        csv_d               = {}
        taxo_id_dict        = {}
        for ti, rr in enumerate(data[:]):
            taxo_value_grp[str(rr['t_id'])]  = ti
            #print '\n====================================='
            #print rr['t_l']
            for ii, ph in enumerate(phs):
                if ph['k'] not in rr:continue
                #print '\t', ph
                v_d         = rr[ph['k']]
                t           = v_d['v']
                clean_value = t
                try:
                    clean_value = numbercleanup_obj.get_value_cleanup(t)
                except:
                    clean_value = ''
                    pass
                if 'f_col' in v_d:
                    del v_d['f_col']
                if 'f_col' in res[0]['data'][ti][ph['k']]:
                    del res[0]['data'][ti][ph['k']]['f_col']
                tk   = self.get_quid(v_d['t']+'_'+v_d['x'])
                c_id        = txn_m.get('XMLID_MAP_'+tk)
                if c_id:
                    ktup    = (rr['t_id'], v_d['t'], int(c_id.split('_')[1]))
                    #print '\t', ktup, ktup in mat_date
                    if ktup in mat_date:
                        v_d['mdate']    = mat_date[ktup][2]
                        res[0]['data'][ti][ph['k']]['mdate']    = mat_date[ktup][2]
                        v_d.setdefault('phcsv', {})['mdate']    =  mat_date[ktup][2]
                        res[0]['data'][ti][ph['k']].setdefault('phcsv', {})['mdate']    = mat_date[ktup][2]
                clean_v_d.setdefault(ti, {})[ph['k']]   = clean_value
                ph_ti.setdefault(ph['n'], {})[ti]  = ph['k']
                phk_ind_data.setdefault(ph['k'], {})[ti]    = clean_value
                if clean_value == '':continue
                phk_data.setdefault(ph['k'], []).append((ti, clean_value))
                #ph_value_d.setdefault(ph['n'], {}).setdefault(clean_value, {})[ph['k']]          = 1
        #print phs
        f_phs, reported_phs               = self.get_order_phs(phs, dphs, self.report_map_d, ijson)
        dphs_all         = report_year_sort.year_sort(dphs.keys())
        exists_ph       = {}
        for ph in f_phs:
            exists_ph[ph['ph']] = 1
        n_ph    = {}
        for ph in res[0]['phs']:
            if ph['n'] in exists_ph:continue
            if not ph['n']:continue
            try:
                ttyear  = int(ph['n'][-4:])
            except:continue
            #if ph['n'] in ignore_dphs:continue
            n_ph.setdefault(ph['n'], []).append( ph['k'])

        d_new_phs   = []
        if n_ph:
            ph_map_d    = {}
            f_phs_d     = {}
            for ph in f_phs:
                ph_map_d[ph['ph']]   = ph
                f_phs_d[ph['ph']]   = ph['k']
            for tph, pks in n_ph.items():
                ph  = {'ph':tph, 'new':"Y", 'k':pks[-1], 'g':tph, 'dph':(tph[:-4], int(tph[-4:]))}
                ph_map_d[ph['ph']]   = ph
                f_phs_d[ph['ph']]   = ph['k']
                reported_phs[tph]    = {}
                for tk in pks:
                    reported_phs[tph][tk]    = 1
                d_new_phs.append(copy.deepcopy(ph))
            dphs_ar             = report_year_sort.year_sort(f_phs_d.keys())
            dphs_all            = copy.deepcopy(dphs_ar)
            dphs_ar.reverse()
            f_phs               = map(lambda x:ph_map_d[x], dphs_ar)

        col_m               = self.get_column_map(ph_kindex, reported_phs, f_phs, phk_ind_data, data, ph_formula_d, taxo_value_grp, table_type)
        #sys.exit()
        f_phs_d             = {}
        done_ph             = {}
        ph_map_d    = {}
        for ph in f_phs:
            f_phs_d[ph['ph']]   = ph['k']
            done_ph[ph['ph']]  = 1
            ph_map_d[ph['ph']]    = ph['k']
        new_phs         = []
        n_d             = {}
        #for ph in dphs_ar:
        #    if ph in done_ph or ph in n_d:continue
        tt_ph_f             = self.read_ph_config_formula(ijson)
        if not tt_ph_f:
            tt_ph_f             = self.read_ph_config_formula(ijson, '')
        if tt_ph_f:
            remain_phs = tt_ph_f.keys()+ph_f_map.keys()
        else:
            remain_phs = ph_f_map.keys()
        system_formulas = {}
        for k, v in tt_ph_f.items():
            for v1 in v:
                system_formulas[v1[0]]   = 1
        #print tt_ph_f
        #sys.exit()
        #print ph_f_map
        #print ph_gid_map
            
        for ti, rr in enumerate(data[:]):
            if t_ids and rr['t_id'] not in t_ids:continue
            t_ph_f      = ph_formula_d.get(str(rr['t_id']), {})
            for ii, ph in enumerate(res[0]['phs']):
                if ph['k'] in t_ph_f:
                    if ph['n'] not in n_d and  ph['n'] not in done_ph:
                        n_d[ph['n']]    = 1
                        tph             = ph['n']
                        new_phs.append({'k':tph, 'ph':tph, 'new':'Y'})
        for ph in remain_phs:
            for year in years.keys():
                tph  = ph+year
                if tph in done_ph or tph in n_d:continue
                n_d[tph]             =1
                new_phs.append({'k':tph, 'ph':tph, 'new':'Y'})
            
        if new_phs:
            ph_map_d    = {}
            #for k, v in f_phs_d.items():
            #    print k, v
            for ph in f_phs+new_phs:
                #print [ph]
                if not ph['ph']:continue
                ph_map_d[ph['ph']]   = ph
                f_phs_d[ph['ph']]   = ph['k']
                done_ph[ph['ph']]  = 1
            #print f_phs_d.keys()
            #print n_phs.keys()
        
            dphs_ar             = report_year_sort.year_sort(f_phs_d.keys())
            dphs_all            = copy.deepcopy(dphs_ar)
            dphs_ar.reverse()
            f_phs               = map(lambda x:ph_map_d[x], dphs_ar)

        rem_phs  = {}
        for ph in res[0]['phs']:
            if ph['k'] not in phk_ind_data:continue
            if ph['n'] not in f_phs_d:
                rem_phs[ph['n']]    = 1
        if rem_phs and 0:
            tnew_phs = []
            for tph in rem_phs.keys():
                if tph in done_ph or tph in n_d:continue
                n_d[tph]             =1
                new_phs.append({'k':tph, 'ph':tph, 'new':'Y'})
            if tnew_phs:
                ph_map_d    = {}
                #for k, v in f_phs_d.items():
                #    print k, v
                for ph in f_phs+tnew_phs:
                    ph_map_d[ph['ph']]   = ph
                    f_phs_d[ph['ph']]   = ph['k']
                    done_ph[ph['ph']]  = 1
                #print f_phs_d.keys()
                #print n_phs.keys()
                dphs_ar             = report_year_sort.year_sort(f_phs_d.keys())
                dphs_ar.reverse()
                f_phs               = map(lambda x:ph_map_d[x], dphs_ar)
            new_phs += tnew_phs
        tdphs_ar             = report_year_sort.year_sort(list(sets.Set(dphs_all+dphs.keys())))
        dphs_all            = copy.deepcopy(tdphs_ar)
        pk_rev_d    = {}
        for ph in f_phs:
            pk_rev_d[ph['k']]   = ph['ph']
        rep_docids  = {}
        for ti, rr in enumerate(data[:]):
            taxo_id_dict[str(rr['t_id'])]        = rr
            rr['f_taxo']    = str(rr['t_id'])
            t_s_f       = ph_formula_d.get(('S', str(rr['t_id'])), {})
            ph_value_d  = {}
            for ii, ph in enumerate(phs):
                csv_d.setdefault(ti, {})[ph['k']]   = (0, '','', '', '')
                if (ph['k'] not in rr or clean_v_d.get(ti, {}).get(ph['k'], '') == '') and (ti in col_m.get(ph['k'], {})):
                    if col_m[ph['k']][ti] in rr:
                        clean_v_d.setdefault(ti, {})[ph['k']]   = clean_v_d[ti][col_m[ph['k']][ti]]
                        rr[ph['k']] = rr[col_m[ph['k']][ti]]
                        res[0]['data'][ti][ph['k']]    = rr[col_m[ph['k']][ti]]
                if ph['k'] not in rr:continue
                if 'ng_f' in rr[ph['k']]:
                    del rr[ph['k']]['ng_f']
                if 'g_f' in rr[ph['k']]:
                    del rr[ph['k']]['g_f']
                v_d         = rr[ph['k']]
                rep_docids[str(v_d['d'])]   = 1
                t           = v_d['v']
                clean_value = clean_v_d[ti][ph['k']]
                ph_value_d.setdefault(ph['n'], {}).setdefault(clean_value, {})[ph['k']]          = 1
                d_al, scale, tph_map, mscale, form   = (0, '', '', '', '')
                key     = v_d['t']+'_'+self.get_quid(v_d['x'])
                tph_map  = txn.get('PH_MAP_'+str(key))
                if tph_map:
                    period_type, period, currency, scale, value_type    = tph_map.split('^')
                if scale in sconvert_obj.scale_map_d:
                    mscale  = sconvert_obj.scale_map_d[scale]
                sgrpid  = ''
                if ph['k'] in t_s_f and clean_value:
                    sgrpid, tform    = t_s_f[ph['k']]
                    v, factor   = sconvert_obj.convert_frm_to_1(tform, scale, clean_value)
                    if v and factor != '':
                        form    = tform
                        v       = str(v)
                        clean_value = v
                        mscale  = nscale
                        rr[ph['k']]['expr_str']   = form+'('+str(rr[ph['k']]['v'])+') = '+str(v)
                        rr[ph['k']]['v']    = v
                        rr[ph['k']]['fv']   = 'Y'
                        rr[ph['k']]['fv']   = 'Y'
                        #scale_d.setdefault(mscale, {})[ph['k']]  = 'Y'
                        #mscale_d.setdefault(mscale, {}).setdefault(scale, {})[ph['k']]  = 'Y'
                        d_al    = 1
                csv_d.setdefault(ti, {})[ph['k']]   = (d_al, scale, mscale, form, sgrpid)
            if t_ids and rr['t_id'] not in t_ids:continue
            row += 1
            row_ind_map[ti] = row
            if ijson.get('restated', '') != 'N':
                for ii, ph in enumerate(f_phs):
                    trs_value    = []
                    if ph['ph'] not in dphs:continue
                    if ph['k'] not in rr or clean_v_d[ti].get(ph['k'], '') == '':
                        other_values            = ph_value_d.get(ph['ph'], {});
                        for v in other_values.keys():
                            if v and v != clean_v_d[ti].get(ph['k'], ''):
                                for key in other_values[v].keys():
                                        if 1:#indx_d[key] > c_index:
                                            trs_value.append(key)
                    else:
                        other_values            = ph_value_d.get(ph['ph'], {})
                        clean_value             = clean_v_d[ti][ph['k']]
                        for v in other_values.keys():
                            if v and v != clean_v_d[ti][ph['k']]:
                                for key in other_values[v].keys():
                                    if 1:#indx_d[key] > c_index:
                                        if  rr[key]['t'] != v_d['t']:
                                            trs_value.append(key)
                    trs_value.sort(key=lambda x:(dphs_all.index(doc_d[rr[x]['d']][0]), rr[x]['d']))
                    if trs_value:
                        prev_v      =  copy.deepcopy(rr.get(ph['k'], {'v':'','x':'','bbox':[], 'd':'', 't':''}))
                        rr[ph['k']] = copy.deepcopy(rr[trs_value[-1]])
                        rr[ph['k']]['rest_ar']  = trs_value[:]
                        rr[ph['k']]['org_d']    = prev_v
                        rr[ph['k']]['x'] = rr[ph['k']]['x']
                        csv_d.setdefault(ti, {})[ph['k']]   = csv_d[ti][trs_value[-1]]
                        clean_v_d[ti][ph['k']]   = clean_v_d[ti][trs_value[-1]]
                        res[0]['data'][ti][ph['k']]    = copy.deepcopy(rr[ph['k']])
                        #print 'RESTATED_ ', [rr['t_l'], ph, ph['ph'] in dphs, trs_value]


        row                 = 1
        remain_phs = ph_f_map.keys()
        all_grps        = {}
        cf_all_grps     = {}
        d_all_grps        = {}
        n_phs           = {}    
        scale_grps      = {}
        d_s_grps        = {}
        xml_row_map     = {}
        row                 = 1
        row_formula         = {}
        tmp_f_phs   = []
        for tr in f_phs:
            tr  = copy.deepcopy(tr) 
            tr['n']     = tr['ph']
            tmp_f_phs.append(tr)  
        self.calculate_missing_values_by_fx(t_ids, res, data, ph_formula_d, taxo_id_dict, table_type, group_id, row_formula, tmp_f_phs, str(deal_id))
        tmp_phs = copy.deepcopy(f_phs)
        if tmp_phs and int(tmp_phs[0]['ph'][-4:]) > int(tmp_phs[-1]['ph'][-4:]):
            tmp_phs.reverse()
        for ti, rr in enumerate(data[:]):
            if t_ids and rr['t_id'] not in t_ids:continue
            row += 1
            tmpphs              = [] 
            key_value           = {}
            ph_value_d          = {}
            indx_d              = {}
            xml_col_map         = {}
            ph_map              = {}
            scale_d             = {}
            d_scale_d             = {}
            t_ph_f      = ph_formula_d.get(str(rr['t_id']), {})
            t_s_f       = ph_formula_d.get(('S', str(rr['t_id'])), {})
            t_cf_f      = ph_formula_d.get(('CF', str(rr['t_id'])), {})
            t_cfs_f     = ph_formula_d.get(('CF_F', str(rr['t_id'])), ())
            tmp_sgrps    = {}
            r_key       = {}
            rs_value    = {}
            grps        = {}
            cfgrps        = {}
            d_grps      = {}
            mscale_d    = {}
            for ii, ph in enumerate(phs):
                if ph['k'] in rr:
                    v_d = rr[ph['k']]
                    xml_row_map.setdefault((v_d['t'], v_d['x']), {})[ti]    = 1
                    v_d             = rr[ph['k']]
                    
                if ph['k'] not in rr:continue
                ph_map[ph['k']]              = ph['ph']
                v_d = rr[ph['k']]
                t       = v_d['v']
                xml_col_map[(v_d['t'], v_d['x'])]         = ph['k']
                clean_value = t
                try:
                    clean_value = numbercleanup_obj.get_value_cleanup(t)
                except:
                    clean_value = ''
                    pass
                #if clean_value == '':continue
                tmpphs.append(ph)
                #key_value[ph['k']]       = clean_value
                #print [ph['n'], clean_value, ph['k']]
                ph_value_d.setdefault(ph['n'], {}).setdefault(clean_value, {})[ph['k']]          = 1
                indx_d[ph['k']]              = ii
            key_value   = clean_v_d.get(ti, {})
            #f_phs       = self.get_order_phs(tmpphs, dphs, report_map_d)
            #print '\n===================================='
            #print rr['t_l']
            #print t_ph_f
            done_s    = {}
            r_key, rs_value, operand_rows, row_form, row_op = self.read_formula(ti, rr, f_phs, txn, xml_row_map, xml_col_map)
            consider_rows[rr['t_id']]       = 1
            if tt_ph_f:
                #t_ph_f  = {}
                for ii, ph in enumerate(f_phs):
                    if ph['k'] not in rr or not key_value.get(ph['k'], ''):
                        if ph['ph'][:-4] in tt_ph_f and ph['k'] not in t_ph_f:
                            t_ph_f[ph['k']] = tt_ph_f[ph['ph'][:-4]]
                            t_ph_f[('rid', ph['k'])] = 'RID-0'
            for ii, ph in enumerate(tmp_phs):
                d_al, scale, mscale, form,sgrpid  = csv_d[ti].get(ph['k'], (0, '', '','', ''))
                if form:
                    tmp_sgrps[sgrpid]  = (form, scale);
                    scale_d.setdefault(mscale, {})[ph['k']]  = 'Y'
                    mscale_d.setdefault(mscale, {}).setdefault(scale, {})[ph['k']]  = 'Y'
                if ph['k'] in rr:
                    consider_ks[ph['k']]         = 1
                if (ph['k'] not in rr or not key_value.get(ph['k'], '')):
                    f   = 0
                    if str(rr['t_id']) not in cfstend_d:
                        f   = self.calculate_user_ph_values(ph, rr, ti, t_cf_f, t_ph_f, t_cfs_f, taxo_value_grp, ph_ti, res, data, f_phs_d, key_value, ph_map_d, cfgrps, cf_all_grps, grps, all_grps, {}, ph_f_map, ph_gid_map, d_all_grps, taxo_id_dict, ph_formula_d, str(deal_id), table_type, group_id, 1, {}, '', '')
                    if f == 0:
                        continue
                r_key[ph['k']]   = 1
                consider_ks[ph['k']]         = 1
                rs_value[ph['k']]  = []
                #other_values            = ph_value_d[ph['ph']]
                #c_index                 = indx_d[ph['k']]
                v_d                     = rr[ph['k']]
                key     = v_d['t']+'_'+self.get_quid(v_d['x'])
                t                       = v_d['v']
                consider_table[v_d['t']]       =  1
                consider_rows[rr['t_id']]       = 1
                clean_value             = key_value[ph['k']]
                disp_name   = ''
                tmpdd   = {}
                self.add_row_formula(ti, tmpdd, operand_rows, res, data, ph, key_map_rev, key_map_rev_t, row_op, txn, disp_name, row_ind_map, {}, '', {}, {})
                if rr[ph['k']].get('f_col', []):
                    row_formula[str(rr['t_id'])]    = rr[ph['k']]['f_col'][0]
                if d_al == 0 and ph['k'] in csv_d[ti]:
                    scale_d.setdefault(mscale, {})[ph['k']]  = 'N'
                    mscale_d.setdefault(mscale, {}).setdefault(scale, {})[ph['k']]  = 'N'
                trs_value   = v_d.get('rest_ar', [])
                if trs_value:
                    del v_d['rest_ar']
                    self.add_restated_values(ti, rr, v_d['org_d'], {}, res, trs_value, key_map_rev, key_map_rev_t, ph, txn, dphs_ar, doc_d, csv_d, {}, dphs, ijson, ph['dph'], table_type, rep_docids, 'Y')
                    del v_d['org_d']
            rr['mscale_d']  = mscale_d
            rr['scale_d']  = scale_d
            scales  = scale_d.keys()
            scales.sort()
            scales  = tuple(scales)
            if scales:
                if scales in scale_grps:
                    g_idd    = scale_grps[scales][0]
                else:
                    scale_grps[scales]  = ('S-'+str(len(scale_grps.keys())), map(lambda x:x+' - '+str(mscale_d[x].keys()), scales))
                    g_idd    = scale_grps[scales][0]
                s_ids   = {}
                for scale, pks in scale_d.items():
                    s_ids.update(pks)
                rr['s_gids']  = {g_idd:s_ids}
                for grp in tmp_sgrps.keys():
                    d_s_grps.setdefault(grp, {'v':tmp_sgrps[grp]})
                    s_ids   = {}
                    s_ids.update(d_scale_d.get(grp, {}))
                    rr['s_gids'][grp]   = s_ids
                 
            #for ph in remain_phs:
            #    for year in years.keys():
            #        if ph+year in done_ph:continue
            #        f   = 0
            #        tmpph   = ph+year
            #        pd_d    = {'k':tmpph, 'ph':tmpph}
            #        f   = self.calculate_user_ph_values(pd_d, rr, ti, t_cf_f, t_ph_f, t_cfs_f, taxo_value_grp, ph_ti, res, data, f_phs_d, key_value, ph_map_d, cfgrps, cf_all_grps, grps, all_grps, n_phs, ph_f_map, ph_gid_map, d_all_grps, taxo_id_dict, 1)
            
            rr['gids']  = grps
            rr['cfgids']  = cfgrps
            rr['d_gids']  = d_grps
        if cfstend_d:
            import pyapi_db
            cf_obj  = pyapi_db.PYAPI()
            for pair in cfstend_d['pairs']:
                t1, t2  = pair
                row1 = data[taxo_value_grp[t1]]
                row2 = data[taxo_value_grp[t2]]
                map_dict    = cf_obj.get_cash_begin_end_pos(t1, t2, row1, row2, f_phs)
                for tid1, ph_d in map_dict.items():
                    ti1 = taxo_value_grp[tid1]
                    row = data[ti1]
                    for pk, taxotup in ph_d.items():
                        tid2, pk1   = taxotup
                        ti2 = taxo_value_grp[tid2]
                        row2    =  data[ti2]
                        v_d     = row2[pk1]
                        ref_d   = {'x':v_d['x'], 'bbox':v_d['bbox'], 't':v_d['t'], 'd':v_d['d'], 'phcsv':v_d['phcsv'], 'FORM_VALUE':v_d.get('FORM_VALUE', ''), 'v':v_d['v']}
                        row[pk] = ref_d
                        row[pk]['expr_str']   = 'CF ( '+str([row2['t_l'], pk_rev_d[pk1]])+' )'
                        row[pk]['fv']   = 'Y'
                        res[0]['data'][ti1][pk] = copy.deepcopy(row[pk])
        tmp_ar  = []
        for rr in ph_grp_ar:
            #print rr
            rr  = copy.deepcopy(rr)
            if str(rr['g_id'])+'~RID-0' in d_all_grps:
                #print 'YES'
                if 'RID-0' not in d_all_grps.get(str(rr['g_id'])+'~RID-0', {}).keys():continue
                #print 
                #print rr
                #print ph_formula_d.get(('Ignore_taxo', rr['f']), {})
                if ph_formula_d.get(('Ignore_taxo', rr['f']), {}):
                    rr['ignore_taxo']   = filter(lambda x:str(x) in taxo_value_grp, ph_formula_d.get(('Ignore_taxo', rr['f']), {}).keys())
                rr['c'] = len(d_all_grps.get(str(rr['g_id'])+'~RID-0', {}).keys())
                rr['done']  = 'Y'
                rr['rid']  = d_all_grps.get(str(rr['g_id'])+'~RID-0', {}).keys()[0].split('-')[1]
                tmp_ar.append(rr)
                rr['g_id']  = str(rr['g_id'])+'~RID-0'
            if rr.get('done') != 'Y' and rr['f'] in system_formulas and 0:
                if ph_formula_d.get(('Ignore_taxo', rr['f']), {}):
                    rr['ignore_taxo']   = filter(lambda x:str(x) in taxo_value_grp, ph_formula_d.get(('Ignore_taxo', rr['f']), {}).keys())
                rr['c'] = len(d_all_grps.get(str(rr['g_id'])+'~RID-0', {}).keys())
                rr['done']  = 'Y'
                rr['rid']  = d_all_grps.get(str(rr['g_id'])+'~RID-0', {'0-0':1}).keys()[0].split('-')[1]
                tmp_ar.append(rr)
                rr['g_id']  = str(rr['g_id'])+'~RID-0'
        for rr in ph_grp_ar:
            #if ph_formula_d.get(('Ignore_taxo', rr['f']), {}):
            #    rr['ignore_taxo']   = filter(lambda x:str(x) in taxo_value_grp, ph_formula_d.get(('Ignore_taxo', rr['f']), {}).keys())
            u_keys  = d_all_grps.get(rr['g_id'], {}).keys()
            if 'RID-0' in u_keys:
                u_keys.remove('RID-0')
            if (rr['g_id'] in d_all_grps and u_keys):# or (rr['f'] in system_formulas):
                #if not u_keys:
                #    u_keys.append('RID-0')
                rr['c'] = len(u_keys)
                rr['done']  = 'Y'
                rr['rid']  = u_keys[0].split('-')[1]
                tmp_ar.append(rr)
            elif rr['g_id'] in all_grps:
                #if all_grps.get(rr['g_id'], {'-0':1}).keys()[0].split('-')[1] == '0':continue
                rr['c'] = len(all_grps.get(rr['g_id'], {}).keys())
                rr['done']  = 'N'
                tmp_ar.append(rr)
        #if not tmp_ar:
        #    res = [{'message':'done', 'grps':[]}]
        #    return res
        s_grp   = []
        sks     = scale_grps.keys()
        sks.sort(key=lambda x:scale_grps[x])
        for k in sks:
            s_grp.append({'ph':'S:'+'-'.join(k), 'op':map(lambda x:str(x), scale_grps[k][1]), 'g_id':scale_grps[k][0]})
        for k, v in d_s_grps.items():
            s_grp.append({'ph':v['v'][0], 'op':[v['v'][1]], 'g_id':k, 'r_id':k, 'done':'Y', 'rid':k.split('-')[1]})
        res[0]['s_grps']  = s_grp

        cf_grp   = []
        cfks     = cf_all_grps.keys()
        for k in cfks:
            cf_t, cf_c, r_ph, cf_ph = ph_formula_d[k]['v']
            cid =  ph_formula_d[k]['i'].keys()
            taxo_id, celid  = cid[0]
            dd  = {'done':"Y",'gid':k, 'r':{'ph':r_ph, 't_id':taxo_id, 't_l':data[taxo_value_grp[taxo_id]]['t_l'], 'k':celid, 'rks':map(lambda x:x[1], cid)}, 'cf':{'ph':cf_ph, 't_id':cf_t, 't_l':data[taxo_value_grp[cf_t]]['t_l'], 'k':cf_c}, 'rid':k.split('-')[1], 'row':ph_formula_d[k]['r'], 'col':ph_formula_d[k]['c']}
            cf_grp.append(dd)
        if not cf_grp:
            cf_grp.append({'r':{},'cf':{},'row':False, 'col':False})
        res[0]['cf_grps']  = cf_grp
        ph_orgd    = {}
        for ph in res[0]['phs']:
            ph_orgd[ph['k']]   = 1
            
            
        ph_grp_ar   = tmp_ar
        ph_grp_ar.sort(key=lambda x:(1 if x['done'] == 'Y' else 0, x['c']), reverse=True)
        res[0]['grps']  = ph_grp_ar
        new_phs = new_phs+d_new_phs
        if new_phs:
            ph_map_d    = {}
            #for k, v in f_phs_d.items():
            #    print k, v
            for ph in filter(lambda x:x['k'] in consider_ks, res[0]['phs'])+new_phs:
                #print [ph]
                if ph.get('new', '') == 'Y':
                    if not ph.get('g'):
                        ph['g'] = ph['ph']
                    ph_map_d[ph['ph']]   = ph
                    ph['n']              = ph['ph']
                else:
                    ph_map_d[ph['n']]   = ph
            #for k, v in n_phs.items():
            #    ph_map_d[k] = {'g':'NEW-'+k,'k':k, 'n':k}
            #print f_phs_d.keys()
            #print n_phs.keys()
            dphs_ar             = report_year_sort.year_sort(f_phs_d.keys()+n_phs.keys())
            dphs_ar.reverse()
            res[0]['phs']       = map(lambda x:ph_map_d[x], dphs_ar)
        else:
            res[0]['phs']       = filter(lambda x:x['k'] in consider_ks, res[0]['phs'])
        f_phs   = res[0]['phs']
        res[0]['data']      = filter(lambda x:x['t_id'] in consider_rows, res[0]['data'])
        res[0]['table_ar']  = filter(lambda x:x['t'] in consider_table, res[0]['table_ar'])
        res[0]['n_phs']     = n_phs
        if 1:
                ph_d    = {}
                for ph in res[0]['phs']:
                    ph_d[ph['k']]   = 1
                for rr in res[0]['data']:
                    ks  = rr.keys()
                    for k in ks:
                        if ':$$:' in k:
                            del rr[k]
                    for k in ['f_taxo', 'tids', 'order', 'da', 'dbf','fd']:
                        if k in rr:
                            del rr[k]
                    for k in ['th_flg']:
                        if k in rr and not rr.get(k):
                            del rr[k]
                    for pk in ph_orgd.keys():
                        if pk in rr and (pk not in ph_d):
                            del rr[pk]
                    for pk in ph_d.keys():
                        if pk in rr:
                            for k in ['rest_ar', 'org_d', 'PH_FORM', 'PH_FORM_STR', 'PH_D']:
                                if k in rr[pk]:
                                    del rr[pk][k]
        return res

    def calculate_missing_values_by_fx(self, t_ids, res, data, ph_formula_d, taxo_id_dict, table_type, group_id, row_formula, f_phs, deal_id):
        for ti, rr in enumerate(data[:]):
            if t_ids and rr['t_id'] not in t_ids:continue
            ftype   = ''
            t_row_f     = ph_formula_d.get(('SYS F', str(rr['t_id'])), ())
            op_d    = {}
            if not t_row_f:
                t_row_f     = ph_formula_d.get(('F', str(rr['t_id'])), ())
                if t_row_f:
                    ftype   = 'USER'
            else:
                ftype   = 'SYSTEM'
                op_d    = ph_formula_d.get(("OP", t_row_f[0]), {})
                
            if 0:#not t_row_f and row_formula.get(str(rr['t_id']), []):# and str(deal_id) == '214':
                tmpar   = []
                for tr in row_formula[str(rr['t_id'])]:
                    tmpar.append({'txid':str(tr['taxo_id']), 'type':'t', 't_type':table_type, 'g_id':group_id, 'op':tr['operator']})
                t_row_f = ('NEW F', tmpar)
            #if deal_id == '216':
            #    print '\n========================================='
            if t_row_f:
                print rr['t_l'], [t_row_f]
                ph_key_map  = {}
                for tr in f_phs:
                    ph_key_map[tr['k']]  = tr['n']
                val_dict, form_d, flip_sign = self.get_formula_evaluation(t_row_f[1], taxo_id_dict, map(lambda x:x['k'], f_phs), {}, None, 'Y', op_d)
                if val_dict:
                    for k, v in val_dict.get(str(rr['t_id']), {}).items():
                        if rr.get(k, {}).get('v', None) != None:
                            v_d = rr.get(k, {})
                            try:
                                clean_value = numbercleanup_obj.get_value_cleanup(v_d['v'])
                            except:
                                clean_value = '0'
                                pass
                            if clean_value == '':
                                clean_value = '0'
                            clean_value = float(clean_value)
                            n_value     = clean_value - float(v['v'].replace(',', '')) 
                            if n_value != 0.0:
                                rr[k]['c_s']    = n_value
                                res[0]['data'][ti][k]['c_s']   = n_value
                                #if deal_id == '216':
                                #    print [ph_key_map[k], clean_value, ' - ',float(v['v'].replace(',', '')), ' = ',n_value]
                            continue
                        v   = copy.deepcopy(v)
                        rr[k]   = v
                        #print '\t', k, v
                        res[0]['data'][ti][k]   = copy.deepcopy(v)
                        f_ar    = []
                        for ft in form_d[k]:#t_row_f[1]:
                            if ft['type'] == 'v':
                                dd  = {}
                                dd['clean_value']   = ft['txid']
                            else:
                                dd  = copy.deepcopy(taxo_id_dict[ft['txid']].get(k, {}))
                                dd['clean_value']   = dd.get('v', '')
                                dd['description']   = taxo_id_dict[ft['txid']]['t_l']
                                dd['ph']            = ph_key_map[k]
                                dd['taxo_id']       = ft['txid']
                            dd['operator']      = ft['op']
                            dd['k']      = k
                            f_ar.append(dd)
                        f_ar[0]['R']    = 'Y'
                        f_ar[0]['rid']    = t_row_f[0].split('-')[-1]
                        v['f_col']  = [f_ar]
                        rr['f']  = 'Y'
                for ph in f_phs[:]:
                    if ph['k'] in rr:
                        k   = ph['k']
                        v   = rr[ph['k']]
                        #if v.get('f_col', []):continue
                        #print '\tIN', k, v
                        
                        f_ar    = []
                        for ft in form_d[ph['k']]:#t_row_f[1]:
                            if ft['type'] == 'v':
                                dd  = {}
                                dd['clean_value']   = ft['txid']
                            else:
                                dd  = copy.deepcopy(taxo_id_dict.get(ft['txid'], {}).get(k, {}))
                                dd['clean_value']   = dd.get('v', '')
                                dd['description']   = taxo_id_dict.get(ft['txid'], {}).get('t_l', '')
                                dd['ph']            = ph_key_map[k]
                                dd['taxo_id']       = ft['txid']
                            dd['operator']      = ft['op']
                            dd['k']      = k
                            f_ar.append(dd)
                        f_ar[0]['R']    = 'Y'
                        f_ar[0]['rid']    = t_row_f[0].split('-')[-1]
                        v['f_col']  = [f_ar]
                        v['fv']  = 'Y'
                        rr['f']  = 'Y'

    ## 33 PH Derivation            
    def save_ph_derivation_data(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        user_name       = ijson['user']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        ph_grps         = ijson['grps']
        table_type      = ijson['table_type']
        group_id        = ijson['grpid']
        df_flg          = int(ijson['df'])
        formula_type    = ijson.get('type', '')
        del_rows        = ijson.get('rids', [])
        saved_rid       = ''
        if 'taxo_flg' in ijson:
            del ijson['taxo_flg']

        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)

        try:
            create_stmt = "CREATE TABLE IF NOT EXISTS ph_derivation (row_id INTEGER PRIMARY KEY AUTOINCREMENT, formula VARCHAR(256), table_type VARCHAR(100), group_id VARCHAR(50), ph VARCHAR(10), formula_type VARCHAR(20), formula_str VARCHAR(256), user_name VARCHAR(100));"
            cur.execute(create_stmt)
            conn.commit()
        except:pass

        def delete_formula_rows(del_rows):
            del_row = ["'"+str(e)+"'" for e in del_rows]
            del_stmt = "delete from ph_derivation where row_id in (%s)"%(', '.join(del_row))
            cur.execute(del_stmt)
            conn.commit()
            return 'done'

        if df_flg == 1:
            msg = delete_formula_rows(del_rows)
            conn.close()
            res = [{'message':'done', 'rid':saved_rid}]
            return res

        elif df_flg == 0: 
            del_rows    = []
            for info_dict in ph_grps:
                try:
                    del_rows.append(str(int(info_dict['rid'])))
                except:pass
                
            msg = delete_formula_rows(del_rows)

        data_rows = []
        for info_dict in ph_grps:
            #print info_dict
            ph              = info_dict['ph']
            formula         = info_dict.get('f', '')
            formula_info    = []
            taxo_dict       = info_dict['tids']
            for taxo_id, cell_dict in taxo_dict.iteritems():
                tmp = [taxo_id]
                for cid, f_cell in cell_dict.iteritems():
                    if not isinstance(f_cell, list):
                        f_cell  = [f_cell]
                    fcell_str = '^^'.join([cid]+f_cell)
                    tmp.append(fcell_str)
                formula_info.append('$'.join(tmp))
            data_rows.append((formula, table_type, group_id, ph, formula_type, '##'.join(formula_info), user_name))

        if df_flg != 2: 
            cur.executemany('insert into ph_derivation (formula, table_type, group_id, ph, formula_type, formula_str, user_name) values(?, ?, ?, ?, ?, ?, ?)', data_rows)
            conn.commit()
            cur.execute('select max(row_id) from ph_derivation;')
            max_row_tup = cur.fetchone()
            saved_rid = str(max_row_tup[0])
        elif df_flg == 2:
            sel_stmt = "select formula_str from ph_derivation where row_id = '%s'"%(del_rows[0])
            cur.execute(sel_stmt)
            drow = cur.fetchone()
            formula_str = ''
            if drow is not None:
                formula_str = str(drow[0]) 
            stored_fstr = ''
            if data_rows and data_rows[0]:
                stored_fstr = data_rows[0][5]
            new_fstr = '##'.join([formula_str, stored_fstr])
            upd_stmt = 'update ph_derivation set formula_str = "%s" where row_id = "%s"'%(new_fstr, del_rows[0])
            cur.execute(upd_stmt)
            conn.commit()
        conn.close()
        res = [{'message':'done', 'rid':saved_rid}]
        return res 

    def read_ph_derivation_poss(self, ijson):
        company_name, model_number, company_id, table_type = ijson['company_name'], ijson['model_number'], ijson['project_id'] + '_' + ijson['deal_id'], ijson['table_type']
        gid = str(ijson['grpid'])
        fname = os.path.join(self.model_db, company_name, model_number, 'derived_presults', table_type)
        env         = lmdb.open(fname, readonly=True)
        txn_p       = env.begin()

        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        group_id        = gid
        ftype   = 'PH'
        if group_id:
            sql         = "select row_id, ph, formula, formula_type, formula_str, group_id from ph_derivation where formula_type='%s' and table_type='%s' and group_id = '%s'"%(ftype, table_type, group_id)
        else:
            sql         = "select row_id, ph, formula, formula_type, formula_str, group_id from ph_derivation where formula_type='%s' and table_type='%s'"%(ftype, table_type)
        cur.execute(sql)
        res         = cur.fetchall()
        exists_formla_d = {}
        for rr in res:
            row_id, ph, formula, formula_type, formula_str, tgroup_id    = rr
            if not group_id and tgroup_id:continue
            exists_formla_d[('ROWID', formula)] = row_id
            if formula_str:
                for t_k in formula_str.split('##'):
                    c_sp    = t_k.split('$')
                    taxo_id = c_sp[0]
                    for cell_id in c_sp[1:]:
                        exists_formla_d.setdefault(formula, {}).setdefault(taxo_id, {})[cell_id]    = 1

        if not gid:
            gid = 'ALL'
        output = eval(txn_p.get(gid, '[]'))
        g_id    = 0
        eq_d    = {}
        feq_d   = {}
        done_grps   = {}
        for rr in output:
            rr['g_id']  = g_id
            g_id        += 1
            done_d      = {}
            eqs         = {}
            tids        = []
            for tr in rr['taxo_arr']:
                tids.append(tr['t_id'])
                eqs[tr['ex']]         = 1
                done    = 0
                for ph in tr['ph_info'].keys():
                    if ph in exists_formla_d.get(tr['ex'], {}).get(str(tr['t_id']), {}):
                        done    = 1
                if done == 1:
                    #tr['done']  = 'Y'
                    done_d[tr['t_id']]  = 1
            if len(done_d.keys()) == len(rr['taxo_arr']):
                done_grps[rr['g_id']]   = 1
                rr['done']  = 'Y'
            else:
                for tr in rr['taxo_arr']:
                    if tr['t_id'] in done_d:
                        tr['prtly_done']    = 'Y'
            feq_d.update(done_d)
            eqs = eqs.keys()
            eqs.sort()
            tids.sort()
            eq_d.setdefault(tuple(eqs), {}).setdefault(tuple(tids), {})[rr['g_id']]    = 1
        grps    = eq_d.keys()
        grps.sort(key=lambda x:len(eq_d[x].keys()), reverse=True)
        grp_ar  = []
        for grp in grps:
            gids    = {}
            done_all    = 'Y'
            for ttup, tgids in eq_d[grp].items():
                for gid in tgids.keys():
                    gids[gid]   = 1
                    if gid not in done_grps:
                        done_all    = 'N'
            grp_ar.append({'n':'( '+str(len(eq_d[grp].keys()))+') '+' ~ '.join(grp), 'gids':gids, 'done':done_all, 'l':len(eq_d[grp].keys())})
        grp_ar.sort(key=lambda x:({'Y':1}.get(x['done'], 0), 999999 - x['l']))
        
        return [{'message':'done', 'data':output, 'g_ar':grp_ar, 'feq_d':feq_d}]

    ## PH Derivation Save            
    def save_ph_derivation_from_POSS(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        user_name       = ijson['user']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        table_type      = ijson['table_type']
        group_id        = ijson.get('grpid', '')
        formula_type    = 'PH'
        if 'taxo_flg' in ijson:
            del ijson['taxo_flg']

        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)

        try:
            create_stmt = "CREATE TABLE IF NOT EXISTS ph_derivation (row_id INTEGER PRIMARY KEY AUTOINCREMENT, formula VARCHAR(256), table_type VARCHAR(100), group_id VARCHAR(50), ph VARCHAR(10), formula_type VARCHAR(20), formula_str VARCHAR(256), user_name VARCHAR(100));"
            cur.execute(create_stmt)
            conn.commit()
        except:pass
        if group_id:
            sql         = "select row_id, ph, formula, formula_type, formula_str, group_id from ph_derivation where formula_type='PH' and table_type='%s' and group_id = '%s'"%(table_type, group_id)
        else:
            sql         = "select row_id, ph, formula, formula_type, formula_str, group_id from ph_derivation where formula_type='PH' and table_type='%s'"%(table_type)
        try:
            cur.execute(sql)
            res         = cur.fetchall()
        except:
            res  = []
        exists_formla_d = {}
        for rr in res:
            row_id, ph, formula, formula_type, formula_str, tgroup_id    = rr
            if not group_id and tgroup_id:continue
            exists_formla_d[('ROWID', formula)] = row_id
            if formula_str:
                for t_k in formula_str.split('##'):
                    c_sp    = t_k.split('$')
                    taxo_id = c_sp[0]
                    for cell_id in c_sp[1:]:
                        cell_id = cell_id.split('^^')[0]
                        exists_formla_d.setdefault(formula, {}).setdefault(taxo_id, {})[cell_id]    = 1
                    


        #"taxo_arr":[{"t_id":139023,"ph_info":{"H22009":11121},"ex":"H2=FY","$$hashKey":"object:4022"},{"t_id":139024,"ph_info":{"H22009":-86330},"ex":"H2=FY","$$hashKey":"object:4023"},{"t_id":139025,"ph_info":{"H22009":-7115},"ex":"H2=FY","$$hashKey":"object:4024"},{"t_id":139832,"ph_info":{"H22009":0},"ex":"H2=FY-H1","$$hashKey":"object:4025"},{"t_id":139026,"ph_info":{"H22009":1862},"ex":"H2=FY","$$hashKey":"object:4026"},{"t_id":139027,"ph_info":{"H22009":-80462},"ex":"H2=FY","$$hashKey":"object:4027"}]
        form_d  = {}
        for rr in ijson['taxo_arr']:
            taxo_id     = rr['t_id']
            cellinfo    = rr['ph_info'].keys()
            for cinfo in cellinfo:
                form_d.setdefault(rr['ph_info'][cinfo][0], {}).setdefault(str(taxo_id), {})[cinfo] = 1
        i_rows  = []
        u_rows  = []
        final_d = {}
        if ijson.get('del') == 'Y': 
            for formula, tinfo in exists_formla_d.items():
                if isinstance(formula, str) or isinstance(formula, unicode):
                    formula_info    = []
                    for tid, cinfo in tinfo.items():
                        if tid in form_d.get(formula, {}):continue
                        tmpd    = {}
                        tmpd.update(cinfo)
                        formula_info.append(str(tid)+'$'+'$'.join(tmpd.keys()))
                        final_d.setdefault(tid, {}).setdefault(formula, {})
                        for pk in tmpd.keys():
                            final_d[tid][formula][pk]   = 1
                    u_rows.append((formula, table_type, group_id, '', formula_type, '##'.join(formula_info), user_name, exists_formla_d[('ROWID', formula)]))
            
        else:
            for formula, tinfo in form_d.items():
                formula_info    = []
                for tid, cinfo in tinfo.items():
                    tmpd    = exists_formla_d.get(formula, {}).get(tid, {})
                    tmpd.update(cinfo)
                    formula_info.append(str(tid)+'$'+'$'.join(tmpd.keys()))
                    final_d.setdefault(tid, {}).setdefault(formula, {})
                    for pk in tmpd.keys():
                        final_d[tid][formula][pk]   = 1
                for tid, cinfo in exists_formla_d.get(formula, {}).items():
                    final_d.setdefault(tid, {}).setdefault(formula, {})
                    for pk in cinfo.keys():
                        final_d[tid][formula][pk]   = 1
                    formula_info.append(tid+'$'+'$'.join(cinfo.keys()))
                if ('ROWID', formula) not in exists_formla_d:
                    i_rows.append((formula, table_type, group_id, '', formula_type, '##'.join(formula_info), user_name))
                else:
                    u_rows.append((formula, table_type, group_id, '', formula_type, '##'.join(formula_info), user_name, exists_formla_d[('ROWID', formula)]))
        #print (ijson.get('del'), len(i_rows), len(u_rows))
        if i_rows:
            cur.executemany('insert into ph_derivation (formula, table_type, group_id, ph, formula_type, formula_str, user_name) values(?, ?, ?, ?, ?, ?, ?)',i_rows)
        if u_rows:
            cur.executemany('update ph_derivation  set formula=?, table_type=?, group_id=?, ph=?, formula_type=?, formula_str=?, user_name=? where row_id=?', u_rows)
        conn.commit()
        conn.close()
        res = [{'message':'done', "data":final_d}]
        return res 

    def create_group_id(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        sql             = "select table_type, vgh_id, group_txt from vgh_group_map"
        cur.execute(sql)
        res             = cur.fetchall()
        dd              = {}
        for rr in res:
            table_type, vgh_id, group_text  = rr
            try:
                group_text  = int(group_text)
            except:
                dd.setdefault((table_type, group_text), {})[vgh_id]              = 1

        sql = "CREATE TABLE IF NOT EXISTS vgh_group_info(row_id INTEGER PRIMARY KEY AUTOINCREMENT, group_id VARCHAR(20), table_type VARCHAR(100), group_txt VARCHAR(20), user_name VARCHAR(100), datetime TEXT);"
        cur.execute(sql)
        sql = "select group_id, table_type, group_txt from vgh_group_info"
        cur.execute(sql)
        res = cur.fetchall()
        grp_info    = {}
        for rr in res:
            group_id, table_type, group_txt = rr
            group_txt = group_txt.replace('\t', '')
            grp_info[(table_type, group_txt)]   = group_id
        dtime   = str(datetime.datetime.now()).split('.')[0]
        i_ar    = []
        with conn:
            sql = "select seq from sqlite_sequence WHERE name = 'vgh_group_info'"
            cur.execute(sql)
            r       = cur.fetchone()
            if r:
                g_id    = int(r[0])+2
            else:
                g_id    = 1
            for grptup, vids in dd.items():
                if grptup in grp_info:continue
                i_ar.append((g_id, )+grptup+('SYSTEM', dtime))
                g_id    += 1
        cur.executemany('insert into vgh_group_info(group_id, table_type, group_txt, user_name, datetime) values(?,?,?,?,?)', i_ar)
        u_ar    = []
        for k, v in grp_info.items():
            u_ar.append((v, )+k)
        cur.executemany("update vgh_group_map set group_txt=? where table_type=? and group_txt=?", u_ar)
        sql         = 'CREATE TABLE IF NOT EXISTS final_output(row_id INTEGER PRIMARY KEY AUTOINCREMENT, gen_id TEXT,table_type TEXT, gen_type TEXT, taxo_ids TEXT, group_text, user_name TEXT, datetime TEXT, error_txt TEXT)'
        cur.execute(sql)
        cur.executemany("update final_output set group_text=? where table_type=? and group_text=?", u_ar)
        conn.commit()
        conn.close()
        res = [{'message':'done'}]
        return res
        

    def update_order_frm_backup(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number)
        #db_file         = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        #conn, cur       = conn_obj.sqlite_connection(db_file)
        display_data    = {}
        path            = '/var/www/html/muthu/FINAL_OUTPUT_%s/'%(deal_id)
        env1        = lmdb.open(path, readonly=True)
        with env1.begin() as txn1:
            for k, v in txn1.cursor():
                if 'DISPLAYOUTPUT_' in k:
                    display_data[k.split('DISPLAYOUTPUT_')[1]]  = eval(v)
        #ijson['table_type'] = 'BS'
        #ijson['update_db'] = 'Y'
        #self.re_order_seq(ijson)
        #sys.exit()
        new_order   = []
        for table_type in rev_m_tables.keys():
            if table_type in ['IS', 'COS', 'RNTL', 'COGS']:continue
            #if table_type != 'BS':continue
            if table_type not in self.order_d:continue
            table_id    = self.order_d[table_type]
            txtpath     = '/root/Muthu/%s/'%(company_name)
            fname       = txtpath+'/'+str(table_id)+'-P.txt'
            if str(table_id)+'-P' in display_data:#os.path.exists(fname):
                ijson['table_type'] = table_type
                res = self.create_seq_across(ijson)
                c_data      = res[0]['data']
                c_od        = {}
                for rr in c_data:
                    c_od[rr['t_id']]    = rr
                old_data    = display_data[str(table_id)+'-P'][0]['data']
                o_od        = {}
                print '\n======================================================'
                print 'table_type ', table_type
                for rr in old_data:
                    tid = rr['t_id']
                    if tid not in c_od:
                        print 'Missing ', tid, [ rr['t_l']]
                        continue
                    c_order = c_od[tid]['order']
                    if rr['t_l'] != c_od[tid]['t_l']:
                        print 'Error', [table_type, rr['t_l'], c_od[tid]['t_l']]
                        sys.exit()
                    if c_order == rr['order']:continue
                    new_order.append((rr['order'], tid, table_type))
                    print [tid, c_order, rr['order'], rr['t_l'], c_od[tid]['t_l']]
                    #o_od[rr['t_id']]    = rr['order']
                continue
                data    = eval(open(fname, 'r').read())
                for k, v in data.items():
                    if k == 'key_map':continue
                    ks  = list(sets.Set(map(lambda x:x[0], v['data'].keys())))
                    ks.sort()
                    for r in ks:
                        print r, v['data'][(r, 0)][1]
                    
            
        
        #db_file         = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        #conn, cur       = conn_obj.sqlite_connection(db_file)

    def read_table_info(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        m_tables, rev_m_tables, doc_m_d, table_type_m = self.get_main_table_info(company_name, model_number, ijson['table_types'])
        res = [{'message':'done', 'table_ids':m_tables.keys()}]
        return res

    def run_ph_csv(self, ijson):
        import index_all_table
        i_obj = index_all_table.Index()
        if 'g_cinfo' in ijson:
            del ijson['g_cinfo']
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        #print company_id
        ijson.update({"eq":"N","O":"Y"})
        if not ijson.get('table_ids', []) and ijson.get('table_str') and ijson.get('table_str') != 'ALL':
            ijson['table_ids']  = ijson['table_str'].split('#')
            
        if not ijson.get('table_ids', []):
            m_tables, rev_m_tables, doc_m_d, table_type_m = self.get_main_table_info(company_name, model_number, ijson.get('table_types', []))
            ijson['table_ids']  = m_tables.keys()
        print 'tables ', len(ijson['table_ids'])
        if ijson.get('re_run') == 'Y':# or (ijson.get('only_index') == 'Y' and fli_info):
            i_obj.index_table_norm_info(company_id, list(sets.Set(ijson['table_ids'])))
        if ijson.get('only_index') == 'Y':
            res = [{'message':'done'}]
            return res
        ijson['re_run'] = 'N'    
        if ijson.get('print', '') != 'N':
            cmd = "cd /root/Muthu;python table_tree_group.py 7 %s_%s '%s' ;cd - "%(ijson['project_id'], deal_id, json.dumps(ijson))
        else:
            cmd = "cd /root/Muthu &>/dev/null;python table_tree_group.py 7 %s_%s '%s' >/dev/null;cd - &>/dev/null"%(ijson['project_id'], deal_id, json.dumps(ijson))
        print cmd
        os.system(cmd)
        if ijson.get('PH', '') == 'Y':
            self.update_ph(ijson)
        res = [{'message':'done'}]
        return res

    def read_triplet_data(self,ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        lmdb_path2      =  "/mnt/eMB_db/%s/%s/table_phcsv_data"%(company_name, model_number)
        env1             = lmdb.open(lmdb_path2, readonly=True)
        txn_trip             = env1.begin()
        #print lmdb_path2

        lmdb_path2      =  "/mnt/eMB_db/%s/%s/default_table_phcsv_data"%(company_name, model_number)
        #print lmdb_path2
        env1             = lmdb.open(lmdb_path2, readonly=True)
        txn_trip_default             = env1.begin()
        triplet =  self.show_triplet(ijson['table_id'], txn_trip, ijson['x'], txn_trip_default)
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()
        tk   = self.get_quid(ijson['table_id']+'_'+ijson['x'])
        print 'C_ID ', [txn_m.get("XMLID_MAP_"+tk)]

    def update_vgh_text(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        m_tables, rev_m_tables, doc_m_d, table_type_m = self.get_main_table_info(company_name, model_number)
        db_file         = self.get_db_path(ijson)
        print db_file
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_trip         = env.begin()

        lmdb_path2      =  "/mnt/eMB_db/%s/%s/default_table_phcsv_data"%(company_name, model_number)
        #print lmdb_path2
        env1             = lmdb.open(lmdb_path2, readonly=True)
        txn_trip_default             = env1.begin()
        if ijson.get('DF', '') == 'Y':
            txn_trip    = txn_trip_default

        conn, cur       = conn_obj.sqlite_connection(db_file)
        vgh_text_d      = {}
        sql             = "select table_type, vgh, vgh_id from vgh_info"
        cur.execute(sql)
        res = cur.fetchall()
        for rr in res:
            table_type, vgh, vgh_id = rr
            vgh_text_d.setdefault(table_type, {})[vgh.lower()] = (vgh_id, vgh)

        table_ids       = map(lambda x:str(x), ijson['table_ids'])
        sql             = "select row_id, table_type, table_id, xml_id from mt_data_builder where isvisible='Y' and table_id in (%s)"%(','.join(table_ids))
        cur.execute(sql)
        res             = cur.fetchall()
        u_ar            = []
        new_vgh_text_d  = {}
        for rr in res:
            row_id, table_type, table_id, xml_id    = rr
            if xml_id:
                tk   = self.get_quid(table_id+'_'+xml_id)
                c_id    = txn_m.get('XMLID_MAP_'+tk)
                if not c_id:continue
                triplet =  self.read_triplet(table_id, txn_trip, xml_id, txn_trip_default)
                vgh_txt = triplet.get('VGH', {}).keys()
                if not vgh_txt:
                    vgh_txt = [[txn_m.get('COLTEXT_'+table_id+'_'+str(c_id.split('_')[2]))]]
                    print 'NO Triplet ', [table_id, xml_id]
                    sys.exit()
                vgh_txt = ' '.join(' '.join(vgh_txt[0]).strip().split())
                if vgh_txt.lower() in vgh_text_d.get(table_type, {}):
                    vgh_id  = vgh_text_d[table_type][vgh_txt.lower()][0]
                else:
                    vgh_id  = len(vgh_text_d.get(table_type, {}).keys())+1
                    vgh_text_d.setdefault(table_type, {})[vgh_txt.lower()] = (vgh_id, vgh_txt)
                    new_vgh_text_d.setdefault(table_type, {})[vgh_txt.lower()] = (vgh_id, vgh_txt)
                u_ar.append((vgh_id, row_id))

        vgh_groups  = []
        for k, v in new_vgh_text_d.items():
            for t, vtup in v.items():
                vgh_groups.append((k, vtup[1], vtup[0]))
        print vgh_groups
        print 'Total ', len(u_ar)
        cur.executemany("update mt_data_builder set vgh_text=? where row_id=?", u_ar)
        cur.executemany("insert into vgh_info(table_type, vgh, vgh_id)values(?,?, ?)", vgh_groups)
        conn.commit()
        conn.close()
            
    def get_doc_map_info(self, project_id, deal_id):
        path    = "%s/%s/%s/1_1/21/sdata/doc_map.txt"%(self.doc_path, project_id, deal_id)
        if os.path.exists(path):
            fin = open(path, 'r')
            lines   = fin.readlines()
            fin.close()
        else:
            lines   = []
        doc_ph_info = DD(set)
        for line in lines[1:]:
            line    = line.split('\t')
            if len(line) < 8:continue
            line    = map(lambda x:str(x).strip(), line)
            ph      = line[3]+line[7]
            doc_id  = line[0]
            doc_type= line[2]
            doc_ph_info[doc_id] = '_'.join([ph, doc_type])
        return doc_ph_info

    def get_vgh_txt_info(self, conn, cur, table_type):
        sql         = "select vgh, vgh_id from vgh_info where table_type='%s'"%(table_type)
        cur.execute(sql)
        res         = cur.fetchall()
        vgh_d       = {}
        for rr in res:
            vgh_text, vgh_id    = rr
            #try:
            #    vgh_text    = binascii.a2b_hex(vgh_text)
            #except:pass
            vgh_d[vgh_id]       = self.convert_html_entity(vgh_text)
        return vgh_d

    def ph_csv_doc_vgh_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        table_type      = ijson['table_type']
        taxo_ids        = map(lambda x:str(x), ijson['t_ids'])
        stype           = ijson['type']
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)

        lmdb_path       =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env             = lmdb.open(lmdb_path, readonly=True)
        txn_m           = env.begin()

        if stype == 'doc':
            # DOC PH and DOC TYPE Info
            doc_map_info    = self.get_doc_map_info(project_id, deal_id)

        elif stype == 'vgh':
            # VGH TEXT INFO
            vgh_txt_info = self.get_vgh_txt_info(conn, cur, table_type)

        sql             = "select row_id, taxo_id, doc_id, table_id, xml_id, vgh_text from mt_data_builder where isvisible='Y' and table_type = '%s' and taxo_id in (%s)"%(table_type, ','.join(taxo_ids))
        cur.execute(sql)
        res             = cur.fetchall()
        conn.close()
        
        doc_taxoid_dict = DD(lambda : DD(dict))  # DD represent Default Dict 
        doc_vgh_info_dict = DD(set)
        for rr in res:
            rr = map(str, rr)
            row_id, taxo_id, doc_id, table_id, xml_id, vgh_text    = rr

            tk   = self.get_quid(table_id+'_'+xml_id)
            c_id    = txn_m.get('XMLID_MAP_'+tk)
            tid, row, col = c_id.split('_')

            k = '%s-%s-'%(tid, col)
            if stype == 'doc':
                unique_id = doc_id
            elif stype == 'vgh':
                unique_id = vgh_text
            doc_taxoid_dict[unique_id][taxo_id][k] = row_id
            doc_vgh_info_dict[unique_id] = 1

        res_data = []
        for unique_id in doc_vgh_info_dict.keys():
            tmp = {'d':unique_id, 'taxos':doc_taxoid_dict.get(unique_id, {})}
            if stype == 'doc':
                tmp['disp'] = doc_map_info.get(unique_id, '')
            elif stype == 'vgh':
                tmp['disp'] = vgh_txt_info.get(unique_id, '')
            res_data.append(tmp)
        res_data.sort(key=lambda x:x['d'])
        return [{'message':'done', 'data':res_data}]

    def save_ph_csv_info(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        table_type      = ijson['table_type']
        row_ids         = map(lambda x:str(x), ijson['r_ids'])
        data_dict       = ijson['data']
        if 0:

            db_file         = self.get_db_path(ijson)
            conn, cur       = conn_obj.sqlite_connection(db_file)

            try:
                sql     = "CREATE TABLE IF NOT EXISTS mt_data_builder(row_id INTEGER PRIMARY KEY AUTOINCREMENT, taxo_id INTEGER, prev_id INTEGER,order_id INTEGER, table_type, taxonomy TEXT, user_taxonomy TEXT, missing_taxo varchar(1), table_id TEXT, c_id TEXT, gcom TEXT, ngcom TEXT, ph TEXT, ph_label TEXT, user_name TEXT, datetime TEXT, isvisible varchar(1), m_rows TEXT, vgh_text TEXT, vgh_group TEXT, doc_id INTEGER, xml_id TEXT, period VARCHAR(50), period_type VARCHAR(50), scale VARCHAR(50), currency VARCHAR(50), value_type VARCHAR(50))"
                cur.execute(sql)
                cur.commit()
            except:pass

            col_info_stmt   = 'pragma table_info(mt_data_builder);'
            cur.execute(col_info_stmt)
            all_cols        = cur.fetchall()
            cur_coldef      = set(map(lambda x:str(x[1]), all_cols))
            coldef          = ["row_id", "taxo_id", "prev_id", "order_id", "table_type", "taxonomy", "user_taxonomy", "missing_taxo", "table_id", "c_id", 
                               "gcom", "ngcom", "ph", "ph_label", "user_name", "datetime", "isvisible", "m_rows", "vgh_text", "vgh_group", "doc_id", 
                               "xml_id", "period", "period_type", "scale", "currency", "value_type"
                              ]
            exists_coldef   = set(coldef)
            new_cols        = list(exists_coldef.difference(cur_coldef))

            col_list = []
            for new_col in coldef:
                if new_col not in new_cols:continue
                col_list.append(' '.join([new_col, 'VARCHAR(50)']))

            for col in col_list:
                alter_stmt = 'alter table mt_data_builder add column %s;'%(col)
                cur.execute(alter_stmt)
            conn.commit()

        # {pt: "Q3", p: "2043", vt: "BNUM", c: "NOK", s: "Ton"}
        period  = data_dict.get('p', None)
        p_type  = data_dict.get('pt', None)
        scale   = data_dict.get('s', None)
        curr    = data_dict.get('c', None)
        value   = data_dict.get('vt', None)

        if ijson["stype"] == 'ROW' or ijson.get('ptype_info') == 'Y':
            td  = {}
            if ijson.get('ptype_info') == 'Y':
                lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
                env         = lmdb.open(lmdb_path1, readonly=True)
                txn_m       = env.begin()
                for t_c in ijson['pk_D'].keys():
                    table_id, col   = t_c.split('-')
                    c_ids           = txn_m.get('GV_'+str(table_id))
                    if c_ids:
                        for c_id in c_ids.split('#'):
                            if c_id.split('_')[2] != col:continue
                            xml = txn_m.get('XMLID_'+c_id)
                            if xml:
                                td.setdefault(table_id, []).append(xml)
                pass
            else:
                for txml in ijson["r_ids"]:
                    table_id, xml   = txml.split(':$$:')
                    td.setdefault(table_id, []).append(xml)
            sh_list = data_dict.keys()
            save_phcsv_ijson = {"row":data_dict,"rc":[],"xml_dict":td,"sh_list":sh_list,"pid":"","deal_id":deal_id,"company_name":company_name,"project_id":project_id,"model_number":model_number}
            
            res_json = self.save_phcsv_rest_data(save_phcsv_ijson)
            return [{'message':'done'}]
        
        data_rows = []
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        sql             = "select row_id, table_id, xml_id from mt_data_builder where row_id in(%s)"%(', '.join(row_ids))
        cur.execute(sql)
        res = cur.fetchall()
        conn.close()
        r_d = {}
        for rr in res:
            row_id, table_id, xml_id    = rr
            r_d[str(row_id)]    = (table_id, xml_id)
        
        path    =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
        env1    = lmdb.open(path)
        f_d     = {}
        if period != None:
            f_d[1]  = period
        if p_type != None:
            f_d[0]  = p_type
        if scale != None:
            f_d[3]  = scale
        if curr != None:
            f_d[2]  = curr
        if value != None:
            f_d[4]  = value
        w_d = {}    
        with env1.begin(write=False) as txn1:
            for rid in row_ids:
                table_id, xml_id    = r_d[rid]
                key = table_id+'_'+self.get_quid(xml_id)
                ph_map  = txn.get('PH_MAP_'+str(key))
                if ph_map:
                    ph_map    = ph_map.split('^')
                else:
                    ph_map   = ['', '', '', '', '']
                for ind, v in f_d.items():
                    ph_map[ind] = v
                w_d['PH_MAP_'+str(key)] = '^'.join(ph_map)
                #data_rows.append((period, p_type, scale, curr, value, rid))
        env1        = lmdb.open(path, map_size=2**39)
        with env1.begin(write=True) as txn1:
            for k, v in w_d.items():
                txn1.put(str(k), str(v))
        #cur.executemany('update mt_data_builder set period=?, period_type=?, scale=?, currency=?, value_type=? where row_id = ?', data_rows)
        #conn.commit()
        #conn.close()
        return [{'message':'done'}]

    ## 43 PH Derivation            
    def copy_ph_derivation_groupid_data(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        user_name       = ijson['user']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        table_type      = ijson['table_type']
        group_ids       = ijson['grpids']
        row_id          = ijson.get('rid', '')

        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)

        sel_stmt = "select formula, table_type, group_id, ph, formula_type, formula_str, user_name from ph_derivation where row_id = '%s'"%(row_id)
        cur.execute(sel_stmt)
        row = cur.fetchone()

        data_li = []
        if row and row[0]:
            data_tup = row
            data_li = list(map(str, data_tup))
            
        data_rows = []
        if data_li:
            for grpid in group_ids:
                data_li[2] = grpid
                data_rows.append(tuple(data_li))
            cur.executemany('insert into ph_derivation (formula, table_type, group_id, ph, formula_type, formula_str, user_name) values(?, ?, ?, ?, ?, ?, ?)', data_rows)
            conn.commit()
        conn.close()
        res = [{'message':'done'}]
        return res 

    def get_map_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        ID      = ijson['uid']
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)

        sql = "select group_id, table_type, group_txt from vgh_group_info"
        try:
            cur.execute(sql)
            res = cur.fetchall()
        except:
            res = []
        grp_info    = {}
        for rr in res:
            group_id, table_type, group_txt = rr
            group_txt = group_txt.replace('\t', '')
            grp_info[(table_type, str(group_id))]   = group_txt

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        sql         = "select row_id, table_type, gen_type, taxo_ids, group_text, datetime from final_output where gen_id=%s"%(ID)
        cur.execute(sql)
        res = cur.fetchall()
        final_ar    = []
        sql         = "select table_type, vgh_id, group_txt, table_str from vgh_group_map"
        cur.execute(sql)
        tres        = cur.fetchall()
        vgh_grp     = {}
        for rr in tres:
            table_type, vgh_id, group_txt, table_str   = rr
            table_type  = str(table_type)
            group_txt   = str(group_txt)
            tc_d    = {}
            if table_str:
                for tcid in table_str.split('#'):
                    tc_d[tcid]  = 1
            vgh_grp.setdefault((table_type, group_txt), {})[vgh_id] = tc_d
        grp_ar_d    = {}
        for rr in res:
            row_id, table_type, gen_type, taxo_ids, group_text, tdatetime  = rr
            grp_ar_d.setdefault(table_type, {})[group_text] = 1
        g_d = {}
        for table_type in grp_ar_d.keys():
            ijson['table_type'] = table_type
            g_ar    = self.create_group_ar(ijson, txn_m)
            for rr in g_ar:
                g_d[(table_type, rr['grpid'])] =  rr
        data = []
        for rr in res:
            row_id, table_type, gen_type, taxo_ids, group_text, tdatetime  = rr
            table_type  = str(table_type)
            group_text   = str(group_text)
            ijson_c   = copy.deepcopy(ijson)
            ijson_c['table_type']   = table_type
            ijson_c['type']         = gen_type
            ijson_c['row_id']         = row_id
            ijson_c['print']         = 'Y'
            if taxo_ids:
                ijson_c['t_ids']         = map(lambda x:int(x), taxo_ids.split(','))
            if gen_type == 'group' and  group_text:
                #print (table_type, group_text), (table_type, group_text) not in vgh_grp
                if (table_type, group_text) not in g_d:continue
                ijson_c['data']     = [g_d[(table_type, group_text)]['n']]
                ijson_c['grpid']    = group_text
                ijson_c['vids']     = g_d[(table_type, group_text)]['vids']
            res = self.create_seq_across(ijson_c)
            data    = res[0]['data']
        return data

    # Generate page_coord.txt
    def store_bobox_data(self, ijson):
        company_name        = ijson['company_name']
        project_id          = ijson['project_id']
        deal_id             = ijson['deal_id']
        company_id          = str(project_id) + '_'+str(deal_id)
        #self.output_path    = '/var/www/html/fundamentals_intf/output/'
        lmdb_folder         = os.path.join(self.output_path, company_id, 'doc_page_adj_cords')
        doc_page_dict       = {}

        if os.path.exists(lmdb_folder):
            env = lmdb.open(lmdb_folder, readonly=True)
            with env.begin() as txn:
                cursor = txn.cursor()
                for doc_id, res_str in cursor:
                    if res_str:
                        page_dict = ast.literal_eval(res_str)
                        doc_page_dict[doc_id] = page_dict
        #print doc_page_dict
        for f_dname in ['DB_Model', 'DB_Model_Reported']:
            fpath = os.path.join('/var/www/html/', f_dname, company_name)
            if not os.path.exists(fpath):
                os.system('mkdir -p %s'%fpath)

            i_project_name  = ''.join(ijson['project_name'].split())
            fname = os.path.join(fpath, 'page_coord.txt')
            with open(fname, 'w') as f:
                f.write(str(doc_page_dict))
            tfpath = os.path.join(fpath, i_project_name, 'Norm_Scale')
            if not os.path.exists(tfpath):
                os.system('mkdir -p %s'%tfpath)
            fname = os.path.join(fpath, i_project_name, 'Norm_Scale', 'page_coord.txt')
            with open(fname, 'w') as f:
                f.write(str(doc_page_dict))
            print fname 
        return 'done'

    ## 44 Delete/Update formula_str PH Derivation
    def update_ph_derivation_formula_str_data(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        user_name       = ijson['user']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        row_id          = ijson.get('rid', '')
        taxok_map       = ijson.get('tids', {})
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)

        sel_stmt = "select formula_str from ph_derivation where row_id = '%s'"%(row_id)
        cur.execute(sel_stmt)
        drow = cur.fetchone()
        formula_str = ''
        if drow is not None:
            formula_str = str(drow[0])

        #print formula_str
        if formula_str:
            new_formula_li = []
            all_taxos = formula_str.split('##')
            for taxo_str in all_taxos:
                cell_info_li = map(lambda x:x.split('^^')[0], taxo_str.split('$'))
                taxoid = cell_info_li[0]
                if taxoid in taxok_map:
                    k_li = []
                    all_del_k = taxok_map[taxoid]
                    for ek in cell_info_li[1:]:
                        if ek in all_del_k:
                            continue
                        k_li.append(ek)
                    if not k_li:
                        continue
                    else:
                        new_taxo_str = '$'.join([taxoid]+k_li)
                        new_formula_li.append(new_taxo_str)
                else:
                    new_formula_li.append(taxo_str)

            new_fstr = '##'.join(new_formula_li)
            #print new_fstr
            upd_stmt = 'update ph_derivation set formula_str = "%s" where row_id = "%s"'%(new_fstr, row_id)
            #print upd_stmt
            cur.execute(upd_stmt)
            conn.commit()
        conn.close()
        res = [{'message':'done'}]
        return res

    ## TAXO MAPPING STORAGE
    def store_taxonomy_mapping(self):
        fpath = '/tmp/data_builder_tree.txt'
        data_rows = []
        with open(fpath, 'r') as f:
            lines = f.readlines()
            for line in lines[1:]:
                level, desc = line.split('\t')
                data_rows.append((str(level), unicode(desc).strip(), '0'))
        db_file         = '/mnt/eMB_db/node_mapping.db'
        conn, cur       = conn_obj.sqlite_connection(db_file)

        try:
            drop_stmt = 'drop table taxonomy_grouping;'
            cur.execute(drop_stmt)
        except:pass

        try:
            create_stmt = "CREATE TABLE IF NOT EXISTS taxonomy_grouping (row_id INTEGER PRIMARY KEY AUTOINCREMENT, level VARCHAR(256), description TEXT, review_flg VARCHAR(10));"
            cur.execute(create_stmt)
            conn.commit()
        except:pass
       
        cur.executemany('insert into taxonomy_grouping (level, description, review_flg) values(?, ?, ?)', data_rows)
        conn.commit()
        conn.close()
        return 'done'

    ## 45 Tree View for Taxonomy Mapping
    def taxonomy_mapping_tree_view_data(self, ijson):
        db_file         = '/mnt/eMB_db/node_mapping.db'
        conn, cur       = conn_obj.sqlite_connection(db_file)
        table_name      = 'taxonomy_grouping'

        sel_stmt        = 'select * from taxonomy_grouping'
        cur.execute(sel_stmt)
        rows = cur.fetchall()
        conn.close()
        fmap, tmap      = {}, {} 
        for row in rows:
            row = map(str, row)
            rid, level, desc, rflg = row
            tmap[level]  = [rid, desc, rflg]

            lli = level.split('.')
            nkey = '.'.join(lli[:-1])
            if not nkey:
                fmap.setdefault('root', []).append(level)
                continue
            fmap.setdefault(nkey, []).append(level)
        tmap['root'] = ['999', 'Root', '0']
        #print fmap
        #print tmap
        return [{'fmap':fmap, 'tmap':tmap, 'f1':["root"]}]

    def split_row_multi_all(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        sql             = "select row_id, taxo_id, order_id, table_id, xml_id, table_type from mt_data_builder where isvisible='Y'"
        cur.execute(sql)
        res             = cur.fetchall()
        order_d         = {}
        taxo_o_d        = {}
        new_row_id      = {}
        table_taxo_d    = {}
        for rr in res:
            row_id, taxo_id, order_id, table_id, xml_id, table_type         = rr
            taxo_o_d[taxo_id]                           = order_id
            tk   = self.get_quid(table_id+'_'+xml_id)
            c_id        = txn_m.get('XMLID_MAP_'+tk)
            if not c_id:continue
            #if c_id.split('_')[1]   != '5':continue
            gv_xml  = c_id
            new_row_id[str(row_id)]      = 1
            table_taxo_d.setdefault(taxo_id, {}).setdefault((table_type, table_id), {}).setdefault(int(c_id.split('_')[1]), {})[str(row_id)]   = 1
        with conn:
            #sql = "select max(taxo_id) from kpi_input"
            sql = "select seq from sqlite_sequence WHERE name = 'mt_data_builder'"
            cur.execute(sql)
            r       = cur.fetchone()
            g_id    = int(r[0])+1
            sql     = "select max(taxo_id) from mt_data_builder" 
            cur.execute(sql)
            r       = cur.fetchone()
            tg_id    = int(r[0])+1
            g_id    = max(g_id, tg_id)
            for taxo_id, table_d in table_taxo_d.items():
                for table_id, row_d in table_d.items():
                    if len(row_d.keys()) <= 1:continue
                    rows    = row_d.keys()
                    rows.sort() 
                    for r in rows[1:]:
                        sql     = "insert into mt_data_builder(taxo_id)values(-1)"
                        #cur.execute(sql)
                        #conn.commit()
                        sql     = "update mt_data_builder set taxo_id=%s where row_id in (%s)"%(g_id, ', '.join(row_d[r].keys()))
                        print [table_id, sql]
                        #cur.execute(sql)
                        #conn.commit()
                        g_id    += 1


    '''
        ALTER TABLE COLUMNS
        coldef is list of all columns names [new col names also should be there]
        eg. ['row_id', 'taxo_id', ......]
    '''
    def alter_table_coldef(self, conn, cur, table_name, coldef):
        col_info_stmt   = 'pragma table_info(%s);'%table_name
        cur.execute(col_info_stmt)
        all_cols        = cur.fetchall()
        cur_coldef      = set(map(lambda x:str(x[1]), all_cols))
        exists_coldef   = set(coldef)
        new_cols        = list(exists_coldef.difference(cur_coldef))
        col_list = []
        for new_col in coldef:
            if new_col not in new_cols:continue
            col_list.append(' '.join([new_col, 'TEXT']))
        for col in col_list:
            alter_stmt = 'alter table %s add column %s;'%(table_name, col)
            #print alter_stmt
            try:
                cur.execute(alter_stmt)
            except:pass
        conn.commit()
        return 'done'

    def get_tt_desc_map(self, conn, cur):
        sel_stmt        = 'select node_name, description from node_mapping'
        cur.execute(sel_stmt)
        rows = cur.fetchall()
        tt_desc_map = {}
        for row in rows:
            row = map(str, row)
            node_name, desc = row
            tt_desc_map[node_name]  = desc
        return tt_desc_map

    # 48 View Excel Sheet Header
    def get_excel_sheet_names(self, ijson):
        company_name    = str(ijson['company_name'])
        model_number    = str(ijson['model_number'])
        url_id          = str(ijson['deal_id'])
        project_id      = str(ijson['project_id'])
        company_id      = '_'.join([project_id, url_id])

        classification_db_path = '/mnt/eMB_db/node_mapping.db'
        conn, cur       = conn_obj.sqlite_connection(classification_db_path)
        tt_desc_map     = self.get_tt_desc_map(conn, cur)
        cur.close()
        conn.close()

        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)

        mname = '/var/www/html/DB_Model/%s/' %(company_name)

        try:
            sql         = 'CREATE TABLE IF NOT EXISTS final_output(row_id INTEGER PRIMARY KEY AUTOINCREMENT, gen_id TEXT,table_type TEXT, gen_type TEXT, taxo_ids TEXT, group_text, user_name TEXT, datetime TEXT, error_txt TEXT, header_name TEXT)'
            cur.execute(sql)
            conn.commit()
        except:pass

        coldef    = ['row_id', 'gen_id', 'table_type', 'gen_type', 'taxo_ids', 'group_text', 'user_name', 'datetime', 'error_txt', 'header_name']
        alter_msg = self.alter_table_coldef(conn, cur, 'final_output', coldef)

        sel_stmt        = "select gen_id, table_type, gen_type, group_text, header_name from final_output;"
        cur.execute(sel_stmt)
        rows            = cur.fetchall()
        data_rows = []
        for row in rows:
            row = map(str, row)
            sid, tt, gen_type, group_text, new_header_name = row
            if new_header_name == 'None':
                new_header_name = ''
            '''
            fname = '%s-P.txt'%(sid)
            ff = os.path.join(mname, fname)
            if not os.path.exists(ff):
                continue
            fin = open(ff, 'r')
            lines = fin.readlines()
            fin.close()
            '''
            old_header_name = tt_desc_map.get(tt, tt)
            if gen_type == 'group':
                txt_stmt = 'select group_txt from vgh_group_info where group_id = "%s";' % group_text
                cur.execute(txt_stmt)
                txt_tup = cur.fetchone()
                sub_header = ''
                if txt_tup and txt_tup[0]:
                    sub_header = str(txt_tup[0])
                old_header_name = '%s - %s'%(old_header_name, sub_header)
            data_rows.append({'sid':sid, 'tt':tt, 'on':old_header_name, 'nn':new_header_name}) 
        #print data_rows
        conn.close()
        #data_rows.sort(key = lambda x:x['sid'])
        return [{'message':'done', 'data':data_rows}]

    # 49 Save New Excel Sheet Header
    def save_new_excel_sheet_headers(self, ijson):
        company_name    = str(ijson['company_name'])
        model_number    = str(ijson['model_number'])
        url_id          = str(ijson['deal_id'])
        project_id      = str(ijson['project_id'])
        company_id      = '_'.join([project_id, url_id])

        data_rows       = ijson['data']

        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)

        for data_dict in data_rows:
            sid, new_header_name = data_dict['sid'], data_dict['nn']
            update_stmt = 'update final_output set header_name = "%s" where gen_id = "%s"'%(new_header_name, sid)
            cur.execute(update_stmt)
        conn.commit()
        conn.close()
        return [{'message':'done'}]

    def update_vgh_doc_group_text(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        try:
            cur.execute("alter table vgh_group_map add column table_str TEXT")
        except:
            pass
        conn.commit()
        conn.close()
        conn, cur       = conn_obj.sqlite_connection(db_file)
        sql             = 'select table_type, doc_id, table_id, vgh_id, group_txt from doc_group_map' 
        try:
            cur.execute(sql)
            res             = cur.fetchall()
        except:
            res = []
        if not res:
            res = [{'message':'done'}]
            conn.close()
            return res

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()
        
        dd              = {}
        all_tables      = {}
        all_vgh         = {}
        for rr in res:
            table_type, doc_id, table_id, vgh_id, group_txt = rr
            k   = (table_type, vgh_id, group_txt)
            dd.setdefault(k, {})[table_id]  = 1
            all_tables[str(table_id)]   = 1
            all_vgh[str(vgh_id)]         = 1
        sql = "select table_id, xml_id, vgh_text from mt_data_builder where table_id in(%s) and vgh_text in (%s)"%(', '.join(all_tables.keys()), ', '.join(all_vgh.keys()))
        cur.execute(sql)
        res = cur.fetchall()
        v_d     = {}
        for rr in res:
            table_id, xml_id, vgh_id    = rr
            table_id    = str(table_id)
            vgh_id      = str(vgh_id)
            tk   = self.get_quid(table_id+'_'+xml_id)
            c_id        = txn_m.get('XMLID_MAP_'+tk)
            c           = c_id.split('_')[2]
            v_d.setdefault((vgh_id, table_id), {})[c]     = 1
            
        u_ar    = []
        for k, v in dd.items():
            table_type, vgh_id, group_txt   = k
            t_ar    = []
            for table_id in v.keys():
                for c in v_d[(vgh_id, table_id)].keys():
                    t_ar.append(table_id+'-'+c)
            u_ar.append(('#'.join(t_ar), group_txt, vgh_id, table_type))
            #print  u_ar[-1]
        print 'Total Update ', len(u_ar)
        cur.executemany('update vgh_group_map set table_str=? where group_txt=? and vgh_id=? and table_type=?', u_ar)
        conn.commit()
        conn.close()
        res = [{'message':'done'}]
        return res

    ## 51_old Distinguish Taxonomy View
    def get_taxonomy_distinguish_view_data_old(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        ph_d        = {}
        path    = "%s/%s/%s/1_1/21/sdata/doc_map.txt"%(self.doc_path, project_id, deal_id)
        if os.path.exists(path):
            fin = open(path, 'r')
            lines   = fin.readlines()
            fin.close()
        else:
            lines   = []
        doc_d       = {}
        dphs        = {}
        #c_year      = int(datetime.datetime.now().strftime('%Y'))
        c_year      = self.get_cyear(lines)
        #start_year  = c_year - int(ijson['year'])
        start_year  = c_year - int(ijson.get('year', 5))
        for line in lines[1:]:
            line    = line.split('\t')
            if len(line) < 8:continue
            line    = map(lambda x:x.strip(), line)
            ph      = line[3]+line[7]
            try:
                year    = int(ph[-4:])
            except:continue
            if ph and start_year<year:
                doc_id  = line[0]
                if ijson.get('ignore_doc_ids', {}) and doc_id in  ijson.get('ignore_doc_ids', {}):continue
                doc_d[doc_id]   = (ph, line[2])
                dphs[ph]        = 1

        def update_table_coldef(conn, cur):
            try:
                sql     = "CREATE TABLE IF NOT EXISTS mt_data_builder(row_id INTEGER PRIMARY KEY AUTOINCREMENT, taxo_id INTEGER, prev_id INTEGER,order_id INTEGER, table_type, taxonomy TEXT, user_taxonomy TEXT, missing_taxo varchar(1), table_id TEXT, c_id TEXT, gcom TEXT, ngcom TEXT, ph TEXT, ph_label TEXT, user_name TEXT, datetime TEXT, isvisible varchar(1), m_rows TEXT, vgh_text TEXT, vgh_group TEXT, doc_id INTEGER, xml_id TEXT, period VARCHAR(50), period_type VARCHAR(50), scale VARCHAR(50), currency VARCHAR(50), value_type VARCHAR(50), target_taxonomy TEXT, numeric_flg VARCHAR(10), tb_flg VARCHAR(10))"
                cur.execute(sql)
                cur.commit()
            except:pass
            coldef          = ["row_id", "taxo_id", "prev_id", "order_id", "table_type", "taxonomy", "user_taxonomy", "missing_taxo", "table_id", "c_id", 
                               "gcom", "ngcom", "ph", "ph_label", "user_name", "datetime", "isvisible", "m_rows", "vgh_text", "vgh_group", "doc_id", 
                               "xml_id", "period", "period_type", "scale", "currency", "value_type", "target_taxonomy", "numeric_flg", "tb_flg"
                              ]
            alter_msg       = self.alter_table_coldef(conn, cur, 'mt_data_builder', coldef)
            return 'done'

        i_table_type    = ijson['table_type']

        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, [i_table_type])

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn         = env.begin()

        lmdb_path   = '/var/www/html/Rajeev/BBOX/'+str(project_id)+'_'+str(deal_id)
        lmdb_path    = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        env1    = lmdb.open(lmdb_path, readonly=True)
        txn1    = env1.begin()
        table_ph_d  = {}
        all_ph_d    = {}
        table_type  = str(i_table_type)
        f_taxo_arr  = [] #json.loads(txn.get('TABLE_RESULTS_'+table_type, "[]"))
        taxo_d      = {} #json.loads(txn.get('TABLE_RESULTS_'+table_type, "[]"))
        table_ids   = {}
        g_ar        = []
        table_col_phs   = {}
        consider_tables = {}
        if not taxo_d:
            db_file         = self.get_db_path(ijson)
            conn, cur   = conn_obj.sqlite_connection(db_file)
            alter_msg   = update_table_coldef(conn, cur)
            if not ijson.get('vids', {}):
                sql = "select group_id, table_type, group_txt from vgh_group_info where table_type='%s'"%(table_type)
                try:
                    cur.execute(sql)
                    res = cur.fetchall()
                except:
                    res = []
                grp_info    = {}
                for rr in res:
                    group_id, table_type, group_txt = rr
                    group_txt = group_txt.replace('\t', '')
                    grp_info[str(group_id)]   = group_txt
                sql         = "select vgh_id, group_txt from vgh_group_map where table_type='%s'"%(table_type)
                try:
                    cur.execute(sql)
                    res         = cur.fetchall()
                except:
                    res = []
                group_d     = {}
                for rr in res:
                    vgh_id, group_txt   = rr
                    group_d.setdefault(group_txt, {})[vgh_id]   = 1
                g_ar    = []
                for k, v in group_d.items():
                    dd  = {'n':grp_info.get(k,k), 'vids':v.keys(),'grpid':k}
                    g_ar.append(dd)
            if not ijson.get('vids', {}):
                sql         = "select row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, doc_id, m_rows, vgh_text, vgh_group, xml_id, target_taxonomy, numeric_flg from mt_data_builder where table_type='%s' and isvisible='Y'"%(table_type)
            else:
                sql         = "select row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, doc_id, m_rows, vgh_text, vgh_group, xml_id, target_taxonomy, numeric_flg from mt_data_builder where table_type='%s' and isvisible='Y' and vgh_text in (%s)"%(table_type, ', '.join(ijson['vids']))
            try:
                cur.execute(sql)
                res         = cur.fetchall()
            except:
                res = []
            for rr in res:
                row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, gv_xml, ph, ph_label,gcom, ngcom, doc_id,m_rows, vgh_text, vgh_group, xml_id, target_taxonomy, numeric_flg    = rr
                doc_id      = str(doc_id)
                if doc_id not in doc_d:continue
                table_id    = str(table_id)
                if table_id not in m_tables:continue
                tk   = self.get_quid(table_id+'_'+xml_id)
                c_id        = txn_m.get('XMLID_MAP_'+tk)
                if not c_id:continue
                gv_xml  = c_id
                consider_tables[table_id] = 1
                c   = int(c_id.split('_')[2])
                table_col_phs.setdefault((table_id, c), {})[ph]   = table_col_phs.setdefault((table_id, c), {}).get(ph, 0) +1
                table_ids[table_id]   = 1
                comp    = ''
                #if gcom == 'Y' or ngcom == 'Y':
                #    comp    = 'Y'
                taxo_d.setdefault(taxo_id, {'l_change':{},'order_id':order_id, 'rid':row_id, 'u_label':user_taxonomy, 't_l':taxonomy, 'missing':missing_taxo, 'comp':comp, 'ks':[], 'm_rows':m_rows, 'target_tx':target_taxonomy, 'num_flg':numeric_flg})['ks'].append((table_id, gv_xml, ph, ph_label,gcom, ngcom, doc_id, xml_id))
                if vgh_group == 'N':
                    taxo_d[taxo_id]['l_change'][xml_id]  = 1
            conn.close()
        r_ld    = {}
        for table_id in table_ids.keys():
            k       = 'HGH_'+str(table_id)
            ids     = txn_m.get(k)
            if ids:
                ids     = ids.split('#')
                row_d   = {}
                for c_id in ids:
                    r       = int(c_id.split('_')[1])
                    c       = int(c_id.split('_')[2])
                    x       = txn_m.get('XMLID_'+c_id)
                    key     = table_id+'_'+self.get_quid(x)
                    t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                    t       = ' '.join(t.split())
                    rs      = int(txn_m.get('rowspan_'+c_id))
                    for tr in range(rs): 
                        #r_ld.setdefault(table_id, {}).setdefault(r+tr, []).append((c, t))
                        row_d.setdefault(r+tr, []).append((c, t, x))
                r_ld[table_id]  = {}
                for r, c_ar in row_d.items():
                    c_ar.sort()
                    txt = []
                    xml = []
                    for tr in c_ar:
                        txt.append(tr[1])
                        xml.append(tr[2])
                    bbox        = self.get_bbox_frm_xml(txn1, table_id, ':@:'.join(xml))
                    r_ld[table_id][r]  = (' '.join(txt), ':@:'.join(xml), bbox)
        rc_ld    = {}
        if ijson.get('vids', []):
            for table_id in table_ids.keys():
                k       = 'VGH_'+str(table_id)
                ids     = txn_m.get(k)
                if ids:
                    col_d   = {}
                    ids     = ids.split('#')
                    for c_id in ids:
                        r       = int(c_id.split('_')[1])
                        c       = int(c_id.split('_')[2])
                        cs      = int(txn_m.get('colspan_'+c_id))
                        for tr in range(cs): 
                            col_d.setdefault(c+tr, {})[r]   = c_id
                    for c, rows in col_d.items():
                        rs= rows.keys()
                        rs.sort(reverse=True)
                        for r in rs:
                            c_id        = str(table_id)+'_'+str(r)+'_'+str(c)
                            if not c_id or not txn_m.get('TEXT_'+c_id):continue
                            x       = txn_m.get('XMLID_'+c_id)
                            t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                            t       = ' '.join(t.split())
                            rc_ld.setdefault(table_id, {})[c]   = (x, t, self.get_bbox_frm_xml(txn1, table_id, x))
                            break
                        
        f_ar        = []
        done_d      = {}
        tmptable_col_phs    = {}
        not_found_ph    = {}
        for k, v in table_col_phs.items():
            phs = v.keys()
            phs.sort(key=lambda x:v[x], reverse=True)
            if len(phs) == 1 and phs[0] == '':
                table_id    = k[0]
                tk       = 'VGH_'+str(table_id)
                ids     = txn_m.get(tk)
                if ids:
                    col_d   = {}
                    ids     = ids.split('#')
                    for c_id in ids:
                        r       = int(c_id.split('_')[1])
                        c       = int(c_id.split('_')[2])
                        cs      = int(txn_m.get('colspan_'+c_id))
                        for tr in range(cs): 
                            col_d.setdefault(c+tr, {})[r]   = c_id
                    for c, rows in col_d.items():
                        if c != k[1]:continue
                        rs= rows.keys()
                        rs.sort(reverse=True)
                        for r in rs:
                            c_id        = str(table_id)+'_'+str(r)+'_'+str(c)
                            if not c_id or not txn_m.get('TEXT_'+c_id):continue
                            x       = txn_m.get('XMLID_'+c_id)
                            t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                            t       = ' '.join(t.split())
                            tmptable_col_phs[k]   = t
                            not_found_ph.setdefault(k[0], {})[k[1]] = 1
                            break
                pass
            if k not in tmptable_col_phs:
                tmptable_col_phs[k] = phs[0]
        row_ids = taxo_d.keys()
        row_ids.sort(key=lambda x:len(taxo_d[x]['ks']), reverse=True)
        #for row_id, dd in taxo_d.items():
        if ijson.get('gen_output', '') == 'Y':
            tmprows = []
            done_d  = {}
            for row_id in row_ids:
                dd      = taxo_d[row_id]
                ks      = dd['ks']
                tmp_arr = []
                for table_id, c_id, ph, tlabel, gcom, ngcom, doc_id, xml_id in ks:
                    if (table_id, xml_id) in done_d:continue
                    tmp_arr.append((table_id, c_id, ph, tlabel, gcom, ngcom, doc_id, xml_id))
                    done_d[(table_id, xml_id)]=1
                if not tmp_arr:continue
                tmprows.append(row_id)
            row_ids = tmprows[:]
                    
        
        for row_id in row_ids:
            dd      = taxo_d[row_id]
            ks      = dd['ks']
            taxos   = dd['t_l'].split(' / ')[0]
            row     = {'t_id':row_id} #'t_l':taxo}
            f_dup   = ''
            label_d = {}
            target_tx   = dd['target_tx']
            numeric_flg = dd['num_flg']
            label_r_d   = {}
            for table_id, c_id, ph, tlabel, gcom, ngcom, doc_id, xml_id in ks:
                #tk   = self.get_quid(table_id+'_'+xml_id)
                #c_id        = txn_m.get('XMLID_MAP_'+tk)
                #if not c_id:continue
                row.setdefault('tids', {})[table_id]    = 1
                table_id    = str(table_id)
                c_id        = str(c_id)
                r           = int(c_id.split('_')[1])
                c           = int(c_id.split('_')[2])
                x           = txn_m.get('XMLID_'+c_id)
                t           = self.convert_html_entity(binascii.a2b_hex(txn_m.get('TEXT_'+c_id)))
                if tlabel:
                    tlabel  = self.convert_html_entity(tlabel)
                #print [taxo, table_id, c_id, t, ph, tlabel]
                if (deal_id, table_type) in self.ph_grp_d :
                    c_ph        = ph
                    c_tlabel    = tlabel
                else:
                    c_ph        = str(c)
                    c_tlabel    = ''
                row[table_id+'-'+c_ph+'-'+c_tlabel]    = {'v':t, 'x':x, 'bbox':self.get_bbox_frm_xml(txn1, table_id, x), 'd':doc_id, 't':table_id, 'r':r}
                #if c_id in done_d:
                #    f_dup   = 'Y'
                #    row[table_id+'-'+c_ph+'-'+c_tlabel]['m']    = 'Y'
                if gcom == 'Y':
                    row[table_id+'-'+c_ph+'-'+c_tlabel]['g_f']    = 'Y'
                    row['f']    = 'Y'
                if ngcom == 'Y':
                    row[table_id+'-'+c_ph+'-'+c_tlabel]['ng_f']    = 'Y'
                    row['f']    = 'Y'
                done_d[c_id]    = 1
                if (deal_id, table_type) in self.ph_grp_d :
                    table_ph_d.setdefault((doc_id, table_id), {})[(c_ph, c_tlabel)]   = (c, ph)
                else:
                    table_ph_d.setdefault((doc_id, table_id), {})[(c_ph, c_tlabel)]   = (c, tmptable_col_phs[(table_id, c)])
                #print table_id, c_id
                txts, xml_ar, bbox    = r_ld[table_id].get(r, ('', '', ''))
                txts        = self.convert_html_entity(txts)
                grm_txts    = txts.lower() #self.remove_grm_mrks(txts).lower()
                label_r_d[grm_txts] = 1
                label_d.setdefault(grm_txts, {'id':xml_id, 'txt':txts, 'bbox':bbox, 'x':xml_ar, 'd':doc_id, 't':table_id, 'v':{}})['v'][doc_d[doc_id][0]]    = {'bbox':bbox, 'x':xml_ar, 'd':doc_id, 't':table_id}

                if xml_id not in dd.get('l_change', {}):
                    label_d[grm_txts]['s']  = 'Y'
                col_txt = rc_ld.get(table_id, {}).get(c, ())
                if col_txt:
                    txts        = self.convert_html_entity(col_txt[1])
                    grm_txts    = txts.lower() #self.remove_grm_mrks(txts).lower()
                    label_d.setdefault(grm_txts, {'id':xml_id, 'txt':txts, 'bbox':col_txt[2], 'x':col_txt[0], 'd':doc_id, 't':table_id, 'v':{}})['v'][doc_d[doc_id][0]]    = {'bbox':col_txt[2], 'x':col_txt[0], 'd':doc_id, 't':table_id}
                    if xml_id not in dd.get('l_change', {}):
                        label_d[grm_txts]['s']  = 'Y'
            if len(label_d.keys()) > 1:
                row['lchange']  = 'Y'
                row['ldata']    = label_d.values()
            lble    = label_r_d.keys()
            lble.sort(key=lambda x:len(x), reverse=True)
            row['taxo']  = dd['t_l']
            if dd.get('u_label', ''):
                row['t_l']  = dd.get('u_label', '')
            else:
                row['t_l']  = label_d[lble[0]]['txt']
            xml_ar  = label_d[ lble[0]]['x'].split(':@:')
            if xml_ar and xml_ar[0]:
                    table_id    =  label_d[ lble[0]]['t']
                    doc_id      = label_d[ lble[0]]['d']
                    p_key   = txn.get('TRPLET_HGH_PINFO_'+table_id+'_'+self.get_quid(xml_ar[0]))
                    if p_key:
                        tmp_xar  = []
                        t_ar    = []
                        for pkinfo in p_key.split(':!:'):
                            pxml, ptext = pkinfo.split('^')
                            tmp_xar.append(pxml)
                            t_ar.append(binascii.a2b_hex(ptext))
                        pxml    = ':@:'.join(tmp_xar)
                        row['parent_txt']   = ' '.join(t_ar) 

            row['x']    = label_d[ lble[0]]['x']
            row['bbox'] = label_d[ lble[0]]['bbox']
            row['t']    = label_d[ lble[0]]['t']
            row['d']    = label_d[ lble[0]]['d']
            row['l']    = len(ks)
            row['fd']   = f_dup
            row['order']= dd['order_id']
            row['rid']  = dd['rid']
            row['target_tx'] = target_tx
            row['num_flg'] = numeric_flg
            if dd.get('m_rows', ''):
                row['merge']    = 'Y'
            if dd.get('missing', ''):
                row['missing']  = dd['missing']
            f_ar.append(row)

        f_ar.sort(key=lambda x:(x['order'], x['rid']))
        data = []
        idx = 1
        for data_dict in f_ar:
            #print x['taxo'], '===', x.get('t_l', ''),'>>>', x.get('parent_txt', ''), '===', x['t_id']
            taxo    = data_dict.get('taxo', '')
            num_flg = data_dict.get('num_flg', 0)
            taxo_ar = taxo.split(' / ')

            inflg = False
            if num_flg is not None and int(num_flg):
                inflg = True
            taxos = []
            for tx in taxo_ar:
                fflg = False
                if tx == data_dict['target_tx']:
                    fflg = True
                taxos.append({'t':str(tx), 'f':fflg, 'id':idx})
                idx += 1
            
            x, bbox, t, d = data_dict['x'], data_dict['bbox'], data_dict['t'], data_dict['d']
            tmp     = {'t_l':str(data_dict.get('t_l', '')), 'taxos':taxos, 't_id':data_dict['t_id'], 'in':inflg, 'x':x, 'bbox':bbox, 'd':d, 't':t}
            data.append(tmp)
        return [{'message':'done', 'data':data, 'tb_flg':'tb'}]

    def taxo_grouping(self, conn, cur, table_type, target_taxo, taxo_id):
        tstmt = "select row_id from taxonomy_mapping where target_taxo = '%s' and table_type = '%s';"%(target_taxo, table_type)
        cur.execute(tstmt)
        drows = cur.fetchall()

        grouped_ar = []
        for row in drows:
            row_id = str(row[0])
            grouped_ar.append((taxo_id, row_id))

        if grouped_ar:
            cur.executemany('update taxonomy_mapping set taxo_id = ? where row_id = ?', grouped_ar)
            conn.commit() 
        return 'done'

    def taxo_table_split_merging(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']

        table_type      = ijson['table_type']
        source_data     = ijson['source']
        merge_split_flg = ijson['ms_flg']

        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)

        if merge_split_flg == 'm':
            t_taxo     = ijson['t_taxo']
            stmt = 'select distinct(target_taxo) from taxonomy_mapping where taxo_id = "%s" and table_type = "%s"'%(t_taxo, table_type)
            cur.execute(stmt)
            cur_tar_taxo_tup = cur.fetchone()
            cur_tar_taxo = str(cur_tar_taxo_tup[0])

            data_ar = []
            for taxo_id, table_ar in source_data.iteritems():
                for table in table_ar:
                    data_ar.append((t_taxo, cur_tar_taxo, taxo_id))
            if data_ar:
                cur.executemany('update taxonomy_mapping set taxo_id = ?, target_taxo = ? where row_id = ?', data_ar)
                conn.commit()

        elif merge_split_flg == 's':
            data_ar = []
            new_taxo_id = hashlib.md5(str(datetime.datetime.now())).hexdigest() 
            for taxo_id, table_ar in source_data.iteritems():
                for table in table_ar:
                    data_ar.append((new_taxo_id, '', taxo_id))
            if data_ar:
                cur.executemany('update taxonomy_mapping set taxo_id = ?, target_taxo = ? where row_id = ?', data_ar)
                conn.commit()
        conn.close()
        return [{'message':'done'}]

    ## 52 Save Asigned Taxonomy
    def save_taxonomy_distinguish_info(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']

        table_type      = ijson['table_type']
        data_ar         = ijson['list']
        tb_flg          = ijson['tb_flg']

        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)

        data_rows       = []
        edit_data_rows  = []
        for dd in data_ar:
            target_taxo = dd['taxo']
            numeric_flg = dd['in']
            taxo_id     = dd['t_id']
            edit_flg    = dd.get('ef', 0)

            #tmsg = self.taxo_grouping(conn, cur, table_type, target_taxo, taxo_id)

            if edit_flg:
                edit_data_rows.append((target_taxo, numeric_flg, taxo_id, table_type))
            else:
                data_rows.append((target_taxo, numeric_flg, taxo_id, table_type))

        #print edit_data_rows
        #print '>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>'
        #print data_rows

        if edit_data_rows:
            updated_edit_data_rows = []
            for ktup in edit_data_rows:
                ktup = list(ktup)
                target_taxo = ktup[0]
                taxo_id = ktup[2]
                tstmt = "select distinct(taxo_str) from taxonomy_mapping where taxo_id = '%s'"%(taxo_id)
                cur.execute(tstmt)
                drows = cur.fetchone()
                taxo_str = ''
                if drows and drows[0]:  
                    taxo_li = drows[0].split('^!!^')
                    taxo_li = list(set(taxo_li))
                    if target_taxo not in taxo_li:
                        taxo_li.append(target_taxo)
                    taxo_str = '^!!^'.join(taxo_li)
                    ktup.insert(0, taxo_str)
                updated_edit_data_rows.append(tuple(ktup[:]))
            cur.executemany('update taxonomy_mapping set taxo_str=?, target_taxo=?, num_flg=? where taxo_id=? and table_type=?', updated_edit_data_rows)
            conn.commit()

        cur.executemany('update taxonomy_mapping set target_taxo=?, num_flg=? where taxo_id = ? and table_type = ?', data_rows)
        conn.commit()

        cur.execute('update taxonomy_mapping set tb_flg = "%s" where table_type = "%s"'%(tb_flg, table_type))
        conn.commit()

        conn.close()
        return [{'message':'done'}]

    # 51
    def get_taxonomy_distinguish_view_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        table_type    = ijson['table_type']
        db_file         = self.get_db_path(ijson)
        conn, cur   = conn_obj.sqlite_connection(db_file)

        table_name = 'taxonomy_mapping'
        column_list = [('row_id', 'INTEGER PRIMARY KEY AUTOINCREMENT'), ('order_id', 'INTEGER'), ('taxo_id', 'INTEGER'), ('label', 'TEXT'), ('xml_id', 'TEXT'), ('doc_id', 'VARCHAR(256)'), ('table_id', 'VARCHAR(256)'), ('table_type', 'VARCHAR(256)'), ('taxo_str', 'TEXT'), ('bbox', 'TEXT'), ('target_taxo', 'VARCHAR(256)'), ('num_flg', 'INTEGER'), ('tb_flg', 'VARCHAR(256)')]
        try:
            sql     = "CREATE TABLE IF NOT EXISTS taxonomy_mapping ( %s );"%(', '.join([' '.join(list(x)) for x in column_list]))
            cur.execute(sql)
            conn.commit()
        except:pass

        stmt = "select row_id, order_id, taxo_id, label, xml_id, doc_id, table_id, table_type, taxo_str, bbox, target_taxo, num_flg, tb_flg from taxonomy_mapping where table_type = '%s'"%(table_type)
        cur.execute(stmt)

        rows = cur.fetchall()
        taxo_map = {}
        label_map = {}

        global_tb_flg = 'tb'

        for rr in rows:
            row_id, order_id, taxo_id, label, xml_id, doc_id, table_id, table_type, taxo_str, bbox, target_taxonomy, numeric_flg, tb_flg = rr
            global_tb_flg = tb_flg
            unique_key = str(target_taxonomy).strip()
            if not unique_key:
                unique_key = str(taxo_id).strip()
            taxo_map.setdefault(unique_key, {'taxo_id':taxo_id, 'rid':row_id, 'order_id':order_id, label:[], 'tx_str':[], 'ks':[], 'target_tx':target_taxonomy, 'num_flg':numeric_flg})['ks'].append((table_id, doc_id, xml_id, bbox))
            if taxo_str not in taxo_map[unique_key]['tx_str']:
                taxo_map[unique_key]['tx_str'].append(taxo_str)
            if unique_key not in label_map:
                label_map[unique_key] = {}
            if label not in label_map[unique_key]:
                label_map[unique_key][label] = []
            label_map[unique_key][label].append((table_id, doc_id, xml_id, bbox, row_id))

        f_ar = []
        table_id_map = []
        kid = 1
        for unique_key, ddict in taxo_map.iteritems():
            row                 = {}
            row['t_id']         = ddict['taxo_id']
            label_dict          = label_map.get(unique_key, [])
            lmap = []
            for label, t_info_li in label_dict.iteritems():
                tmap = []
                for t_tup in t_info_li:
                    table_id, doc_id, xml_id, bbox, rid = t_tup
                    tmpbbox = [bbox.split('@^@')]
                    try:
                        tmpbbox = eval(bbox)
                    except:pass
                    tmap.append({'t':table_id, 'd':doc_id, 'x':xml_id, 'b':tmpbbox, 'r':rid})
                tmp = {'l':label, 't_info':tmap, 'kid':kid}
                kid += 1
                lmap.append(tmp)
            row['t_l']          = lmap

            taxo_str_li         = ddict['tx_str']
            ftaxo_str_li        = []
            for taxo_str in taxo_str_li:
                li = taxo_str.split('^!!^')
                ftaxo_str_li += li

            row['taxo']         = '^!!^'.join(set(ftaxo_str_li))
            row['num_flg']      = ddict['num_flg']
            row['target_tx']    = ddict['target_tx']
            row['order_id']     = ddict['order_id']
            ks_ar               = ddict['ks']
            row['table_ids']    = []
            for ks in ks_ar:
                table_id, doc_id, xml_id, bbox = ks
                row['t'] = table_id
                row['x'] = xml_id
                tmpbbox = [bbox.split('@^@')]
                try:
                    tmpbbox = eval(bbox)
                except:pass
                    
                row['bbox'] = tmpbbox
                row['d'] = doc_id
                row['table_ids'].append({'t':table_id, 'x':xml_id, 'd':doc_id, 'bbox':tmpbbox})
            f_ar.append(row)

        f_ar.sort(key=lambda x:x['order_id'])
        data = []
        idx = 1
        for data_dict in f_ar:
            taxo    = data_dict.get('taxo', '')
            table_ids    = data_dict.get('table_ids', [])
            num_flg = data_dict.get('num_flg', 0)
            taxo_ar = taxo.split('^!!^')
            label   = data_dict.get('t_l', [])

            inflg = False
            if num_flg is not None and int(num_flg):
                inflg = True
            taxos = []
            f   = 0
            if not data_dict['target_tx']:
                for tx in taxo_ar[:]:
                    fflg = False
                    if tx == data_dict['target_tx']:
                        fflg = True
                        f   = 1
                    taxos.append({'t':str(tx), 'f':fflg, 'id':idx,'ef':False})
                    idx += 1
            if f == 0:
                for tx in [data_dict['target_tx']]:
                    fflg = False
                    if tx == data_dict['target_tx']:
                        fflg = True
                        f   = 1
                    taxos.append({'t':str(tx), 'f':fflg, 'id':idx,'ef':False})
                    idx += 1
            
            
            x, bbox, t, d = data_dict['x'], data_dict['bbox'], data_dict['t'], data_dict['d']
            tmp  = {'t_l':label, 'taxos':taxos, 't_id':data_dict['t_id'], 'in':inflg, 'x':x, 'bbox':[bbox], 'd':d, 't':t, 'table_ids':table_ids}
            data.append(tmp)
        return [{'message':'done', 'data':data, 'tb_flg':global_tb_flg}]


    def compute_hop_score(self, chain):

        chain_li = list(chain)
        first_elm = chain_li[0]
        diff_measure = []

        for e1 in chain_li[1:]:
            cur_diff = e1-first_elm-1

            diff_measure.append(cur_diff)
            first_elm = e1

        #return max(diff_measure)
        return sum(diff_measure)


    def create_computation_groups_multi_new(self, ijson):

        from common.clean_cell_value_by_lang import CleanCellValueByLang
        clean_obj = CleanCellValueByLang()

        pattern_file = '/mnt/eMB_db/GComp_txt_files/1/ExtnGCPatterns.txt'
        formula_file = '/mnt/eMB_db/GComp_txt_files/1/Extnadd_formula.txt'

        ijson['restated'] = 'N'
        #data = self.create_ph_group(ijson)[0]
        data = self.create_seq_across(ijson)[0]
        #data['data']    = filter(lambda x:x['t_id'] in [50985, 50998, 51002, 51007], data['data'])

        #print len(data['data']); sys.exit()

        all_phs = [each['k'] for each in data['phs']]
        ph_wise_data = dd(lambda : {
                                    'xml_ids': [], 
                                    'values': [], 
                                    'actual_values': []
                                })
        rownum_tid_dict = {}

        for row_num, row in enumerate(data['data']):

            tid = row['t_id']
            rownum_tid_dict[row_num] = tid

            for ph in all_phs:

                v = row.get(ph, {}).get('v', '')
                x = row.get(ph, {}).get('x', '')

                clean_value = 0

                if v.strip():
                    try:
                        clean_value = float(clean_obj.get_clean_value_new(v))
                    except:
                        pass

                ph_wise_data[ph]['values'].append(clean_value)
                ph_wise_data[ph]['xml_ids'].append(x)
                ph_wise_data[ph]['actual_values'].append(v)

        #print dict(ph_wise_data); sys.exit()

        # For col wise computation
        all_comp_results = []

        #print 'mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm', ph_wise_data.keys() 
        if 0:
            for (k, v) in ph_wise_data.items()[:]:
                if '451' not in k: continue 
                print   k, v
                all_comp_results.append(calculate_comp_results((k, v)))
                for v_ind, t_val in enumerate(v['values'][:]):
                    print v_ind, t_val   
                  
        else:
            from multiprocessing import Pool
            pool = Pool(4)
            all_comp_results = pool.map(calculate_comp_results, [[ph, values] for ph, values in ph_wise_data.items()[:]])

        gcomp_resultant_dict = dd(set)
        ng_resultant_dict = dd(set)

        #print 'lllllllllllllllllllll' 
        for (ph, gresults, ngresults) in all_comp_results:
            #print '========================'  
            #print ph  
            for r in gresults:
                #rtup = (tuple(r[0]), tuple(r[1]), r[2])
                #gcomp_resultant_dict[rtup].add(ph)
                   
                rtup = tuple(r[1]), r[2]
                gcomp_resultant_dict[rtup].add((tuple(r[0]), ph))

            '''
            for r in ngresults:
                rtup = (r[0], tuple(r[1]), r[2])
                ng_resultant_dict[rtup].add(ph)
            #'''
                
        #sys.exit()
        gcomp_result_groups_list = []
        nong_result_groups_list = []

        signs = ('+', '-')
        ng_signs = {
                    'a/b' : ('+', '+'),
                    '(a/b)*100' : ('+', '+'),
                    '((a-b)/b)*100' : ('+', '+'),
                    'a/-b' : ('+', '-'),
                    '(a/-b)*100' : ('+', '-'),
                    '((a-b)/-b)*100' : ('+', '-'),
                    '(a/b)*1000000' : ('+', '+'),
                    '(a/-b)*1000000' : ('+', '-'),
                    '(a/b)*1000' : ('+', '+'),
                    '(a/-b)*1000' : ('+', '-')
                    }

        done_dict = dd(set)

        for result, op_matched_phs in gcomp_resultant_dict.iteritems():

            if result[-1]:
                res_index = result[0][0]
            else:
                res_index = result[0][-1]

            empty_value_sequence = [True] * len(result[0])

            matched_phs = set([p[1] for p in op_matched_phs])
            all_operators_set = set([o[0] for o in op_matched_phs])

            #print all_operators_set; continue; #sys.exit()

            for ph, ph_data in ph_wise_data.iteritems():

                xml_ids = ph_data['xml_ids']
                actual_values = ph_data['actual_values']

                for i, v in enumerate(result[0]):
                    if empty_value_sequence[i] and actual_values[v].strip():
                        empty_value_sequence[i] = False

                # If resultant is present and any operand is valid, then reject the current sequence as its not present in all phs
                if ph not in matched_phs:
                    if xml_ids[res_index].strip() and any(xml_ids[x].strip() for x in result[0]):
                        break

            else: # No break

                # Reject if resultant row is all null
                res_pos = 0 if result[-1] else -1
                if empty_value_sequence[res_pos]: continue

                # Get only those rows where atleast one valid value is present
                new_result = tuple([op_index for op_index, is_valid in zip(result[0], empty_value_sequence) if not is_valid])

                if len(new_result) < 3: continue

                if result[-1]:
                    m, n = 1, len(new_result)
                else:
                    m, n = 0, len(new_result) - 1

                #hop_score = self.compute_hop_score(result[1])
                hop_score = self.compute_hop_score(new_result)
                res_group = []

                valid_operators_set = set()

                empty_value_sequence = empty_value_sequence[m:n]

                for operators in all_operators_set:

                    new_operators = tuple([op for op, is_valid in zip(operators, empty_value_sequence) if not is_valid])

                    # If its already considered
                    if len(new_operators) < 2 or new_operators in done_dict[new_result]: continue

                    valid_operators_set.add(new_operators)
                    done_dict[new_result].add(new_operators)

                for new_operators in valid_operators_set:

                    cur_row = []
                    for s, r in zip(new_operators, new_result[m:n]):
                        cur_row.append({'tid': rownum_tid_dict[r], 's': signs[s]})

                    # Change the resultant sign
                    if result[-1]:
                        cur_row = [{'tid': rownum_tid_dict[res_index], 's': '=', 'R': 'Y', 'c': 'G'}] + cur_row
                    else:
                        cur_row.append({'tid': rownum_tid_dict[res_index], 's': '=', 'R': 'Y', 'c': 'G'})

                    res_group.append(cur_row)

                if res_group:
                    gcomp_result_groups_list.append((res_group, hop_score, new_result[0]))

        '''
        for ngresult, matched_phs in ng_resultant_dict.iteritems():
            for ph, ph_data in ph_wise_data.iteritems():

                xml_ids = ph_data['xml_ids']
                res_index = ngresult[1][0]

                # If resultant is present and any operand is valid, then reject the current sequence as its not present in all phs
                if ph not in matched_phs:
                    if xml_ids[res_index].strip() and any(xml_ids[x].strip() for x in ngresult[1]):
                        break

            else: # No break

                ftype = ngresult[0]
                cur_row = []

                for (r, s) in zip(ngresult[1][1:], ng_signs[ftype]):
                    cur_row.append({'tid': rownum_tid_dict[r], 's': s})

                # If the resultant is present at the beginning
                if ngresult[-1]:
                    cur_row = [{'tid': rownum_tid_dict[res_index], 's': '=', 'R': 'Y', 'c': 'NG', 'ft': ftype}] + cur_row
                else:
                    cur_row.append({'tid': rownum_tid_dict[res_index], 's': '=', 'R': 'Y', 'c': 'NG', 'ft': ftype})

                nong_result_groups_list.append((cur_row, hop_score, ngresult[1][0]))
        #'''

        resultant_f = [each[0] for each in sorted(gcomp_result_groups_list, key=lambda x: (x[1], x[2], len(x[0][0])))]

        return resultant_f


    def create_computation_groups_partial(self, ijson):

        from common.clean_cell_value_by_lang import CleanCellValueByLang
        clean_obj = CleanCellValueByLang()

        pattern_file = '/mnt/eMB_db/GComp_txt_files/1/ExtnGCPatterns.txt'
        formula_file = '/mnt/eMB_db/GComp_txt_files/1/Extnadd_formula.txt'

        ijson['restated'] = 'N'
        #data = self.create_ph_group(ijson)[0]
        data = self.create_seq_across(ijson)[0]

        #print data['phs']; sys.exit()

        ph_groups_dict = {}

        for each in data['phs']:
            ph_groups_dict[each['k']] = each['g']

        all_phs = [each['k'] for each in data['phs']]
        ph_wise_data = dd(lambda : {
                                    'xml_ids': [], 
                                    'values': [], 
                                    'actual_values': []
                                })
        rownum_tid_dict = {}
        tid_phvalue_dict = dd(dict)

        for row_num, row in enumerate(data['data']):

            tid = row['t_id']
            rownum_tid_dict[row_num] = tid

            for ph in all_phs:

                v = row.get(ph, {}).get('v', '')
                x = row.get(ph, {}).get('x', '')

                clean_value = 0

                if v.strip():
                    try:
                        clean_value = float(clean_obj.get_clean_value_new(v))
                    except:
                        pass

                ph_wise_data[ph]['values'].append(clean_value)
                ph_wise_data[ph]['xml_ids'].append(x)
                ph_wise_data[ph]['actual_values'].append(v)
                tid_phvalue_dict[tid][ph] = v

        num_rows = len(rownum_tid_dict)

        #print dict(ph_wise_data); sys.exit()

        # For col wise computation
        all_comp_results = []

        #print 'mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm', ph_wise_data.keys() 
        if 0:
            for (k, v) in ph_wise_data.items()[:]:
                #if '302' not in k: continue 
                #if k.split('-')[0].strip() not in ['302', '437', '451', '149', '559', '767', '899', '1218', '1229', '1271']: continue
                #if k.split('-')[0].strip() not in ['548', '466', '1236', '1195']: continue
                #if k.split('-')[0].strip() not in ['548']: continue
                #if k.split('-')[0].strip() not in ['478']: continue
                #if k.split('-')[0].strip() not in ['1195']: continue
                print   k, v
                all_comp_results.append(calculate_comp_results((k, v)))
                for v_ind, t_val in enumerate(v['values'][:]):
                    print v_ind, t_val   

            print 'lllllllllllllllllllll' 
            for (ph, gresults, ngresults) in all_comp_results:
                print '========================'  
                print ph  
                for r in gresults:
                    print r  
        else:

            '''
            null_index_map = {}                   
            for ph, vals in ph_wise_data.items():
                #print ph, vals
                #sys.exit()     
                values = vals['actual_values']
                for val_it, val in enumerate(values):
                    if val == '':     
                       if val_it not in null_index_map:
                          null_index_map[val_it] = [] 
                       null_index_map[val_it].append(ph)

            for n_k, n_phs in null_index_map.items():
            '''

            from multiprocessing import Pool
            pool = Pool(6)
            all_comp_results = pool.map(calculate_comp_results, [[ph, values] for ph, values in ph_wise_data.items()[:]])

        gcomp_resultant_dict = dd(set)
        ng_resultant_dict = dd(set)

        for (ph, gresults, ngresults) in all_comp_results:
            for r in gresults:
                rtup = tuple(r[1]), r[2]
                gcomp_resultant_dict[rtup].add((tuple(r[0]), ph))
                #print r

            '''
            for r in ngresults:
                rtup = (r[0], tuple(r[1]), r[2])
                ng_resultant_dict[rtup].add(ph)
            #'''

        #sys.exit()
        gcomp_result_groups_list = []
        nong_result_groups_list = []

        signs = ('+', '-')
        ng_signs = {
                    'a/b' : ('+', '+'),
                    '(a/b)*100' : ('+', '+'),
                    '((a-b)/b)*100' : ('+', '+'),
                    'a/-b' : ('+', '-'),
                    '(a/-b)*100' : ('+', '-'),
                    '((a-b)/-b)*100' : ('+', '-'),
                    '(a/b)*1000000' : ('+', '+'),
                    '(a/-b)*1000000' : ('+', '-'),
                    '(a/b)*1000' : ('+', '+'),
                    '(a/-b)*1000' : ('+', '-')
                    }

        done_dict = dd(set)
        done_set = set()

        for result, op_matched_phs in gcomp_resultant_dict.iteritems():
            if result[-1]:
                res_index = result[0][0]
            else:
                res_index = result[0][-1]

            empty_value_sequence = [True] * len(result[0])
            matched_phs = set([p[1] for p in op_matched_phs])
            all_operators_set = set([o[0] for o in op_matched_phs])

            ph_operandsign_dict = dd(list)

            correct_formulas = [] 
            incorrect_formulas = [] 
            break_flg = 0 

            for ph, ph_data in ph_wise_data.iteritems():
                xml_ids = ph_data['xml_ids']
                actual_values = ph_data['actual_values']

                # Find if any valid entry is present
                for i, v in enumerate(result[0]):
                    if empty_value_sequence[i] and actual_values[v].strip():
                        empty_value_sequence[i] = False

            if result[-1]:
               # first one is resultant
               if empty_value_sequence[0] == True:
                  continue  
            else:
               # last one is resultant   
               if empty_value_sequence[-1] == True:
                  continue 

            if True in empty_value_sequence:
               false_count = 0
               for e in empty_value_sequence:
                   if e == False:
                      false_count += 1
               if false_count < 2:
                  # Number of resultant and operands are less than 2
                  continue

               #print ' old: ', result
               #print ' all_operators_set: ', all_operators_set 
               new_result = tuple([op_index for op_index, is_valid in zip(result[0], empty_value_sequence) if not is_valid])
               result = (new_result, result[-1])
               #print ' new: ', new_result         
               
               new_all_operators_set = []
               for operators in all_operators_set:
                    new_operators = tuple([op for op, is_valid in zip(operators, empty_value_sequence) if not is_valid])
                    new_all_operators_set.append(new_operators)
 
               #print ' new: ', new_all_operators_set 
               result = (new_result, result[-1])
               all_operators_set = new_all_operators_set[:] 

            for ph, ph_data in ph_wise_data.iteritems():
                xml_ids = ph_data['xml_ids']
                actual_values = ph_data['actual_values']
                # If resultant is present and any operand is valid, then reject the current sequence as its not present in all phs
                if ph not in matched_phs:
                    #if xml_ids[res_index].strip() and any(xml_ids[x].strip() for x in result[0]):
                    if result[-1]:
                       # first one is resultant
                       res_operands = result[0][1:]
                    else:
                       # last one is resultant
                       res_operands = result[0][:-1]
                    if actual_values[res_index].strip() and any(actual_values[x].strip() for x in res_operands[:]):
                        break_flg = 1
                        ##break
                        #print '==================' 
                        #print ' Incorrect reason: ', xml_ids[res_index].strip(), ph
                        #print 'Incorrect ', result[0], ph, ' ==> ', matched_phs 
                        #print actual_values  
                        incorrect_formulas.append(ph)           
                else:
                   if result[-1]:
                       # first one is resultant
                       res_operands = result[0][1:]
                   else:
                       # last one is resultant
                       res_operands = result[0][:-1]
                   if actual_values[res_index].strip() and (any(actual_values[x].strip() for x in res_operands[:])): 
                      #Resultant should be non-null
                      correct_formulas.append(ph)          
                   else:
                      break_flg = 1
                          
            correct_tabs = map(lambda x:x.split('-')[0], correct_formulas[:])
            incorrect_tabs = map(lambda x:x.split('-')[0], incorrect_formulas[:])

            ctabs = dd(list)
            ictabs = dd(list)

            for tab in correct_formulas:
                tab_id, col, _ = tab.split('-')
                ctabs[tab_id].append(tab.strip())

            for tab in incorrect_formulas:
                tab_id, col, _ = tab.split('-')
                ictabs[tab_id].append(tab.strip())

            correct_tabs  = list(sets.Set(correct_tabs))
            incorrect_tabs = list(sets.Set(incorrect_tabs))

            if break_flg: 
               s1 = sets.Set(correct_tabs)
               s2 = sets.Set(incorrect_tabs)
                  
               if s1.intersection(s2): 
                  s = s1 - s2
                  correct_tabs = list(s)
                    
                  #continue
               if (len(correct_tabs) < 3):
                  continue
               if 0:    
                  print result 
                  print correct_formulas, correct_tabs 
                  print incorrect_formulas, incorrect_tabs 
                  
                  #sys.exit() 
            #else: # Present in all cols, hence don't include
            #   continue
                    
            if 0:
                # Reject if resultant row is all null
                res_pos = 0 if result[-1] else -1
                if empty_value_sequence[res_pos]: continue

                # Get only those rows where atleast one valid value is present
                new_result = tuple([op_index for op_index, is_valid in zip(result[0], empty_value_sequence) if not is_valid])

                if len(new_result) < 3: continue

                if result[-1]:
                    m, n = 1, len(new_result)
                else:
                    m, n = 0, len(new_result) - 1

                #hop_score = self.compute_hop_score(result[1])
                hop_score = self.compute_hop_score(new_result)

                valid_operators_set = set()

                empty_value_sequence = empty_value_sequence[m:n]

                for operators in all_operators_set:
                    new_operators = tuple([op for op, is_valid in zip(operators, empty_value_sequence) if not is_valid])

                    # If its already considered
                    if len(new_operators) < 2 or new_operators in done_dict[new_result]: continue

                    #valid_operators_set.add(new_operators)
                    done_dict[new_result].add(new_operators)

            if 1:
                hop_score = self.compute_hop_score(result[0])
                res_group = []

                if result[-1]:
                    m, n = 1, len(result[0])
                else:
                    m, n = 0, len(result[0]) - 1

                for new_operators in all_operators_set:
                    tids = []
                    cur_row = []
                    for s, r in zip(new_operators, result[0][m:n]):
                        cur_row.append({'tid': rownum_tid_dict[r], 's': signs[s]})
                        tids.append(rownum_tid_dict[r])

                    # Change the resultant sign
                    if result[-1]:
                        cur_row = [{'tid': rownum_tid_dict[res_index], 's': '=', 'R': 'Y', 'c': 'G', 'ctabs': dict(ctabs), 'ictabs': dict(ictabs)}] + cur_row
                        tids = [rownum_tid_dict[res_index]] + tids
                    else:
                        cur_row.append({'tid': rownum_tid_dict[res_index], 's': '=', 'R': 'Y', 'c': 'G', 'ctabs': dict(ctabs), 'ictabs': dict(ictabs)})
                        tids.append(rownum_tid_dict[res_index])

                    tids = tuple(tids), result[-1]
                    #print result , ' tids: ', tids , tids not in done_set 
                    if tids not in done_set:
                        res_group.append(cur_row)
                        done_set.add(tids)
                        break

                if res_group:
                    gcomp_result_groups_list.append((res_group, hop_score, result[0][0], break_flg))

        '''
        for ngresult, matched_phs in ng_resultant_dict.iteritems():
            for ph, ph_data in ph_wise_data.iteritems():

                xml_ids = ph_data['xml_ids']
                res_index = ngresult[1][0]

                # If resultant is present and any operand is valid, then reject the current sequence as its not present in all phs
                if ph not in matched_phs:
                    if xml_ids[res_index].strip() and any(xml_ids[x].strip() for x in ngresult[1]):
                        break

            else: # No break

                ftype = ngresult[0]
                cur_row = []

                for (r, s) in zip(ngresult[1][1:], ng_signs[ftype]):
                    cur_row.append({'tid': rownum_tid_dict[r], 's': s})

                # If the resultant is present at the beginning
                if ngresult[-1]:
                    cur_row = [{'tid': rownum_tid_dict[res_index], 's': '=', 'R': 'Y', 'c': 'NG', 'ft': ftype}] + cur_row
                else:
                    cur_row.append({'tid': rownum_tid_dict[res_index], 's': '=', 'R': 'Y', 'c': 'NG', 'ft': ftype})

                nong_result_groups_list.append((cur_row, hop_score, ngresult[1][0]))
        #'''

        #print gcomp_result_groups_list; sys.exit()
        #for each in gcomp_result_groups_list:
        #    print each; sys.exit()

        gcomp_result_groups_list = sorted(gcomp_result_groups_list, key=lambda x: (x[1], x[2], len(x[0][0])))

        resultant_f = [(each[0], each[-1]) for each in gcomp_result_groups_list]# if each[-1]==0] + [(each[0], each[-1]) for each in gcomp_result_groups_list if each[-1]==1]

        #print resultant_f; sys.exit()

        return resultant_f

    def create_computation_groups_for_selected_rows(self, ijson, data):
        from common.clean_cell_value_by_lang import CleanCellValueByLang
        clean_obj = CleanCellValueByLang()

        pattern_file = '/mnt/eMB_db/GComp_txt_files/1/ExtnGCPatterns.txt'
        formula_file = '/mnt/eMB_db/GComp_txt_files/1/Extnadd_formula.txt'

        ijson['restated'] = 'N'
        #data = self.create_ph_group(ijson)[0]
        #data = self.create_seq_across(ijson)[0]

        #print data['phs']; sys.exit()

        ph_groups_dict = {}

        for each in data['phs']:
            ph_groups_dict[each['k']] = each['g']

        all_phs = [each['k'] for each in data['phs']]
        ph_wise_data = dd(lambda : {
                                    'xml_ids': [], 
                                    'values': [], 
                                    'actual_values': []
                                })
        rownum_tid_dict = {}
        tid_phvalue_dict = dd(dict)
        sel_rc_cnt = 0
        for row_num, row in enumerate(data['data']):
            if sel_rc_cnt > 10:
                break
            tid = row['t_id']
            rownum_tid_dict[row_num] = tid
            for ph in all_phs:

                v = row.get(ph, {}).get('v', '')
                x = row.get(ph, {}).get('x', '')

                clean_value = 0

                if v.strip():
                    try:
                        clean_value = float(clean_obj.get_clean_value_new(v))
                    except:
                        pass

                ph_wise_data[ph]['values'].append(clean_value)
                ph_wise_data[ph]['xml_ids'].append(x)
                ph_wise_data[ph]['actual_values'].append(v)
                tid_phvalue_dict[tid][ph] = v
            sel_rc_cnt += 1

        if 0 and sel_rc_cnt > 10:
            return 0, []
        
        num_rows = len(rownum_tid_dict)
        # For col wise computation
        all_comp_results = []
        if 0:
            for (k, v) in ph_wise_data.items()[:]:
                #if k.split('-')[0].strip() not in ['302', '437', '451', '149', '559', '767', '899', '1218', '1229', '1271']: continue
                print   k, v
                all_comp_results.append(calculate_comp_results((k, v)))
                for v_ind, t_val in enumerate(v['values'][:]):
                    print v_ind, t_val   

            for (ph, gresults, ngresults) in all_comp_results:
                print '========================'  
                print ph  
                for r in gresults:
                    print r  
        else:

            from multiprocessing import Pool
            pool = Pool(6)
            all_comp_results = pool.map(calculate_comp_results, [[ph, values] for ph, values in ph_wise_data.items()[:]])

        gcomp_resultant_dict = dd(set)
        ng_resultant_dict = dd(set)

        for (ph, gresults, ngresults) in all_comp_results:
            for r in gresults:
                rtup = tuple(r[1]), r[2]
                gcomp_resultant_dict[rtup].add((tuple(r[0]), ph))
                #print r

        #sys.exit()
        gcomp_result_groups_list = []
        nong_result_groups_list = []

        signs = ('+', '-')
        
        done_dict = dd(set)
        done_set = set()

        for result, op_matched_phs in gcomp_resultant_dict.iteritems():
            if result[-1]:
                res_index = result[0][0]
            else:
                res_index = result[0][-1]

            empty_value_sequence = [True] * len(result[0])
            matched_phs = set([p[1] for p in op_matched_phs])
            all_operators_set = set([o[0] for o in op_matched_phs])
            default_operator_signs = tuple([0 for _ in range(len(result[0]))])

            ph_operandsign_dict = dd(list)

            correct_formulas = [] 
            incorrect_formulas = [] 
            break_flg = 0 

            for ph, ph_data in ph_wise_data.iteritems():
                xml_ids = ph_data['xml_ids']
                actual_values = ph_data['actual_values']

                # Find if any valid entry is present
                for i, v in enumerate(result[0]):
                    if empty_value_sequence[i] and actual_values[v].strip():
                        empty_value_sequence[i] = False

            if result[-1]:
               # first one is resultant
               if empty_value_sequence[0] == True:
                  continue  
            else:
               # last one is resultant   
               if empty_value_sequence[-1] == True:
                  continue 

            if True in empty_value_sequence:
               false_count = 0
               for e in empty_value_sequence:
                   if e == False:
                      false_count += 1
               if false_count < 2:
                  # Number of resultant and operands are less than 2
                  continue

               new_result = tuple([op_index for op_index, is_valid in zip(result[0], empty_value_sequence) if not is_valid])
               result = (new_result, result[-1])
               #print ' new: ', new_result         
               
            #'''
            ## NEW CHANGE
            if result[-1]:
                 m, n = 1, len(result[0])
            else:
                m, n = 0, len(result[0]) - 1

            empty_value_sequence = empty_value_sequence[m:n]

            for (operators, ph) in op_matched_phs:
                new_signs = tuple([op for op, is_valid in zip(operators, empty_value_sequence) if not is_valid])
                if len(new_signs) == len(result[0]) - 1: # # of signs should be one less than # of operands
                    ph_operandsign_dict[ph].append(new_signs)
            #'''

            for ph, ph_data in ph_wise_data.iteritems():
                xml_ids = ph_data['xml_ids']
                actual_values = ph_data['actual_values']
                # If resultant is present and any operand is valid, then reject the current sequence as its not present in all phs
                if ph not in matched_phs:
                    #if xml_ids[res_index].strip() and any(xml_ids[x].strip() for x in result[0]):
                    if result[-1]:
                       # first one is resultant
                       res_operands = result[0][1:]
                    else:
                       # last one is resultant
                       res_operands = result[0][:-1]
                    if actual_values[res_index].strip() and any(actual_values[x].strip() for x in res_operands[:]):
                        break_flg = 1
                        incorrect_formulas.append(ph)           
                else:
                   if result[-1]:
                       # first one is resultant
                       res_operands = result[0][1:]
                   else:
                       # last one is resultant
                       res_operands = result[0][:-1]
                   if actual_values[res_index].strip() and (any(actual_values[x].strip() for x in res_operands[:])): 
                      #Resultant should be non-null
                      correct_formulas.append(ph)          
                   else:
                      break_flg = 1
                          
            correct_tabs = map(lambda x:x.split('-')[0], correct_formulas[:])
            incorrect_tabs = map(lambda x:x.split('-')[0], incorrect_formulas[:])

            ctabs = dd(list)
            ictabs = dd(list)

            '''
            for tab in correct_formulas:
                tab_id, col, _ = tab.split('-')
                ctabs[tab_id].append(tab.strip())

            for tab in incorrect_formulas:
                tab_id, col, _ = tab.split('-')
                ictabs[tab_id].append(tab.strip())
            #'''

            correct_tabs  = list(sets.Set(correct_tabs))
            incorrect_tabs = list(sets.Set(incorrect_tabs))

            if break_flg: 
               s1 = sets.Set(correct_tabs)
               s2 = sets.Set(incorrect_tabs)
                  
               if s1.intersection(s2): 
                  s = s1 - s2
                  correct_tabs = list(s)
                    
               if (len(correct_tabs) < 3):
                  continue

            for tab in correct_formulas:
                tab_id, col, _ = tab.split('-')
                if tab_id in correct_tabs and tab_id not in incorrect_tabs: ctabs[tab_id].append(tab.strip())

            for tab in incorrect_formulas:
                tab_id, col, _ = tab.split('-')
                ictabs[tab_id].append(tab.strip())

            if 0:
                # Reject if resultant row is all null
                res_pos = 0 if result[-1] else -1
                if empty_value_sequence[res_pos]: continue

                # Get only those rows where atleast one valid value is present
                new_result = tuple([op_index for op_index, is_valid in zip(result[0], empty_value_sequence) if not is_valid])

                if len(new_result) < 3: continue

                if result[-1]:
                    m, n = 1, len(new_result)
                else:
                    m, n = 0, len(new_result) - 1

                #hop_score = self.compute_hop_score(result[1])
                hop_score = self.compute_hop_score(new_result)

                valid_operators_set = set()

                empty_value_sequence = empty_value_sequence[m:n]

                for operators in all_operators_set:
                    new_operators = tuple([op for op, is_valid in zip(operators, empty_value_sequence) if not is_valid])

                    # If its already considered
                    if len(new_operators) < 2 or new_operators in done_dict[new_result]: continue

                    #valid_operators_set.add(new_operators)
                    done_dict[new_result].add(new_operators)

            if 1:
                hop_score = self.compute_hop_score(result[0])
                res_group = []

                if result[-1]:
                    m, n = 1, len(result[0])
                else:
                    m, n = 0, len(result[0]) - 1

                if len(result[0]) < 3: continue

                default_operator_signs = [0] * (n-m)

                #'''
                ## NEW CHANGE
                group_dict = dd(list)

                for ph in all_phs:
                #for ph, operand_signs_list in ph_operandsign_dict.iteritems():
                    operand_signs_list = ph_operandsign_dict.get(ph, [default_operator_signs])

                    group = ph_groups_dict[ph]

                    # Add the operator signs for this set of tids(index) under this ph
                    operand_signs = operand_signs_list[0] # Consider only the first entry

                    tids = []
                    cur_row = []

                    if n-m != len(operand_signs):
                        print n, m, operand_signs
                        continue

                    for s, r in zip(operand_signs, result[0][m:n]):
                        cur_row.append({'tid': rownum_tid_dict[r], 's': signs[s], 'v': tid_phvalue_dict[rownum_tid_dict[r]][ph]})
                        tids.append(rownum_tid_dict[r])

                    # Change the resultant sign
                    if result[-1]:
                        #cur_row = [{'tid': rownum_tid_dict[res_index], 's': '=', 'R': 'Y', 'c': 'G', 'ctabs': dict(ctabs), 'ictabs': dict(ictabs), 'ph': ph, 'v': tid_phvalue_dict[rownum_tid_dict[res_index]][ph] }] + cur_row
                        cur_row = [{'tid': rownum_tid_dict[res_index], 's': '=', 'R': 'Y', 'ph': ph, 'v': tid_phvalue_dict[rownum_tid_dict[res_index]][ph] }] + cur_row

                        tids = [rownum_tid_dict[res_index]] + tids

                    else:
                        #cur_row.append({'tid': rownum_tid_dict[res_index], 's': '=', 'R': 'Y', 'c': 'G', 'ctabs': dict(ctabs), 'ictabs': dict(ictabs), 'ph': ph, 'v': tid_phvalue_dict[rownum_tid_dict[res_index]][ph] })
                        cur_row.append({'tid': rownum_tid_dict[res_index], 's': '=', 'R': 'Y', 'ph': ph, 'v': tid_phvalue_dict[rownum_tid_dict[res_index]][ph] })

                        tids.append(rownum_tid_dict[res_index])

                    tid_tup = tuple(tids), result[-1]
                    if ph in done_dict[tid_tup]:
                        continue
                    done_dict[tid_tup].add(ph)

                    if len(cur_row) > 2: group_dict[group].append(cur_row)#, hop_score, result[0][0], break_flg))

                if group_dict:
                    gcomp_result_groups_list.append([dict(group_dict), hop_score, result[0][0], len(result[0]), 'G', dict(ctabs), dict(ictabs), break_flg])

                #'''

        ## NEW CHANGE
        gcomp_result_groups_list = sorted(gcomp_result_groups_list, key=lambda x: (x[1], x[2], x[3]))
        resultant_f = [[each[0], each[-4], each[-3], each[-2], each[-1]] for each in gcomp_result_groups_list if each[-1]==0] + [(each[0], each[-4], each[-3], each[-2], each[-1]) for each in gcomp_result_groups_list if each[-1]==1]
        return 1, resultant_f

    def create_computation_groups_partial_new(self, ijson):

        from common.clean_cell_value_by_lang import CleanCellValueByLang
        clean_obj = CleanCellValueByLang()

        pattern_file = '/mnt/eMB_db/GComp_txt_files/1/ExtnGCPatterns.txt'
        formula_file = '/mnt/eMB_db/GComp_txt_files/1/Extnadd_formula.txt'

        ijson['restated'] = 'N'
        #data = self.create_ph_group(ijson)[0]
        data = self.create_seq_across(ijson)[0]

        #print data['phs']; sys.exit()

        ph_groups_dict = {}

        for each in data['phs']:
            ph_groups_dict[each['k']] = each['g']

        all_phs = [each['k'] for each in data['phs']]
        ph_wise_data = dd(lambda : {
                                    'xml_ids': [], 
                                    'values': [], 
                                    'actual_values': []
                                })
        rownum_tid_dict = {}
        tid_phvalue_dict = dd(dict)

        for row_num, row in enumerate(data['data']):

            tid = row['t_id']
            rownum_tid_dict[row_num] = tid

            for ph in all_phs:

                v = row.get(ph, {}).get('v', '')
                x = row.get(ph, {}).get('x', '')

                clean_value = 0

                if v.strip():
                    try:
                        clean_value = float(clean_obj.get_clean_value_new(v))
                    except:
                        pass

                ph_wise_data[ph]['values'].append(clean_value)
                ph_wise_data[ph]['xml_ids'].append(x)
                ph_wise_data[ph]['actual_values'].append(v)
                tid_phvalue_dict[tid][ph] = v

        num_rows = len(rownum_tid_dict)

        # For col wise computation
        all_comp_results = []

        if 1:
            for (k, v) in ph_wise_data.items()[:]:
                #if k.split('-')[0].strip() not in ['302', '437', '451', '149', '559', '767', '899', '1218', '1229', '1271']: continue
                print   k, v
                all_comp_results.append(calculate_comp_results((k, v)))
                for v_ind, t_val in enumerate(v['values'][:]):
                    print v_ind, t_val   

            for (ph, gresults, ngresults) in all_comp_results:
                print '========================'  
                print ph  
                for r in gresults:
                    print r  
        else:

            from multiprocessing import Pool
            pool = Pool(6)
            all_comp_results = pool.map(calculate_comp_results, [[ph, values] for ph, values in ph_wise_data.items()[:]])

        gcomp_resultant_dict = dd(set)
        ng_resultant_dict = dd(set)

        for (ph, gresults, ngresults) in all_comp_results:
            for r in gresults:
                rtup = tuple(r[1]), r[2]
                gcomp_resultant_dict[rtup].add((tuple(r[0]), ph))
                #print r

        #sys.exit()
        gcomp_result_groups_list = []
        nong_result_groups_list = []

        signs = ('+', '-')
        
        done_dict = dd(set)
        done_set = set()

        for result, op_matched_phs in gcomp_resultant_dict.iteritems():
            if result[-1]:
                res_index = result[0][0]
            else:
                res_index = result[0][-1]

            empty_value_sequence = [True] * len(result[0])
            matched_phs = set([p[1] for p in op_matched_phs])
            all_operators_set = set([o[0] for o in op_matched_phs])

            ph_operandsign_dict = dd(list)

            correct_formulas = [] 
            incorrect_formulas = [] 
            break_flg = 0 

            for ph, ph_data in ph_wise_data.iteritems():
                xml_ids = ph_data['xml_ids']
                actual_values = ph_data['actual_values']

                # Find if any valid entry is present
                for i, v in enumerate(result[0]):
                    if empty_value_sequence[i] and actual_values[v].strip():
                        empty_value_sequence[i] = False

            if result[-1]:
               # first one is resultant
               if empty_value_sequence[0] == True:
                  continue  
            else:
               # last one is resultant   
               if empty_value_sequence[-1] == True:
                  continue 

            if True in empty_value_sequence:
               false_count = 0
               for e in empty_value_sequence:
                   if e == False:
                      false_count += 1
               if false_count < 2:
                  # Number of resultant and operands are less than 2
                  continue

               new_result = tuple([op_index for op_index, is_valid in zip(result[0], empty_value_sequence) if not is_valid])
               result = (new_result, result[-1])
               #print ' new: ', new_result         
               
            #'''
            ## NEW CHANGE
            if result[-1]:
                m, n = 1, len(result[0])
            else:
                m, n = 0, len(result[0]) - 1

            empty_value_sequence = empty_value_sequence[m:n]

            for (operators, ph) in op_matched_phs:
                new_signs = tuple([op for op, is_valid in zip(operators, empty_value_sequence) if not is_valid])
                ph_operandsign_dict[ph].append(new_signs)
            #'''

            for ph, ph_data in ph_wise_data.iteritems():
                xml_ids = ph_data['xml_ids']
                actual_values = ph_data['actual_values']
                # If resultant is present and any operand is valid, then reject the current sequence as its not present in all phs
                if ph not in matched_phs:
                    #if xml_ids[res_index].strip() and any(xml_ids[x].strip() for x in result[0]):
                    if result[-1]:
                       # first one is resultant
                       res_operands = result[0][1:]
                    else:
                       # last one is resultant
                       res_operands = result[0][:-1]
                    if actual_values[res_index].strip() and any(actual_values[x].strip() for x in res_operands[:]):
                        break_flg = 1
                        incorrect_formulas.append(ph)           
                else:
                   if result[-1]:
                       # first one is resultant
                       res_operands = result[0][1:]
                   else:
                       # last one is resultant
                       res_operands = result[0][:-1]
                   if actual_values[res_index].strip() and (any(actual_values[x].strip() for x in res_operands[:])): 
                      #Resultant should be non-null
                      correct_formulas.append(ph)          
                   else:
                      break_flg = 1
                          
            correct_tabs = map(lambda x:x.split('-')[0], correct_formulas[:])
            incorrect_tabs = map(lambda x:x.split('-')[0], incorrect_formulas[:])

            ctabs = dd(list)
            ictabs = dd(list)

            for tab in correct_formulas:
                tab_id, col, _ = tab.split('-')
                ctabs[tab_id].append(tab.strip())

            for tab in incorrect_formulas:
                tab_id, col, _ = tab.split('-')
                ictabs[tab_id].append(tab.strip())

            correct_tabs  = list(sets.Set(correct_tabs))
            incorrect_tabs = list(sets.Set(incorrect_tabs))

            if break_flg: 
               s1 = sets.Set(correct_tabs)
               s2 = sets.Set(incorrect_tabs)
                  
               if s1.intersection(s2): 
                  s = s1 - s2
                  correct_tabs = list(s)
                    
               if (len(correct_tabs) < 3):
                  continue
                    
            if 0:
                # Reject if resultant row is all null
                res_pos = 0 if result[-1] else -1
                if empty_value_sequence[res_pos]: continue

                # Get only those rows where atleast one valid value is present
                new_result = tuple([op_index for op_index, is_valid in zip(result[0], empty_value_sequence) if not is_valid])

                if len(new_result) < 3: continue

                if result[-1]:
                    m, n = 1, len(new_result)
                else:
                    m, n = 0, len(new_result) - 1

                #hop_score = self.compute_hop_score(result[1])
                hop_score = self.compute_hop_score(new_result)

                valid_operators_set = set()

                empty_value_sequence = empty_value_sequence[m:n]

                for operators in all_operators_set:
                    new_operators = tuple([op for op, is_valid in zip(operators, empty_value_sequence) if not is_valid])

                    # If its already considered
                    if len(new_operators) < 2 or new_operators in done_dict[new_result]: continue

                    #valid_operators_set.add(new_operators)
                    done_dict[new_result].add(new_operators)

            if 1:
                hop_score = self.compute_hop_score(result[0])
                res_group = []

                if result[-1]:
                    m, n = 1, len(result[0])
                else:
                    m, n = 0, len(result[0]) - 1

                #'''
                ## NEW CHANGE
                group_dict = dd(list)

                for ph, operand_signs_list in ph_operandsign_dict.iteritems():

                    group = ph_groups_dict[ph]

                    # Add the operator signs for this set of tids(index) under this ph
                    operand_signs = operand_signs_list[0] # Consider only the first entry

                    tids = []
                    cur_row = []
                    for s, r in zip(operand_signs, result[0][m:n]):
                        cur_row.append({'tid': rownum_tid_dict[r], 's': signs[s], 'v': tid_phvalue_dict[rownum_tid_dict[r]][ph]})
                        tids.append(rownum_tid_dict[r])

                    # Change the resultant sign
                    if result[-1]:
                        cur_row = [{'tid': rownum_tid_dict[res_index], 's': '=', 'R': 'Y', 'c': 'G', 'ctabs': dict(ctabs), 'ictabs': dict(ictabs), 'ph': ph, 'v': tid_phvalue_dict[rownum_tid_dict[res_index]][ph] }] + cur_row

                        tids = [rownum_tid_dict[res_index]] + tids

                    else:
                        cur_row.append({'tid': rownum_tid_dict[res_index], 's': '=', 'R': 'Y', 'c': 'G', 'ctabs': dict(ctabs), 'ictabs': dict(ictabs), 'ph': ph, 'v': tid_phvalue_dict[rownum_tid_dict[res_index]][ph] })

                        tids.append(rownum_tid_dict[res_index])

                    group_dict[group].append(cur_row)#, hop_score, result[0][0], break_flg))

                if group_dict:
                    gcomp_result_groups_list.append((dict(group_dict), hop_score, result[0][0], len(result[0]), break_flg))

                #'''

        ## NEW CHANGE
        gcomp_result_groups_list = sorted(gcomp_result_groups_list, key=lambda x: (x[1], x[2], x[3]))

        resultant_f = [(each[0], each[-1]) for each in gcomp_result_groups_list]# if each[-1]==0] + [(each[0], each[-1]) for each in gcomp_result_groups_list if each[-1]==1]

        #print resultant_f; sys.exit()

        #for each in resultant_f:
        #    print each; sys.exit()

        return resultant_f


    def store_sys_formula(self, ijson):

        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        res = self.read_doc_info(ijson)[0]
        if res['message'] != 'done':
            return [{'message': 'error', 'data' : []}]

        all_table_types = [each['k'] for each in res['mt_list']]

        #results_path = os.path.join('/tmp/ak_comp/', company_id)
        results_path = os.path.join(self.output_path, company_id, 'grid_computation')

        '''
        # Remove old results
        try:
            os.remove(os.path.join(results_path, 'data.mdb'))
            os.remove(os.path.join(results_path, 'lock.mdb'))
        except OSError:
            pass
        #'''

        # Make the directory
        try:
            os.makedirs(results_path)
        except OSError:
            pass

        error_dict = {}

        # Make changes to the ijson input, tt and gid and call create_computation_groups_multi_new and store the results
        for tt in all_table_types[:]:

            #print tt
            #if tt != 'BS': continue
        
            new_ijson = copy.deepcopy(ijson)
            new_ijson['table_type'] = tt

            res = self.create_seq_across(new_ijson)[0]

            all_group_ids = [str(each['grpid']) for each in res['g_ar']]

            all_group_ids.append('') # default, no group
            #print all_group_ids

            for gid in all_group_ids:

                new_ijson = copy.deepcopy(ijson)
                new_ijson['table_type'] = tt

                new_ijson['grpid'] = gid

                if 1:
                    result = self.create_computation_groups_multi_new(new_ijson)

                #except Exception as e:
                #    error_dict[(tt, gid)] = e

                if 1:
                    # Store the result into a lmdb
                    with lmdb.open(results_path) as env:
                        with env.begin(write=True) as txn:
                            txn.put(str((tt, gid)), str(result))

        msg = 'error' if error_dict else 'done'

        return [{'message': msg, 'data': error_dict}]


    def store_sys_formula_partial(self, ijson):

        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        res = self.read_doc_info(ijson)[0]
        if res['message'] != 'done':
            return [{'message': 'error', 'data' : []}]

        all_table_types = [each['k'] for each in res['mt_list']]

        #results_path = os.path.join('/tmp/ak_comp_partial/', company_id)
        results_path = os.path.join(self.output_path, company_id, 'grid_computation')

        '''
        # Remove old results
        try:
            os.remove(os.path.join(results_path, 'data.mdb'))
            os.remove(os.path.join(results_path, 'lock.mdb'))
        except OSError:
            pass
        #'''

        # Make the directory
        try:
            os.makedirs(results_path)
        except OSError:
            pass

        error_dict = {}

        # Make changes to the ijson input, tt and gid and call create_computation_groups_partial and store the results
        for tt in all_table_types[:]:

            #if tt != 'RBS': continue
        
            new_ijson = copy.deepcopy(ijson)
            new_ijson['table_type'] = tt

            res = self.create_seq_across(new_ijson)[0]

            all_group_ids = [str(each['grpid']) for each in res['g_ar']]

            all_group_ids.append('') # default, no group
            #print tt
            #print all_group_ids

            for gid in all_group_ids:

                #if gid != '': continue

                new_ijson = copy.deepcopy(ijson)
                new_ijson['table_type'] = tt

                new_ijson['grpid'] = gid

                if 1:
                    result = self.create_computation_groups_partial(new_ijson)

                #except Exception as e:
                #    error_dict[(tt, gid)] = e

                if 1:
                    # Store the result into a lmdb
                    with lmdb.open(results_path, map_size=30*1024*1024*1024) as env:
                        with env.begin(write=True) as txn:
                            txn.put(str((tt, gid)), str(result))

        msg = 'error' if error_dict else 'done'
        #sys.exit()

        return [{'message': msg, 'data': error_dict}]

    def compute_eq(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        sel_table_type  = ijson['table_type']
        sel_group_id    = ijson['grpid']
        company_id      = "%s_%s"%(project_id, deal_id)
        res = self.read_doc_info(ijson)[0]
        if res['message'] != 'done':
            return [{'message': 'error', 'data' : []}]
        all_table_types = [each['k'] for each in res['mt_list'] if each['k'] == sel_table_type]
        results_path = os.path.join(self.output_path, company_id, 'grid_computation_new')
        # Make the directory
        try:
            os.makedirs(results_path)
        except OSError:
            pass
        error_dict = {}
        result = []
        # Make changes to the ijson input, tt and gid and call create_computation_groups_partial and store the results
        for tt in all_table_types[:]:
            new_ijson = copy.deepcopy(ijson)
            new_ijson['table_type'] = tt
            res = self.create_seq_across(new_ijson)[0]
            all_group_ids = list(set([''] + [str(each1['grpid']) for each1 in res['g_ar']]))
            all_group_ids = [e for e in all_group_ids if e == sel_group_id]
            for gid in all_group_ids:
                new_ijson = copy.deepcopy(ijson)
                new_ijson['table_type'] = tt
                new_ijson['grpid'] = gid
                rflg, cres = self.create_computation_groups_for_selected_rows(new_ijson, res)
                if rflg and cres:
                    result = []
                    for each in cres:
                        cur_formula = {}
                        rtid = -1
                        res_loc = ''
                        for doc_group, ph_results_list in each[0].iteritems():
                            ctabs = {}
                            ictabs = {}
                            ctype = 'G'
                            if ph_results_list:
                                rrows = [{} for _ in range(len(ph_results_list[0]))]
                            else:
                                continue
                            if len(rrows) < 3: continue
                            for ph_data in ph_results_list[:]:
                                res_pos = 0 if 'R' in ph_data[0] else -1
                                res_loc = 'F' if not res_pos else 'L'
                                res_element = ph_data[res_pos]
                                #res_element['ps_f'] = each[-1]
                                rtid = res_element['tid']
                                cur_ph = res_element['ph']
                                df = []
                                for i, op in enumerate(ph_data):
                                    cur_ph_data = {}
                                    cur_ph_data['s'] = op['s']
                                    cur_ph_data['v'] = op['v']
                                    rrows[i][cur_ph] = cur_ph_data
                                    rrows[i]['tid'] = op['tid']
                                    df.append(op['tid'])
                                rrows[res_pos][cur_ph]['R'] = 'Y'
                            cur_formula[doc_group] = rrows
                        if cur_formula:# and (int(each[-1]) == 0):
                            df = '@@'.join(map(str, df)) + '^^' + res_loc
                            result.append({'formula': cur_formula, 'tid_str': df, 'rtid': rtid, 'ctabs': each[2], 'ictabs': each[3], 'ctype': each[1], 'ps_f': each[-1]});

                    msg = 'done'

        try:
            sFormula_data = self.get_sFormula_data(ijson)
        except:
            sFormula_data = []

        return [{'message': 'done', 'data': result, 'sformula':sFormula_data}]
        
    def store_sys_formula_partial_new(self, ijson):

        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        res = self.read_doc_info(ijson)[0]
        if res['message'] != 'done':
            return [{'message': 'error', 'data' : []}]

        all_table_types = [each['k'] for each in res['mt_list']]

        #results_path = os.path.join('/tmp/ak_comp_partial/', company_id)
        results_path = os.path.join(self.output_path, company_id, 'grid_computation_new')

        '''
        # Remove old results
        try:
            os.remove(os.path.join(results_path, 'data.mdb'))
            os.remove(os.path.join(results_path, 'lock.mdb'))
        except OSError:
            pass
        #'''

        # Make the directory
        try:
            os.makedirs(results_path)
        except OSError:
            pass

        error_dict = {}

        # Make changes to the ijson input, tt and gid and call create_computation_groups_partial and store the results
        for tt in all_table_types[:]:

            #if tt != 'RBS': continue
        
            new_ijson = copy.deepcopy(ijson)
            new_ijson['table_type'] = tt

            res = self.create_seq_across(new_ijson)[0]

            all_group_ids = [str(each['grpid']) for each in res['g_ar']]

            all_group_ids.append('') # default, no group
            #print tt
            #print all_group_ids

            for gid in all_group_ids:

                #if gid != '': continue

                new_ijson = copy.deepcopy(ijson)
                new_ijson['table_type'] = tt

                new_ijson['grpid'] = gid

                if 1:
                    result = self.create_computation_groups_partial_new(new_ijson)

                #except Exception as e:
                #    error_dict[(tt, gid)] = e

                if 1:
                    # Store the result into a lmdb
                    with lmdb.open(results_path, map_size=30*1024*1024*1024) as env:
                        with env.begin(write=True) as txn:
                            txn.put(str((tt, gid)), str(result))

        msg = 'error' if error_dict else 'done'
        #sys.exit()

        return [{'message': msg, 'data': error_dict}]


    def get_stored_sys_formula(self, ijson):

        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        tt = str(ijson['table_type'])
        gid = str(ijson.get('grpid', ''))

        #results_path = os.path.join('/tmp/ak_comp/', company_id)
        results_path = os.path.join(self.output_path, company_id, 'grid_computation')
        result = []

        if not os.path.exists(results_path):
            msg = 'Results not found'

        else:
            with lmdb.open(results_path) as env:
                with env.begin(write=False) as txn:
                    res = eval(txn.get(str((tt, gid)), '[]'))
                    msg = 'done'
                    result = [each[0] for each in res]

        try:
            sFormula_data = self.get_sFormula_data(ijson)
        except:
            sFormula_data = []

        return [{'message': 'done', 'data': result, 'sformula':sFormula_data}]


    def get_stored_sys_formula_partial(self, ijson):

        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        tt = str(ijson['table_type'])
        gid = str(ijson.get('grpid', ''))
        partial_flag = ijson.get('ps_f', 0)

        #results_path = os.path.join('/tmp/ak_comp_partial/', company_id)
        results_path = os.path.join(self.output_path, company_id, 'grid_computation')
        result = []

        if not os.path.exists(results_path):
            msg = 'Results not found'

        else:
            with lmdb.open(results_path) as env:
                with env.begin(write=False) as txn:
                    res = eval(txn.get(str((tt, gid)), '[]'))
                    result = []
                    for each in res:
                        for op in each[0][0]:
                            if 'R' in op:
                                op['ps_f'] = each[-1]
                                break

                        result.append(each[0][0])
                    msg = 'done'

                    '''
                    sform = []
                    pform = []
                    for each in res:
                        for op in each[0][0]:
                            if 'R' in op:
                                op['ps_f'] = each[-1]
                                break
                        if each[-1]: pform.append(each[0][0])
                        else: sform.append(each[0][0])

                    result = sform + pform

                    #'''

        try:
            sFormula_data = self.get_sFormula_data(ijson)
        except:
            sFormula_data = []

        return [{'message': 'done', 'data': result, 'sformula':sFormula_data}]

    def index_group_taxonomy_partial_data(self, ijson):
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        #tt = str(ijson['table_type'])
        gid = str(ijson.get('grpid', ''))
        partial_flag = ijson.get('ps_f', 0)

        #results_path = os.path.join('/tmp/ak_comp_partial/', company_id)
        results_path = os.path.join(self.output_path, company_id, 'grid_computation_new')
        result = []
        partial_indexing_data_dict = {}
        if not os.path.exists(results_path):
            msg = 'Results not found'

        else:
            with lmdb.open(results_path) as env:
                with env.begin(write=False) as txn:
                    for gkey, res in txn.cursor():
                        res = eval(res)
                        org_group_key = gkey
                        gkey = eval(gkey)
                        tt, gid = gkey
                        partial_indexing_data_dict[org_group_key] = {}
                        result = []
                        for each in res:
                            #print len(res)
                            cur_formula = {}
                            rtid = -1
                            res_loc = ''
                            for doc_group, ph_results_list in each[0].iteritems():

                                ctabs = {}
                                ictabs = {}
                                ctype = 'G'

                                if ph_results_list:
                                    rrows = [{} for _ in range(len(ph_results_list[0]))]
                                else:
                                    continue

                                if len(rrows) < 3: continue

                                for ph_data in ph_results_list[:]:

                                    res_pos = 0 if 'R' in ph_data[0] else -1
                                    res_loc = 'F' if not res_pos else 'L'
                                    res_element = ph_data[res_pos]
                                    #res_element['ps_f'] = each[-1]
                                    rtid = res_element['tid']
                                    cur_ph = res_element['ph']
                                    df = []
                                    for i, op in enumerate(ph_data):
                                        cur_ph_data = {}
                                        cur_ph_data['s'] = op['s']
                                        cur_ph_data['v'] = op['v']
                                        rrows[i][cur_ph] = cur_ph_data
                                        rrows[i]['tid'] = op['tid']
                                        df.append(op['tid'])
                                    rrows[res_pos][cur_ph]['R'] = 'Y'
                                    #ctabs = res_element['ctabs']
                                    #ictabs = res_element['ictabs']
                                    #ctype = res_element['c']

                                cur_formula[doc_group] = rrows
                            
                            if cur_formula:
                                rtid = str(rtid)
                                if rtid not in partial_indexing_data_dict[org_group_key]:
                                    partial_indexing_data_dict[org_group_key][rtid] = [] 
                                df = '@@'.join(map(str, df)) + '^^' + res_loc
                                partial_indexing_data_dict[org_group_key][rtid].append({'formula': cur_formula, 'tid_str': df, 'rtid': rtid, 'ctabs': each[2], 'ictabs': each[3], 'ctype': each[1], 'ps_f': each[-1]})
        ###################################################################################
        results_path = os.path.join(self.output_path, company_id, 'grid_computation_indxdata') 
        print results_path
        with lmdb.open(results_path, map_size=30*1024*1024*1024) as env:
            with env.begin(write=True) as txn:
                for k, v in partial_indexing_data_dict.items():
                    txn.put(k, str(v))
        ###################################################################################

    def get_stored_sys_formula_partial_new_sel(self, ijson):
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        tt = str(ijson['table_type'])
        gid = str(ijson.get('grpid', ''))
        partial_flag = ijson.get('ps_f', 0)
        sel_tid = str(ijson.get('tid', ''))
        results_path = os.path.join(self.output_path, company_id, 'grid_computation_indxdata')
        result = []
        partial_indexing_data_dict = {}
        if not os.path.exists(results_path):
            msg = 'Results not found'
        else:
            with lmdb.open(results_path) as env:
                with env.begin(write=False) as txn:
                    tdict = eval(txn.get(str((tt, gid)), '{}'))
                    result = tdict.get(sel_tid, [])
                    new_result = []
                    for ar in result:
                        if int(ar['ps_f']) == 0:
                            cnt = 100
                        else:
                            cnt = len(ar['ctabs'].keys())
                        new_result.append((cnt, ar))
                    new_result.sort()
                    new_result.reverse()
                    result = map(lambda x:x[1], new_result[:])
        try:
            sFormula_data = self.get_sFormula_data(ijson)
        except:
            sFormula_data = []
        return [{'message': 'done', 'data': result, 'sformula':sFormula_data, 'res_tids':[]}]


    def get_stored_sys_formula_partial_new(self, ijson):

        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        tt = str(ijson['table_type'])
        gid = str(ijson.get('grpid', ''))
        partial_flag = ijson.get('ps_f', 0)

        #results_path = os.path.join('/tmp/ak_comp_partial/', company_id)
        results_path = os.path.join(self.output_path, company_id, 'grid_computation_new')
        result = []

        if not os.path.exists(results_path):
            msg = 'Results not found'

        else:
            with lmdb.open(results_path) as env:
                with env.begin(write=False) as txn:
                    res = eval(txn.get(str((tt, gid)), '[]'))
                    result = []
                    for each in res:
                        cur_formula = {}
                        rtid = -1
                        res_loc = ''

                        for doc_group, ph_results_list in each[0].iteritems():

                            ctabs = {}
                            ictabs = {}
                            ctype = 'G'

                            try:
                                rrows = [{} for _ in range(len(ph_results_list[0]))]
                            except IndexError:
                                continue

                            if len(rrows) < 3: continue

                            for ph_data in ph_results_list[:]:

                                res_pos = 0 if 'R' in ph_data[0] else -1
                                res_loc = 'F' if not res_pos else 'L'
                                res_element = ph_data[res_pos]
                                #res_element['ps_f'] = each[-1]
                                rtid = res_element['tid']

                                cur_ph = res_element['ph']
                                df = []

                                for i, op in enumerate(ph_data):
                                    cur_ph_data = {}
                                    cur_ph_data['s'] = op['s']
                                    cur_ph_data['v'] = op['v']
                                    rrows[i][cur_ph] = cur_ph_data
                                    rrows[i]['tid'] = op['tid']
                                    df.append(op['tid'])

                                rrows[res_pos][cur_ph]['R'] = 'Y'
                                #ctabs = res_element['ctabs']
                                #ictabs = res_element['ictabs']
                                #ctype = res_element['c']

                            cur_formula[doc_group] = rrows
                        if len(result) > 200:
                            break
                        if cur_formula and (int(each[-1]) == 0):
                            df = '@@'.join(map(str, df)) + '^^' + res_loc
                            result.append({'formula': cur_formula, 'tid_str': df, 'rtid': rtid, 'ctabs': each[2], 'ictabs': each[3], 'ctype': each[1], 'ps_f': each[-1]});
                        #break

                    msg = 'done'

        new_result = []
        for ar in result:
            if int(ar['ps_f']) == 0:
                cnt = 100
            else:
                cnt = len(ar['ctabs'].keys())
            new_result.append((cnt, ar))
        new_result.sort()
        new_result.reverse()
        result = map(lambda x:x[1], new_result[:])
  
        try:
            sFormula_data = self.get_sFormula_data(ijson)
        except:
            sFormula_data = []

        return [{'message': 'done', 'data': result, 'sformula':sFormula_data, 'res_tids':[]}]


    def get_stored_sys_formula_partial_new_reduced(self, ijson):

        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        tt = str(ijson['table_type'])
        gid = str(ijson.get('grpid', ''))
        partial_flag = ijson.get('ps_f', 0)

        required_res_tid = str(ijson.get('tid', ''))
        res_tids = set()

        #results_path = os.path.join('/tmp/ak_comp_partial/', company_id)
        #results_path = os.path.join(self.output_path, company_id, 'grid_computation_new')
        results_path = os.path.join(self.output_path, company_id, 'grid_computation') # New change
        result = []

        if not os.path.exists(results_path):
            msg = 'Results not found'

        else:
            with lmdb.open(results_path) as env:
                with env.begin(write=False) as txn:
                    res = eval(txn.get(str((tt, gid)), '[]'))
                    result = []
                    for each in res:
                        cur_formula = {}
                        rtid = -1
                        res_loc = ''

                        #print each[0]; sys.exit()
                        for doc_group, ph_results_list in each[0].iteritems():

                            ctabs = {}
                            ictabs = {}
                            ctype = 'G'

                            try:
                                rrows = [{} for _ in range(len(ph_results_list[0]))]
                            except IndexError:
                                #print 'No results'
                                continue

                            if len(rrows) < 3: continue

                            for ph_data in ph_results_list[:]:

                                res_pos = 0 if 'R' in ph_data[0] else -1
                                res_loc = 'F' if not res_pos else 'L'
                                res_element = ph_data[res_pos]
                                #res_element['ps_f'] = each[-1]
                                rtid = res_element['tid']

                                cur_ph = res_element['ph']
                                df = []

                                for i, op in enumerate(ph_data):
                                    cur_ph_data = {}
                                    cur_ph_data['s'] = op['s']
                                    cur_ph_data['v'] = op['v']
                                    rrows[i][cur_ph] = cur_ph_data
                                    rrows[i]['tid'] = op['tid']
                                    df.append(op['tid'])

                                rrows[res_pos][cur_ph]['R'] = 'Y'
                                #ctabs = res_element['ctabs']
                                #ictabs = res_element['ictabs']
                                #ctype = res_element['c']

                            cur_formula[doc_group] = rrows
                        #if len(result) > 200:
                        #    break
                        res_tids.add(rtid)

                        if (required_res_tid and str(rtid) != required_res_tid): continue

                        if cur_formula:
                            df = '@@'.join(map(str, df)) + '^^' + res_loc
                            result.append({'formula': cur_formula, 'tid_str': df, 'rtid': rtid, 'ctabs': each[2], 'ictabs': each[3], 'ctype': each[1], 'ps_f': each[-1]});
                        #break

                    msg = 'done'

        '''
        for r in result[:10]:
            print r
        sys.exit() 
        #'''

        try:
            sFormula_data = self.get_sFormula_data(ijson)
        except:
            sFormula_data = []

        return [{'message': 'done', 'data': result, 'sformula':sFormula_data, 'res_tids': list(res_tids)}]


    def alter_sel_table(self, company_names, table_name, col_names):
        model_number = '1'
        for company_name in company_names:
            db_file         = self.get_db_path(ijson)
            conn, cur   = conn_obj.sqlite_connection(db_file)

            for col_name in col_names:
                sql         = "alter table %s add column %s TEXT"%(table_name, col_name)
                try:
                    cur.execute(sql)
                except:pass
            conn.commit()
            conn.close()
        res = [{'message':'done'}]
        return res

    def alter_table_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        table_name      = ijson['t']
        cols            = ijson['c']
        self.alter_sel_table([company_name], table_name, cols)


    def alter_table(self, ijson):
        if 0:
            return self.alter_table_data(ijson)
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        db_file         = self.get_db_path(ijson)
        conn, cur   = conn_obj.sqlite_connection(db_file)
        if 1:
            sql         = "alter table vgh_group_map add column doc_vgh TEXT"
            try:
                cur.execute(sql)
            except:pass
            sql         = "alter table vgh_group_map add column table_str TEXT"
            try:
                cur.execute(sql)
            except:pass
            try:
                sql = "update vgh_group_map set doc_vgh='VGH'"
                cur.execute(sql)
            except:pass
        sql = "CREATE TABLE IF NOT EXISTS vgh_doc_map(row_id INTEGER PRIMARY KEY AUTOINCREMENT, vgh_group_id TEXT, doc_group_id TEXT, table_type VARCHAR(100), group_txt TEXT, user_name VARCHAR(100), datetime TEXT);"
        cur.execute(sql)
        conn.commit()
        conn.close()
        res = [{'message':'done'}]
        return res



    def validate_csv(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        table_type      = ijson['table_type']
        company_id      = "%s_%s"%(project_id, deal_id)
        res = self.read_doc_info(ijson)[0]
        if res['message'] != 'done':
            return [{'message': 'error', 'data' : []}]
        #################################################################################
        data = self.create_seq_across(ijson)[0]
        all_phs = [each['k'] for each in data['phs']]
        #taxo_wise_currency_dict = {}
        #taxo_wise_scale_dict = {}
        gbl_col_dict = {}
        high_ligh_lst = []
        for row_num, row in enumerate(data['data']):
            tid = row['t_id']
            currency_dict = {}
            scale_dict = {}
            for ph in all_phs:
                ph_info = row.get(ph, {}).get('phcsv', {})
                v = row.get(ph, {}).get('v', {})
                x = row.get(ph, {}).get('x', {})
                c = ph_info.get('c', '').strip()
                s = ph_info.get('s', '').strip()
                if 1:
                    if ph not in gbl_col_dict:
                        gbl_col_dict[ph] = {}
                    gbl_col_dict[ph][tid] = row.get(ph, {})
                if c:
                    if c not in currency_dict:
                        currency_dict[c] = []
                    currency_dict[c].append((ph, x, tid, c))
                if s:
                    if s not in scale_dict:
                        scale_dict[s] = []
                    scale_dict[s].append((ph, x, tid, s))
            
            if len(currency_dict.keys()) > 1:
                high_ligh_lst += self.get_diff_refs(currency_dict)
            if len(scale_dict.keys()) > 1:
                high_ligh_lst += self.get_diff_refs(scale_dict)
        ###############################################################################
        for ph, tx_dict in gbl_col_dict.items():
            currency_dict = {}
            scale_dict = {}
            for tid, row_info in tx_dict.items():
                x = row_info.get('x', '')
                c = row_info.get('c', '')
                s = row_info.get('s', '')
                ph_info = row_info.get('phcsv', {})
                if c:
                    if c not in currency_dict:
                        currency_dict[c] = []
                    currency_dict[c].append((ph, x, tid, c))
                if s:
                    if s not in scale_dict:
                        scale_dict[s] = []
                    scale_dict[s].append((ph, x, tid, s))
            
            if len(currency_dict.keys()) > 1:
                high_ligh_lst += self.get_diff_refs(currency_dict)
            if len(scale_dict.keys()) > 1:
                high_ligh_lst += self.get_diff_refs(scale_dict)
        mdict_new = {}
        for k in high_ligh_lst:
            if k[2] not in mdict_new:
                mdict_new[k[2]] = {}
            mdict_new[k[2]][k[0]] = 'N' 
            
        return [{'message': 'done', 'data' : mdict_new}]

    def get_diff_refs(self, ddict):
        ddlst = []
        for k, vs in ddict.items():
            #for v in vs:
            n = len(vs)
            ddlst.append((n, k, vs))
        ddlst.sort()
        ddlst.reverse()
        diff_lst = []
        for dtup  in ddlst[1:]:
            xs = dtup[2]
            diff_lst += xs
        return diff_lst


    def create_HGH_group(self, ijson):
        import data_builder.db_data as db_data
        obj = db_data.PYAPI()
        if ijson.get("PRINT") != 'Y':
                disableprint()
        if ijson.get('SINGLE') == 'Y':
            td_d    = ijson['t_ids']
            for ii, tid in enumerate(td_d.keys()):
                lbl = td_d[tid]
                print 'Running ', ii, ' / ', len(td_d.keys()),  {tid:lbl}
                ijson_c           = copy.deepcopy(ijson)
                ijson_c['t_ids']  = {tid:lbl}
                obj.create_HGH_group(ijson_c)
            enableprint()
            return [{"message":'done'}]
        res = obj.create_HGH_group(ijson)
        enableprint()
        return res
        if 0:#len(ijson.get('t_ids', {}).keys()) > 1:
            res = [{'message':'Error More than taxonomy not allowed'}]
            return res
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        i_table_type    = ijson['table_type']
        ph_d        = {}
        path    = "%s/%s/%s/1_1/21/sdata/doc_map.txt"%(self.doc_path, project_id, deal_id)
        if os.path.exists(path):
            fin = open(path, 'r')
            lines   = fin.readlines()
            fin.close()
        else:
            lines   = []
        doc_d       = {}
        dphs        = {}
        c_year      = self.get_cyear(lines)
        start_year  = c_year - int(ijson.get('year', 5))
        for line in lines[1:]:
            line    = line.split('\t')
            if len(line) < 8:continue
            line    = map(lambda x:x.strip(), line)
            ph      = line[3]+line[7]
            if ph and start_year<int(ph[2:]):
                doc_id  = line[0]
                doc_d[doc_id]   = (ph, line[2])
                dphs[ph]        = 1

                    

        i_table_type    = ijson['table_type']
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, [i_table_type])

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        lmdb_path   = '/var/www/html/Rajeev/BBOX/'+str(project_id)+'_'+str(deal_id)
        lmdb_path    = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        env1    = lmdb.open(lmdb_path, readonly=True)
        txn1    = env1.begin()

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn         = env.begin()


        table_type  = str(i_table_type)
        group_d     = {}
        revgroup_d  = {}

        d_group_d     = {}
        d_revgroup_d  = {}

        vgh_id_d    = {}
        db_file         = self.get_db_path(ijson)
        conn, cur   = conn_obj.sqlite_connection(db_file)
        #sql         = "select vgh_id, doc_id, table_id, group_txt from doc_group_map where table_type='%s'"%(table_type)
        #try:
        #    cur.execute(sql)
        #    res = cur.fetchall()
        #except:
        #    res = []
        grp_doc_map_d   = {}
        rev_doc_map_d   = {}
        #for rr in res:
        #    vgh_id, doc_id, table_id, group_txt = rr
        #    grp_doc_map_d.setdefault(group_txt, {})[table_id]   = doc_id #.setdefault(doc_id, {})[table_id]    = 1
        #    rev_doc_map_d.setdefault((doc_id, table_id), {})[group_txt] = 1
        #sql = "CREATE TABLE IF NOT EXISTS vgh_doc_map(row_id INTEGER PRIMARY KEY AUTOINCREMENT, vgh_group_id TEXT, doc_group_id TEXT, table_type VARCHAR(100), group_txt TEXT, user_name VARCHAR(100), datetime TEXT);"
        self.alter_table_coldef(conn, cur, 'mt_data_builder', ['target_taxonomy', 'numeric_flg', 'th_flg'])
        sql = "select vgh_group_id, doc_group_id, group_txt from vgh_doc_map where table_type='%s'"%(table_type)
        cur.execute(sql)
        doc_vgh_map = {}
        res = cur.fetchall()
        for rr in res:
            vgh_group_id, doc_group_id, group_txt   = rr
            doc_vgh_map[(vgh_group_id, doc_group_id)]   = group_txt

        sql = "select group_id, table_type, group_txt from vgh_group_info where table_type='%s'"%(table_type)
        try:
            cur.execute(sql)
            res = cur.fetchall()
        except:
            res = []
        grp_info    = {}
        for rr in res:
            group_id, table_type, group_txt = rr
            group_txt = group_txt.replace('\t', '')
            grp_info[("GRP", str(group_id))]   = group_txt


        sql         = "select row_id, vgh_id, group_txt, table_str, doc_vgh from vgh_group_map where table_type='%s'"%(table_type)
        cur.execute(sql)
        res         = cur.fetchall()
        for rr in res:
            row_id, vgh_id, group_txt, table_str, doc_vgh   = rr
            if doc_vgh == 'DOC':
                d_group_d.setdefault(group_txt, {})[vgh_id]   = (row_id, table_str)
                d_revgroup_d.setdefault(vgh_id, {})[group_txt]    = (row_id, table_str)
                pass
            else:
                group_d.setdefault(group_txt, {})[vgh_id]   = (row_id, table_str)
                revgroup_d.setdefault(vgh_id, {})[group_txt]    = (row_id, table_str)

        t_ids   = map(lambda x:str(x), ijson.get('t_ids', {}).keys())
        if t_ids:
            sql         = "select row_id, taxo_id, prev_id, order_id, table_type, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, gcom, ngcom, ph, ph_label, user_name, datetime, isvisible, m_rows, vgh_text, vgh_group, doc_id, xml_id, period, period_type, scale, currency, value_type, target_taxonomy, numeric_flg, th_flg from mt_data_builder where table_type='%s' and taxo_id in (%s)"%(table_type, ', '.join(t_ids))
        else:
            sql         = "select row_id, taxo_id, prev_id, order_id, table_type, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, gcom, ngcom, ph, ph_label, user_name, datetime, isvisible, m_rows, vgh_text, vgh_group, doc_id, xml_id, period, period_type, scale, currency, value_type, target_taxonomy, numeric_flg, th_flg from mt_data_builder where table_type='%s'"%(table_type)
        cur.execute(sql)
        res         = cur.fetchall()
        docinfo_d   = {}
        vgh_id_d_all    = {}
        tmp_grpd        = {}
        table_ids       = {}
        all_vgh         = {}
        l_list          = ijson.get('t_ids', {}).values()
        l_list.sort(key=lambda x:len(x), reverse=True)
        f_taxo          = l_list[0]
        for rr in res:
            row_id, taxo_id, prev_id, order_id, table_type, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, gcom, ngcom, ph, ph_label, user_name, tdatetime, isvisible, m_rows, vgh_text, vgh_group, doc_id, xml_id, period, period_type, scale, currency, value_type, target_taxonomy, numeric_flg, th_flg   = rr
            tk   = self.get_quid(table_id+'_'+xml_id)
            c_id        = txn_m.get('XMLID_MAP_'+tk)
            if not c_id:continue
            doc_id  = str(doc_id)
            vgh_id  = vgh_text
            if str(doc_id) in doc_d and str(table_id) in m_tables:
                all_vgh[vgh_id]         = 1
                table_ids[table_id]       = 1
                tstr    = table_id+'-'+c_id.split('_')[2]
                #print '\n==========================================='
                rr  = (table_type, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, gcom, ngcom, ph, ph_label, user_name, tdatetime, isvisible, m_rows, vgh_text, vgh_group, doc_id, xml_id, period, period_type, scale, currency, value_type, target_taxonomy, numeric_flg, th_flg)
                if vgh_id in revgroup_d:
                    for grp_id in revgroup_d.get(vgh_id, {}).keys():
                        t   = {}
                        if not revgroup_d[vgh_id][grp_id][1]:
                            t[tstr]  = doc_id
                        elif tstr in revgroup_d[vgh_id][grp_id][1].split('#'):
                            t[tstr]  = doc_id
                        if t:
                            doc_grpid   = ''
                            if doc_id in d_revgroup_d:
                                doc_grpid   = d_revgroup_d[doc_id].keys()[0]
                            tmp_grpd.setdefault(doc_grpid, {}).setdefault(("GRP", grp_id), {})[rr]   = 1
                        #print doc_grpid, ("GRP", grp_id), c_id
                else:
                    doc_grpid   = ''
                    if doc_id in d_revgroup_d:
                        doc_grpid   = d_revgroup_d[doc_id].keys()[0]
                    tmp_grpd.setdefault(doc_grpid, {}).setdefault(("VGH", vgh_id), {})[rr]   = 1
                    #print doc_grpid, ("VGH", vgh_id), c_id
        sql         = "select vgh, vgh_id from vgh_info where table_type='%s' and vgh_id in (%s)"%(table_type, ', '.join(all_vgh.keys()))
        cur.execute(sql)
        res         = cur.fetchall()
        vgh_d       = {}
        for rr in res:
            vgh_text, vgh_id    = rr
            if vgh_id:
                try:
                    vgh_text    = binascii.a2b_hex(vgh_text)
                except:pass
                grp_info[("VGH", vgh_id)]       = self.convert_html_entity(vgh_text)
        r_ld    = {}
        for table_id in table_ids.keys():
            k       = 'HGH_'+str(table_id)
            ids     = txn_m.get(k)
            if ids:
                ids     = ids.split('#')
                row_d   = {}
                for c_id in ids:
                    r       = int(c_id.split('_')[1])
                    c       = int(c_id.split('_')[2])
                    x       = txn_m.get('XMLID_'+c_id)
                    key     = table_id+'_'+self.get_quid(x)
                    t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                    t       = ' '.join(t.split())
                    rs      = int(txn_m.get('rowspan_'+c_id))
                    for tr in range(rs): 
                        #r_ld.setdefault(table_id, {}).setdefault(r+tr, []).append((c, t))
                        row_d.setdefault(r+tr, []).append((c, t, x))
                r_ld[table_id]  = {}
                for r, c_ar in row_d.items():
                    c_ar.sort()
                    txt = []
                    xml = []
                    for tr in c_ar:
                        txt.append(tr[1])
                        xml.append(tr[2])
                    bbox        = self.get_bbox_frm_xml(txn1, table_id, ':@:'.join(xml))
                    r_ld[table_id][r]  = (' '.join(txt), ':@:'.join(xml), bbox)
        rc_ld    = {}
        for table_id in table_ids.keys():
            k       = 'VGH_'+str(table_id)
            ids     = txn_m.get(k)
            if ids:
                col_d   = {}
                ids     = ids.split('#')
                for c_id in ids:
                    r       = int(c_id.split('_')[1])
                    c       = int(c_id.split('_')[2])
                    cs      = int(txn_m.get('colspan_'+c_id))
                    for tr in range(cs): 
                        col_d.setdefault(c+tr, {})[r]   = c_id
                for c, rows in col_d.items():
                    rs= rows.keys()
                    rs.sort(reverse=True)
                    for r in rs:
                        c_id        = str(table_id)+'_'+str(r)+'_'+str(c)
                        if not c_id or not txn_m.get('TEXT_'+c_id):continue
                        x       = txn_m.get('XMLID_'+c_id)
                        t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                        t       = ' '.join(t.split())
                        rc_ld.setdefault(table_id, {})[c]   = (x, t, self.get_bbox_frm_xml(txn1, table_id, x))
                        break
        g_i_ar  = []
        grp_ar  = []
        t_ars   = []
        all_table_types = {}
        for doc_grpid, tgrp_info in tmp_grpd.items(): 
            tmp_grp_name    = f_taxo
            doc_name    = ''
            if ("GRP", doc_grpid) in grp_info:
                tmp_grp_name    = f_taxo+' - '+grp_info[("GRP", doc_grpid)]
                doc_name    = doc_grpid
            f_ar    = []
            table_ph_d  = {}
            f_taxo_ar   = []
            tmpt_d      = {}
            oid         = 1
            for grp_id_tup, rows in tgrp_info.items():
                flg, grp_id = grp_id_tup
                grp_name    = grp_info.get(grp_id_tup, ' - '.join(grp_id_tup))
                grp_name    = doc_vgh_map.get((grp_id, doc_grpid), grp_name)    
                ks      = rows
                taxos   = grp_name
                t_id    = '_'.join(grp_id_tup)+' - '+doc_grpid
                row     = {'t_id':t_id} #'t_l':taxo}
                f_dup   = ''
                label_d = {}
                label_r_d   = {}
                ph_cnt      = {}
                ks  = []
                t_ar    = []
                #for rr in rows.keys():
                r_ks    = rows.keys()
                r_ks.sort(key=lambda x:tuple(map(lambda x1:int(x1), x[5].split('_'))))
                for rr in r_ks:
                    table_type, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, gcom, ngcom, ph, ph_label, user_name, tdatetime, isvisible, m_rows, vgh_text, vgh_group, doc_id, xml_id, period, period_type, scale, currency, value_type, target_taxonomy, numeric_flg, th_flg    = rr
                    ks.append((table_id, c_id))
                    row.setdefault('tids', {})[table_id]    = 1
                    table_id    = str(table_id)
                    c_id        = str(c_id)
                    r           = int(c_id.split('_')[1])
                    c           = int(c_id.split('_')[2])
                    x           = txn_m.get('XMLID_'+c_id)
                    t           = self.convert_html_entity(binascii.a2b_hex(txn_m.get('TEXT_'+c_id)))
                    tlabel      = ph_label
                    if tlabel:
                        tlabel  = self.convert_html_entity(tlabel)
                    c_ph        = ph
                    c_tlabel    = ''
                    if (table_id, c_ph) in ph_cnt:
                        c_tlabel    = str(ph_cnt[(table_id, c_ph)]+1)
                        ph_cnt[(table_id, c_ph)]    = ph_cnt[(table_id, c_ph)]+1
                    else:   
                        ph_cnt[(table_id, c_ph)]    = 0
                    row[table_id+'-'+c_ph+'-'+c_tlabel]    = {'v':t, 'x':x, 'bbox':self.get_bbox_frm_xml(txn1, table_id, x), 'd':doc_id, 't':table_id, 'r':r}
                    table_ph_d.setdefault((doc_id, table_id), {}).setdefault((c_ph, c_tlabel), {})[(c, ph)] = 1
                    #print table_id, c_id
                    txts, xml_ar, bbox    = r_ld[table_id].get(r, ('', '', ''))
                    txts        = self.convert_html_entity(txts)
                    grm_txts    = txts.lower() #self.remove_grm_mrks(txts).lower()
                    #label_r_d[grm_txts] = 1
                    #label_d.setdefault(grm_txts, {'id':xml_id, 'txt':txts, 'bbox':bbox, 'x':xml_ar, 'd':doc_id, 't':table_id, 'v':{}})['v'][doc_d[doc_id][0]]    = {'bbox':bbox, 'x':xml_ar, 'd':doc_id, 't':table_id}

                    col_txt = rc_ld.get(table_id, {}).get(c, ())
                    if col_txt:
                        txts        = self.convert_html_entity(col_txt[1])
                        grm_txts    = txts.lower() #self.remove_grm_mrks(txts).lower()
                        label_r_d[grm_txts] = 1
                        label_d.setdefault(grm_txts, {'id':xml_id, 'txt':txts, 'bbox':col_txt[2], 'x':col_txt[0], 'd':doc_id, 't':table_id, 'v':{}})['v'][doc_d[doc_id][0]]    = {'bbox':col_txt[2], 'x':col_txt[0], 'd':doc_id, 't':table_id}
                    rr      = list(rr)
                    rr[0]   =  'HGHGROUP-'+i_table_type+'-'+'_'.join(t_ids)+'-'+doc_grpid
                    all_table_types['"'+rr[0]+'"'] = {}
                    t_ar.append(rr)
                if len(label_d.keys()) > 1:
                    row['lchange']  = 'Y'
                    row['ldata']    = label_d.values()
                lble    = label_r_d.keys()
                lble.sort(key=lambda x:len(x), reverse=True)
                row['taxo']     = grp_name
                row['t_l']      = grp_name
                xml_ar  = label_d[ lble[0]]['x'].split(':@:')
                if xml_ar and xml_ar[0]:
                        table_id    =  label_d[ lble[0]]['t']
                        doc_id      = label_d[ lble[0]]['d']
                        p_key   = txn.get('TRPLET_HGH_PINFO_'+table_id+'_'+self.get_quid(xml_ar[0]))
                        if p_key:
                            tmp_xar  = []
                            t1_ar    = []
                            for pkinfo in p_key.split(':!:'):
                                pxml, ptext = pkinfo.split('^')
                                tmp_xar.append(pxml)
                                t1_ar.append(binascii.a2b_hex(ptext))
                            pxml    = ':@:'.join(tmp_xar)
                            row['parent_txt']   = ' '.join(t1_ar) 

                row['x']    = label_d[ lble[0]]['x']
                row['bbox'] = label_d[ lble[0]]['bbox']
                row['t']    = label_d[ lble[0]]['t']
                row['d']    = label_d[ lble[0]]['d']
                row['l']    = len(ks)
                row['fd']   = f_dup
                f_taxo_ar.append({'t_l':[t_id], 'ks':ks})
                f_ar.append(row)
                tmpt_d[t_id]  = {'d':t_ar, 'o':oid, 't_l':grp_name}
                oid += 1

            tdphs = report_year_sort.year_sort(dphs.keys())
            tdphs.reverse()
            #alphs = report_year_sort.year_sort(all_ph_d.keys())
            #alphs.reverse()
            table_ids   = table_ph_d.keys()
            try:
                table_ids.sort(key=lambda x:(tdphs.index(doc_d[x[0]][0]), x[1]))
            except:
                table_ids.sort(key=lambda x:(x[0], x[1]))
                pass
            f_taxo_ar  = self.order_by_table_structure_by_col(f_taxo_ar, table_ids)
            order_d = {}
            for ii, rr in enumerate(f_taxo_ar):
                order_d[rr['t_l'][0]]   = ii+1
                tmpt_d[rr['t_l'][0]]['o']    = ii+1
            f_ar.sort(key=lambda x:order_d[x['t_id']])
            t_ars.append(tmpt_d)    
            phs = []
            t_ar    = []
            dtime   = str(datetime.datetime.now()).split('.')[0]
            for table_id in table_ids:
                t_ar.append({'t':table_id[1], 'l':len(table_ph_d[table_id].keys()), 'd':table_id[0], 'dt':doc_d.get(table_id[0], '')})
                all_phs = table_ph_d[table_id].keys()
                for k in table_ph_d[table_id].keys():
                    vs  = table_ph_d[table_id][k].keys()
                    ph_cnt_d    = {}
                    for (c, ph) in vs:
                        ph_cnt_d[ph]    = ph_cnt_d.get(ph, 0)+1
                    tphs    = ph_cnt_d.keys()
                    tphs.sort(key=lambda x:ph_cnt_d[x], reverse=True)
                    table_ph_d[table_id][k] = (vs[0][0], tphs[0])
                all_phs.sort(key=lambda x:table_ph_d[table_id][x][0])
                for ph, tlabel in all_phs:
                    tph = table_ph_d[table_id][(ph, tlabel)][1]
                    phs.append({'k':table_id[1]+'-'+ph+'-'+tlabel, 'n':tph, 'g':table_id[0]+'-'+table_id[1]+' ( '+'_'.join(doc_d.get(table_id[0], []))+' )', 'ph':ph})
            dd  = {'n':tmp_grp_name, 'data':f_ar, 'phs':phs, 'table_ar':t_ar, 'grpid':'_'.join(t_ids)+'-'+doc_grpid}
            g_i_ar.append(('HGHGROUP-'+i_table_type+'-'+'_'.join(t_ids)+'-'+doc_grpid, tmp_grp_name, ijson['user'], dtime))
            grp_ar.append(dd)
        tmpg_i_ar   = []
        with conn:
            #sql = "select max(taxo_id) from kpi_input"
            sql = "select seq from sqlite_sequence WHERE name = 'vgh_group_info'"
            cur.execute(sql)
            r       = cur.fetchone()
            g_id    = 1
            if r and r[0]:
                g_id    = int(r[0])+1
            
            sql = "select max(group_id) from vgh_group_info"
            cur.execute(sql)
            r       = cur.fetchone()
            tg_id   = 1
            if r and r[0]:
                tg_id    = int(r[0])+1
            g_id     = max(g_id, tg_id)
            for ng in g_i_ar:
                tmpg_i_ar.append((g_id, )+ng)
                g_id    += 1
        i_ar    = []
        with conn:
            #sql = "select max(taxo_id) from kpi_input"
            sql = "select seq from sqlite_sequence WHERE name = 'mt_data_builder'"
            cur.execute(sql)
            r       = cur.fetchone()
            g_id    = int(r[0])+1
            sql     = "select max(taxo_id) from mt_data_builder" 
            cur.execute(sql)
            r       = cur.fetchone()
            tg_id    = int(r[0])+1
            g_id    = max(g_id, tg_id)
            for t_d in t_ars:
                for t_id, vd in t_d.items():
                    for tup in vd['d']:
                        tup = (g_id, vd['o'])+tuple(tup)
                        tup = tup[:4]+(vd['t_l'], )+tup[5:]
                        i_ar.append(tup)
                    g_id    += 1
        cols    = 'taxo_id, order_id, table_type, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, gcom, ngcom, ph, ph_label, user_name, datetime, isvisible, m_rows, vgh_text, vgh_group, doc_id, xml_id, period, period_type, scale, currency, value_type, target_taxonomy, numeric_flg, th_flg'
        cols_v  =', '.join(map(lambda x:'?', cols.split(',')))
        if 1:#ijson.get('update_db', '') == 'Y':
            sql = "delete from mt_data_builder where table_type in (%s)"%(','.join(all_table_types.keys()))
            #print sql
            cur.execute(sql)
            sql = "delete from vgh_group_info where table_type in (%s)"%(','.join(all_table_types.keys()))
            #print sql
            cur.execute(sql)
            cur.executemany('insert into mt_data_builder(%s) values(%s)'%(cols, cols_v), i_ar)
            cur.executemany('insert into vgh_group_info(group_id, table_type, group_txt, user_name, datetime) values(?,?,?,?,?)', tmpg_i_ar)
            conn.commit()
        conn.close()
        res = [{'message':'done', 'data':grp_ar, 't_ids':ijson.get('t_ids', {})}]
        return res

    def form_HGH_group(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        i_table_type    = ijson['table_type']
        ph_d        = {}
        path    = "%s/%s/%s/1_1/21/sdata/doc_map.txt"%(self.doc_path, project_id, deal_id)
        if os.path.exists(path):
            fin = open(path, 'r')
            lines   = fin.readlines()
            fin.close()
        else:
            lines   = []
        doc_d       = {}
        dphs        = {}
        c_year      = self.get_cyear(lines)
        start_year  = c_year - int(ijson.get('year', 10))
        for line in lines[1:]:
            line    = line.split('\t')
            if len(line) < 8:continue
            line    = map(lambda x:x.strip(), line)
            ph      = line[3]+line[7]
            if ph and start_year<int(ph[2:]):
                doc_id  = line[0]
                doc_d[doc_id]   = (ph, line[2])
                dphs[ph]        = 1

                    

        i_table_type    = ijson['table_type']
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, [i_table_type])

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        lmdb_path   = '/var/www/html/Rajeev/BBOX/'+str(project_id)+'_'+str(deal_id)
        lmdb_path    = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        env1    = lmdb.open(lmdb_path, readonly=True)
        txn1    = env1.begin()

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn         = env.begin()


        table_type  = str(i_table_type)
        group_d     = {}
        revgroup_d  = {}

        d_group_d     = {}
        d_revgroup_d  = {}

        vgh_id_d    = {}
        db_file         = self.get_db_path(ijson)
        conn, cur   = conn_obj.sqlite_connection(db_file)
        #sql         = "select vgh_id, doc_id, table_id, group_txt from doc_group_map where table_type='%s'"%(table_type)
        #try:
        #    cur.execute(sql)
        #    res = cur.fetchall()
        #except:
        #    res = []
        grp_doc_map_d   = {}
        rev_doc_map_d   = {}
        #for rr in res:
        #    vgh_id, doc_id, table_id, group_txt = rr
        #    grp_doc_map_d.setdefault(group_txt, {})[table_id]   = doc_id #.setdefault(doc_id, {})[table_id]    = 1
        #    rev_doc_map_d.setdefault((doc_id, table_id), {})[group_txt] = 1
        #sql = "CREATE TABLE IF NOT EXISTS vgh_doc_map(row_id INTEGER PRIMARY KEY AUTOINCREMENT, vgh_group_id TEXT, doc_group_id TEXT, table_type VARCHAR(100), group_txt TEXT, user_name VARCHAR(100), datetime TEXT);"
        if ijson.get('vids', {}) and isinstance(ijson.get('vids', {}), list):
            ijson['vids']   = ijson['vids'][0]
        else:
            ijson['vids']   = {}
        g_vids  = ijson.get('vids', {})
        t_ids   = map(lambda x:str(x), ijson.get('t_ids', []))
        self.alter_table_coldef(conn, cur, 'mt_data_builder', ['target_taxonomy', 'numeric_flg', 'th_flg'])
        if ijson.get('vids', {}):
            sql         = "select row_id, taxo_id, prev_id, order_id, table_type, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, gcom, ngcom, ph, ph_label, user_name, datetime, isvisible, m_rows, vgh_text, vgh_group, doc_id, xml_id, period, period_type, scale, currency, value_type, target_taxonomy, numeric_flg, th_flg from mt_data_builder where table_type='%s' and taxo_id in (%s) and  isvisible='Y' and vgh_text in (%s)"%(table_type, ', '.join(t_ids),  ', '.join(ijson['vids'].keys()))
        else:
            sql         = "select row_id, taxo_id, prev_id, order_id, table_type, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, gcom, ngcom, ph, ph_label, user_name, datetime, isvisible, m_rows, vgh_text, vgh_group, doc_id, xml_id, period, period_type, scale, currency, value_type, target_taxonomy, numeric_flg, th_flg from mt_data_builder where table_type='%s' and taxo_id in (%s) and isvisible='Y'"%(table_type, ', '.join(t_ids))
        cur.execute(sql)
        res         = cur.fetchall()
        docinfo_d   = {}
        vgh_id_d_all    = {}
        tmp_grpd        = {}
        table_ids       = {}
        all_vgh         = {}
        f_d             = {}
        f_o_d          = {}
        for rr in res:
            row_id, taxo_id, prev_id, order_id, table_type, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, gcom, ngcom, ph, ph_label, user_name, tdatetime, isvisible, m_rows, vgh_text, vgh_group, doc_id, xml_id, period, period_type, scale, currency, value_type, target_taxonomy, numeric_flg, th_flg   = rr
            tk   = self.get_quid(table_id+'_'+xml_id)
            c_id        = txn_m.get('XMLID_MAP_'+tk)
            if not c_id:continue
            c   = int(c_id.split('_')[2])
            if g_vids.get(vgh_text, {}) and table_id+'-'+str(c) not in g_vids.get(vgh_text, {}):continue
            rr  = (table_type, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, gcom, ngcom, ph, ph_label, user_name, tdatetime, isvisible, m_rows, vgh_text, vgh_group, doc_id, xml_id, period, period_type, scale, currency, value_type, target_taxonomy, numeric_flg, th_flg)
            f_d.setdefault(taxo_id, {})[rr] = 1
            f_o_d[taxo_id]   = order_id
        tmpt_d          = {}
        all_table_types = {}
        for taxo_id, ks in f_d.items():
            t_ar    = []
            for rr in ks:
                rr      = list(rr)
                rr[0]   =  'HGHGROUP-'+i_table_type+'-'+'_'.join(t_ids)+'-HGH'
                all_table_types['"'+rr[0]+'"'] = {}
                t_ar.append(rr)
            tmpt_d[taxo_id]  = {'d':t_ar, 'o':f_o_d[taxo_id]}
        t_ars   = []
        t_ars.append(tmpt_d)    
        g_i_ar  = []
        tmp_grp_name    = ijson['grp_name']
        dtime   = str(datetime.datetime.now()).split('.')[0]
        g_i_ar.append(('HGHGROUP-'+i_table_type+'-'+'_'.join(t_ids)+'-HGH', tmp_grp_name, ijson['user'], dtime))
        tmpg_i_ar   = []
        with conn:
            #sql = "select max(taxo_id) from kpi_input"
            sql = "select seq from sqlite_sequence WHERE name = 'vgh_group_info'"
            cur.execute(sql)
            r       = cur.fetchone()
            g_id    = 1
            if r and r[0]:
                g_id    = int(r[0])+1
            
            sql = "select max(group_id) from vgh_group_info"
            cur.execute(sql)
            r       = cur.fetchone()
            tg_id   = 1
            if r and r[0]:
                tg_id    = int(r[0])+1
            g_id     = max(g_id, tg_id)
            for ng in g_i_ar:
                tmpg_i_ar.append((g_id, )+ng)
                g_id    += 1
        i_ar    = []
        with conn:
            #sql = "select max(taxo_id) from kpi_input"
            sql = "select seq from sqlite_sequence WHERE name = 'mt_data_builder'"
            cur.execute(sql)
            r       = cur.fetchone()
            g_id    = int(r[0])+1
            sql     = "select max(taxo_id) from mt_data_builder" 
            cur.execute(sql)
            r       = cur.fetchone()
            tg_id    = int(r[0])+1
            g_id    = max(g_id, tg_id)
            for t_d in t_ars:
                for t_id, vd in t_d.items():
                    #print  t_id, vd
                    for tup in vd['d']:
                        #print '\n\tBB ',tup
                        tup = (g_id, vd['o'])+tuple(tup)
                        #print '\t',tup
                        i_ar.append(tup)
                    g_id    += 1
        cols    = 'taxo_id, order_id, table_type, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, gcom, ngcom, ph, ph_label, user_name, datetime, isvisible, m_rows, vgh_text, vgh_group, doc_id, xml_id, period, period_type, scale, currency, value_type, target_taxonomy, numeric_flg, th_flg'
        cols_v  =', '.join(map(lambda x:'?', cols.split(',')))
        if 1:#ijson.get('update_db', '') == 'Y':
            sql = "delete from mt_data_builder where table_type in (%s)"%(','.join(all_table_types.keys()))
            #print sql
            cur.execute(sql)
            sql = "delete from vgh_group_info where table_type in (%s)"%(','.join(all_table_types.keys()))
            #print sql
            cur.execute(sql)
            #print 'insert into mt_data_builder(%s) values(%s)'%(cols, cols_v)
            #for rr in i_ar:
            #    print rr
            cur.executemany('insert into mt_data_builder(%s) values(%s)'%(cols, cols_v), i_ar)
            cur.executemany('insert into vgh_group_info(group_id, table_type, group_txt, user_name, datetime) values(?,?,?,?,?)', tmpg_i_ar)
            conn.commit()
        conn.close()
        res = [{'message':'done'}]
        return res

    def order_by_table_structure_by_col(self, f_taxo_arr, table_ids):
        tmptable_ids   = map(lambda x:x[1], table_ids)
        table_match_d   = {}
        for ti, dd in enumerate(f_taxo_arr):
            ks      = dd['ks']
            for table_id, c_id in ks:
                table_match_d.setdefault(table_id, {}).setdefault(ti, {})[c_id] = 1
        tabl_ids    = table_match_d.keys()
        tabl_ids.sort(key=lambda x:tmptable_ids.index(x))
        #tabl_ids.sort(key=lambda x:len(table_match_d[x].keys()), reverse=True)
        final_arr   = []
        for table_id in tabl_ids:
            inds    = table_match_d[table_id].keys()
            inds.sort()
            #print '\n==========================================================='
            #print table_id, sorted(inds)
            inds.sort(key=lambda x:int(table_match_d[table_id][x].keys()[0].split('_')[2]))
            #print 'Ordered ', inds
            if not final_arr:
                final_arr   = inds
            else:
                m_d = list(sets.Set(final_arr).intersection(sets.Set(inds)))
                deletion    = {}
                tmp_arr     = []
                ftmp_arr     = []
                for t in inds:
                    if t in m_d:
                        ftmp_arr    = []
                        if tmp_arr:
                            deletion[t] = copy.deepcopy(tmp_arr[:])
                            #tmp_arr = []    
                        continue
                    tmp_arr.append(t)
                    ftmp_arr.append(t)
                done_d  = {}
                m_d.sort(key=lambda x:final_arr.index(x))
                
                for t in m_d:
                    if t in deletion:
                        tmp_arr = []
                        for t1 in deletion[t]:
                            if t1 not in done_d:
                                tmp_arr.append(t1)
                                done_d[t1]  = 1
                        index       = final_arr.index(t)
                        final_arr   = final_arr[:index]+tmp_arr+final_arr[index:]    
                
                if ftmp_arr:
                    final_arr   = final_arr+ftmp_arr
            
            #print 'FINAL ', final_arr
        missing = sets.Set(range(len(f_taxo_arr))) - sets.Set(final_arr)
        if len(final_arr) > len(f_taxo_arr):
            print 'Duplicate ', final_arr
            sys.exit()
        if len(missing):
            print 'missing ', list(missing)
            sys.exit()
        f_taxo_arr  = map(lambda x:f_taxo_arr[x], final_arr)
        return f_taxo_arr

    def update_gcom_flg(self, ijson):
        #sys.exit()
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        db_file         = self.get_db_path(ijson)
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        conn, cur       = conn_obj.sqlite_connection(db_file)
        try:
            sql = "alter table mt_data_builder add column xml_id TEXT"
            cur.execute(sql)
        except:pass
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn         = env.begin()
        if ijson.get('table_ids', []):
            sql = "select row_id, c_id, xml_id, table_id, ph from mt_data_builder where table_id in (%s)"%(', '.join(map(lambda x:str(x), ijson.get('table_ids', []))))
        elif ijson.get('table_types', []):
            sql = "select row_id, c_id, xml_id, table_id, ph from mt_data_builder where table_type in (%s)"%(', '.join(map(lambda x:'"'+str(x)+'"', ijson.get('table_types', []))))
        else:
            sql = "select row_id, c_id, xml_id, table_id, ph from mt_data_builder"
        cur.execute(sql)
        res = cur.fetchall()
        u_ar    = []
        for rr in res:
            row_id, c_id, xml_id, table_id, ph    = rr
            if table_id and xml_id:
                table_id = str(table_id)
                key     = table_id+'_'+self.get_quid(xml_id)
                gcom_f  = 'N'
                nggcom_f  = 'N'
                for rc in ['COL', 'ROW']:
                    ngcom        = txn.get('COM_NG_'+rc+'MAP_'+key)
                    if ngcom:
                        for eq in ngcom.split('|'):
                            nggcom_f    = 'Y'
                            break
                    
                    gcom        = txn.get('COM_G_'+rc+'MAP_'+key)
                    if gcom:
                        for eq in gcom.split('|'):
                            gcom_f  = 'Y'
                            break
                if gcom_f == 'Y' or nggcom_f == 'Y':
                    u_ar.append((nggcom_f, gcom_f, row_id))
        #for rr in u_ar:
        #    print rr
        #sys.exit()
        print 'Total ', len(u_ar)
        cur.executemany("update mt_data_builder set ngcom=?, gcom=? where row_id=?", u_ar)
        conn.commit()
        conn.close()
    ## 57 T / H Save for Excel View
    def save_ttype_taxo_info(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        taxo_ids        = ijson["t_ids"]
        th_flg          = ijson['key']
        grp_id          = ijson['grpid']
        user_name       = ijson['user']
        table_type      = ijson['table_type']
        company_id      = "%s_%s"%(project_id, deal_id)
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        create_stmt = "CREATE TABLE IF NOT EXISTS ph_derivation (row_id INTEGER PRIMARY KEY AUTOINCREMENT, formula VARCHAR(256), table_type VARCHAR(100), group_id VARCHAR(50), ph VARCHAR(10), formula_type VARCHAR(20), formula_str VARCHAR(256), user_name VARCHAR(100));"
        cur.execute(create_stmt)

        i_ar = []
        d_ar = []
        for tid in taxo_ids:
            i_ar.append((th_flg, table_type, grp_id, tid, 'TTYPE', '', user_name))
            d_ar.append((table_type, grp_id, tid, 'TTYPE'))

        cur.executemany('delete from ph_derivation where table_type = ? and group_id = ? and ph = ? and formula_type = ?', d_ar)

        cur.executemany('insert into ph_derivation (formula, table_type, group_id, ph, formula_type, formula_str, user_name) values (%s)'%(','.join(map(lambda x:'?', i_ar[0]))), i_ar)
        conn.commit()
        conn.close()
        return [{'message':'done'}]

    ## 57 T / H Save for Excel View
    def save_header_resultant_taxo_info(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        taxo_ids        = ijson["t_ids"]
        th_flg          = ijson['key']
        grp_id          = ijson['grpid']
        user_name       = ijson['user']
        table_type      = ijson['table_type']
        company_id      = "%s_%s"%(project_id, deal_id)
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        create_stmt = "CREATE TABLE IF NOT EXISTS ph_derivation (row_id INTEGER PRIMARY KEY AUTOINCREMENT, formula VARCHAR(256), table_type VARCHAR(100), group_id VARCHAR(50), ph VARCHAR(10), formula_type VARCHAR(20), formula_str VARCHAR(256), user_name VARCHAR(100));"
        cur.execute(create_stmt)

        i_ar = []
        d_ar = []
        for tid in taxo_ids:
            i_ar.append((th_flg, table_type, grp_id, tid, 'TH', '', user_name))
            d_ar.append((table_type, grp_id, tid, 'TH'))

        cur.executemany('delete from ph_derivation where table_type = ? and group_id = ? and ph = ? and formula_type = ?', d_ar)

        cur.executemany('insert into ph_derivation (formula, table_type, group_id, ph, formula_type, formula_str, user_name) values (%s)'%(','.join(map(lambda x:'?', i_ar[0]))), i_ar)
        conn.commit()
        conn.close()
        return [{'message':'done'}]

    def validate_table_data(self, table_id, txn):
        lkey        = 'GV_'+table_id
        ids         = txn.get(lkey)
        if not ids:return 'GV NOT FOUND'
        r_d         = {}
        for c_id in ids.split('#'):
            t_id, r, c  = c_id.split('_')
            r               = int(r)
            c               = int(c)
            rs              = int(txn.get('rowspan_'+c_id))
            cs              = int(txn.get('colspan_'+c_id))
            r_d.setdefault(r, {})[c]    = (rs, cs)
            for ri in range(int(rs)):
                r_d.setdefault(r+ri, {})[c]   = (1, 1)
            #for ci in range(int(cs)):
            #    r_d[r][c+ci]   = (1, 1)
        rows    = r_d.keys()
        rows.sort()
        f_d = {}
        for r in rows:
            cols    = r_d[r].keys()
            cols.sort()
            #print r, cols
            f_d.setdefault(tuple(cols),{})[r]    = 1
        if len(f_d.keys()) > 1:
            lkey        = 'HGH_'+table_id
            ids         = txn.get(lkey)
            r_d         = {}
            for c_id in ids.split('#'):
                t_id, r, c  = c_id.split('_')
                r               = int(r)
                c               = int(c)
                text            = binascii.a2b_hex(txn.get('TEXT_'+c_id))
                if text:
                    r_d.setdefault(r, []).append(text)
            tmp_arr = []
            for k, v in f_d.items():
                rows    = v.keys()
                rows.sort()
                txt = ''
                tmpr    = rows[0]
                for r in rows:
                    if r in r_d:
                        txt = r_d[r]
                        tmpr    = r
                        break 
                tmp_arr.append((tmpr, txt, k))
            return sorted(tmp_arr)
        return True

    def validate_normalization(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        ph_d        = {}
        path    = "%s/%s/%s/1_1/21/sdata/doc_map.txt"%(self.doc_path, project_id, deal_id)
        if os.path.exists(path):
            fin = open(path, 'r')
            lines   = fin.readlines()
            fin.close()
        else:
            lines   = []
        doc_d       = {}
        dphs        = {}
        c_year      = self.get_cyear(lines)
        #start_year  = c_year - int(ijson['year'])
        start_year  = c_year - int(ijson.get('year', 5))
        for line in lines[1:]:
            line    = line.split('\t')
            if len(line) < 8:continue
            line    = map(lambda x:x.strip(), line)
            ph      = line[3]+line[7]
            try:
                year    = int(ph[-4:])
            except:continue
            if ph and start_year < year:
                doc_id  = line[0]
                if ijson.get('ignore_doc_ids', {}) and doc_id in  ijson.get('ignore_doc_ids', {}):continue
                doc_d[doc_id]   = (ph, line[2])
                dphs[ph]        = 1

                    

        table_type    = ijson.get('table_type', '')
        if table_type:
            m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, [table_type])
        else:
            m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, [])

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        db_file         = self.get_db_path(ijson)
        conn, cur   = conn_obj.sqlite_connection(db_file)
        if table_type:
            sql         = "select table_id, doc_id,xml_id from mt_data_builder where table_type='%s' and isvisible='Y'"%(table_type)
        else:
            sql         = "select table_id, doc_id,xml_id from mt_data_builder where isvisible='Y'"
        cur.execute(sql)
        res = cur.fetchall()
        table_ids   = {}
        for rr in res:
            table_id, doc_id,xml_id = rr
            doc_id      = str(doc_id)
            if doc_id not in doc_d:continue
            table_id    = str(table_id)
            if table_id not in m_tables:continue
            tk   = self.get_quid(table_id+'_'+xml_id)
            c_id        = txn_m.get('XMLID_MAP_'+tk)
            if not c_id:continue
            table_ids[(doc_id, table_id)]   = 1
        error_tables    = []
        done_tables     = []
        for doc_id, table_id in table_ids.keys():
            is_valid    = self.validate_table_data(table_id, txn_m)
            if is_valid == True:
                dd  = {'d':doc_id, 't':table_id, 'n':doc_id+'-'+str(table_id)+'_'.join(doc_d[doc_id]), 'error':is_valid}
                done_tables.append(dd)
                continue
            dd  = {'d':doc_id, 't':table_id, 'n':doc_id+'-'+str(table_id)+'_'.join(doc_d[doc_id]), 'error':is_valid}
            error_tables.append(dd)
        res = [{'message':'done','data':error_tables, 'done_tables':len(done_tables), 't':len( table_ids.keys())}]
        return res
            

    ## SFORMULA DATA
    def get_sFormula_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        group_id        = ijson["grpid"]
        table_type      = ijson['table_type']

        db_file         = self.get_db_path(ijson)
        conn, cur   = conn_obj.sqlite_connection(db_file)
 
        if group_id: 
            sql         = 'select row_id, formula from ph_derivation where table_type="%s" and formula_type="SFORMULA" and group_id="%s"'%(table_type, group_id)
        elif not group_id:
            sql         = 'select row_id, formula, group_id from ph_derivation where table_type="%s" and formula_type="SFORMULA"'%(table_type)
        cur.execute(sql)
        res         = cur.fetchall()
        conn.close()
        formula_d   = []
        for rr in res:
            if group_id:
                row_id, formula = map(str, rr)
            elif not group_id:
                row_id, formula, gid = map(str, rr) 
                if gid:
                    continue
            operands    = []
            for topr in formula.split('$$'):
                tid, operator, t_type, g_id, ftype = topr.split('@@')
                tmp = {'tid':tid, 's':operator, 'R':'N'}
                if operator == '=':
                    tmp['rid']  = row_id
                    tmp['R']    = 'Y'
                operands.append(tmp)
            formula_d.append(operands)
        return formula_d

    def print_table_types(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        ijson.update({"eq":"N","O":"Y"})
        m_tables, rev_m_tables, doc_m_d, table_type_m = self.get_main_table_info(company_name, model_number, ijson.get('table_types', []))
        t_types = {}
        for tid in ijson['table_ids']:
            t_types[m_tables[str(tid)]] = 1
        print t_types.keys()
           
    def get_sheet_id_map(self):
        db_file     = '/mnt/eMB_db/node_mapping.db'
        conn, cur   = conn_obj.sqlite_connection(db_file)
        sql   = "select sheet_id, node_name from node_mapping where review_flg = 0"
        try:
            cur.execute(sql)
            tres        = cur.fetchall()
        except:
            tres    = []
        conn.close()
        #print rr, len(tres)
        ddict = dd(set)
        for tr in tres:
            sheet_id, node_name = map(str, tr)
            ddict[sheet_id] = node_name
        return ddict

    def clean_actual_value_tableid_data(self, conn, cur):
        crt_qry = "CREATE TABLE IF NOT EXISTS clean_actual_value_status(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_id VARCHAR(100), doc_id VARCHAR(100), table_type VARCHAR(100), value_status VARCHAR(20));"
        cur.execute(crt_qry)
        
        read_qry = 'select table_id, table_type from clean_actual_value_status;'
        cur.execute(read_qry)   
        actual_data = [tuple(map(str, tup))for tup in cur.fetchall()]
        return actual_data
    def get_distinct_docids(self, conn, cur):
        sql             = "select doc_id from company_meta_info"
        cur.execute(sql)
        res             = cur.fetchall()
        doc_d   = {}
        for rr in res:
            doc_d[str(rr[0])]   = 1
        return doc_d

    def tt_group_display_name_info_save(self, ijson):
        save_data         = ijson['save_data']

        db_file           = self.get_db_path(ijson)
        conn, cur         = conn_obj.sqlite_connection(db_file)

        #sql = "CREATE TABLE IF NOT EXISTS vgh_group_info(row_id INTEGER PRIMARY KEY AUTOINCREMENT, group_id VARCHAR(20), table_type VARCHAR(100), group_txt VARCHAR(20), user_name VARCHAR(100), datetime TEXT);"
        crt_qry = 'CREATE TABLE IF NOT EXISTS tt_group_display_name_info(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_type TEXT, group_id TEXT, group_name TEXT, display_name TEXT)'
        cur.execute(crt_qry)
        conn.commit()
     
        read_qry = 'SELECT table_type, group_id FROM tt_group_display_name_info;'
        cur.execute(read_qry)
        check_data = {row:1 for row in cur.fetchall()}
       
        update_rows = []
        insert_rows = [] 
        for rw_dct in save_data:
            tt, grp_id, group_name, n_dis = rw_dct['p'], rw_dct['grpid'], rw_dct['n'], rw_dct['n_dis'] 
            n_dis = n_dis.encode('utf-8')
            if (tt, grp_id) not in check_data:
                insert_rows.append((tt, grp_id, group_name, n_dis))
            elif (tt, grp_id) in check_data:
                update_rows.append((n_dis, tt, grp_id))
        if update_rows:
            cur.executemany("UPDATE tt_group_display_name_info SET display_name=? WHERE table_type=? and group_id=?", update_rows)
        if insert_rows:
            cur.executemany("INSERT INTO tt_group_display_name_info(table_type, group_id, group_name, display_name) VALUES(?, ?, ?, ?)", insert_rows)
        conn.commit()
        conn.close()
        return [{'message':'done'}] 

    def get_tt_group_display_name_info(self, ijson):

        db_file           = self.get_db_path(ijson)
        conn, cur         = conn_obj.sqlite_connection(db_file)

        #sql = "CREATE TABLE IF NOT EXISTS vgh_group_info(row_id INTEGER PRIMARY KEY AUTOINCREMENT, group_id VARCHAR(20), table_type VARCHAR(100), group_txt VARCHAR(20), user_name VARCHAR(100), datetime TEXT);"
        crt_qry = 'CREATE TABLE IF NOT EXISTS tt_group_display_name_info(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_type TEXT, group_id TEXT, group_name TEXT, display_name TEXT)'
        cur.execute(crt_qry)
        conn.commit()
            
        read_qry = 'SELECT table_type, group_id, display_name FROM tt_group_display_name_info;'
        cur.execute(read_qry)
        table_data = cur.fetchall()
        conn.close()

        tt_group_disp_nm_info = {}
        for row in table_data:
            table_type, group_id, display_name = row[:]
            if not display_name:display_name = ''
            if not group_id:
                tt_group_disp_nm_info.setdefault(table_type, {})['n_dis'] = display_name
            elif group_id:
                tt_group_disp_nm_info.setdefault(table_type, {})[group_id] = display_name
        return tt_group_disp_nm_info
    
    def get_docid_docname_map(self, company_id):
         
        m_conn = MySQLdb.connect('172.16.20.229', 'root', 'tas123', 'tfms_urlid_%s'%(company_id))
        m_cur = m_conn.cursor() 
        read_qry = """ SELECT document_id, document_name FROM ir_document_master WHERE active_status='Y' """       
        m_cur.execute(read_qry)
        t_data = m_cur.fetchall()
        m_conn.close()
        did_dn_map = {str(row[0]):str(row[1]) for row in t_data}
        return did_dn_map
        
 
    def get_remaining_tables(self, company_id, classified_set, source_table_data):
        project_id, url_id = company_id.split('_')
        did_dcn_map = self.get_docid_docname_map(company_id)
        import model_view.read_norm_cell_data as Slt_normdata
        sObj = Slt_normdata.Slt_normdata()
        data_lst = sObj.slt_normresids(project_id, url_id)
        all_table_dct = {}
        for row in data_lst:
            did, pg, tid = map(str, row)
            all_table_dct[tid] = did
        #import model_view.cp_company_docTablePh_details as py
        #pObj = py.Company_docTablePh_details()
        #all_table_dct = pObj.get_docId_passing_tableId(company_id)
        get_remaining = set(all_table_dct) - set(classified_set)
        res_lst = []
        for tab in get_remaining:   
            get_dc = all_table_dct.get(tab, '')
            get_dn = did_dcn_map.get(get_dc, '')
            source_info = source_table_data.get(tab, {})
            data_dct = {'t':tab, 'd':get_dc, 'tt':'Not Classified', 'dn':get_dn, 'act_f':'N', 'source_info':source_info, 'rid':''}
            res_lst.append(data_dct)
        return res_lst

    def read_doc_types(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        db_path = '/mnt/eMB_db/%s/%s/tas_company.db'%(company_name, model_number)
        conn = sqlite3.connect(db_path)
        cur  = conn.cursor()
        read_qry = """ SELECT distinct(document_type) FROM company_meta_info; """
        cur.execute(read_qry)
        t_data   = cur.fetchall()
        conn.close()
        res_lst = []
        for row in t_data:
            d_type = row[0]
            res_lst.append(d_type)
        return res_lst 
    
    ## 61 Classification Data
    def get_classified_tables_info(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        table_type      = ijson['table_type']
        sheet_id_map    = self.get_sheet_id_map()

        db_file     = '/mnt/eMB_db/%s/%s/tas_company.db'%(company_name, model_number)
        conn, cur   = conn_obj.sqlite_connection(db_file)

        sql   = "select row_id, sheet_id, doc_id, doc_name, table_id from table_group_mapping;"
        try:
            cur.execute(sql)
            tres        = cur.fetchall()
        except:
            tres    = []
        all_valid_documents = self.get_distinct_docids(conn, cur)
        dbf = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn1 = sqlite3.connect(dbf)
        cur1 = conn1.cursor()
        actual_tables = self.clean_actual_value_tableid_data(conn1, cur1) 
        conn1.commit()
        conn1.close()
        conn.close()
        source_table_data = {}
        if project_id in ('20', ):
           source_table_data =  self.source_table_information(company_id)    
            
        data_rows = []
        classified_set = {}
        for row in tres:
            row = map(str, row)
            row_id, sheet_id, doc_id, doc_name, table_id_str = row
            if doc_id not in all_valid_documents:continue
            table_id_li = table_id_str.split('^!!^')
            for tid in table_id_li:
                if not tid.strip():continue
                tup = tuple([tid, sheet_id_map.get(sheet_id, '')])
                ac = 'N'
                if tup in actual_tables:
                    ac = 'Y'
                classified_set[tid] = doc_id
                source_info = source_table_data.get(tid, {})
                data_rows.append({'t':tid, 'd':doc_id, 'tt':sheet_id_map.get(sheet_id, ''), 'dn':doc_name, 'act_f':ac, 'source_info':source_info, 'rid':row_id})
        
        get_remaining = self.get_remaining_tables(company_id, classified_set, source_table_data)
        data_rows += get_remaining
        tt_group_dict = self.get_tt_sub_groups_info(ijson)  
        companyConfig_data = self.read_companyConfigTable(ijson)
        formula_lst = self.show_all_table_types_and_formulas_company()
        dn_info = self.get_tt_group_display_name_info(ijson)
        doc_type_info      = self.read_doc_types(ijson)
        return [{'message':'done', 'data':data_rows, 'tt_data':tt_group_dict, 'cf_data':companyConfig_data, 'fl':formula_lst, 'dn_info':dn_info, 'doc_type_info':doc_type_info}]
        #return [{'message':'done', 'data':data_rows, 'tt_data':tt_group_dict, 'cf_data':companyConfig_data}]
        
    def read_inc_project_id(self, db_name):
        m_conn = MySQLdb.connect('172.16.20.52', 'root', 'tas123', 'WorkSpaceDb_DB')
        m_cur = m_conn.cursor()
        read_qry = """ SELECT ProjectID FROM ProjectMaster WHERE ProjectCode='%s'; """%(db_name)
        m_cur.execute(read_qry)
        t_data = m_cur.fetchone()
        m_conn.close()
        inc_pid = t_data[0]
        return inc_pid

    def get_source_table_info(self, ijson):
        project_id = ijson['project_id']
        deal_id    = ijson['deal_id']
        company_id  = '%s_%s'%(project_id, deal_id)
        data  = ijson['table_ids']
        table_str = ', '.join(['"'+str(e)+'"'for e in data])
        m_conn = MySQLdb.connect('172.16.20.229', 'root', 'tas123', 'tfms_urlid_%s'%(company_id))
        m_cur = m_conn.cursor()
        read_qry = """ SELECT norm_resid, source_table_info, source_type FROM norm_data_mgmt WHERE norm_resid in (%s); """%(table_str)
        try:
            m_cur.execute(read_qry)
            t_data = m_cur.fetchall()
        except:t_data=()
        m_conn.close() 
        source_table_data = {}
        for r in t_data:
            if not r[1]:continue
            tab, source_table_info, stype = str(r[0]), eval(r[1]), str(r[2])
            doc_id, pg, grid_id = source_table_info
            inc_pid = self.read_inc_project_id(stype)
            st_dct = {'d':doc_id, 'p':pg, 'g':grid_id, 'db_string':stype, 'p_id':inc_pid}
            source_table_data[tab] = st_dct
        res = [{'message':'done', 'data':source_table_data}]
        return res 
        
    def source_table_information(self, company_id):
        m_conn = MySQLdb.connect('172.16.20.229', 'root', 'tas123', 'tfms_urlid_%s'%(company_id))
        m_cur = m_conn.cursor() 
        read_qry = """ SELECT norm_resid, source_table_info FROM norm_data_mgmt; """       
        m_cur.execute(read_qry)
        t_data = m_cur.fetchall()
        m_conn.close()
        source_table_data = {}
        for r in t_data:
            tab, source_table_info = str(r[0]), eval(r[1])
            doc_id, pg, grid_id = source_table_info
            st_dct = {'s_doc':doc_id, 's_pg':pg, 's_grid':grid_id}
            source_table_data[tab] = st_dct
        return source_table_data
    

    
    def gen_DB_data_builder(self, ijson):
        if ijson['user'].strip() not in self.gen_users:
            res = [{'message':'Error : Permission Denied for '+ijson['user']}]
            return res
            
        import create_table_seq
        obj = create_table_seq.TableSeq()
        if ijson.get('PRINT', '') != 'Y':
            disableprint()
        ijson['re_run'] = 'Y'
        ijson['Taxo'] = 'Y'
        if ijson.get('ph', '') == 'Y':
            if ijson.get('PRINT', '') != 'Y':
                ijson['print'] = 'N'
            #self.run_ph_csv(copy.deepcopy(ijson))
        if ijson.get('table_types', []):
            obj.index_seq_across_mt_taxo(ijson)
        enableprint()
        res = [{'message':'done'}]
        return res

    def add_new_table_DB(self, ijson):
        if ijson['user'].strip() not in self.gen_users:
            res = [{'message':'Error : Permission Denied for '+ijson['user']}]
            return res
        disableprint()
        ijson['Taxo'] = 'Y'
        ijson['re_run'] = 'Y'
        if ijson.get('ph', '') == 'Y':
            ijson['print'] = 'N'
            #self.run_ph_csv(copy.deepcopy(ijson))
        self.add_new_table(ijson)
        enableprint()
        res = [{'message':'done'}]
        return res

    def update_ph_DB(self, ijson):
        if ijson['user'].strip() not in self.gen_users:
            res = [{'message':'Error : Permission Denied for '+ijson['user']}]
            return res
        if ijson.get('PRINT', '') != 'Y':
            disableprint()
            ijson['re_run'] = 'Y'
            ijson['print']  = 'N'
        ijson['PH']     = 'Y'
        ijson['print']  = 'N'
        self.run_ph_csv(ijson)
        enableprint()
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        import model_view.table_tagging_v2 as TableTagging
        Omobj = TableTagging.TableTagging()
        table_id_cell_dict = Omobj.all_user_selected_references(company_name, model_number, company_id)
        d_ar            = []
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()
        for table_id, xmls in table_id_cell_dict.items():
            table_id    = str(table_id)
            ignore_col  = {}
            k   = 'GV_'+table_id
            ids = txn_m.get(k)
            if not ids:continue
            ids         = ids.split('#')
            col_d   = {}
            for c_id in ids:
                r       = int(c_id.split('_')[1])
                c       = int(c_id.split('_')[2])
                x       = txn_m.get('XMLID_'+str(c_id))
                col_d.setdefault(c, {})[x]  = 1
            for xml_id in xmls:
                xml_id  = '#'.join(filter(lambda x:x, xml_id.split('#')))
                tk      = self.get_quid(table_id+'_'+xml_id)
                c_id    = txn_m.get('XMLID_MAP_'+tk)
                if not c_id:continue
                c       = int(c_id.split('_')[2])
                cs      = int(txn_m.get('colspan_'+c_id))
                for c_s in range(cs): 
                    for x in col_d.get(c+c_s, {}).keys():
                        d_ar.append((table_id, x))
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        cur.executemany("update mt_data_builder set isvisible='N' where table_id=? and xml_id=?", d_ar)  
        conn.commit()
        conn.close()
        res = [{'message':'done'}]
        return res

    def gen_taxo_grp(self, ijson):
        import data_builder.taxo_group as taxo_group
        taxo_group_obj  = taxo_group.TaxoGroup()
        ijson['from_web_api']   = 'Y'
        taxo_group_obj.create_taxo_grp(ijson)
    def gen_sh_taxo_grp(self, ijson):
        import data_builder.taxo_group_update as taxo_group
        taxo_group_obj  = taxo_group.TaxoGroup()
        if ijson.get('PRINT', '') != 'Y':
            disableprint()
        res = taxo_group_obj.match_SH_group(ijson)
        enableprint()
        return res


    def validate_ph_csv(self, ijson):
        import data_builder.taxo_group as taxo_group
        taxo_group_obj  = taxo_group.TaxoGroup()
        taxo_group_obj.validate_ph_csv(ijson)

    def gen_taxo_grp_intf(self, ijson):
        if ijson['user'].strip() not in self.gen_users:
            res = [{'message':'Error : Permission Denied for '+ijson['user']}]
            return res
        import data_builder.taxo_group as taxo_group
        taxo_group_obj  = taxo_group.TaxoGroup()
        disableprint()
        ijson['re_run'] = 'Y'
        ijson['Taxo'] = 'Y'
        if ijson.get('ph', '') == 'Y':
            ijson['print'] = 'N'
            self.run_ph_csv(copy.deepcopy(ijson))
        if ijson.get('table_types', []):
            ijson['from_web_api']   = 'Y'
            taxo_group_obj.create_taxo_grp(ijson)
        elif ijson.get('table_ids', []):
            ijson['from_web_api']   = 'Y'
            taxo_group_obj.create_taxo_grp(ijson)
        enableprint()
        res = [{'message':'done'}]
        return res

    def gen_taxo_grp_ph(self, ijson):
        import data_builder.taxo_group as taxo_group
        taxo_group_obj  = taxo_group.TaxoGroup()
        ijson['lgroup'] = 'Y'
        ijson['taxo_flg'] = 1
        ijson['table_types'] = [ijson['table_type']]
        disableprint()
        ijson['from_web_api']   = 'Y'
        taxo_group_obj.create_taxo_grp(ijson)
        enableprint()
        res = self.create_seq_across(ijson)
        return res

    def gen_taxo_grp_auto_reorder(self, ijson):
        import data_builder.taxo_group as taxo_group
        taxo_group_obj  = taxo_group.TaxoGroup()
        ijson["lgroup_auto_order"] = 'Y'
        ijson['taxo_flg'] = 1
        ijson['table_types'] = [ijson['table_type']]
        disableprint()
        ijson['from_web_api']   = 'Y'
        taxo_group_obj.create_taxo_grp(ijson)
        enableprint()
        res = self.create_seq_across(ijson)
        return res

    def restore_flip_table(self, ijson):
        company_name = ijson["company_name"]
        project_id  = ijson["project_id"]
        deal_id     = ijson["deal_id"]
        model_number= ijson["model_number"]
        machine_id  = '122'
        company_id  = '%s_%s'%(project_id, deal_id)
        table_ids   = ijson['table_ids']
        if not table_ids:
            return [{'message':'Error empty table list'}]
        disableprint()
        import index_all_table
        i_obj = index_all_table.Index()
        notm_d  = i_obj.index_table_norm_info(company_id, list(sets.Set(table_ids)), 'N')
        enableprint()
        for tid in table_ids:
            org_path    = '%s/%s/Table_Htmls/%s_org.html'%(self.output_path, company_id, tid)
            cpath       = '%s/%s/Table_Htmls/%s.html'%(self.output_path, company_id, tid)
            if os.path.exists(org_path):
                os.system("cp %s %s"%(org_path, cpath))
        lmdb_path2      =  "/mnt/eMB_db/%s/%s/table_phcsv_data"%(company_name, model_number)
        env1             = lmdb.open(lmdb_path2, map_size=2**39)
        with env1.begin(write=True) as txn:
            for tid in table_ids:
                txn.put('FLIP_'+str(tid), 'N')
        lmdb_path2      =  "/mnt/eMB_db/%s/%s/default_table_phcsv_data"%(company_name, model_number)
        env1             = lmdb.open(lmdb_path2, map_size=2**39)
        with env1.begin(write=True) as txn:
            for tid in table_ids:
                txn.put('FLIP_'+str(tid), 'N')
        res = [{'message':'done'}]
        return res
        ijson['Taxo'] = 'Y'
        ijson['print'] = 'N'
        ijson['re_run'] = 'Y'
        disableprint()
        self.run_ph_csv(copy.deepcopy(ijson))
        enableprint()
        res = [{'message':'done'}]
        return res
        if ijson.get('taxo_flg', '') == 1:
            import data_builder.taxo_group as taxo_group
            taxo_group_obj  = taxo_group.TaxoGroup()
            disableprint()
            #ijson['re_run'] = 'Y'
            if ijson.get('table_types', []):
                ijson['from_web_api']   = 'Y'
                taxo_group_obj.create_taxo_grp(ijson)
            elif ijson.get('table_ids', []):
                ijson['from_web_api']   = 'Y'
                taxo_group_obj.create_taxo_grp(ijson)
            enableprint()
            res = [{'message':'done'}]
            return res
        else:
            disableprint()
            ijson['Taxo'] = 'Y'
            self.add_new_table(ijson)
            enableprint()
            res = [{'message':'done'}]
            return res


    def gen_triplet(self, ijson, table_ids, flg):
        company_name = ijson["company_name"]
        project_id  = ijson["project_id"]
        deal_id     = ijson["deal_id"]
        model_number= ijson["model_number"]
        company_id  = '%s_%s'%(project_id, deal_id)
        import model_view.gen_uform_utag_triplet_data as triplet_data
        obj = triplet_data.Comp_Tag_Triplet_data()
        machine_id  = '122'
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()
        for table_id in table_ids:
            normalized_data = self.read_table_data(table_id, txn_m)
            obj.generate_default_triplet(company_name, model_number, company_id, machine_id, table_id, normalized_data, flg)
        
    def header_group_table(self, ijson):
        company_name = ijson["company_name"]
        project_id  = ijson["project_id"]
        deal_id     = ijson["deal_id"]
        model_number= ijson["model_number"]
        machine_id  = '122'
        company_id  = '%s_%s'%(project_id, deal_id)
        table_ids   = ijson['table_ids']

        #table_ids   = table_ids.keys()
        if not table_ids:
            return [{'message':'Error empty table list'}]
        if ijson.get('PRINT', '') != 'Y':
            disableprint()
        import index_all_table
        i_obj = index_all_table.Index()
        notm_d  = i_obj.index_table_norm_info(company_id, list(sets.Set(table_ids)), 'HG')
        enableprint()
        for tid in table_ids:
            npath  = "/var/www/html/muthu/Transpose/%s/%s_HG.html"%(company_id, tid)
            if os.path.exists(npath):
                org_path    = '%s/%s/Table_Htmls/%s_org.html'%(self.output_path, company_id, tid)
                cpath       = '%s/%s/Table_Htmls/%s.html'%(self.output_path, company_id, tid)
                if not os.path.exists(org_path):
                    os.system("cp %s %s"%(cpath, org_path))
                os.system("cp %s %s"%(npath, cpath))
        lmdb_path2      =  "/mnt/eMB_db/%s/%s/table_phcsv_data"%(company_name, model_number)
        env1             = lmdb.open(lmdb_path2, map_size=2**39)
        with env1.begin(write=True) as txn:
            for tid in table_ids:
                txn.put('FLIP_'+str(tid), 'HG')
        lmdb_path2      =  "/mnt/eMB_db/%s/%s/default_table_phcsv_data"%(company_name, model_number)
        env1             = lmdb.open(lmdb_path2, map_size=2**39)
        with env1.begin(write=True) as txn:
            for tid in table_ids:
                txn.put('FLIP_'+str(tid), 'HG')
        #self.gen_triplet(ijson, table_ids, 'FLIP')
        res = [{'message':'done'}]
        return res

    def flip_table(self, ijson):
        company_name = ijson["company_name"]
        project_id  = ijson["project_id"]
        deal_id     = ijson["deal_id"]
        model_number= ijson["model_number"]
        machine_id  = '122'
        company_id  = '%s_%s'%(project_id, deal_id)
        table_ids   = ijson['table_ids']

        #table_ids   = table_ids.keys()
        if not table_ids:
            return [{'message':'Error empty table list'}]
        if ijson.get('PRINT', '') != 'Y':
            disableprint()
        import index_all_table
        i_obj = index_all_table.Index()
        notm_d  = i_obj.index_table_norm_info(company_id, list(sets.Set(table_ids)), 'Y')
        enableprint()
        for tid in table_ids:
            npath  = "/var/www/html/muthu/Transpose/%s/%s.html"%(company_id, tid)
            if os.path.exists(npath):
                org_path    = '%s/%s/Table_Htmls/%s_org.html'%(self.output_path, company_id, tid)
                cpath       = '%s/%s/Table_Htmls/%s.html'%(self.output_path, company_id, tid)
                if not os.path.exists(org_path):
                    os.system("cp %s %s"%(cpath, org_path))
                os.system("cp %s %s"%(npath, cpath))
        lmdb_path2      =  "/mnt/eMB_db/%s/%s/table_phcsv_data"%(company_name, model_number)
        env1             = lmdb.open(lmdb_path2, map_size=2**39)
        with env1.begin(write=True) as txn:
            for tid in table_ids:
                txn.put('FLIP_'+str(tid), 'Y')
        lmdb_path2      =  "/mnt/eMB_db/%s/%s/default_table_phcsv_data"%(company_name, model_number)
        env1             = lmdb.open(lmdb_path2, map_size=2**39)
        with env1.begin(write=True) as txn:
            for tid in table_ids:
                txn.put('FLIP_'+str(tid), 'Y')
        #self.gen_triplet(ijson, table_ids, 'FLIP')
        res = [{'message':'done'}]
        return res
        #sys.exit()
        #import model_view.gen_uform_utag_triplet_data as gen_uform_utag_triplet_data
        #tri_obj = gen_uform_utag_triplet_data.Comp_Tag_Triplet_data()
        #i_ar    = []
        #for gtable_id, normalized_data in notm_d.items():
        #    i_ar.append()
        #    #print 
        #    #for ndata in normalized_data:
        #    #    for rr in ndata:
        #    #        print '\t', rr
        #    tri_obj.generate_default_triplet(company_name, model_number, company_id, machine_id, gtable_id, normalized_data)
        ijson['Taxo'] = 'Y'
        ijson['print'] = 'N'
        ijson['re_run'] = 'Y'
        if ijson.get('PRINT', '') != 'Y':
            disableprint()
        else:
            ijson['print'] = 'Y'
        self.run_ph_csv(copy.deepcopy(ijson))
        enableprint()
        if ijson.get('gen_taxo') != 'Y':
            res = [{'message':'done'}]
            return res
        if ijson.get('taxo_flg', '') == 1:
            import data_builder.taxo_group as taxo_group
            taxo_group_obj  = taxo_group.TaxoGroup()
            if ijson.get('PRINT', '') != 'Y':
                disableprint()
            #print 'TAXO'
            #ijson['re_run'] = 'Y'
            if ijson.get('table_types', []):
                ijson['from_web_api']   = 'Y'
                taxo_group_obj.create_taxo_grp(ijson)
            elif ijson.get('table_ids', []):
                ijson['from_web_api']   = 'Y'
                taxo_group_obj.create_taxo_grp(ijson)
            enableprint()
            res = [{'message':'done'}]
            return res
        else:
            if ijson.get('PRINT', '') != 'Y':
                disableprint()
            ijson['Taxo'] = 'Y'
            self.add_new_table(ijson)
            enableprint()
            res = [{'message':'done'}]
            return res

    def restore_merge_table(self, ijson):
        company_name = ijson["company_name"]
        project_id  = ijson["project_id"]
        deal_id     = ijson["deal_id"]
        model_number= ijson["model_number"]
        machine_id  = '122'
        company_id  = '%s_%s'%(project_id, deal_id)
        table_ids   = ijson['table_ids']
        if not table_ids:
            return [{'message':'Error empty table list'}]
        disableprint()
        import index_all_table
        i_obj = index_all_table.Index()
        notm_d  = i_obj.index_table_norm_info(company_id, list(sets.Set(table_ids)), 'N')
        enableprint()
        for tid in table_ids:
            org_path    = '%s/%s/Table_Htmls/%s_org.html'%(self.output_path, company_id, tid)
            cpath       = '%s/%s/Table_Htmls/%s.html'%(self.output_path, company_id, tid)
            if os.path.exists(org_path):
                os.system("cp %s %s"%(org_path, cpath))
        lmdb_path2      =  "/mnt/eMB_db/%s/%s/table_phcsv_data"%(company_name, model_number)
        env1             = lmdb.open(lmdb_path2, map_size=2**39)
        with env1.begin(write=True) as txn:
            for tid in table_ids:
                txn.put('FLIP_'+str(tid), 'N')
        lmdb_path2      =  "/mnt/eMB_db/%s/%s/default_table_phcsv_data"%(company_name, model_number)
        env1             = lmdb.open(lmdb_path2, map_size=2**39)
        with env1.begin(write=True) as txn:
            for tid in table_ids:
                txn.put('FLIP_'+str(tid), 'N')
        res = [{'message':'done'}]
        return res
        ijson['Taxo'] = 'Y'
        ijson['print'] = 'N'
        ijson['re_run'] = 'Y'
        disableprint()
        self.run_ph_csv(copy.deepcopy(ijson))
        enableprint()
        if ijson.get('taxo_flg', '') == 1:
            import data_builder.taxo_group as taxo_group
            taxo_group_obj  = taxo_group.TaxoGroup()
            disableprint()
            #ijson['re_run'] = 'Y'
            if ijson.get('table_types', []):
                ijson['from_web_api']   = 'Y'
                taxo_group_obj.create_taxo_grp(ijson)
            elif ijson.get('table_ids', []):
                ijson['from_web_api']   = 'Y'
                taxo_group_obj.create_taxo_grp(ijson)
            enableprint()
            res = [{'message':'done'}]
            return res
        else:
            disableprint()
            ijson['Taxo'] = 'Y'
            self.add_new_table(ijson)
            enableprint()
            res = [{'message':'done'}]
            return res

    def merge_table(self, ijson):
        company_name = ijson["company_name"]
        project_id  = ijson["project_id"]
        deal_id     = ijson["deal_id"]
        model_number= ijson["model_number"]
        machine_id  = '122'
        company_id  = '%s_%s'%(project_id, deal_id)
        table_ids   = ijson['table_ids']
        if not table_ids:
            return [{'message':'Error empty table list'}]
        disableprint()
        import index_all_table
        i_obj = index_all_table.Index()
        notm_d  = i_obj.index_table_norm_info(company_id, list(sets.Set(table_ids)), 'NM')
        enableprint()
        for tid in table_ids:
            npath  = "/var/www/html/muthu/Transpose/%s/%s_merge.html"%(company_id, tid)
            if os.path.exists(npath):
                org_path    = '%s/%s/Table_Htmls/%s_org.html'%(self.output_path, company_id, tid)
                cpath       = '%s/%s/Table_Htmls/%s.html'%(self.output_path, company_id, tid)
                if not os.path.exists(org_path):
                    os.system("cp %s %s"%(cpath, org_path))
                os.system("cp %s %s"%(npath, cpath))
        lmdb_path2      =  "/mnt/eMB_db/%s/%s/table_phcsv_data"%(company_name, model_number)
        env1             = lmdb.open(lmdb_path2, map_size=2**39)
        with env1.begin(write=True) as txn:
            for tid in table_ids:
                txn.put('FLIP_'+str(tid), 'NM')
        lmdb_path2      =  "/mnt/eMB_db/%s/%s/default_table_phcsv_data"%(company_name, model_number)
        env1             = lmdb.open(lmdb_path2, map_size=2**39)
        with env1.begin(write=True) as txn:
            for tid in table_ids:
                txn.put('FLIP_'+str(tid), 'NM')
        #self.gen_triplet(ijson, table_ids, 'NMERGE')
        res = [{'message':'done'}]
        return res
        #import model_view.gen_uform_utag_triplet_data as gen_uform_utag_triplet_data
        #tri_obj = gen_uform_utag_triplet_data.Comp_Tag_Triplet_data()
        #i_ar    = []
        #for gtable_id, normalized_data in notm_d.items():
        #    i_ar.append()
        #    #print 
        #    #for ndata in normalized_data:
        #    #    for rr in ndata:
        #    #        print '\t', rr
        #    tri_obj.generate_default_triplet(company_name, model_number, company_id, machine_id, gtable_id, normalized_data)
        ijson['Taxo'] = 'Y'
        ijson['print'] = 'N'
        ijson['re_run'] = 'Y'
        disableprint()
        self.run_ph_csv(copy.deepcopy(ijson))
        enableprint()
        if ijson.get('taxo_flg', '') == 1:
            import data_builder.taxo_group as taxo_group
            taxo_group_obj  = taxo_group.TaxoGroup()
            disableprint()
            #ijson['re_run'] = 'Y'
            if ijson.get('table_types', []):
                ijson['from_web_api']   = 'Y'
                taxo_group_obj.create_taxo_grp(ijson)
            elif ijson.get('table_ids', []):
                ijson['from_web_api']   = 'Y'
                taxo_group_obj.create_taxo_grp(ijson)
            enableprint()
            res = [{'message':'done'}]
            return res
        else:
            disableprint()
            ijson['Taxo'] = 'Y'
            self.add_new_table(ijson)
            enableprint()
            res = [{'message':'done'}]
            return res
            


    def taxoDropDown(self, ijson):
        table_type = ijson["table_type"]
        db_path = '/mnt/eMB_db/TAXO_INFO/taxo_info.db'
        conn = sqlite3.connect(db_path)
        cur = conn.cursor()
        qry = 'select distinct(taxonomy) from taxo_label_map where table_type="%s"'%(table_type) 
        cur.execute(qry)
        data = cur.fetchall()
        res_lst = []
        for row in data:
            taxonomy = str(row[0])
            rs = {'n':taxonomy} 
            res_lst.append(rs)
        conn.close()
        res = [{'message':'done', 'data':res_lst}]
        return res

    def taxoDropDown_cp(self, ijson):
        table_type = ijson["table_type"]
        sys.path.append('/root/databuilder_train_ui/tenkTraining/Working_Table_Tagging_Training_V2/pysrc/model_view/') 
        import company_docTablePh_details_tableType as py
        obj  = py.Company_docTablePh_details()
        company_name = ijson["company_name"]
        project_id  = ijson["project_id"]
        deal_id     = ijson["deal_id"]
        company_id  = '%s_%s'%(project_id, deal_id)

        try:
            tables = obj.getTableIdLst_passingTableType(company_id)[table_type]
        except:
            tables  = []
            pass
        #################################
        lmdb_path = os.path.join("/var/www/html/fill_table/", company_id , "table_info")
        env = lmdb.open(lmdb_path, readonly=True)
        txn  = env.begin()
        #################################
        res_lst = []
        for table_id in tables:
            getCids = txn.get('HGH_'+table_id)
            if getCids == None:continue 
            for cids in getCids.split('#'):
                getTxtHex = txn.get('TEXT_' + cids)
                unhashedTx = self.gen_taxonomy(binascii.a2b_hex(getTxtHex))
                tx = {'n':unhashedTx}
                if tx not in res_lst:
                    res_lst.append(tx) 
        res = [{'message':'done', 'data':res_lst, 'cnt':len(res_lst)}] 
        return res

    def alter_column_datatype(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        alter_stmt = 'ALTER TABLE ph_derivation ADD ph TEXT;'
        try:
            cur.execute(alter_stmt)
            conn.commit()
        except:pass
        conn.close()
        return [{'message':'done'}]

    
    def get_selected_class_data_by_key(self, ijson):
        
        company_name      = ijson["company_name"]
        model_number      = ijson["model_number"]
        project_id        = ijson["project_id"]
        deal_id           = ijson["deal_id"]
        company_id        = '%s_%s'%(project_id, deal_id)
        given_table_id    = ijson["t"]
        given_doc_id    = ijson["d"]
 
        db_file     = os.path.join('/mnt/eMB_db/', company_name, str(model_number), 'tas_company.db')
        conn        = sqlite3.connect(db_file)
        cur         = conn.cursor()
        stmt = "select doc_id, period, reporting_year from company_meta_info" 
        cur.execute(stmt)
        res = cur.fetchall()
        conn.close()
        doc_pdict = {}
        for r in res:
            doc_id, period_type, period = map(str, r[:])
            doc_pdict[doc_id] = (period_type, period)
        ################################################
        db_file     = os.path.join('/mnt/eMB_db/', company_name, str(model_number), 'tas_tagging.db')
        conn        = sqlite3.connect(db_file)
        cur         = conn.cursor()
        
        crt_qry     = 'CREATE TABLE IF NOT EXISTS UsrTableCsvPhInfo(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_id VARCHAR(256), xml_id TEXT, period_type TEXT, period TEXT, currency TEXT, value_type TEXT, scale TEXT, month VARCHAR(20), review_flg INTEGER)'
        cur.execute(crt_qry)

        stmt = "select table_id, xml_id, period, period_type, month, currency, scale, value_type from UsrTableCsvPhInfo where table_id='%s'" %(given_table_id)
        cur.execute(stmt)
        res = cur.fetchall()
        conn.close()
        user_sel_dict = {}
        for r in res:
            table_id, xml_id, period, period_type, month, currency, scale, value_type = map(lambda x:str(x).strip(), r[:]) 
            if table_id not in user_sel_dict:
                user_sel_dict[table_id] = {}
            user_sel_dict[table_id][xml_id] = [period, period_type, month, currency, scale, value_type]
        #####################################################
        db_file     = os.path.join('/mnt/eMB_db/', company_name, str(model_number), 'table_tagging_dbs', '%s.db'%(given_table_id))
        conn        = sqlite3.connect(db_file)
        cur         = conn.cursor()
        stmt = "select nrow, ncol, txt, xml_id, period, period_type, usr_value_type, currency, scale, value_type from TableCsvPhInfo where cell_type='GV'"
        cur.execute(stmt)
        res = cur.fetchall()
        conn.close()
        doc_id = given_doc_id
        doc_key = given_table_id
        table_id = given_table_id
        output_lst = []
        sl = 1
        other_table_ids = []
        doc_table_group_data = {}
        given_group_dict = {}
        col_key = 'green'
        xml_id_key_dict = {}
        for row in res:
            nrow, ncol, txt, xml_id, period, period_type, month, currency, scale, value_type = map(str, row[:])
            nrow, ncol = map(int, [nrow, ncol])
            doc_period_type, doc_period = doc_pdict.get(doc_id, ('', ''))
            doc_p = doc_period_type + doc_period
            if doc_key not in given_group_dict:
                given_group_dict[doc_key] = {}
            if doc_key not in xml_id_key_dict:
                xml_id_key_dict[doc_key] = {}
            xml_id_key_dict[doc_key][xml_id] = (nrow, ncol)
            given_group_dict[doc_key][xml_id] = {'l':txt, 'p':period, 'pt':period_type, 'm':month, 'c':currency, 's':scale, 'vt':value_type, 'd_pt_p':doc_p, 'x':xml_id, 'd':doc_id, 't':table_id}
            if doc_key not in doc_table_group_data:
                doc_table_group_data[doc_key] = {}
            if col_key not in doc_table_group_data[doc_key]:
                doc_table_group_data[doc_key][col_key] = []
            if xml_id not in doc_table_group_data[doc_key][col_key]:
                doc_table_group_data[doc_key][col_key].append(xml_id)

        '''
        fname = os.path.join('/mnt/eMB_db/', 'csv_classes', company_id)
        env = lmdb.open(fname)
        output_lst = []
        sl = 1
        other_table_ids = []
        doc_table_group_data = {}
        given_group_dict = {}
        with env.begin() as txn:
            cursor = txn.cursor()
            for key, vstr in cursor:
                if 'CLS_CNT' in key:continue
                if 'CLS:' in key:continue
                if 'DPC' in key:continue
                col_key = 'yellow'
                if 'RED' in key:
                    col_key = 'red'
                elif 'GREEN' in key:
                    col_key = 'green'
                kstr, prop_lst = eval(vstr)
                doc_id, table_id, xml_id = kstr.split('$::$')
                if table_id != given_table_id:continue
                txt, period, period_type, month, currency, scale, value_type = prop_lst
                doc_key = table_id
                if doc_key not in doc_table_group_data:
                    doc_table_group_data[doc_key] = {}
                if col_key not in doc_table_group_data[doc_key]:
                    doc_table_group_data[doc_key][col_key] = []
                if xml_id not in doc_table_group_data[doc_key][col_key]:
                    doc_table_group_data[doc_key][col_key].append(xml_id)
                if 1:#cls in key:
                    if doc_key not in given_group_dict:
                        given_group_dict[doc_key] = {}
                    doc_period_type, doc_period = doc_pdict.get(doc_id, ('', ''))
                    doc_p = doc_period_type + doc_period
                    given_group_dict[doc_key][xml_id] = {'l':txt, 'p':period, 'pt':period_type, 'm':month, 'c':currency, 's':scale, 'vt':value_type, 'd_pt_p':doc_p, 'x':xml_id, 'd':doc_id, 't':table_id}
        '''
        ########################################################### 
        results_dict = {}
        lmdb_path = os.path.join('/var/www/html/fill_table/', company_id, 'XML_BBOX')
        env       = lmdb.open(lmdb_path, readonly=True)
        txn_mm       = env.begin() 
        #res_str = txn.get('RST:'+str(given_table_id), '')
        #if res_str:
        #    results_dict = ast.literal_eval(res_str)
        #print results_dict
        ###########################################################
        #print 'RRRRRRRRRRRRRRRRRRRRRRRRRR', given_group_dict
        output_lst = []
        sl = 1
        apply_cell_dict = {} 
        for table_id, xdict in given_group_dict.items():
            ucell_dict = user_sel_dict.get(table_id, {})
            xml_cids = xml_id_key_dict[table_id]
            xids = xdict.keys()
            xids = map(lambda x:(xml_cids[x], x), xids[:])
            xids.sort()
            xids = map(lambda x:x[1], xids[:])
            for xml_id in xids:
                dd  = xdict[xml_id]
                dd['sn'] = sl
                if xml_id in ucell_dict:
                    dd['p'], dd['pt'], dd['m'], dd['c'], dd['s'], dd['vt'] = ucell_dict[xml_id]
                    dd['df'] = 1
                    apply_cell_dict[xml_id] = dd['df']
                else:
                    dd['df'] = 0
                bbox_lst = []
                for xm in dd['x'].split('#'):
                    bbox = eval(txn_mm.get(str(table_id)+':$$:'+str(xm), '[]'))
                    #print '>>>>', (str(table_id)+':$$:'+str(xm)), bbox
                    if bbox == []:
                        bbox_lst.append(bbox)
                        continue
                    bbox_lst.append(bbox[0][0])
                dd['bbox'] = bbox_lst 
                output_lst.append(dd)
                sl += 1
        ########################################################### 
        #return 'done', output_lst, doc_table_group_data
        res = [{'message':'done', 'data':output_lst, 't_xdict':doc_table_group_data, 'cdict':apply_cell_dict}]
        return res

    def validate_taxo_ph_scv_info(self, company_name, model_number, company_id, txn_m, txn, table_ids, taxo_dict):
        t_rc_d  = {}
        import model_view.table_tagging_v2 as TableTagging
        Omobj = TableTagging.TableTagging()
        table_id_cell_dict = Omobj.all_user_selected_references(company_name, model_number, company_id)

        ignore_col      = {}
        for table_id, xmls in table_id_cell_dict.items():
            table_id    = str(table_id)
            for xml_id in xmls:
                xml_id  = '#'.join(filter(lambda x:x, xml_id.split('#')))
                #print (table_id, xml_id)
                tk      = self.get_quid(table_id+'_'+xml_id)
                c_id    = txn_m.get('XMLID_MAP_'+tk)
                if not c_id:continue
                c       = int(c_id.split('_')[2])
                cs      = int(txn_m.get('colspan_'+c_id))
                for c_s in range(cs):
                    ignore_col.setdefault(table_id, {})[c]  = 1

        for table_id in table_ids.keys():
            k        = 'HGH_'+table_id
            ids      = txn_m.get(k)
            r_ld     = {}
            if ids:
                ids     = ids.split('#')
                for c_id in ids:
                    r       = int(c_id.split('_')[1])
                    c       = int(c_id.split('_')[2])
                    x       = txn_m.get('XMLID_'+c_id)
                    rs      = int(txn_m.get('rowspan_'+c_id))
                    for tr in range(rs): 
                        r_ld.setdefault(r+tr, {})[x]  = 1
            k   = 'GV_'+table_id
            ids = txn_m.get(k)
            if not ids:continue
            ids         = ids.split('#')
            for c_id in ids:
                r       = int(c_id.split('_')[1])
                c       = int(c_id.split('_')[2])
                if ignore_col.get(table_id, {}).get(c, 0) == 1:
                    continue
                x       = txn_m.get('XMLID_'+c_id)
                col     = 0
                if c_id:
                    col     = int(c_id.split('_')[2])
                key     = table_id+'_'+self.get_quid(x)
                x_txts  = r_ld.get(r, {}).keys()
                for x_p in x_txts:
                    t_rc_d.setdefault((table_id, x_p), {})[(col, x)]   = 1
        error_map_dict = {}
        table_gv_info  = {}
        for tid, ks_dict in taxo_dict.iteritems():
            taxo_map_ar = ks_dict['ks']
            matcher = {}
            max_group_len = 0
            max_key = ''
            for idx, tmap in enumerate(taxo_map_ar):
                set_val_type_map = []
                if str(tmap[2]) == 'Parent':continue
                #print '>>>> AA', tmap
                #print tmap[3]
                hgh_xml = txn_m.get('XMLID_'+str(tmap[1]))
                k = (tmap[0], hgh_xml)

                for (col, xml_id) in sorted(t_rc_d.get(k, {}).keys()):
                    key     = tmap[0]+'_'+self.get_quid(xml_id)
                    ph_map  = txn.get('PH_MAP_'+str(key))
                    if ph_map:
                        tperiod_type, tperiod, tcurrency, tscale, tvalue_type    = ph_map.split('^')
                    else:
                        tperiod_type, tperiod, tcurrency, tscale, tvalue_type   = '', '', '', '', ''
                    #print [tmap[3], tmap[0], '==', hgh_xml, '===', xml_id, '==', tvalue_type], '==', set_val_type_map
                    set_val_type_map.append(tvalue_type)
                k = '^!!^'.join([tmap[0], hgh_xml])
                matcher[k] = set_val_type_map
                if len(set(set_val_type_map)) > max_group_len:
                    max_group_len = len(set(set_val_type_map))
                    max_key = k
            if matcher:
                max_group_data = matcher.get(max_key, [])
                for k, sub_group in matcher.iteritems():
                    t, x = k.split('^!!^')
                    table_gv_info[(t, x)] = sub_group
                    if k == max_key:continue
                    if not (set(sub_group).issubset(set(max_group_data))):
                        error_map_dict[(t, x)] = 1
            
        #print error_map_dict
        #print table_gv_info
        return error_map_dict, table_gv_info

    def save_phcsv_data(self, ijson):
        company_name      = ijson["company_name"]
        model_number      = ijson["model_number"]
        project_id        = ijson["project_id"]
        deal_id           = ijson["deal_id"]
        company_id        = '%s_%s'%(project_id, deal_id)
       
        lmdbPath  = "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)

        ph_comp_flg = int(ijson.get('ph_comp', '0'))
        self.ph_comp_flg = ph_comp_flg
 
        row = ijson['row']
        rc = ijson.get('rc', [])
        xml_list = ijson.get('xml_list', [])
        sh_list = ijson.get('sh_list', [])

        #path   =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
        #key = table_id+'_'+self.get_quid(xml_id)
        #'PH_MAP_'+key -- '^'.join([period_type, period, currency, scale, value_type])

        if not sh_list: 
            msg, apply_cell_dict = self.update_user_csv_row(company_name, model_number, row, rc, xml_list, lmdbPath)
        else:
            msg, apply_cell_dict = self.update_user_csv_row_by_sel(company_name, model_number, row, rc, xml_list, sh_list, lmdbPath)
        for x, x1 in apply_cell_dict.items():
            if int(x1) == 0:
                apply_cell_dict[x] = 2
            else:
                apply_cell_dict[x] = 1
        return [{'message':msg, 'cdict':apply_cell_dict}]


    def validate_cell_ph_info(self, ttup):
        period_type, period, currency, value_type, scale, month = map(lambda x:str(x).strip(), ttup[:])
        vflg = 0
        if self.ph_comp_flg:
            if period_type and period:
                vflg = 1
            return vflg 
   
        if value_type == 'MNUM':
            if period_type and period and currency and scale:
                vflg = 1
        elif value_type == 'Percentage':
            if scale == '1' and period and period_type and (not currency):
                vflg = 1
        elif value_type == 'BNUM':
            if period_type and period and (not currency) and scale:
                vflg = 1
        elif value_type == 'Ratio':
            if (not currency) and scale == '1' and period and period_type:
                vflg = 1
        elif value_type == 'Other':
            if (not currency) and period and period_type:
                vflg = 1    
        return vflg

    def update_user_csv_row(self, company_name, model_number, ijson, rc, xml_list, lmdbPath=''):
        xml_list = map(lambda x:str(x), xml_list[:])
        table_id, doc_id, xml_id = map(str, [ijson['t'], ijson['d'], ijson['x']])
        period_type, period, currency, scale, value_type, month = map(str, [ijson['pt'], ijson['p'], ijson['c'], ijson['s'], ijson['vt'], ijson['m']])
        # project_id, deal_id
        db_file     = os.path.join('/mnt/eMB_db/', company_name, str(model_number), 'tas_tagging.db')
        #conn1, cur1   = self.get_connection(db_file)
        #column_tup = self.create_user_table_schama(conn1, cur1)
        conn1     = sqlite3.connect(db_file)
        cur1      = conn1.cursor()

        crt_qry     = 'CREATE TABLE IF NOT EXISTS UsrTableCsvPhInfo(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_id VARCHAR(256), xml_id TEXT, period_type TEXT, period TEXT, currency TEXT, value_type TEXT, scale TEXT, month VARCHAR(20), review_flg INTEGER)'
        cur1.execute(crt_qry)

        data = []
        apply_xml_ids = []  
        apply_cell_dict = {}
        db_file     = os.path.join('/mnt/eMB_db/', company_name, str(model_number), 'table_tagging_dbs', '%s.db'%table_id)
        #conn, cur   = self.get_connection(db_file)
        conn  = sqlite3.connect(db_file)
        cur   = conn.cursor()

        review_flg = 1
        if rc:
            apply_xml_ids = []
            if len(rc) == 2:
                apply_xml_ids = xml_list
            else:
                stmt = "select nrow, ncol from TableCsvPhInfo where xml_id='%s'" %(xml_id)
                cur.execute(stmt)
                res = cur.fetchone()
                sel_row, sel_col = res[0], res[1]
                for rr in rc:
                    if rr == 'ROW':
                        stmt = "select distinct(xml_id) from TableCsvPhInfo where cell_type='GV' and nrow='%s'" %(sel_row)
                    elif rr == 'COL':
                        stmt = "select distinct(xml_id) from TableCsvPhInfo where cell_type='GV' and ncol='%s'" %(sel_col)
                    cur.execute(stmt)
                    apply_xml_ids += map(lambda x:str(x[0]), cur.fetchall())    
            ################################################################
            apply_xml_ids = list(set(apply_xml_ids))
            rem_xml_ids = []
            for new_xml_id in apply_xml_ids:
                if new_xml_id in xml_list:
                    ttup = (period_type, period, currency, value_type, scale, month)
                    #review_flg = self.validate_cell_ph_info(ttup)
                    apply_cell_dict[new_xml_id] = review_flg
                    data.append((table_id, new_xml_id, period_type, period, currency, value_type, scale, month, review_flg))
                else:
                    rem_xml_ids.append(new_xml_id)
            if xml_list:
                apply_xml_ids = list(set(apply_xml_ids) - set(rem_xml_ids))
            else:
                apply_xml_ids = list(set(apply_xml_ids))
            ################################################################   
        else:
            apply_xml_ids = [xml_id]
            ttup = (period_type, period, currency, value_type, scale, month)
            #review_flg = self.validate_cell_ph_info(ttup)
            apply_cell_dict[xml_id] = review_flg
            data = [(table_id, xml_id, period_type, period, currency, value_type, scale, month, review_flg)]
        ####### ADD TO USER DATA #################    
        app_xml_str = ', '.join(['"'+e+'"' for e in apply_xml_ids]) 
        table_name = 'UsrTableCsvPhInfo'
        stmt = "delete from UsrTableCsvPhInfo where xml_id in (%s) and table_id='%s'" %(app_xml_str, table_id)
        cur1.execute(stmt)
        #msg = self.qObj.insertIntoLite(conn1, cur1, '', table_name, column_tup, data) 
        cur1.executemany('INSERT INTO UsrTableCsvPhInfo(table_id, xml_id, period_type, period, currency, value_type, scale, month, review_flg) VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?)', data)
        conn1.commit()
        conn1.close()
        stmt = "update TableCsvPhInfo set period_type='%s', period='%s', currency='%s', value_type='%s', scale='%s', usr_value_type='%s' where xml_id in (%s)" %(period_type, period, currency, value_type, scale, month, app_xml_str) 
        cur.execute(stmt)
        conn.commit()
        conn.close()      

        if lmdbPath:
            lmdbdict = {}
            for xml_id in apply_xml_ids:
                key = table_id+'_'+self.get_quid(xml_id)
                pkey = 'PH_MAP_'+key
                pval = '^'.join([period_type, period, currency, scale, value_type])
                lmdbdict[pkey] = pval
            msg = self.get_update_lmdb(lmdbPath, lmdbdict) 
        return 'done', apply_cell_dict

    def get_update_lmdb(self, lmdbPath, lmdbdict):
        env = lmdb.open(lmdbPath, map_size=2**39)
        with env.begin(write=True) as txn:
            for key, val in lmdbdict.iteritems():
                txn.put(key, val)
        return 'done'

    def get_connection(self, db_file):
        conn = sqlite3.connect(db_file)
        cur  = conn.cursor()
        return conn, cur

    def update_user_csv_row_by_sel(self, company_name, model_number, ijson, rc, xml_list, sh_list, lmdbPath=''):
        xml_list = map(lambda x:str(x), xml_list[:])
        table_id, doc_id, xml_id = map(str, [ijson['t'], ijson['d'], ijson['x']])
        period_type, period, currency, scale, value_type, month = map(str, [ijson['pt'], ijson['p'], ijson['c'], ijson['s'], ijson['vt'], ijson['m']])
        db_file     = os.path.join('/mnt/eMB_db/', company_name, str(model_number), 'tas_tagging.db')
        conn1, cur1   = self.get_connection(db_file)
        #column_tup = self.create_user_table_schama(conn1, cur1)
        crt_qry     = 'CREATE TABLE IF NOT EXISTS UsrTableCsvPhInfo(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_id VARCHAR(256), xml_id TEXT, period_type TEXT, period TEXT, currency TEXT, value_type TEXT, scale TEXT, month VARCHAR(20), review_flg INTEGER)'
        cur1.execute(crt_qry)
        
        data = []
        apply_xml_ids = []
        apply_cell_dict = {}
        db_file     = os.path.join('/mnt/eMB_db/', company_name, str(model_number), 'table_tagging_dbs', '%s.db'%table_id)
        conn, cur   = self.get_connection(db_file)
        user_sel_flg = 0 
        if rc:
            apply_xml_ids = []
            if len(rc) == 2:
                apply_xml_ids = xml_list[:]
                if not apply_xml_ids:
                    stmt = "select distinct(xml_id) from TableCsvPhInfo where cell_type='GV'"
                    cur.execute(stmt)
                    apply_xml_ids += map(lambda x:str(x[0]), cur.fetchall())    
                    user_sel_flg = 1
            else:
                stmt = "select nrow, ncol from TableCsvPhInfo where xml_id='%s'" %(xml_id)
                cur.execute(stmt)
                res = cur.fetchone()
                sel_row, sel_col = res[0], res[1]
                #print sel_row, sel_col
                for rr in rc:
                    if rr == 'ROW':
                        stmt = "select distinct(xml_id) from TableCsvPhInfo where cell_type='GV' and nrow='%s'" %(sel_row)
                    elif rr == 'COL':
                        stmt = "select distinct(xml_id) from TableCsvPhInfo where cell_type='GV' and ncol='%s'" %(sel_col)
                    cur.execute(stmt)
                    apply_xml_ids += map(lambda x:str(x[0]), cur.fetchall())    
                #print 'HHHH', apply_xml_ids
            ################################################################
            if xml_list:
                apply_xml_ids = [x for x in list(set(apply_xml_ids)) if x in xml_list]
            else:
                apply_xml_ids = list(set(apply_xml_ids))
            ################################################################   
        else:
            apply_xml_ids = xml_list[:]
        ########################################################################
        app_xml_str = ', '.join(['"'+e+'"' for e in apply_xml_ids]) 
        stmt = "select org_row, org_col, nrow, ncol, cell_type, txt, xml_id, period_type, period, currency, value_type, scale, usr_value_type from TableCsvPhInfo where xml_id in (%s)" %(app_xml_str)
        cur.execute(stmt)
        res = cur.fetchall()
        mms = ['pt', 'p', 'c', 'vt', 's', 'm']
        sel_poss = []
        for x in sh_list:
            sel_poss.append((mms.index(x), str(ijson[x])))
        ################################################   
        new_ph_csv_data = [] 
        review_flg = 1
        xml_map_dict = {}
        for row in res:
            org_row, org_col, nrow, ncol, cell_type, txt, cell_ref, speriod_type, speriod, scurrency, svalue_type, sscale, smonth = row[:]
            mm = [speriod_type, speriod, scurrency, svalue_type, sscale, smonth, review_flg]   
            for (p, cc) in sel_poss:
                mm[p] = cc
            ttup = tuple(mm[:-1])
            #review_flg = self.validate_cell_ph_info(ttup)
            mm[-1] = review_flg
            apply_cell_dict[cell_ref] = review_flg
            dtup = tuple([table_id, cell_ref] + mm[:]) 
            data.append(dtup)
            xml_map_dict[cell_ref] = map(str, [mm[0], mm[1], mm[2], mm[4], mm[3]])
            new_ph_csv_data.append((org_row, org_col, nrow, ncol, cell_type, txt, cell_ref, mm[0], '', mm[1], '', mm[2], '', mm[3], mm[5], mm[4], ''))
        ####### ADD TO USER DATA ###############################################    
        table_name = 'UsrTableCsvPhInfo'
        stmt = "delete from UsrTableCsvPhInfo where xml_id in (%s) and table_id='%s'" %(app_xml_str, table_id)
        cur1.execute(stmt)
        #msg = self.qObj.insertIntoLite(conn1, cur1, '', table_name, column_tup, data) 
        cur1.executemany('INSERT INTO UsrTableCsvPhInfo(table_id, xml_id, period_type, period, currency, value_type, scale, month, review_flg) VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?)', data)
        conn1.commit()
        conn1.close()
        #####################ADD TO SYSTEM ######################################
        new_table_name = 'TableCsvPhInfo'
        stmt = "delete from TableCsvPhInfo where xml_id in (%s)" %(app_xml_str)
        cur.execute(stmt)
        #new_column_tup = ('org_row', 'org_col', 'nrow', 'ncol', 'cell_type', 'txt', 'xml_id', 'period_type', 'usr_period_type', 'period', 'usr_period', 'currency', 'usr_currency', 'value_type', 'usr_value_type', 'scale', 'usr_scale')
        #msg = self.qObj.insertIntoLite(conn, cur, '', new_table_name, new_column_tup, new_ph_csv_data) 
        cur.executemany('INSERT INTO TableCsvPhInfo(org_row, org_col, nrow, ncol, cell_type, txt, xml_id, period_type, usr_period_type, period, usr_period, currency, usr_currency, value_type, usr_value_type, scale, usr_scale) VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)', new_ph_csv_data)
        conn.commit()
        conn.close()       
        
        if lmdbPath:
            lmdbdict = {}
            for xml_id in apply_xml_ids:
                key = table_id+'_'+self.get_quid(xml_id)
                pkey = 'PH_MAP_'+str(key)
                period_type, period, currency, scale, value_type = xml_map_dict.get(xml_id, ['', '', '', '', ''])
                pval = '^'.join([period_type, period, currency, scale, value_type])
                #print pkey, table_id, xml_id
                #print pval
                #print
                lmdbdict[pkey] = pval
            msg = self.get_update_lmdb(lmdbPath, lmdbdict)
        return 'done', apply_cell_dict

    ## EXCEL CONFIG GROUP INFO
    def get_tt_sub_groups_info(self, ijson):
        company_name      = ijson["company_name"]
        model_number      = ijson["model_number"]
        project_id        = ijson["project_id"]
        deal_id           = ijson["deal_id"]
        company_id        = '%s_%s'%(project_id, deal_id)

        db_file           = self.get_db_path(ijson)
        conn, cur         = conn_obj.sqlite_connection(db_file)

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        path    = "%s/%s/%s/1_1/21/sdata/doc_map.txt"%(self.doc_path, project_id, deal_id)
        if os.path.exists(path):
            fin = open(path, 'r')
            lines   = fin.readlines()
            fin.close()
        else:
            lines   = []
        import data_builder.db_data as db_data
        obj = db_data.PYAPI()
        doc_d       = {}
        c_year      = self.get_cyear(lines)
        start_year  = c_year - int(ijson.get('year', 5))
        for line in lines[1:]:
            line    = line.split('\t')
            if len(line) < 8:continue
            line    = map(lambda x:x.strip(), line)
            ph      = line[3]+line[7]
            try:
                year    = int(ph[-4:])
            except:continue
            if ph and start_year < year:
                doc_id  = line[0]
                if ijson.get('ignore_doc_ids', {}) and doc_id in  ijson.get('ignore_doc_ids', {}):continue
                doc_d[doc_id]   = (ph, line[2])

        group_info_dict = OD()

        tt_data = self.read_doc_info(ijson)[0]['mt_list']
        table_types = [x['k'] for x in tt_data]
        for table_type in table_types:
            vgh_id_d, vgh_id_d_all, docinfo_d = {},{},{}
            m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, [table_type])
            sql         = "select row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label,gcom, ngcom, doc_id, m_rows, vgh_text, vgh_group, xml_id, period, period_type, scale, currency, value_type from mt_data_builder where table_type='%s' and isvisible='Y'"%(table_type)
            try:
                cur.execute(sql)
                res         = cur.fetchall()
            except:
                res = []
            for rr in res:
                row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, gv_xml, ph, ph_label,gcom, ngcom, doc_id,m_rows, vgh_text, vgh_group, xml_id, period, period_type, scale, currency, value_type    = rr
                doc_id      = str(doc_id)
                if doc_id not in doc_d:continue
                table_id    = str(table_id)
                if table_id not in m_tables:continue
                tk          = self.get_quid(table_id+'_'+xml_id)
                c_id        = txn_m.get('XMLID_MAP_'+tk)
                if not c_id:continue
                c   = int(c_id.split('_')[2])
                key     = table_id+'_'+self.get_quid(xml_id)
                vgh_id_d.setdefault(vgh_text, {})[(table_id, c_id, row_id, doc_id)]        = 1
                vgh_id_d_all[vgh_text]  = 1
                doc_id      = str(doc_id)
                table_id    = str(table_id)
                docinfo_d.setdefault(doc_id, {})[(table_id, c_id, vgh_text)]   = 1
 
            #group_info_dict[table_type] = self.read_all_vgh_groups(table_type, company_name, model_number,vgh_id_d, vgh_id_d_all, docinfo_d)
            group_info_dict[table_type]  = obj.read_all_vgh_groups(table_type, company_name, model_number, doc_m_d, {}) 
        conn.close()
        return group_info_dict

    ## 83 Config Save
    def saveCompanyConfigTable(self, ijson):
        company_name     = ijson["company_name"]
        model_number     = ijson["model_number"]
        project_id       = ijson["project_id"]
        deal_id          = ijson["deal_id"]
        company_id       = '%s_%s'%(project_id, deal_id)
        gbl              = int(ijson["gbl"])
        proj_info_li     = ijson['p_list']
        all_companies    = int(ijson["all"])

        db_file           = self.get_db_path(ijson)
        self.gbl_update(ijson,db_file)
        self.saveTableType_wiseFormula_company(ijson, db_file)
        db_file ='/mnt/eMB_db/node_mapping.db'
        if gbl:
            self.gbl_update(ijson,db_file)
            self.saveTableType_wiseFormula_company(ijson, db_file)
        if all_companies:
            dct_all_companies = self.get_list_all_companies()
            for comp_id, comp_name in dct_all_companies.items():
                if comp_name == company_name:continue
                db_file = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(comp_name, model_number)
                self.gbl_update_all(company_name, company_id, db_file, proj_info_li)
        companyConfig_data = self.read_companyConfigTable(ijson)
        proj_info = self.getProjectId_details_comp(ijson)
        return [{'message':'done','cf_data':companyConfig_data, 'proj_info':proj_info}]

    def gbl_update(self, ijson,db_file):
        company_name     = ijson["company_name"]
        model_number     = ijson["model_number"]
        project_id       = ijson["project_id"]
        deal_id          = ijson["deal_id"]
        company_id       = '%s_%s'%(project_id, deal_id)
        gbl              = int(ijson["gbl"])
        proj_info_li     = ijson['p_list']
        conn, cur         = conn_obj.sqlite_connection(db_file)

        crt_qry          = "CREATE TABLE IF NOT EXISTS company_config(row_id INTEGER PRIMARY KEY AUTOINCREMENT, company_id VARCHAR(20), company_name TEXT, project_id INTEGER, project_name TEXT, reporting_type VARCHAR(256), total_years INTEGER, derived_ph VARCHAR(256), periods VARCHAR(256), ind_group_flg INTEGER, taxo_flg INTEGER, excel_config_str TEXT, rsp_period TEXT, rsp_year TEXT, rfr TEXT, restated_period TEXT, rfr_tt TEXT)"
        cur.execute(crt_qry)
        
        self.alter_table_coldef(conn, cur, 'company_config', ['rsp_period', 'rsp_year', 'rfr', 'restated_period', 'rfr_tt', 'event_year', 'disp_name', 'scale_vt', 'formula_derivation', 'excel_start_period', 'excel_start_year', 'doc_type_info', 'mt_set', 'r_as_r'])

        def get_max_pid():
            slt_qry  = "select max(project_id) from company_config;"
            cur.execute(slt_qry)
            slt_data = cur.fetchone()
            p_id = 1
            if slt_data and slt_data[0]:
                p_id = int(slt_data[0]) + 1
            return p_id

        data_rows   = []
        update_rows = []
        PID         = 0
        for proj_info_dict in proj_info_li:
            project_name, reporting_type, total_years   = proj_info_dict['n'], proj_info_dict['st'], proj_info_dict['yr']
            derived_ph_lst, periods_lst                 = proj_info_dict['pt'], proj_info_dict['phd']
            ind_group_flg, taxo_flg, cur_pid            = proj_info_dict['rp'], proj_info_dict['tx'], proj_info_dict.get('pid', '')
            excel_config_map                            = proj_info_dict['tt_list']
            rsp_period, rsp_year                        = proj_info_dict['re_p'], proj_info_dict['re_yr']
            es_period, es_year                          = proj_info_dict.get('es_p', ''), proj_info_dict.get('es_yr', '')
            rfr, restated_period                        = proj_info_dict['rfr'], proj_info_dict['rest_p']
            rfr_tt                                      = '##'.join(proj_info_dict['rfr_tt'])
            event_year                                  = proj_info_dict['event_yr']        
            disp_name                                   = proj_info_dict['disp_name']
            scale_vt_lst                                   = proj_info_dict.get('scale_t', [])
            formula_derivation                          = proj_info_dict.get('formula_derivation', '0')
            doc_type_info                               = proj_info_dict.get('doc_t_grp', [])
            mt_set_data                                 = proj_info_dict.get('mt_set', [])     
            r_as_r_data                                 = proj_info_dict.get('r_as_r', '')
            
            mt_set_lst = []
            for rmt_dct in mt_set_data:
                rmt_id = rmt_dct['k'] 
                i_tt_dct   = rmt_dct['ignore_tt']
                mt_str = '~'.join((str(rmt_id), '##'.join(i_tt_dct)))
                mt_set_lst.append(mt_str)
            all_mt_str = '@@'.join(mt_set_lst)
            
            scale_vt_dt_lst = []
            for svt_dct in scale_vt_lst:
                sc = svt_dct['s']
                vt = svt_dct['vt']
                ignore_tids = svt_dct.get('ignore_ph', {})
                rw = sc+'~'+vt+'~'+str(ignore_tids)
                scale_vt_dt_lst.append(rw)
            scale_vt = '#'.join(scale_vt_dt_lst)
            qry = 'select company_name, project_id from company_config where project_name like "%s"'%(project_name)
            cur.execute(qry)
            daa = cur.fetchone()

            if daa and daa[1]:
                cur_pid = str(daa[1])
            
            derived_ph  = '##'.join(derived_ph_lst)
            periods     = '##'.join(periods_lst)
            doc_type_lst = []
            for mgw_dct in doc_type_info:   
                group_i, dllst = mgw_dct['p'], mgw_dct['c']
                dllst_str= '~'.join(dllst)
                gdl_str = '##'.join((group_i, dllst_str))
                doc_type_lst.append(gdl_str)
            doc_type_str = '@@'.join(doc_type_lst)

            parent_child_rel = OD()
            for pdict in excel_config_map:
                parent      = pdict['p']
                child_li    = pdict['c']
                grid        = pdict['grpid']
                edit_p      = str(pdict.get('edit_p', 0))
                pm_header   = pdict.get('mh', '')
                ptaxo_ids_lst = map(str, pdict.get('t_ids', []))
                ptaxo_ids_str = '&&'.join(ptaxo_ids_lst)
                parr_phs     = map(str, pdict.get('ignore_phs', []))
                pphs_str     = '##'.join(parr_phs)
                p_rfr        = map(str, pdict.get('restated_from_reported', [])) 
                offset_year   = pdict.get('offset_year', '')
                p_rfr        = '##'.join(p_rfr)
                p_ignore_wa  = pdict.get('ignore_ph^!!^winacr', '')
                p_db_wa      = pdict.get('db^!!^winacr', '')
                p_rfr_wa     = pdict.get('restated_from_reported^!!^winacr', '')
                p_phd_wa  = pdict.get('ph_derivation^!!^winacr', '')
                print [parent, grid, edit_p, pm_header, ptaxo_ids_str, pphs_str, p_rfr, p_ignore_wa, p_db_wa, p_rfr_wa, p_phd_wa, offset_year]
                pkey        = '@:@'.join([parent, grid, edit_p, pm_header, ptaxo_ids_str, pphs_str, p_rfr, p_ignore_wa, p_db_wa, p_rfr_wa, p_phd_wa, offset_year])
                if child_li:
                    for cdict in child_li:
                        ch      = cdict['p']
                        cgrid   = cdict['grpid']
                        main_header = cdict.get('mh', '')
                        ctaxo_ids_lst = map(str, cdict.get('t_ids', []))
                        ctaxo_ids_str = '&&'.join(ctaxo_ids_lst)
                        carr_phs     = map(str, cdict.get('ignore_phs', []))
                        cphs_str     = '##'.join(carr_phs)   
                        c_rfr        = map(str, cdict.get('restated_from_reported', []))
                        offset_year       = cdict.get('offset_year', '')
                        c_rfr        = '##'.join(c_rfr)
                        c_ignore_wa  = cdict.get('ignore_ph^!!^winacr', '')
                        c_db_wa      = cdict.get('db^!!^winacr', '')
                        c_rfr_wa     = cdict.get('restated_from_reported^!!^winacr', '')
                        c_phd_wa  = cdict.get('ph_derivation^!!^winacr', '')
                        ckey = '^^'.join([cgrid, main_header, ctaxo_ids_str, cphs_str, c_rfr, c_ignore_wa, c_db_wa, c_rfr_wa, c_phd_wa, offset_year])
                        #print ckey.split('^^')
                        #ckey = '^^'.join([cgrid, main_header, ctaxo_ids_str, cphs_str])
                        parent_child_rel.setdefault(pkey, []).append(ckey)
                else:
                    parent_child_rel[pkey] = []

            excel_config_str = '^!!^'.join(['$$'.join([pmap, '@@'.join(cli)]) for pmap, cli in parent_child_rel.iteritems()])

            if not daa:
                new_max_pid = get_max_pid()
                PID = str(new_max_pid)
                data_rows.append((company_id, company_name, new_max_pid, project_name, reporting_type, total_years, derived_ph, periods, ind_group_flg, taxo_flg, excel_config_str, rsp_period, rsp_year, rfr, restated_period, rfr_tt, event_year, disp_name, scale_vt, formula_derivation, es_period, es_year, doc_type_str, all_mt_str, r_as_r_data))
            #elif cur_pid != 'new':
            else:
                PID = cur_pid
                update_rows.append((company_id, company_name, project_name, reporting_type, total_years, derived_ph, periods, ind_group_flg, taxo_flg, excel_config_str, rsp_period, rsp_year, rfr, restated_period, rfr_tt, event_year, disp_name, scale_vt, formula_derivation, es_period, es_year, doc_type_str, all_mt_str, r_as_r_data, project_name))

        if update_rows:
            cur.executemany("UPDATE company_config set company_id=?, company_name=?, project_name=?, reporting_type=?, total_years=?, derived_ph=?, periods=?, ind_group_flg=?, taxo_flg=?, excel_config_str=?, rsp_period=?, rsp_year=?, rfr=?, restated_period=?, rfr_tt=?, event_year=?, disp_name=?, scale_vt=?, formula_derivation=?, excel_start_period=?, excel_start_year=?, doc_type_info=?, mt_set=?, r_as_r=? WHERE project_name=?", update_rows)
        if data_rows:
            cur.executemany("INSERT INTO company_config(company_id, company_name, project_id, project_name, reporting_type, total_years, derived_ph, periods, ind_group_flg, taxo_flg, excel_config_str, rsp_period, rsp_year, rfr, restated_period, rfr_tt, event_year, disp_name, scale_vt, formula_derivation, excel_start_period, excel_start_year, doc_type_info, mt_set, r_as_r) VALUES(?, ?, ?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?, ?, ?, ?, ?)", data_rows)
        conn.commit()
        conn.close()

    def txt_get_list_all_companies(self):
        file_path = '/mnt/eMB_db/dealid_company_info.txt'
        f = open(file_path)
        company_details = f.readlines()
        f.close()
        companyName_companyId_map = {}
        for comp in company_details:
            company_id, company_name, ippadrs = comp.split(':$$:')
            companyName_companyId_map[company_id] = company_name
        return companyName_companyId_map

    def get_list_all_companies(self):
        db_path = '/mnt/eMB_db/company_info/compnay_info.db' 
        conn  = sqlite3.connect(db_path)
        cur  =  conn.cursor()
        read_qry = 'select company_name, (project_id|| "_" ||toc_company_id) from company_info;'
        cur.execute(read_qry)
        table_data = cur.fetchall()
        conn.close()
        companyName_companyId_map = {}
        for comp in table_data:
            company_name, company_id = map(str, comp)
            companyName_companyId_map[company_id] = company_name
        return companyName_companyId_map

    def gbl_update_all(self, company_name, company_id, db_file, proj_info_li):
        try:
            conn, cur         = conn_obj.sqlite_connection(db_file)
        except:return

        crt_qry          = "CREATE TABLE IF NOT EXISTS company_config(row_id INTEGER PRIMARY KEY AUTOINCREMENT, company_id VARCHAR(20), company_name TEXT, project_id INTEGER, project_name TEXT, reporting_type VARCHAR(256), total_years INTEGER, derived_ph VARCHAR(256), periods VARCHAR(256), ind_group_flg INTEGER, taxo_flg INTEGER, excel_config_str TEXT, rsp_period TEXT, rsp_year TEXT, rfr TEXT, restated_period TEXT, rfr_tt TEXT)"
        cur.execute(crt_qry)
        self.alter_table_coldef(conn, cur, 'company_config', ['rsp_period', 'rsp_year', 'rfr', 'restated_period', 'rfr_tt', 'event_year', 'disp_name', 'scale_vt', 'doc_type_info', 'r_as_r'])

        update_rows = []
        PID         = 0
        for proj_info_dict in proj_info_li:
            project_name, reporting_type, total_years   = proj_info_dict['n'], proj_info_dict['st'], proj_info_dict['yr']
            derived_ph_lst, periods_lst                 = proj_info_dict['pt'], proj_info_dict['phd']
            ind_group_flg, taxo_flg, cur_pid            = proj_info_dict['rp'], proj_info_dict['tx'], proj_info_dict.get('pid', '')
            #excel_config_map                            = proj_info_dict['tt_list']
            rsp_period, rsp_year                        = proj_info_dict['re_p'], proj_info_dict['re_yr']
            rfr, restated_period                        = proj_info_dict['rfr'], proj_info_dict['rest_p']
            rfr_tt                                     = '##'.join(proj_info_dict['rfr_tt'])
            event_year                                  = proj_info_dict['event_yr']            
            disp_name                                   = proj_info_dict['disp_name'] 
            scale_vt_lst                                   = proj_info_dict.get('scale_t', '')
            scale_vt_dt_lst = []
            for svt_dct in scale_vt_lst:
                sc = svt_dct['s']
                vt = svt_dct['vt']
                rw = sc+'~'+vt
                scale_vt_dt_lst.append(rw)
            scale_vt = '#'.join(scale_vt_dt_lst)
            qry = 'select company_name, project_id from company_config where project_name like "%s"'%(project_name)
            cur.execute(qry)
            daa = cur.fetchone()

            if not daa:
                return 'project name does not exists'

            if daa and daa[1]:
                cur_pid = str(daa[1])
            
            derived_ph  = '##'.join(derived_ph_lst)
            periods     = '##'.join(periods_lst)

            if 1:
                PID = cur_pid
                update_rows.append((reporting_type, total_years, derived_ph, periods, ind_group_flg, taxo_flg, rsp_period, rsp_year, rfr, restated_period, rfr_tt, event_year, disp_name, scale_vt, project_name))

        if update_rows:
            cur.executemany("UPDATE company_config set reporting_type=?, total_years=?, derived_ph=?, periods=?, ind_group_flg=?, taxo_flg=?, rsp_period=?, rsp_year=?, rfr=?, restated_period=?, rfr_tt=?, event_year=?, disp_name=?, scale_vt=? WHERE project_name=?", update_rows)
        conn.commit()
        conn.close()
        return 'done'

    def connect_to_sqlite(self, db_path):
        import sqlite3
        conn = sqlite3.connect(db_path)
        cur  = conn.cursor()
        return conn, cur
        
    def read_project_display_name_from_global(self):
        db_path          = '/mnt/eMB_db/node_mapping.db'
        conn, cur = self.connect_to_sqlite(db_path)
        read_qry         = " select project_name, disp_name from company_config; "
        cur.execute(read_qry)
        t_data = cur.fetchall()     
        conn.close()
        project_disp_name_map = {}
        for row in t_data:
            project_name, disp_name = map(str, row)
            project_disp_name_map[project_name] = disp_name
        return project_disp_name_map 
        
    def read_template_info(self):
        db_path = '/mnt/eMB_db/template_info.db'
        conn = sqlite3.connect(db_path)
        cur  = conn.cursor()
        read_qry = """ SELECT row_id, sheet_name, project_name, display_name, industry FROM meta_info; """
        cur.execute(read_qry)
        t_data = cur.fetchall()
        conn.close()
        template_dct = {}
        for row_data in t_data:
            row_id, sheet_name, project_name, display_name, industry = row_data
            template_dct.setdefault((project_name, industry), []).append({'k':str(row_id), 'n':display_name})
        return template_dct

    def read_template_info_dic(self):
        db_path = '/mnt/eMB_db/template_info.db'
        conn = sqlite3.connect(db_path)
        cur  = conn.cursor()
        read_qry = 'SELECT row_id, sheet_name, project_name, display_name, industry, time_series from meta_info'
        cur.execute(read_qry)
        table_data = cur.fetchall()
        conn.close()
        template_dct = {}
        for row in table_data:
            row_id, sheet_name, project_name, display_name, industry, time_series = row[:]
            template_dct.setdefault((project_name, industry), {})[str(row_id)]  = row
        return template_dct
            

    ## 61 Read Comp Config
    def read_companyConfigTable(self, ijson):
        company_name     = ijson["company_name"]
        model_number     = ijson["model_number"]
        deal_id          = ijson["deal_id"] 
        project_id       = ijson["project_id"]
        company_id       = '%s_%s'%(project_id, deal_id)
        industry_type            = ijson['industry_type']

        get_tt_grp_wise_group_txt = self.read_f_vgh_info(ijson)
        db_file          = self.get_db_path(ijson)
        rs_ls, proj = self.read_gbl_companyConfig(db_file, 0, {}, get_tt_grp_wise_group_txt, industry_type) 
    
        get_tt_grp_wise_group_txt = {}
        db_file          = '/mnt/eMB_db/node_mapping.db'
        res_lst, project = self.read_gbl_companyConfig(db_file, 1, proj, get_tt_grp_wise_group_txt)
        return rs_ls + res_lst 

    def read_f_vgh_info(self, ijson):
        company_name     = ijson["company_name"]
        model_number     = ijson["model_number"]
        deal_id          = ijson["deal_id"] 
        project_id       = ijson["project_id"]
        company_id       = '%s_%s'%(project_id, deal_id)
        db_path = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn = sqlite3.connect(db_path)
        cur  = conn.cursor()
        read_qry = """SELECT row_id, table_type, group_txt FROM f_vgh_group_info"""
        cur.execute(read_qry)
        table_data = cur.fetchall()
        conn.close()
        res_dct = {}
        for row in table_data:
            row_id, table_type, group_txt = row
            group_txt = group_txt.replace('\t', '')
            res_dct[(table_type, row_id)] = group_txt
        return res_dct 

    def read_gbl_companyConfig(self, db_file, gbl, ignore_proj, get_tt_grp_wise_group_txt={}, industry_type=''):
        template_dct = self.read_template_info()
        conn, cur        = conn_obj.sqlite_connection(db_file)
        self.alter_table_coldef(conn, cur, 'company_config', ['rsp_period', 'rsp_year', 'rfr', 'restated_period', 'rfr_tt', 'event_year', 'disp_name', 'scale_vt', 'formula_derivation', 'excel_start_period', 'excel_start_year', 'doc_type_info', 'mt_set', 'r_as_r'])
        read_qry         = "select company_id, company_name, project_id, project_name, reporting_type, total_years, derived_ph, periods, ind_group_flg, taxo_flg, excel_config_str, rsp_period, rsp_year, rfr, restated_period, rfr_tt, event_year, disp_name, scale_vt, formula_derivation, excel_start_period, excel_start_year, doc_type_info, mt_set, r_as_r from company_config"
        try:
            cur.execute(read_qry)
            companyConfig_data      = cur.fetchall()
        except:
            companyConfig_data      = []
        conn.close()
        project_disp_name_dct = self.read_project_display_name_from_global()
        res_lst          = []
        proj = {}
        #db_file_gbl = '/mnt/eMB_db/node_mapping.db'
        proj_n_d = self.formula_read_company(db_file)
        for row in companyConfig_data:
            
            company_id, company_name, project_id, project_name, reporting_type, total_years, derived_ph, periods, ind_group_flg, taxo_flg, excel_config_str, rsp_period, rsp_year, rfr, restated_period, rfr_tt, event_year, disp_name, scale_vt, formula_derivation, excel_start_period, excel_start_year, doc_type_info, mt_set, r_as_r_data = map(str, row)
            
            temp_info_lst  = template_dct.get((project_name, industry_type), [])
            ## Group 1##pd~pd2@@Group 2##pd~pd2
            dp_name = project_disp_name_dct.get(project_name, '')
            if dp_name:
                disp_name = dp_name
            if project_name in ignore_proj:continue
            proj[project_name] = 1
            derived_ph_li   = derived_ph.split('##')
            period_li       = periods.split('##')
            tt_list         = []

            new_ph_list = []
            ph_list = proj_n_d.get(project_name, [])
            res_ph_dct = {}
            for dct in ph_list:
                tt, grpid = dct["mh"], dct["grpid"]
                if tt == '':
                    tt  = dct['p']
                res_ph_dct[(tt, grpid)] = dct
            dtf = []
            if doc_type_info and doc_type_info != 'None':
                for rss in doc_type_info.split('@@'):
                    grp_info, dptype = rss.split('##')          
                    dpct = {'p':grp_info, 'c':dptype.split('~')}
                    dtf.append(dpct)
            
            mt_set_lst = []
            if mt_set and mt_set != 'None':
                for mt_i in mt_set.split('@@'):
                    rmt_id, rmt_data = mt_i.split('~')
                    itd = {e:1 for e in rmt_data.split('##') if e}
                    i_tt_dct = {'k':rmt_id, 'ignore_tt':itd}
                    mt_set_lst.append(i_tt_dct) 

            excel_config_li = excel_config_str.split('^!!^')
            #IS@:@@:@0@:@IS$$^!!^BS@:@@:@0@:@BS$$^!!^CF@:@@:@0@:@CF$$^!!^RBS@:@@:@0@:@RBS$$18^^RBS@@17^^RBS@@HGHGROUP-RBS-56354-^^RBS@@HGHGROUP-RBS-56410-^^RBS@@HGHGROUP-RBS-56392-^^RBS^!!^RBG@:@@:@0@:@RBG$$
            for e in excel_config_li:
                if e == '':continue
                pstr, groups = e.split('$$')
                pstr_sp = pstr.split('@:@')
                p, pgrid, edf, pmh = pstr_sp[:4]
                ptids   = []
                try:
                    ptids   = pstr_sp[4]
                    ptids = ptids.split('&&')
                except:pass
                try:    
                    pa_phs = pstr_sp[5]
                    pa_phs = pa_phs.split('##')
                    if not pa_phs[0]:
                        pa_phs = []
                except:pa_phs = [] 
                try:
                    lp_rfr = pstr_sp[6]
                    lp_rfr = lp_rfr.split('##') if lp_rfr not in ['across', 'within'] else []
                except:lp_rfr = []
                try:
                    p_ig = pstr_sp[7]
                except:p_ig = ''
                try:
                    p_db = pstr_sp[8]
                except:p_db = ''
                try:
                    p_rfr = pstr_sp[9]
                except:p_rfr = ''
                try:
                    p_phd = pstr_sp[10]
                except:p_phd = ''
                try:
                    p_offset_year = pstr_sp[11]
                except:p_offset_year = ''
                cli = []
                if groups:
                    group_li = groups.split('@@')
                    for group in group_li:
                        group_sp    = group.split('^^')
                        cgrid, cmh  = group_sp[:2]
                        try:
                            ctids = group_sp[2].split('&&')
                        except:
                            ctids = []
                        try:
                            ca_phs = group_sp[3]
                            ca_phs = ca_phs.split('##')
                            if not ca_phs[0]:
                                ca_phs = []
                        except:ca_phs = []
                        try:
                            lc_rfr = group_sp[4]
                            lc_rfr = lc_rfr.split('##') if lp_rfr not in ['across', 'within'] else []
                        except:lc_rfr = []
                        try:
                            c_ig = group_sp[5]
                        except:c_ig = ''
                        try:
                            c_db = group_sp[6]
                        except:c_db = ''
                        try:
                            c_rfr = group_sp[7]
                        except:c_rfr = ''
                        try:
                            c_phd = group_sp[8]
                        except:c_phd = ''
                        try:
                            c_offset_year = group_sp[9]
                        except:c_offset_year = ''

                        t = {'p':'', 'mh':cmh, 'c':[], 'grpid':cgrid, 't_ids':ctids, 'ignore_phs':ca_phs, 'restated_from_reported':lc_rfr, 'ignore_ph^!!^winacr':c_ig, 'db^!!^winacr':c_db, 'restated_from_reported^!!^winacr':c_rfr, 'ph_derivation^!!^winacr':c_phd, 'offset_year':c_offset_year}
                        cli.append(t)
                get_p = get_tt_grp_wise_group_txt.get((pmh, pgrid), '')
                if not get_p:
                    get_p = p
                tmp = {'p':get_p, 'c':cli, 'edit_p':int(edf), 'grpid':pgrid, 'mh':pmh, 't_ids':ptids, 'ignore_phs':pa_phs, 'restated_from_reported':lp_rfr, 'ignore_ph^!!^winacr':p_ig, 'db^!!^winacr':p_db, 'restated_from_reported^!!^winacr':p_rfr, 'ph_derivation^!!^winacr':p_phd, 'offset_year':p_offset_year}
                #tmp = {'p':p, 'c':cli, 'edit_p':int(edf), 'grpid':pgrid, 'mh':pmh, 't_ids':ptids, 'ignore_phs':pa_phs}
                tt_list.append(tmp)
                get_tt_dat = res_ph_dct.get((pmh, pgrid), [])
                if not get_tt_dat:
                    get_tt_dat =  res_ph_dct.get((pmh, ''), []) 
                if get_tt_dat:
                    get_tt_dat  = copy.deepcopy(get_tt_dat)
                    get_tt_dat['mh']    = pmh
                    get_tt_dat['grpid']    = pgrid
                    get_tt_dat['p']    = p
                    new_ph_list.append(get_tt_dat)
            
            if r_as_r_data == "None":
                r_as_r_data = ""
            if rsp_period == "None":
                rsp_period = ""
            if rsp_year == "None":
                rsp_year = ""
            if rfr == "None":
                rfr = ""
            if excel_start_period == "None":
                excel_start_period = ""
            if excel_start_year == "None":
                excel_start_year = ""
            if rfr == "None":
                rfr = ""
            if restated_period == "None":
                restated_period = "" 
            if rfr_tt == "None" or (not rfr_tt):
                rfr_tt = []
            elif (rfr_tt != "None") and rfr_tt and rfr == 'Y':
                rfr_tt = rfr_tt.split('##')
            if event_year == "None" or (not event_year):
                event_year = ""
            if disp_name == "None" or (not disp_name):
                disp_name = ""
            if scale_vt == "None" or (not scale_vt):
                scale_vt_lst = []
            elif scale_vt != "None" and scale_vt:
                scale_vt_lst = []
                for el_str in scale_vt.split('#'):
                    if len(el_str.split('~')) == 2:
                        sc, vt  = el_str.split('~')
                        igtids  = {}
                    else:
                        sc, vt, igtids  = el_str.split('~')
                        igtids  = eval(igtids)
                    dt_dct = {'s':sc, 'vt':vt, 'ignore_ph':igtids}
                    scale_vt_lst.append(dt_dct)
            if not formula_derivation or formula_derivation == 'None':
                formula_derivation  = '0'
                 
            res_dct = {'pid':project_id, 'n':project_name, 'st':reporting_type, 'yr':total_years, 'pt':derived_ph_li, 'phd':period_li, 'rp':ind_group_flg, 'tx':taxo_flg, 'ef':0, 'tt_list':tt_list, 'gbl':gbl, 'ph_list':ph_list, 're_p':rsp_period, 're_yr':rsp_year, 'rfr':rfr, 'rest_p':restated_period, 'rfr_tt':rfr_tt, "event_yr":event_year, 'disp_name':disp_name, 'scale_t':scale_vt_lst, 'formula_derivation':formula_derivation, 'es_p':excel_start_period, 'es_yr':excel_start_year, 'doc_t_grp':dtf, 'template_info_lst':temp_info_lst, 'mt_set':mt_set_lst, 'r_as_r':r_as_r_data}
            res_lst.append(res_dct)
        res_lst = sorted(res_lst, key=lambda x:int(x['pid']))
        return res_lst, proj

    ## 84 Delete Comp Config
    def delete_companyConfigTable_pid(self, ijson):
        company_name     = ijson["company_name"]
        model_number     = ijson["model_number"]
        deal_id          = ijson["deal_id"] 
        project_id       = ijson["project_id"]
        company_id       = '%s_%s'%(project_id, deal_id)

        gbl              = ijson["gbl"]
        project_name     = ijson["project_name"]
        p_id             = ijson["pid"]
        db_file          = self.get_db_path(ijson)
        conn, cur        = conn_obj.sqlite_connection(db_file)

        del_stmt = 'delete from company_config where project_name="%s"'%(project_name)
        cur.execute(del_stmt)
        conn.commit()
        conn.close()
        if gbl:
            db_file  = '/mnt/eMB_db/node_mapping.db'
            conn = sqlite3.connect(db_file)
            cur  = conn.cursor()
            del_stmt = 'delete from company_config where project_name="%s"'%(project_name)
            cur.execute(del_stmt)
            conn.commit()
            conn.close()
        return [{'message':'done'}]

    ## 89 Delete HGH group
    def delete_hgh_group(self, ijson):
        company_name     = ijson["company_name"]
        model_number     = ijson["model_number"]
        deal_id          = ijson["deal_id"]
        project_id       = ijson["project_id"]
        company_id       = '%s_%s'%(project_id, deal_id)
        if ijson.get('ALL_HGH') == 'Y':
            return self.delete_all_hgh_group(ijson)

        hgh_gid          = ijson["gid"]

        if 'HGHGROUP' not in hgh_gid:
            return [{'message':'done'}]

        table_type       = ijson["table_type"]
        db_file          = self.get_db_path(ijson)
        conn, cur        = conn_obj.sqlite_connection(db_file)

        del_stmt = 'delete from mt_data_builder where table_type = "%s"'%(hgh_gid)
        cur.execute(del_stmt)
        #print '>>MT ', del_stmt

        del_stmt = 'delete from vgh_group_info where table_type = "%s"'%(hgh_gid)
        try:
            cur.execute(del_stmt)
        except:pass

        del_stmt = 'delete from f_vgh_group_info where table_type = "%s"'%(hgh_gid)
        try:
            cur.execute(del_stmt)
        except:pass
        #print '>>VGH ', del_stmt

        conn.commit()
        conn.close()
        return [{'message':'done'}]

    ## 89 Delete HGH group
    def delete_all_hgh_group(self, ijson):
        company_name     = ijson["company_name"]
        model_number     = ijson["model_number"]
        deal_id          = ijson["deal_id"]
        project_id       = ijson["project_id"]
        company_id       = '%s_%s'%(project_id, deal_id)

        table_type       = ijson["table_type"]
        db_file          = self.get_db_path(ijson)
        conn, cur        = conn_obj.sqlite_connection(db_file)
        sql = "select row_id, table_type, group_txt from f_vgh_group_info where table_type like '%s'"%('HGHGROUP-'+table_type+'%')
        cur.execute(sql)
        res = cur.fetchall()
        ids = {}
        for rr in res:
            group_id, table_type, group_txt = rr
            ids[table_type] = 1
        
        for hgh_gid in ids.keys():
            del_stmt = 'delete from mt_data_builder where table_type = "%s"'%(hgh_gid)
            cur.execute(del_stmt)
            #print '>>MT ', del_stmt

            del_stmt = 'delete from vgh_group_info where table_type = "%s"'%(hgh_gid)
            cur.execute(del_stmt)

            del_stmt = 'delete from f_vgh_group_info where table_type = "%s"'%(hgh_gid)
            try:
                cur.execute(del_stmt)
            except:pass
        #print '>>VGH ', del_stmt

        conn.commit()
        conn.close()
        return [{'message':'done'}]

    def getProjectId_details(self, ijson):
        company_name = ijson["company_name"]
        model_number = ijson["model_number"]
        project_id   = ijson["project_id"]
        deal_id      = ijson["deal_id"]
        company_id   = "%s_%s"%(project_id, deal_id)
        db_path      = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn         = sqlite3.connect(db_path)
        cur          = conn.cursor() 
        
        comp_qry     = 'select project_id, project_name from company_config;'
        try:
            cur.execute(comp_qry)
            comp_data    = cur.fetchall()
        except:
            comp_data = []
        
        res_lst      = []
        required_lst      = []
        for row in comp_data:
            project_id, project_name = map(str, row)
            res_lst.append({'n':project_name, 'pid':project_id})
            required_lst.append(project_name)

        conn.close()
        
        required_str = ['"'+e+'"' for e in required_lst]
        gdb_path  = '/mnt/eMB_db/node_mapping.db'
        gconn     = sqlite3.connect(gdb_path)
        gcur      = gconn.cursor()
        gbl_qry   = "select project_id, project_name from company_config where project_name not in (%s)"%(', '.join(required_str))
        try:
            gcur.execute(gbl_qry)
            gbl_data  = gcur.fetchall()
        except:
            gbl_data = []
        for row in gbl_data:
            project_id, project_name = map(str, row)
            res_lst.append({'n':project_name, 'pid':project_id})
        gconn.close()    
        return res_lst
    def bugetProjectId_details_comp(self, ijson):
        company_name = ijson["company_name"]
        model_number = ijson["model_number"]
        project_id   = ijson["project_id"]
        deal_id      = ijson["deal_id"]
        company_id   = "%s_%s"%(project_id, deal_id)
        db_path      = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn         = sqlite3.connect(db_path)
        cur          = conn.cursor() 
        
        comp_qry     = 'select project_id, project_name from company_config;'
        try:
            cur.execute(comp_qry)
            comp_data    = cur.fetchall()
        except:
            comp_data = []
        
        res_lst      = []
        required_lst      = []
        for row in comp_data:
            project_id, project_name = map(str, row)
            res_lst.append({'n':project_name, 'pid':project_id})
            required_lst.append(project_name)

        conn.close()
        
        return res_lst

    def get_year_data(self, ijson):
        if ijson.get('project_name', ''):
            company_name    = ijson['company_name']
            mnumber         = ijson['model_number']
            db_file         = self.get_db_path(ijson)
            conn, cur   = conn_obj.sqlite_connection(db_file)
            for col in ['rsp_period', 'rsp_year', 'rfr', 'restated_period', 'rfr_tt', 'excel_start_period', 'excel_start_year']:
                try:
                    sql = 'alter table company_config add column %s TEXT'%(col)
                    cur.execute(sql)
                except:pass
            sql         = "select reporting_type, total_years, derived_ph, periods, ind_group_flg, taxo_flg, excel_config_str, rsp_period, rsp_year, rfr, restated_period, rfr_tt, excel_start_period, excel_start_year from company_config where project_name='%s'"%(ijson['project_name'])
            try:
                cur.execute(sql)
                res = cur.fetchall()
            except:
                res = []
            conn.close()
            for rr in res:
                reporting_type, total_years, derived_ph, periods, ind_group_flg, taxo_flg, excel_config_str, rsp_period, rsp_year, rfr, restated_period, rfr_tt, excel_start_period, excel_start_year  = rr
                if reporting_type == 'Reported':
                    rfr, restated_period    ='',''
                if rfr_tt and rfr == 'Y':
                    rfr_tt  = rfr_tt.split('##')
                else:
                    rfr_tt  = []
                ex_d    = {}
                for pc in excel_config_str.split('^!!^'):
                    if not pc:continue
                    p, cs    = pc.split('$$')
                    p_sp = p.split('@:@')
                    text, group_id, tmpy, ttype = p_sp[:4]
                    t_ids   = []
                    if len(p_sp) > 4:
                        t_ids   = filter(lambda x:x, p_sp[4].split('&&'))
                    ignore_dphs   = []
                    if len(p_sp) > 5:
                        ignore_dphs   = filter(lambda x:x, p_sp[5].split('##'))
                    remove_not_restated = []
                    if len(p_sp) > 6:
                        remove_not_restated = filter(lambda x:x, p_sp[6].split('##'))
                    print p_sp
                    if tmpy == '0' and ttype:
                        try:
                            p_ig = p_sp[7]
                        except:p_ig = ''
                        try:
                            p_db = p_sp[8]
                        except:p_db = ''
                        try:
                            p_rfr = p_sp[9]
                        except:p_rfr = ''
                        try:
                            p_phd = p_sp[10]
                        except:p_phd = ''
                        if p_rfr == 'within':
                            ex_d[('RFR', ttype)] = remove_not_restated+ex_d.get(('RFR', ttype), [])
                        elif p_rfr == 'across':
                                ex_d[('RFR', 'DEAL')] = remove_not_restated+ex_d.get(('RFR', 'DEAL'), [])
                        if group_id:
                            ex_d[(ttype, group_id)] = (t_ids, ignore_dphs, remove_not_restated)
                        else:
                            ex_d[(ttype, '')] = (t_ids, ignore_dphs, remove_not_restated)
                    for c in cs.split('@@'):
                        if not c:continue
                        c_sp    = c.split('^^')
                        group_id, table_type    = c_sp[:2]
                        t_ids   = []
                        if len(c_sp) > 2:
                            t_ids   = filter(lambda x:x, c_sp[2].split('&&'))
                        ignore_dphs   = []
                        if len(c_sp) > 3:
                            ignore_dphs   = filter(lambda x:x, c_sp[3].split('##'))
                        remove_not_restated   = []
                        if len(c_sp) > 4:
                            remove_not_restated   = filter(lambda x:x, c_sp[4].split('##'))
                        print '\tC', c_sp

                        try:
                            c_ig = c_sp[5]
                        except:c_ig = ''
                        try:
                            c_db = c_sp[6]
                        except:c_db = ''
                        try:
                            c_rfr = c_sp[7]
                        except:c_rfr = ''
                        try:
                            c_phd = c_sp[8]
                        except:c_phd = ''
                        if c_rfr == 'within':
                            ex_d[('RFR', table_type)] = remove_not_restated+ex_d.get(('RFR', table_type), [])
                        elif c_rfr == 'across':
                            ex_d[('RFR', 'DEAL')] = remove_not_restated+ex_d.get(('RFR', 'DEAL'), [])


                        if group_id:
                            ex_d[(table_type, group_id)] = (t_ids, ignore_dphs, remove_not_restated)
                        else:
                            ex_d[(table_type, '')] = (t_ids, ignore_dphs, remove_not_restated)
                ijson["ex_d"]   = ex_d
        return ijson
    def cppgetProjectId_details_comp(self, ijson):
        company_name = ijson["company_name"]
        model_number = ijson["model_number"]
        deal_id      = ijson["deal_id"]
        project_id   = ijson["project_id"]
        company_id   = "%s_%s"%(project_id, deal_id)
        db_path      = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn         = sqlite3.connect(db_path)
        cur          = conn.cursor() 
        
        sql = "CREATE TABLE IF NOT EXISTS vgh_group_info(row_id INTEGER PRIMARY KEY AUTOINCREMENT, group_id VARCHAR(20), table_type VARCHAR(100), group_txt VARCHAR(20), user_name VARCHAR(100), datetime TEXT);"
        cur.execute(sql)

        try:
            sql     = "CREATE TABLE IF NOT EXISTS mt_data_builder(row_id INTEGER PRIMARY KEY AUTOINCREMENT, taxo_id INTEGER, prev_id INTEGER,order_id INTEGER, table_type, taxonomy TEXT, user_taxonomy TEXT, missing_taxo varchar(1), table_id TEXT, c_id TEXT, gcom TEXT, ngcom TEXT, ph TEXT, ph_label TEXT, user_name TEXT, datetime TEXT, isvisible varchar(1), m_rows TEXT, vgh_text TEXT, vgh_group TEXT, doc_id INTEGER, xml_id TEXT, period VARCHAR(50), period_type VARCHAR(50), scale VARCHAR(50), currency VARCHAR(50), value_type VARCHAR(50))"
            cur.execute(sql)
            cur.commit()
        except:pass
        read_qry = 'select table_type, group_id, group_txt from vgh_group_info;'
        cur.execute(read_qry)
        tt_grp_data = {tuple(map(str, row[:2])):row[2] for row in cur.fetchall()}
        for k in tt_grp_data.keys():
            table_type  = k[0]
            sql = "select group_id, table_type, group_txt from vgh_group_info where table_type like '%s'"%('HGHGROUP-'+table_type+'%')
            try:
                cur.execute(sql)
                res1 = cur.fetchall()
            except:
                res1 = []
            for rr in res1:
                group_id, ttable_type, group_txt = rr
                group_txt = group_txt.replace('\t', '')
                tt_grp_data[(table_type, ttable_type)]  = group_txt
        # mt_data_builder
        read_qry = 'select table_type, taxo_id from mt_data_builder where isvisible="Y"'
        cur.execute(read_qry)
        tx_ids_data = {(row[0], int(row[1])):1 for row in cur.fetchall()}
        comp_qry     = 'select project_id, project_name from company_config;'

        #check table_type
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number)

        try:
            cur.execute(comp_qry)
            comp_data    = cur.fetchall()
        except:
            comp_data = []

        res_lst      = []
        required_lst      = []
        for row in comp_data:
            project_id, project_name = map(str, row)
            ijson["project_name"] = project_name
            get_lst_ex_d = self.get_year_data(ijson).get('ex_d', {})
            error_flg_lst = []
            for tt_grp, taxo_ids_lst in get_lst_ex_d.items():
                get_grp_txt = tt_grp_data.get(tt_grp, '')
                table_type = tt_grp[0]

                if table_type not in rev_m_tables:
                    error_flg_lst.append("%s - Does not exists"%(table_type))

                if tt_grp[1]:
                    if tt_grp not in tt_grp_data:
                        error_flg_lst.append('%s-%s Not exists %s '%(table_type, get_grp_txt, tt_grp[1])) #{'e':list(tt_grp), 'm':'This table_type and group_id does not exists'})
                taxo_error = []
                for taxo_id in taxo_ids_lst[0]:
                    if int(taxo_id) <0 :continue
                    if (table_type, int(taxo_id)) not in tx_ids_data:
                        taxo_error.append(taxo_id)
                if not taxo_error:continue
                error_flg_lst.append("Taxo ids are not exists in %s-%s "%(table_type, get_grp_txt)) #{'e':taxo_error, 'm':'This taxo_ids does not exists'})
           
            ef = 0
            if error_flg_lst:
                ef = 1 
            res_lst.append({'n':project_name, 'pid':project_id, 'ef':ef, 'em':error_flg_lst, 'rev_m_tables':rev_m_tables.keys()})
            required_lst.append(project_name)
        conn.close()
        return res_lst
        
    def read_doc_type_wise_doc_ids(self, company_name, model_number):
        db_path = '/mnt/eMB_db/%s/%s/tas_company.db'%(company_name, model_number)
        conn = sqlite3.connect(db_path)
        cur  = conn.cursor()
        read_qry = """ SELECT document_type, doc_id FROM company_meta_info; """
        cur.execute(read_qry)
        t_data = cur.fetchall() 
        conn.close()    
        res_dct = {}
        for row in t_data:
            document_type, doc_id = map(str, row)    
            res_dct.setdefault(document_type, []).append(doc_id)
        return res_dct            

    def getProjectId_details_comp(self, ijson, grp_map_d={}):
        company_name = ijson["company_name"]
        model_number = ijson["model_number"]
        deal_id      = ijson["deal_id"]
        project_id   = ijson["project_id"]
        company_id   = "%s_%s"%(project_id, deal_id)
        doc_type_doc_lst_map = self.read_doc_type_wise_doc_ids(company_name, model_number)
        db_path      = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn         = sqlite3.connect(db_path)
        cur          = conn.cursor() 
        
        sql = "CREATE TABLE IF NOT EXISTS f_vgh_group_info(row_id INTEGER PRIMARY KEY AUTOINCREMENT, group_id VARCHAR(20), table_type VARCHAR(100), group_txt VARCHAR(20), table_str TEXT,user_name VARCHAR(100), datetime TEXT);"
        cur.execute(sql)

        try:
            sql     = "CREATE TABLE IF NOT EXISTS mt_data_builder(row_id INTEGER PRIMARY KEY AUTOINCREMENT, taxo_id INTEGER, prev_id INTEGER,order_id INTEGER, table_type, taxonomy TEXT, user_taxonomy TEXT, missing_taxo varchar(1), table_id TEXT, c_id TEXT, gcom TEXT, ngcom TEXT, ph TEXT, ph_label TEXT, user_name TEXT, datetime TEXT, isvisible varchar(1), m_rows TEXT, vgh_text TEXT, vgh_group TEXT, doc_id INTEGER, xml_id TEXT, period VARCHAR(50), period_type VARCHAR(50), scale VARCHAR(50), currency VARCHAR(50), value_type VARCHAR(50))"
            cur.execute(sql)
            cur.commit()
        except:pass
        read_qry = 'select table_type, row_id, group_txt from f_vgh_group_info;'
        cur.execute(read_qry)
        tt_grp_data = {tuple(map(str, row[:2])):row[0]+' - '+row[2] for row in cur.fetchall()}
        for k in tt_grp_data.keys():
            table_type  = k[0]
            sql = "select row_id, table_type, group_txt from f_vgh_group_info where table_type like '%s'"%('HGHGROUP-'+table_type+'%')
            try:
                cur.execute(sql)
                res1 = cur.fetchall()
            except:
                res1 = []
            for rr in res1:
                group_id, ttable_type, group_txt = rr
                group_txt = group_txt.replace('\t', '')
                tt_grp_data[(table_type, ttable_type)]  = table_type+' - '+group_txt
        
        # taxo_data_builder
        d_pt = '/mnt/eMB_db/%s/%s/taxo_data_builder.db'%(company_name, model_number) 
        conn1         = sqlite3.connect(d_pt)
        cur1          = conn1.cursor() 
        sql     = "CREATE TABLE IF NOT EXISTS mt_data_builder(row_id INTEGER PRIMARY KEY AUTOINCREMENT, taxo_id INTEGER, prev_id INTEGER,order_id INTEGER, table_type, taxonomy TEXT, user_taxonomy TEXT, missing_taxo varchar(1), table_id TEXT, c_id TEXT, gcom TEXT, ngcom TEXT, ph TEXT, ph_label TEXT, user_name TEXT, datetime TEXT, isvisible varchar(1), m_rows TEXT, vgh_text TEXT, vgh_group TEXT, doc_id INTEGER, xml_id TEXT, period VARCHAR(50), period_type VARCHAR(50), scale VARCHAR(50), currency VARCHAR(50), value_type VARCHAR(50))"
        cur1.execute(sql)
        conn1.commit()
        read_qry = 'select table_type, taxo_id from mt_data_builder where isvisible="Y"'
        cur1.execute(read_qry)
        tx_ids_data = {(row[0], int(row[1])):1 for row in cur1.fetchall()}
        conn1.close()


        #check table_type
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number)

        comp_qry     = 'select project_id, project_name, periods, excel_config_str, doc_type_info from company_config;'
        try:
            cur.execute(comp_qry)
            comp_data    = cur.fetchall()
        except:
            comp_data = []

        res_lst      = []
        required_lst      = []
        for row in comp_data:
            project_id, project_name, derived_ph, excel_config_str, doc_type_info = map(str, row)
            ijson["project_name"] = project_name
            get_lst_ex_d = self.get_year_data(ijson).get('ex_d', {})
            error_flg_lst = []
            for tt_grp, taxo_ids_lst in get_lst_ex_d.items():
                get_grp_txt = tt_grp_data.get(tt_grp, '')
                table_type = tt_grp[0]

                if table_type not in rev_m_tables:
                    error_flg_lst.append("%s - Does not exists"%(table_type))

                if tt_grp[1]:
                    if tt_grp not in tt_grp_data:
                        error_flg_lst.append('%s-%s Not exists %s '%(table_type, get_grp_txt, tt_grp[1])) #{'e':list(tt_grp), 'm':'This table_type and group_id does not exists'})
                taxo_error = []
                if taxo_ids_lst:
                    for taxo_id in taxo_ids_lst[0]:
                        try:
                            if int(taxo_id) <0 :continue
                        except:continue
                        if (table_type, int(taxo_id)) not in tx_ids_data:
                            #print (table_type, int(taxo_id)),project_name
                            #print [get_grp_txt]
                            taxo_error.append(taxo_id)
                if not taxo_error:continue
                error_flg_lst.append("Taxo ids are not exists in %s-%s "%(table_type, get_grp_txt)) #{'e':taxo_error, 'm':'This taxo_ids does not exists'})
           
            ef = 0
            if error_flg_lst:   
                ef = 1
            phd = derived_ph.split('##')
            tt_list = []
            excel_config_li = excel_config_str.split('^!!^')
            #IS@:@@:@0@:@IS$$^!!^BS@:@@:@0@:@BS$$^!!^CF@:@@:@0@:@CF$$^!!^RBS@:@@:@0@:@RBS$$18^^RBS@@17^^RBS@@HGHGROUP-RBS-56354-^^RBS@@HGHGROUP-RBS-56410-^^RBS@@HGHGROUP-RBS-56392-^^RBS^!!^RBG@:@@:@0@:@RBG$$
            for e in excel_config_li:
                if e == '':continue
                pstr, groups = e.split('$$')
                pstr_sp = pstr.split('@:@')
                p, pgrid, edf, pmh = pstr_sp[:4]
                ptids   = []
                try:
                    ptids   = pstr_sp[4]
                    ptids = ptids.split('&&')
                except:pass
                try:    
                    pa_phs = pstr_sp[5]
                    pa_phs = pa_phs.split('##')
                    if not pa_phs[0]:
                        pa_phs = []
                except:pa_phs = [] 
                cli = []
                if groups:
                    group_li = groups.split('@@')
                    for group in group_li:
                        group_sp    = group.split('^^')
                        cgrid, cmh  = group_sp[:2]
                        try:
                            ctids = group_sp[2].split('&&')
                        except:
                            ctids = []
                        try:
                            ca_phs = group_sp[3]
                            ca_phs = ca_phs.split('##')
                            if not ca_phs[0]:
                                ca_phs = []
                        except:ca_phs = []
                        t = {'p':'', 'mh':cmh, 'c':[], 'grpid':cgrid, 't_ids':ctids, 'ignore_phs':ca_phs, 'name':tt_grp_data.get((cmh, cgrid), '') if cgrid else cmh, 'vids':grp_map_d.get((cmh, cgrid), [])}
                        cli.append(t)
                if int(edf):
                    tmp = {'p':p, 'c':cli, 'edit_p':int(edf), 'grpid':pgrid, 'mh':pmh, 't_ids':ptids, 'ignore_phs':pa_phs, 'name':p, 'k': pmh, 'l': p}
                else:
                    tmp = {'p':p, 'c':cli, 'edit_p':int(edf), 'grpid':pgrid, 'mh':pmh, 't_ids':ptids, 'ignore_phs':pa_phs, 'name':tt_grp_data.get((pmh, pgrid), '') if pgrid else pmh, 'vids':grp_map_d.get((pmh, pgrid), []), 'k': pmh, 'l': tt_grp_data.get((pmh, pgrid), '') if pgrid else pmh}
                tt_list.append(tmp)
            dtf = []
            if doc_type_info and doc_type_info != 'None':
                for rss in doc_type_info.split('@@'):
                    grp_info, dptype = rss.split('##')          
                    cis = []
                    grp_doc_ids = []
                    for rbs in dptype.split('~'):
                        dcs = doc_type_doc_lst_map.get(rbs, [])
                        grp_doc_ids.extend(dcs)
                        cis.append(rbs)
                    dpct = {'p':grp_info, 'c':cis, 'grp_doc_ids':{e:1 for e in grp_doc_ids}}
                    dtf.append(dpct)
             
            res_lst.append({'n':project_name, 'pid':project_id, 'ef':ef, 'em':error_flg_lst, 'rev_m_tables':rev_m_tables.keys(), 'phd':phd, 'tt_list':tt_list, 'doc_t_grp':dtf})
            required_lst.append(project_name)
        conn.close()
        #print res_lst;sys.exit()
        return res_lst

    def saveCompanyConfigTable_gbl(self, update_rows, data_rows):
        db_path          = '/mnt/eMB_db/node_mapping.db'
        conn             = sqlite3.connect(db_path)      
        cur              = conn.cursor() 

        crt_qry          = "CREATE TABLE IF NOT EXISTS company_config(row_id INTEGER PRIMARY KEY AUTOINCREMENT, company_id VARCHAR(20), company_name TEXT, project_id INTEGER, project_name TEXT, reporting_type VARCHAR(256), total_years INTEGER, derived_ph VARCHAR(256), periods VARCHAR(256), ind_group_flg INTEGER, taxo_flg INTEGER, excel_config_str TEXT)"
        cur.execute(crt_qry)

        if update_rows:
            cur.executemany("UPDATE company_config set company_id=?, company_name=?, project_name=?, reporting_type=?, total_years=?, derived_ph=?, periods=?, ind_group_flg=?, taxo_flg=?, excel_config_str=? WHERE project_id=?", update_rows)
        if data_rows:
            cur.executemany("INSERT INTO company_config(company_id, company_name, project_id, project_name, reporting_type, total_years, derived_ph, periods, ind_group_flg, taxo_flg, excel_config_str) VALUES(?,?,?,?,?,?,?,?,?,?,?)", data_rows)

        conn.commit()
        conn.close()
        return 'done'


    def show_all_table_types_and_formulas_company(self):
        period_wise_formula  = self.ph_deivation_ar
        formula_lst =  []
        for formula in period_wise_formula:
            period, expression = formula.split('=')
            formula_lst.append({'p':period, 'ex':expression})
        return formula_lst  
    
    def cpformula_read_company(self, db_path):
        conn    = sqlite3.connect(db_path)
        cur     = conn.cursor()

        crt_qry = "CREATE TABLE IF NOT EXISTS table_type_wise_formula(row_id INTEGER PRIMARY KEY AUTOINCREMENT, project_name VARCHAR(100),table_type VARCHAR(100), period VARCHAR(20), period_expression VARCHAR(100));"
        cur.execute(crt_qry)
        self.alter_table_coldef(conn, cur, 'table_type_wise_formula', ['null_type', 'not_null', 'main_header', 'group_id'])

        read_qry = 'select project_name, table_type, period, period_expression, null_type, not_null, main_header, group_id from table_type_wise_formula'
        cur.execute(read_qry)
        tableData = cur.fetchall()
        conn.close()
        dataLst_comp_dct = {}
        for row in tableData:
            project_name, table_type, period, period_expression, null_type, not_null, main_header, group_id = map(str, row)
            if (null_type == "None") or (not null_type):null_type = []
            elif null_type:null_type = null_type.split('##')
            if (not_null  == "None") or (not not_null):not_null = []
            elif not_null:not_null = not_null.split('##')
            if main_header == "None":main_header = ''
            if (group_id == "None") or (not group_id):group_id = ''
            
            dataLst_comp_dct.setdefault(project_name, {}).setdefault((main_header, table_type, group_id), []).append({'p':period, 'ex':period_expression, 'null':null_type, 'not_null':not_null})
        f_d = {}
        for pname, dd_dct in dataLst_comp_dct.items():
            dataLst_comp = []
            for tt_mh_tup, flst in dd_dct.items():
                dataLst_comp.append({'p':tt_mh_tup[1].replace('\t', ''), 'c':flst, 'mh':tt_mh_tup[0], 'grpid':tt_mh_tup[2]})
            f_d[pname]  = dataLst_comp[:]
        return f_d
    def formula_read_company_d(self, db_path):
        conn    = sqlite3.connect(db_path)
        cur     = conn.cursor()

        crt_qry = "CREATE TABLE IF NOT EXISTS table_type_wise_formula(row_id INTEGER PRIMARY KEY AUTOINCREMENT, project_name VARCHAR(100),table_type VARCHAR(100), period VARCHAR(20), period_expression VARCHAR(100));"
        cur.execute(crt_qry)
        self.alter_table_coldef(conn, cur, 'table_type_wise_formula', ['null_type', 'not_null', 'main_header', 'group_id', 'pt_operator'])

        read_qry = 'select project_name, table_type, period, period_expression, null_type, not_null, main_header, group_id, pt_operator from table_type_wise_formula'
        cur.execute(read_qry)
        tableData = cur.fetchall()
        conn.close()
        dataLst_comp_dct = {}
        for row in tableData:
            project_name, table_type, period, period_expression, null_type, not_null, main_header, group_id, pt_operator = map(str, row)
            if (null_type == "None") or (not null_type):null_type = []
            elif null_type:null_type = null_type.split('##')
            if (not_null  == "None") or (not not_null):not_null = []
            elif not_null:not_null = not_null.split('##')
            if main_header == "None":main_header = ''
            if (group_id == "None") or (not group_id):group_id = ''
            if (pt_operator == "None") or (not pt_operator):pt_op_lst = []
            elif pt_operator:#null_type = null_type.split('##')
                pt_op_lst = []
                for ptop in pt_operator.split('##'):
                    atr_splt = ptop.split('_')
                    dt = {'pt':atr_splt[0], 'op':atr_splt[1]}
                    pt_op_lst.append(dt)
            dataLst_comp_dct.setdefault(project_name, {}).setdefault((main_header, table_type, group_id), []).append({'p':period, 'ex':period_expression, 'null':null_type, 'not_null':not_null, 'pt_op':pt_op_lst})
        f_d = {}
        for pname, dd_dct in dataLst_comp_dct.items():
            dataLst_comp = []
            for tt_mh_tup, flst in dd_dct.items():
                dataLst_comp.append({'p':tt_mh_tup[1].replace('\t', ''), 'c':flst, 'mh':tt_mh_tup[0], 'grpid':tt_mh_tup[2]})
            f_d[pname]  = dataLst_comp[:]
        return f_d

    def formula_read_company(self, db_path):
        conn    = sqlite3.connect(db_path)
        cur     = conn.cursor()

        crt_qry = "CREATE TABLE IF NOT EXISTS table_type_wise_formula(row_id INTEGER PRIMARY KEY AUTOINCREMENT, project_name VARCHAR(100),table_type VARCHAR(100), period VARCHAR(20), period_expression VARCHAR(100));"
        cur.execute(crt_qry)
        self.alter_table_coldef(conn, cur, 'table_type_wise_formula', ['null_type', 'not_null', 'main_header', 'group_id', 'pt_operator'])

        read_qry = 'select project_name, table_type, period, period_expression, null_type, not_null, main_header, group_id, pt_operator from table_type_wise_formula'
        cur.execute(read_qry)
        tableData = cur.fetchall()
        conn.close()
        dataLst_comp_dct = {}
        for row in tableData:
            project_name, table_type, period, period_expression, null_type, not_null, main_header, group_id, pt_operator = map(str, row)
            if (null_type == "None") or (not null_type):null_type = []
            elif null_type:null_type = null_type.split('##')
            if (not_null  == "None") or (not not_null):not_null = []
            elif not_null:not_null = not_null.split('##')
            if main_header == "None":main_header = ''
            if (group_id == "None") or (not group_id):group_id = ''
            if (pt_operator == "None") or (not pt_operator):pt_op_lst = []
            elif pt_operator:#null_type = null_type.split('##')
                pt_op_lst = []
                for ptop in pt_operator.split('##'):
                    atr_splt = ptop.split('_')
                    dt = {'pt':atr_splt[0], 'op':atr_splt[1]}
                    pt_op_lst.append(dt)
            dataLst_comp_dct.setdefault(project_name, {}).setdefault((main_header, table_type, group_id), []).append({'p':period, 'ex':period_expression, 'null':null_type, 'not_null':not_null, 'pt_op':pt_op_lst})
        f_d = {}
        for pname, dd_dct in dataLst_comp_dct.items():
            dataLst_comp = []
            for tt_mh_tup, flst in dd_dct.items():
                t_lst = []
                for t in flst:
                    if t not in t_lst:
                        t_lst.append(t) 
                dataLst_comp.append({'p':tt_mh_tup[1].replace('\t', ''), 'c':t_lst, 'mh':tt_mh_tup[0], 'grpid':tt_mh_tup[2]})
            f_d[pname]  = dataLst_comp[:]
        return f_d

    def saveTableType_wiseFormula_company(self, ijson, db_path):
        company_name = ijson["company_name"]
        model_number = ijson["model_number"]
        project_name = ijson["p_list"][0]["n"]
        storeData = ijson["p_list"][0]["ph_list"]
        conn    = sqlite3.connect(db_path)
        cur     = conn.cursor()

        crt_qry = "CREATE TABLE IF NOT EXISTS table_type_wise_formula(row_id INTEGER PRIMARY KEY AUTOINCREMENT, project_name VARCHAR(100),table_type VARCHAR(100), period VARCHAR(20), period_expression VARCHAR(100));"
        cur.execute(crt_qry)
        self.alter_table_coldef(conn, cur, 'table_type_wise_formula', ['null_type', 'not_null', 'main_header', 'group_id', 'pt_operator'])

        dlt_qry = 'delete from table_type_wise_formula where project_name="%s";'%(project_name)
        cur.execute(dlt_qry)
        data_rows = []
        for row in storeData:
            table_type, getC, group_id, main_header  = row["p"], row["c"], row.get("grpid",''), row.get("mh", '')
            if not getC:continue 
            for data in getC:
                period, period_expression, null_type, not_null, pt_operator = data["p"], data["ex"], data.get("null", ''), data.get("not_null", ''), data.get("pt_op", [])
                if null_type:
                    null_type = '##'.join(null_type)
                elif not null_type:
                    null_type = ''
                if not_null:
                    not_null = '##'.join(not_null)
                elif not not_null:
                    not_null = ''
                if pt_operator:
                    ptop_data_lst = []
                    for ptop in pt_operator:
                        pt, op = ptop['pt'], ptop['op']
                        c_dt = '_'.join([pt, op])
                        ptop_data_lst.append(c_dt)
                    ptop_str = '##'.join(ptop_data_lst)
                elif not pt_operator:ptop_str = ''
                tup = (project_name, table_type, period, period_expression, null_type, not_null, main_header, group_id, ptop_str)
                data_rows.append(tup)
        cur.executemany("INSERT INTO table_type_wise_formula(project_name, table_type, period, period_expression, null_type, not_null, main_header, group_id, pt_operator) VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?)", data_rows)
        conn.commit()
        conn.close()
        res = [{'message':'done'}]
        return res

    def cpsaveTableType_wiseFormula_company(self, ijson, db_path):
        company_name = ijson["company_name"]
        model_number = ijson["model_number"]
        project_name = ijson["p_list"][0]["n"]
        storeData = ijson["p_list"][0]["ph_list"]
        conn    = sqlite3.connect(db_path)
        cur     = conn.cursor()

        crt_qry = "CREATE TABLE IF NOT EXISTS table_type_wise_formula(row_id INTEGER PRIMARY KEY AUTOINCREMENT, project_name VARCHAR(100),table_type VARCHAR(100), period VARCHAR(20), period_expression VARCHAR(100));"
        cur.execute(crt_qry)
        self.alter_table_coldef(conn, cur, 'table_type_wise_formula', ['null_type', 'not_null', 'main_header', 'group_id'])

        dlt_qry = 'delete from table_type_wise_formula where project_name="%s";'%(project_name)
        cur.execute(dlt_qry)
        data_rows = []
        for row in storeData:
            table_type, getC, group_id, main_header  = row["p"], row["c"], row.get("grpid",''), row.get("mh", '')
            if not getC:continue 
            for data in getC:
                period, period_expression, null_type, not_null = data["p"], data["ex"], data.get("null", ''), data.get("not_null", '')
                if null_type:
                    null_type = '##'.join(null_type)
                elif not null_type:
                    null_type = ''
                if not_null:
                    not_null = '##'.join(not_null)
                elif not not_null:
                    not_null = ''
                tup = (project_name, table_type, period, period_expression, null_type, not_null, main_header, group_id)
                data_rows.append(tup)
        cur.executemany("INSERT INTO table_type_wise_formula(project_name, table_type, period, period_expression, null_type, not_null, main_header, group_id) VALUES(?, ?, ?, ?, ?, ?, ?, ?)", data_rows)
        conn.commit()
        conn.close()
        res = [{'message':'done'}]
        return res


    def copy_line_items(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        i_table_type    = ijson['table_type']
        ph_d        = {}
        path    = "%s/%s/%s/1_1/21/sdata/doc_map.txt"%(self.doc_path, project_id, deal_id)
        if os.path.exists(path):
            fin = open(path, 'r')
            lines   = fin.readlines()
            fin.close()
        else:
            lines   = []
        doc_d       = {}
        dphs        = {}
        c_year      = self.get_cyear(lines)
        start_year  = c_year - int(ijson.get('year', 10))
        for line in lines[1:]:
            line    = line.split('\t')
            if len(line) < 8:continue
            line    = map(lambda x:x.strip(), line)
            ph      = line[3]+line[7]
            if ph and start_year<int(ph[2:]):
                doc_id  = line[0]
                doc_d[doc_id]   = (ph, line[2])
                dphs[ph]        = 1

                    

        i_table_type    = ijson['table_type']
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, [i_table_type])

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        lmdb_path   = '/var/www/html/Rajeev/BBOX/'+str(project_id)+'_'+str(deal_id)
        lmdb_path    = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        env1    = lmdb.open(lmdb_path, readonly=True)
        txn1    = env1.begin()

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn         = env.begin()


        table_type  = str(i_table_type)
        group_d     = {}
        revgroup_d  = {}

        d_group_d     = {}
        d_revgroup_d  = {}

        vgh_id_d    = {}
        db_file         = self.get_db_path(ijson)
        conn, cur   = conn_obj.sqlite_connection(db_file)
        #sql         = "select vgh_id, doc_id, table_id, group_txt from doc_group_map where table_type='%s'"%(table_type)
        #try:
        #    cur.execute(sql)
        #    res = cur.fetchall()
        #except:
        #    res = []
        grp_doc_map_d   = {}
        rev_doc_map_d   = {}
        #for rr in res:
        #    vgh_id, doc_id, table_id, group_txt = rr
        #    grp_doc_map_d.setdefault(group_txt, {})[table_id]   = doc_id #.setdefault(doc_id, {})[table_id]    = 1
        #    rev_doc_map_d.setdefault((doc_id, table_id), {})[group_txt] = 1
        #sql = "CREATE TABLE IF NOT EXISTS vgh_doc_map(row_id INTEGER PRIMARY KEY AUTOINCREMENT, vgh_group_id TEXT, doc_group_id TEXT, table_type VARCHAR(100), group_txt TEXT, user_name VARCHAR(100), datetime TEXT);"
        if ijson.get('vids', {}) and isinstance(ijson.get('vids', {}), list):
            ijson['vids']   = ijson['vids'][0]
        else:
            ijson['vids']   = {}
        g_vids  = ijson.get('vids', {})
        t_ids   = map(lambda x:str(x), ijson.get('t_ids', []))
        self.alter_table_coldef(conn, cur, 'mt_data_builder', ['target_taxonomy', 'numeric_flg', 'th_flg'])
        if ijson.get('vids', {}):
            sql         = "select row_id, taxo_id, prev_id, order_id, table_type, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, gcom, ngcom, ph, ph_label, user_name, datetime, isvisible, m_rows, vgh_text, vgh_group, doc_id, xml_id, period, period_type, scale, currency, value_type, target_taxonomy, numeric_flg, th_flg from mt_data_builder where table_type='%s' and taxo_id in (%s) and  isvisible='Y' and vgh_text in (%s)"%(table_type, ', '.join(t_ids),  ', '.join(ijson['vids'].keys()))
        else:
            sql         = "select row_id, taxo_id, prev_id, order_id, table_type, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, gcom, ngcom, ph, ph_label, user_name, datetime, isvisible, m_rows, vgh_text, vgh_group, doc_id, xml_id, period, period_type, scale, currency, value_type, target_taxonomy, numeric_flg, th_flg from mt_data_builder where table_type='%s' and taxo_id in (%s) and isvisible='Y'"%(table_type, ', '.join(t_ids))
        cur.execute(sql)
        res         = cur.fetchall()
        docinfo_d   = {}
        vgh_id_d_all    = {}
        tmp_grpd        = {}
        table_ids       = {}
        all_vgh         = {}
        f_d             = {}
        f_o_d          = {}
        for rr in res:
            row_id, taxo_id, prev_id, order_id, table_type, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, gcom, ngcom, ph, ph_label, user_name, tdatetime, isvisible, m_rows, vgh_text, vgh_group, doc_id, xml_id, period, period_type, scale, currency, value_type, target_taxonomy, numeric_flg, th_flg   = rr
            tk   = self.get_quid(table_id+'_'+xml_id)
            c_id        = txn_m.get('XMLID_MAP_'+tk)
            if not c_id:continue
            c   = int(c_id.split('_')[2])
            if g_vids.get(vgh_text, {}) and table_id+'-'+str(c) not in g_vids.get(vgh_text, {}):continue
            rr  = (table_type, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, gcom, ngcom, ph, ph_label, user_name, tdatetime, isvisible, m_rows, vgh_text, vgh_group, doc_id, xml_id, period, period_type, scale, currency, value_type, target_taxonomy, numeric_flg, th_flg)
            f_d.setdefault(taxo_id, {})[rr] = 1
            f_o_d[taxo_id]   = order_id
        tmpt_d          = {}
        all_table_types = {}
        for taxo_id, ks in f_d.items():
            t_ar    = []
            for rr in ks:
                rr      = list(rr)
                rr[0]   =  ijson['target_tt']
                rr[15]  =  'COPY_'+ijson['table_type']+"_"+str(taxo_id)
                t_ar.append(rr)
            tmpt_d[taxo_id]  = {'d':t_ar, 'o':f_o_d[taxo_id]}
        t_ars   = []
        t_ars.append(tmpt_d)    
        dtime   = str(datetime.datetime.now()).split('.')[0]
        i_ar    = []
        with conn:
            #sql = "select max(taxo_id) from kpi_input"
            sql = "select seq from sqlite_sequence WHERE name = 'mt_data_builder'"
            cur.execute(sql)
            r       = cur.fetchone()
            g_id    = int(r[0])+1
            sql     = "select max(taxo_id) from mt_data_builder" 
            cur.execute(sql)
            r       = cur.fetchone()
            tg_id    = int(r[0])+1
            g_id    = max(g_id, tg_id)
            for t_d in t_ars:
                for t_id, vd in t_d.items():
                    #print  t_id, vd
                    for tup in vd['d']:
                        #print '\n\tBB ',tup
                        tup = (g_id, vd['o'])+tuple(tup)
                        #print '\t',tup
                        i_ar.append(tup)
                    g_id    += 1
        cols    = 'taxo_id, order_id, table_type, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, gcom, ngcom, ph, ph_label, user_name, datetime, isvisible, m_rows, vgh_text, vgh_group, doc_id, xml_id, period, period_type, scale, currency, value_type, target_taxonomy, numeric_flg, th_flg'
        cols_v  =', '.join(map(lambda x:'?', cols.split(',')))
        if 1:#ijson.get('update_db', '') == 'Y':
            cur.executemany('insert into mt_data_builder(%s) values(%s)'%(cols, cols_v), i_ar)
            conn.commit()
        conn.close()
        res = [{'message':'done'}]
        return res

    def clean_and_actual_value_status_crt_del(self, ijson):
        company_name = ijson["company_name"]
        model_number = ijson["model_number"]
        project_id   = ijson["project_id"]
        deal_id      = ijson["deal_id"]
        company_id   = "%s_%s"%(project_id, deal_id)
        data_lst     = ijson["list"]        
        del_flg      = int(ijson["del_flg"])

        db_file = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn    = sqlite3.connect(db_file)
        cur     = conn.cursor()
       
        # create table clean_actual_value_status
        crt_qry = "CREATE TABLE IF NOT EXISTS clean_actual_value_status(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_id VARCHAR(100), doc_id VARCHAR(100), table_type VARCHAR(100), value_status VARCHAR(20));"
        cur.execute(crt_qry)

        if not del_flg: 
            # preparing data to insert
            read_qry = 'select table_id, table_type from clean_actual_value_status;'
            cur.execute(read_qry)   
            actual_data = [tuple(map(str, tup))for tup in cur.fetchall()]

            data_rows = []
            for data in data_lst:
                table_id, doc_id, table_type = data["t"], data["d"], data["tt"]
                if tuple([table_id, table_type]) not in actual_data:
                    data_rows.append((table_id, doc_id, table_type, 'Y'))
            # inserting data 
            cur.executemany("INSERT INTO clean_actual_value_status(table_id, doc_id, table_type, value_status) VALUES(?, ?, ?, ?)", data_rows)

        elif del_flg:
            for data in data_lst:
                table_id, doc_id, table_type = data["t"], data["d"], data["tt"]
                del_stmt = 'delete from clean_actual_value_status where table_id="%s" and table_type="%s"'%(table_id, table_type)
                cur.execute(del_stmt)

        conn.commit()
        conn.close()
        return [{'message':'done'}]

    def match_main_sub_table(self, ijson, ret_flg=None, res_d={}, i_ar=[]):
        #ijson['table_types']    = ["IS", "BS", "CF"]
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number)
        if not i_ar:
            i_ar            = []
            for table_type in rev_m_tables.keys():
                if not table_type:continue
                if ijson.get("table_types", []) and table_type not in ijson.get("table_types", []):continue
                ijson['table_type'] = table_type
                g_ar    = self.create_group_ar(ijson, txn_m)
                ijson_c = copy.deepcopy(ijson)
                ijson_c['table_type']   = table_type
                ijson_c['type']         = 'display'
                i_ar.append(ijson_c)
                for rr in g_ar:
                    #g_d[(table_type, rr['grpid'])] =  rr
                    ijson_c = copy.deepcopy(ijson)
                    ijson_c['type']         = 'display'
                    ijson_c['table_type']   = table_type
                    ijson_c['grpid']        = rr['grpid']
                    ijson_c['vids']         = rr['vids']
                    ijson_c['data']         = [rr['n']]
                    i_ar.append(ijson_c)

        data_d  = {}
        i_d     = {}
        v_index_d   = {}
        grp_info_d  = {}
        for ii, ijson_c in enumerate(i_ar):
            #print 'Running ', ii, '/', len(i_ar), [ijson_c['table_type'], ijson_c.get('grpid', ''), ijson_c.get('data', '')]
            ijson_c['NO_FORM']  = 'Y'
            grp_info_d[(ijson_c['table_type'], ijson_c.get('grpid', ''))]  = ijson_c.get('data', [''])[0]
            if (ijson_c['table_type'], ijson_c.get('grpid', '')) in res_d:
                res     = res_d[(ijson_c['table_type'], ijson_c.get('grpid', ''))] #self.create_final_output_with_ph(ijson_c,'P', 'N')
            else:
                res     = self.create_final_output_with_ph(ijson_c,'P', 'N')
            rev_map = {}
            db_ar   = []
            db_d    = {}
            for ti, rr in enumerate(res[0]['data']):
                rows_ar = []
                rows_ar_org = []
                for ph in res[0]['phs'] :
                    v, tph, scale   = '', ph['n'], ''
                    if ph['k'] in rr:
                        v_d = rr[ph['k']]
                        v, scale, vt, cry   = v_d['v'], v_d.get('phcsv', {}).get('s', ''), v_d.get('phcsv', {}).get('vt', ''), v_d.get('phcsv', {}).get('c', '')
                        v_org   = v
                        try:
                            clean_value = numbercleanup_obj.get_value_cleanup(v)
                        except:
                            clean_value = ''
                            pass
                        if clean_value:
                            v_org = str(self.convert_floating_point(float(clean_value)).replace(',', ''))
                            v = str(self.convert_floating_point(abs(float(clean_value))).replace(',', ''))
                        else:
                            v   = ''
                        if v:
                            rows_ar.append((tph, scale, vt, cry , v))
                            db_d.setdefault(ti, {})[tph] = (tph, scale, vt, cry , v_org)
                rev_map[rr['t_id']] = ti
                if not rows_ar:continue
                rows_ar = list(sets.Set(rows_ar))
                rows_ar.sort()
                row_tup    = tuple(rows_ar)
                db_ar.append((ti, rr['t_id'], row_tup))
                i_d.setdefault(row_tup, {}).setdefault((ijson_c['table_type'], ijson_c.get('grpid', '')), {})[(ti, rr['t_id'])]    = 1
                for rtup in row_tup:
                    v_index_d.setdefault(rtup, {})[row_tup]    = 1
            data_d.setdefault(ijson_c['table_type'], {})[ijson_c.get('grpid', '')]  = (res, rev_map, db_ar, db_d) 
        map_d   = {
                    'IS':["IS", "BS","CF"][:1],
                    'BS':["IS","BS", "CF"][:1],
                    'CF':["IS","BS", "CF"][:1],
                    }
        if ijson.get("table_types", []):
            map_d   = {}
            for tt in ijson.get("table_types", []):
                map_d[tt]   = [tt]
        final_d = {}
        match_d = {}
        for ttype in  ['IS', 'BS', 'CF']:
            self.match_db_row(ttype, map_d[ttype], data_d, i_d, v_index_d, grp_info_d, final_d, match_d)
            pass
        ks  = final_d.keys()
        ks.sort(key=lambda x:x[0])
        f_ar    = []
        for (table_type, grp) in ks:
            data_tup    = data_d[table_type][grp]
            #print '\n===================================================='
            #print [table_type, grp], grp_info_d[(table_type, grp)]
            t_ar    = []
            tis     = final_d[(table_type, grp)].keys()
            tis.sort()
            for ti in tis:
                mgrp = final_d[(table_type, grp)][ti]
                #print '\n\tLabel ', [ti], data_tup[0][0]['data'][ti]['t_l']
                tks = mgrp.keys()
                tks.sort()
                phs     = data_tup[3][ti]
                data_ar = []
                t_phs   = copy.deepcopy(data_tup[0][0]['phs'])
                dd      = copy.deepcopy(data_tup[0][0]['data'][ti])
                dd['M'] = 'Y'
                dd['tt']    = table_type
                dd['grp']   = grp_info_d[(table_type, grp)]
                for ph in t_phs:
                    if (ph['k'] not in dd):continue
                    if  (ph['k'] not in phs):
                        if ph['n'] in phs:
                            dd[ph['n']]    = dd[ph['k']]
                        del dd[ph['k']]
                data_ar.append(dd)
                for (m_tt, m_grp) in tks:
                    #print '\n\t\t-------------------------------------------------'
                    #print '\t\t', (m_tt, m_grp), grp_info_d[(m_tt, m_grp)]
                    for tmp_ti, nm_d in mgrp[(m_tt, m_grp)].items():
                        #print '\t\t\tLabel ', [tmp_ti], data_d[m_tt][m_grp][0][0]['data'][tmp_ti]['t_l']
                        dd      = copy.deepcopy(data_d[m_tt][m_grp][0][0]['data'][tmp_ti])
                        dd['tt']    = m_tt
                        dd['grp']   = grp_info_d[(m_tt, m_grp)]
                        for ph in data_d[m_tt][m_grp][0][0]['phs']:
                            if (ph['k'] not in dd):continue
                            if  (ph['k'] not in phs):
                                if ph['n'] in phs:
                                    dd[ph['n']]    = dd[ph['k']]
                                del dd[ph['k']]
                            if ph['n'] in nm_d:
                                dd[ph['n']]['nm']  = 'Y'
                        data_ar.append(dd)
                phs = filter(lambda x:x['n'] in phs, t_phs)
                tmpphs  = []
                for ph in phs:
                    ph['k'] = ph['n']
                    tmpphs.append(ph)
                f_ar.append({'n':table_type+' - '+ grp_info_d[(table_type, grp)], 'phs':tmpphs, 'data':data_ar, 'm_tt':map(lambda x:x[0], tks)})
            #f_ar.append({'n':table_type+' - '+ grp_info_d[(table_type, grp)], 'info':t_ar})
        if ret_flg == 'Y':
            return f_ar
        res = [{"message":'done', 'data':f_ar}]
        return res
                
        

    def match_db_row(self, table_type, ignore, data_d, i_d, v_index_d, grp_info_d, final_d, match_d):
        if table_type not in ignore:
            ignore.append(table_type)
        ignore  = sets.Set(ignore)
        for grp, data_tup in data_d.get(table_type, {}).items():
            for ti, t_id, rs in data_tup[2]:
                m_ar    = []
                m_d = {}
                for r in rs:
                    for rtup in v_index_d.get(r, {}).keys():
                        m_d.setdefault(rtup, {})[r] = 1
                for rtup, mtups in m_d.items():
                    mtups   = mtups.keys()
                    mtups.sort()
                    rem_types   = list(sets.Set(map(lambda x:x[0], i_d[rtup].keys())) - ignore)
                    if rtup == tuple(mtups) and rem_types:
                        m_ar.append((filter(lambda x:x[0] in rem_types, i_d[rtup].keys()), rtup))
                org_rd    = data_tup[3][ti]
                if m_ar:
                    #print '\n===================================================='
                    #print [table_type, grp], [ti, t_id], grp_info_d[(table_type, grp)]
                    #print 'Label ', data_tup[0][0]['data'][ti]['t_l']
                    #print rs, org_rd
                    ktup    = (table_type, grp)
                    for (rem_types, m_r) in m_ar:
                        for (m_tt, m_grp) in rem_types:
                            #print '\n\t-------------------------------------------------'
                            #print '\t', (m_tt, m_grp), grp_info_d[(m_tt, m_grp)]
                            for (tmp_ti, tmp_t_id) in i_d[m_r][(m_tt, m_grp)].keys():
                                #print '\t\tLabel ', (tmp_ti, tmp_t_id), data_d[m_tt][m_grp][0][0]['data'][tmp_ti]['t_l']
                                org_md    = data_d[m_tt][m_grp][3][tmp_ti]
                                all_eq  = 1
                                cks = list(sets.Set(org_rd.keys()).intersection(sets.Set(org_md.keys())))
                                not_c   = {}
                                #print '\t\tcks', cks, org_md
                                for ck in cks:
                                    if org_rd[ck] != org_md[ck]:
                                        all_eq      = 0
                                        not_c[ck]   = (org_rd[ck], org_md[ck])
                                        #break
                                #print '\t\tall_eq ', all_eq, not_c
                                if 1:
                                    final_d.setdefault(ktup, {}).setdefault(ti, {}).setdefault((m_tt, m_grp), {})[tmp_ti]   = not_c
                                    match_d.setdefault((table_type, grp, data_tup[0][0]['data'][ti]['t_id']), {})[(m_tt, m_grp, data_d[m_tt][m_grp][0][0]['data'][tmp_ti]['t_id'])]  = 1 

    def validate_csv(self, ijson):
        res = [{'message':'done'}]
        return res

    def read_data_error_class(self, ijson):
        company_name    = str(ijson['company_name'])
        model_number   = str(ijson['model_number'])
        deal_id         = str(ijson['deal_id'])
        project_id      = str(ijson['project_id'])
        company_id      = "%s_%s"%(project_id, deal_id)
        #error_type      = str(ijson["error_type"])
        user_name       = str(ijson["user"])

        import model_view.cp_company_docTablePh_details as py 
        obj = py.Company_docTablePh_details()
        table_doc_map  = obj.get_docId_passing_tableId(company_id)        


        db_file = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn    = sqlite3.connect(db_file)
        cur     = conn.cursor()
        
        # create table
        #sql = "CREATE TABLE IF NOT EXISTS vgh_doc_map(row_id INTEGER PRIMARY KEY AUTOINCREMENT, vgh_group_id TEXT, doc_group_id TEXT, table_type VARCHAR(100), group_txt TEXT, user_name VARCHAR(100), datetime TEXT);"
        crt_qry = 'CREATE TABLE IF NOT EXISTS error_xmlid_storage(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_type VARCHAR(100), table_id VARCHAR(100), error_xmlid TEXT, error_type TEXT, error_msg TEXT, accept_flg VARCHAR(10), datetime TEXT, user_name TEXT);'
        cur.execute(crt_qry)
    
        read_qry = 'select table_type, table_id, error_xmlid, error_type, error_msg from error_xmlid_storage;'
        cur.execute(read_qry)
        table_data = cur.fetchall()

        table_id_wise_dct = {}
        for row in table_data:
            table_type, table_id, error_xmlid, error_type, error_msg = map(str, row)
            
            if table_type not in table_id_wise_dct:
                table_id_wise_dct[table_type] = {}
        
            if table_id not in table_id_wise_dct[table_type]:   
                table_id_wise_dct[table_type][table_id]  = {}
            
            if error_type not in table_id_wise_dct[table_type][table_id]:
                table_id_wise_dct[table_type][table_id][error_type] = {}
        
            if error_msg not in table_id_wise_dct[table_type][table_id][error_type]:
                table_id_wise_dct[table_type][table_id][error_type][error_msg] = []
            table_id_wise_dct[table_type][table_id][error_type][error_msg].append(error_xmlid)
             
        res_lst = []
        for table_type, table_id_dct in table_id_wise_dct.items():
            tt_dct = {'tt':table_type, 't_list':[]}
            for table_id, error_class_dct in table_id_dct.items():
                doc_id    = table_doc_map.get(table_id, '')  
                table_dct = {'t':table_id, 'd':doc_id, 'ecl':[]}
                for error_class, error_msg_dct in error_class_dct.items():
                    ec_dct = {'e_c':error_class, 'eml':[]}
                    for error_msg, xml_lst in error_msg_dct.items():
                        em_dct = {'em':error_msg, 'xml':xml_lst}  
                        ec_dct['eml'].append(em_dct)
                    table_dct['ecl'].append(ec_dct)
                tt_dct['t_list'].append(table_dct)
            res_lst.append(tt_dct)
        return [{'message':'done', 'data':res_lst}]

    def multiclass_error(self, ijson):
        
        import datetime     
        company_name    = str(ijson['company_name'])
        model_number   = str(ijson['model_number'])
        deal_id         = str(ijson['deal_id'])
        project_id      = str(ijson['project_id'])
        company_id      = "%s_%s"%(project_id, deal_id)
        #error_type      = str(ijson["error_type"])
        user_name       = str(ijson["user"])


        import model_view.phcsv_check as PhCsv_Validata
        obj = PhCsv_Validata.PhCsv_Validata()         
        error_class_elms = obj.validate(company_name, model_number, company_id)
        
        import model_view.cp_company_docTablePh_details as py 
        obj = py.Company_docTablePh_details()
        tableId_tableType_map = obj.get_table_sheet_map(company_name, model_number, company_id)

        db_file = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn    = sqlite3.connect(db_file)
        cur     = conn.cursor()
        
        # create table
        #sql = "CREATE TABLE IF NOT EXISTS vgh_doc_map(row_id INTEGER PRIMARY KEY AUTOINCREMENT, vgh_group_id TEXT, doc_group_id TEXT, table_type VARCHAR(100), group_txt TEXT, user_name VARCHAR(100), datetime TEXT);"
        crt_qry = 'CREATE TABLE IF NOT EXISTS error_xmlid_storage(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_type VARCHAR(100), table_id VARCHAR(100), error_xmlid TEXT, error_type TEXT, error_msg TEXT, accept_flg VARCHAR(10), datetime TEXT, user_name TEXT);'
        cur.execute(crt_qry)
        
        lmdb_path = '/var/www/html/fill_table/%s/table_info'%(company_id)
        env = lmdb.open(lmdb_path, readonly=True)
        txn = env.begin()
                
        data_rows = []    
        # save error_phcsv data 
        for table_type, error_msg_dct in error_class_elms.items():
            #row = [table_type]
            for error_msg, tableId_Doc_lst in  error_msg_dct.items():
                #print error_msg, tableId_Doc_lst;continue
                for tableId_Doc in tableId_Doc_lst:
                    #print tableId_Doc;continue
                    if type(tableId_Doc) == tuple:
                        table_id, doc_id = tableId_Doc
                        gv_key = 'GV_' + table_id
                        getGVcids = txn.get(gv_key)
                        if getGVcids == None:continue
                        for gcids in getGVcids.split('#'):
                            getXmlid_key = 'XMLID_' + gcids
                            gv_xml  = txn.get(getXmlid_key)
                            if getXmlid_key == None:continue  
                            entry_time = datetime.datetime.now()
                            data = (table_type, table_id, gv_xml, error_type, error_msg,'0', str(entry_time), user_name) 
                            data_rows.append(data)
                    elif type(tableId_Doc) == list:
                        for lst in tableId_Doc:
                            table_id, doc_id = lst
                            gv_key = 'GV_' + table_id
                            getGVcids = txn.get(gv_key)
                            if getGVcids == None:continue
                            for gcids in getGVcids.split('#'):
                                getXmlid_key = 'XMLID_' + gcids
                                gv_xml  = txn.get(getXmlid_key)
                                if getXmlid_key == None:continue  
                                entry_time = datetime.datetime.now()
                                data = (table_type, table_id, gv_xml, error_type, error_msg,'0', str(entry_time), user_name) 
                                data_rows.append(data)
                           
        del_qry = 'delete from error_xmlid_storage where error_type="%s"'%(error_type)
        cur.execute(del_qry)
        
        cur.executemany('INSERT INTO error_xmlid_storage(table_type, table_id, error_xmlid, error_type, error_msg, accept_flg, datetime, user_name) VALUES(?, ?, ?, ?, ?, ?, ?, ?)', data_rows)
    
        conn.commit()
        conn.close()
        return [{'message':'done'}] 

    def phcsv_error_data(self, ijson):
    
        import datetime     
        company_name    = str(ijson['company_name'])
        model_number   = str(ijson['model_number'])
        deal_id         = str(ijson['deal_id'])
        project_id      = str(ijson['project_id'])
        company_id      = "%s_%s"%(project_id, deal_id)
        #error_type      = str(ijson["error_type"])
        user_name       = str(ijson["user"])

        import model_view.phcsv_check_new as PhCsv_Validata
        obj = PhCsv_Validata.PhCsv_Validata()
        #print company_name, model_number, company_id;sys.exit()
        error_phcsv_dct = obj.validate_csv(company_name, model_number, company_id)

        import model_view.cp_company_docTablePh_details as py 
        obj = py.Company_docTablePh_details()
        tableId_tableType_map = obj.get_table_sheet_map(company_name, model_number, company_id)
        db_file = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn    = sqlite3.connect(db_file)
        cur     = conn.cursor()
        
        # create table
        #sql = "CREATE TABLE IF NOT EXISTS vgh_doc_map(row_id INTEGER PRIMARY KEY AUTOINCREMENT, vgh_group_id TEXT, doc_group_id TEXT, table_type VARCHAR(100), group_txt TEXT, user_name VARCHAR(100), datetime TEXT);"
        crt_qry = 'CREATE TABLE IF NOT EXISTS error_xmlid_storage(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_type VARCHAR(100), table_id VARCHAR(100), error_xmlid TEXT, error_type TEXT, error_msg TEXT, accept_flg VARCHAR(10), datetime TEXT, user_name TEXT);'
        cur.execute(crt_qry)
                    
        # save error_phcsv data 
        data_rows = []
        for table_id, error_xmlId_dct in error_phcsv_dct.items():
            table_type_lst = tableId_tableType_map.get(table_id, '')
            if not table_type_lst:continue 
            for table_type in table_type_lst:
                for error_xmlid in error_xmlId_dct.keys():
                    entry_time = datetime.datetime.now()
                    data = (table_type, table_id, error_xmlid, 'phcsv_error', 'phcsv_error', '0', str(entry_time), user_name)
                    data_rows.append(data) 
        #print data_rows   
        #sys.exit()
     
        del_qry = 'delete from error_xmlid_storage where error_type="%s"'%(error_type)
        cur.execute(del_qry)
        
        cur.executemany('INSERT INTO error_xmlid_storage(table_type, table_id, error_xmlid, error_type, error_msg, accept_flg, datetime, user_name) VALUES(?, ?, ?, ?, ?, ?, ?, ?)', data_rows)
    
        conn.commit()
        conn.close()
        return [{'message':'done'}] 

        
    def error_classes_wise(self, ijson):
        company_name    = str(ijson['company_name'])
        model_number   = str(ijson['model_number'])
        deal_id         = str(ijson['deal_id'])
        project_id      = str(ijson['project_id'])
        company_id      = "%s_%s"%(project_id, deal_id)
        error_type      = str(ijson["error_type"])
        user_name       = str(ijson["user"])

        import model_view.cp_company_docTablePh_details as py 
        obj = py.Company_docTablePh_details()
        table_doc_map  = obj.get_docId_passing_tableId(company_id)        


        db_file = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn    = sqlite3.connect(db_file)
        cur     = conn.cursor()
        
        # create table
        #sql = "CREATE TABLE IF NOT EXISTS vgh_doc_map(row_id INTEGER PRIMARY KEY AUTOINCREMENT, vgh_group_id TEXT, doc_group_id TEXT, table_type VARCHAR(100), group_txt TEXT, user_name VARCHAR(100), datetime TEXT);"
        crt_qry = 'CREATE TABLE IF NOT EXISTS error_xmlid_storage(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_type VARCHAR(100), table_id VARCHAR(100), error_xmlid TEXT, error_type TEXT, error_msg TEXT, group_id TEXT, accept_flg VARCHAR(10), datetime TEXT, user_name TEXT);'
        cur.execute(crt_qry)
    
        read_qry = 'select table_type, table_id, error_xmlid, error_type, error_msg from error_xmlid_storage;'
        cur.execute(read_qry)
        table_data = cur.fetchall()

        table_id_wise_dct = {}
        for row in table_data:
            table_type, table_id, error_xmlid, error_type, error_msg = map(str, row)
            if table_type not in table_id_wise_dct:
                table_id_wise_dct[table_type] = {}
            if table_id not in table_id_wise_dct[table_type]:   
                table_id_wise_dct[table_type][table_id]  = {}
            if error_type not in table_id_wise_dct[table_type][table_id]:
                table_id_wise_dct[table_type][table_id][error_type] = {}
            if error_msg not in table_id_wise_dct[table_type][table_id][error_type]:
                table_id_wise_dct[table_type][table_id][error_type][error_msg] = []
            table_id_wise_dct[table_type][table_id][error_type][error_msg].append(error_xmlid)
            
        res_lst = []
        for table_type, table_id_dct in table_id_wise_dct.items():
            tt_dct = {'tt':table_type, 'error_type':[], 'table_info':[]}
            for table_id, error_type_dct in table_id_dct.items():
                table_dct = {'t':table_id}
                for error_type, error_msg_dct in error_type_dct.items():
                    ec_dct = {error_type:[]}
                    if error_type not in tt_dct['error_type']:
                        tt_dct['error_type'].append(error_type)
                    for error_msg, xml_lst in error_msg_dct.items():
                        ec_dct[error_type].append({'em':error_msg, 'x':xml_lst})
                    table_dct.update(ec_dct)
                tt_dct['table_info'].append(table_dct)
            res_lst.append(tt_dct)
        return [{'message':'done', 'data':res_lst}]



    def table_connection_validation(self, ijson, res_d, i_ar):
        ijson['table_types']    = ["IS", "BS", "CF"]
        i_ar    = filter(lambda x:x['table_type'] in ijson['table_types'], i_ar)
        #print ijson
        #for rr in i_ar:
        #    if 'DB_DATA' in rr:
        #        del rr['DB_DATA']
        #    print rr
        res = self.match_main_sub_table(ijson, 'Y', res_d, i_ar)
        dd  = {}
        for rr in res:
            tt1 = rr['n'].split(' - ')[0]
            for tt2 in rr['m_tt']:
                if tt1 == tt2:continue
                tup = [tt1, tt2]
                tup.sort()
                dd[tuple(tup)]  = 1
        missing_conn    = {}
        for tt in ijson['table_types']:
            for tt1 in ijson['table_types']:
                if tt == tt1:continue
                tup = [tt, tt1]
                tup.sort()
                tup = tuple(tup)
                if tup not in dd:
                    missing_conn[tup]    = 1
        return missing_conn


    def checksum_validation(self, ijson, res1=[]):
        ijson_c = ijson
        if not res1:
            res1    = self.create_final_output(ijson)
            if res1[0]['message'] != 'done':
                res1    = res1[0]['res']
        
        ph_value_d  = {}
        taxo_value_grp  = {}
        for ti, rr in enumerate(res1[0]['data']):
            taxo_value_grp[str(rr['t_id'])]  = ti
            if ijson_c.get('t_ids', []) and rr['t_id'] not in ijson_c.get('t_ids', []):continue
            for ph in res1[0]['phs']:
                if ph['k'] in rr:
                    try:
                        clean_value = numbercleanup_obj.get_value_cleanup(rr[ph['k']]['v'])
                    except:
                        clean_value = ''
                    if clean_value == '':
                        clean_value = '0'
                    clean_value = float(clean_value)
                    ph_value_d.setdefault(ph['k'], {})[ti]   = clean_value
                    
        # preview final output
        pos_form_d  = {}
        for ti, rr in enumerate(res1[0]['data']):
            if ijson_c.get('t_ids', []) and rr['t_id'] not in ijson_c.get('t_ids', []):continue
            if rr.get('f') == 'Y':
                c_sd    = {}
                ph_map_d    = {}
                for ph in res1[0]['phs']:
                    ph_map_d[ph['k']]    = ph
                    if rr.get(ph['k'], {}).get('c_s') and not rr.get(ph['k'], {}).get('f_ph_str'):
                        c_sd[ph['k']]    = (rr.get(ph['k'], {}).get('c_s'), ph_value_d[ph['k']][ti])
                if c_sd:
                    new_ops     = {}
                    exists_op   = {}
                    for pk, c_s in c_sd.items():
                        opnds   = []
                        for ft in rr[pk]['f_col'][0]:
                            if ft.get('taxo_id') and ft['operator'] != '=':
                                opnds.append(taxo_value_grp[str(ft['taxo_id'])])
                        opnds.sort()
                        exists_op[tuple(opnds)]   = 1
                        v_tids  = list(sets.Set(ph_value_d[pk].keys()))
                        v_tids.sort()
                        if opnds[-1] < ti:
                            v_tids  = filter(lambda x:x<ti, v_tids)
                        else:
                            v_tids  = filter(lambda x:x>ti, v_tids)
                        opvs    = map(lambda x:ph_value_d[pk][x], v_tids)
                        r_ar    = self.check_formula_specific(opvs, c_s[1])
                        #print '\n\n\n=================================='
                        #print [rr['t_l'], ph_map_d[pk]['n'], rr[pk]['v']]
                        #print '****************************************************************'
                        #print 'OLD OPNDS'
                        #for op in opnds:
                        #    print '\t',op, [res1[0]['data'][op]['t_l'], ph_value_d[pk][op]]
                        opnds_set   = sets.Set(opnds)
                        for f_r in r_ar:
                            sign_ar, f_ar, f    = f_r
                            ti_inds = sets.Set(map(lambda x:v_tids[x], f_ar[:-1]))
                            #print f_r
                            if ti_inds != opnds_set and opnds_set.issubset(ti_inds):  
                                #print '\n\t****************************************************************'
                                #print '\tNEW OPNDS'
                                rem_value   = []
                                for op in ti_inds:
                                    #print '\t\t',op, [res1[0]['data'][op]['t_l'], ph_value_d[pk][op]]
                                    if op not in opnds:
                                        rem_value.append((op, ph_value_d[pk][op]))
                                sumval  = sum(map(lambda x:x[1], rem_value))
                                sumval  = self.convert_floating_point(sumval)
                                #print '\tSUM', [sumval, c_s[0]]
                                if sumval == c_s[0]:
                                    ops = map(lambda x:x[0], rem_value)
                                    ops.sort()
                                    ops = tuple(ops)
                                    new_ops[ops] = 1
                                    #print '\n\tYES'
                    if new_ops and len(exists_op.keys()) == 1:
                        #print '\n\nPossible Formula'
                        #print rr['t_l']
                        exists_op   = exists_op.keys()[0]
                        for ops in new_ops:
                            #print '\n\t********************************************************'
                            ops = list(exists_op+ops)
                            ops.sort()
                            pos_form_d.setdefault(res1[0]['data'][ti]['t_id'], []).append(ops)
                            #for op in ops:
                            #    print '\t\t',op, [res1[0]['data'][op]['t_l']]
                        
                    pass
        return pos_form_d
                    
                    
            

    def db_validation(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()
        final_ar    = self.gen_final_output(ijson, 'Y')
        tc_d        = {}
        res_d       = {}
        table_err   = []
        m_tables, rev_m_tables, doc_m_d, table_type_md = self.get_main_table_info(company_name, model_number)
        for ii, ijson_c in enumerate(final_ar):
            ijson_c['gen_output'] = 'Y'
            table_type  = ijson_c['table_type']
            print 'Running ', ii, ' / ', len(final_ar), [ijson_c['table_type'], ijson_c['type'], ijson_c.get('data', []), ijson_c.get('grpid', '')]
            if ijson_c['type'] == 'group':
                for vid, t_d in ijson_c['vids'][0].items():
                    for tid in t_d.keys():
                        tc_d.setdefault(table_type, {}).setdefault(tid, {})[ijson_c['grpid']]   = ijson_c['data']
            # data builder
            res                 = self.create_seq_across(ijson_c)
            rc_d    = {}
            ph_cd   = {}
            for ph in res[0]['phs']:
                ph_cd.setdefault(ph['g'], {}).setdefault(ph['n'], []).append(ph)
            empty_scale = {}
            for rr in res[0]['data']:
                vt_d    = {}
                p_scale = {}
                scal_d  = {}
                t_scal_d    = {}
                lbl_d   = {}
                for ph in res[0]['phs']:
                    if 'phcsv' in rr.get(ph['k'], {}):
                        phcsv   = rr[ph['k']]['phcsv']
                        if not phcsv:continue
                        if not phcsv['s']:
                            try:
                                clean_value = numbercleanup_obj.get_value_cleanup(rr[ph['k']]['v'])
                            except:
                                clean_value = ''
                            if clean_value:
                                empty_scale.setdefault(rr[ph['k']]['t'], {})[rr[ph['k']]['x']]  = 1
                        table_id,   xml_id  = rr[ph['k']]['t'], rr[ph['k']]['x']
                        l_d = rr[table_id+':$$:'+xml_id]
                        lbl_d.setdefault(table_id, {}).setdefault(l_d['x'], {'v':{}, 'x':{}})['v']    = l_d
                        lbl_d[table_id][l_d['x']]['x'][xml_id]  = 1
                        t_scal_d.setdefault(rr[ph['k']]['t'], {}).setdefault(phcsv['s'], {})[rr[ph['k']]['x']]   = 1
                        if phcsv['vt']:
                            vt_d.setdefault(phcsv['vt'], {})[rr[ph['k']]['t']]   = 1 #, {})[rr[ph['k']]['x']]  = 1
                        if phcsv['vt'] == 'Percentage' and phcsv['s'] != '1':
                            p_scale.setdefault(rr[ph['k']]['t'], {})[rr[ph['k']]['x']]  = 1
                scal_d  = {}
                for tid, s_d in t_scal_d.items():
                    scals   = s_d.keys()
                    scals.sort()
                    scals   = tuple(scals)
                    scal_d.setdefault(scals, {}).setdefault(tid, [])
                    xmls    = []
                    for sc, xml_d in s_d.items():
                        xmls    += xml_d.keys()
                    scal_d[scals][tid]  = xmls
                if len(scal_d.keys()) > 1:
                    table_ar    = []    
                    for scals, tids in scal_d.items():
                        tinfo   = []
                        for tid in tids.keys():
                            doc_id = doc_m_d.get(tid, '')
                            phinfo  = []
                            for tx, txv in lbl_d[tid].items():
                                phinfo.append({'em':txv['v']['v'], 'x':txv['x'].keys(), 't':tid, 'd':doc_id, 'dp':'P'})
                            tinfo.append({'t':tid, 'x':tids[tid], 't_id':rr['t_id'], 'em':'', 'd':doc_id, 'dp':'P', 'info':phinfo})
                        table_ar.append({'sig':'('+', '.join(scals)+') - '+rr['t_l'], 'info':tinfo})
                    table_err.append((table_type, '', 'CSV', [{'e':'More than one Scale in DB ROW %s'%(rr['t_l']), 'info':table_ar, 'xmls':[]}]))
                if len(vt_d.keys()) > 1:
                    table_err.append((table_type, '', 'Final Output', [{'e':'More than one Value Type in DB ROW %s - %s'%(rr['t_l'], vt_d), 'info':[vt_d], 'xmls':[]}]))
                for table_id, xml_d in p_scale.items():
                    table_err.append((table_type, table_id, 'PH-CSV', [{'e':'Percentage value typw has Incorrect scale', 'info':[], 'xmls':xml_d.keys()}]))
            print 'empty_scale ',table_type, empty_scale
            for table_id, xml_d in empty_scale.items():
                table_err.append((table_type, table_id, 'PH-CSV', [{'e':'Empty Scale', 'info':[], 'xmls':xml_d.keys()}]))
                    
            for g, ph_d in ph_cd.items():
                phs = filter(lambda x:len(ph_d[x])> 1, ph_d.keys())
                if not phs:continue
                print '\n=============================================================='
                print 'More than one column has same PH in table ', (table_type, g)
                xml_ids = {}
                tid     = ''
                for ph in phs:
                    print '\t', ph, map(lambda x:x['k'], ph_d[ph])
                    for ph in ph_d[ph]:
                        for rr in res[0]['data']:
                            if ph['k'] not in rr: continue
                            tid = rr[ph['k']]['t']
                            xml_ids[rr[ph['k']]['x']]   = 1
                if tid:
                    table_err.append((table_type, tid, 'Final Output', [{'e':'More than one column has same PH in table', 'info':[phs], 'xmls':xml_ids.keys()}]))
            duplicate_taxonomy  = {}
            #ijson_c['DB_DATA']  = copy.deepcopy(res)
            ijson_c['type'] = 'display'
            res1     = self.create_final_output_with_ph(ijson_c,'P', 'Y')
            if res1[0]['message'] != 'done':
                res1    = res1[0]['res']
            for ti, rr in enumerate(res1[0]['data']):
                if ijson_c.get('t_ids', []) and rr['t_id'] not in ijson_c.get('t_ids', []):continue
                duplicate_taxonomy.setdefault(rr['t_l'].lower, {})[str(rr['t_id'])]   = rr['t_l']
            pos_form    = self.checksum_validation(ijson_c, res1)
                    
            tmp_d = {}
            for f_taxo_id, tid_dict in duplicate_taxonomy.items():
                if len(tid_dict.keys()) > 1:
                    print '\n======================================================================='
                    print 'Duplcate taxonomy ', [table_type, f_taxo_id]
                    tmp_d.update(tid_dict)
                    table_err.append((table_type, '', 'Final Output', [{'e':'Duplicate taxonomy %s'%(f_taxo_id), 'info':tid_dict.keys(), 'xmls':[]}])) 
            if table_type in ['IS', 'BS', 'CF']:
                ijson_c['NO_FORM']  = 'Y'
                #ijson_c['DB_DATA']  = copy.deepcopy(res)
                res0     = self.create_final_output_with_ph(ijson_c,'P', 'N')
                if res0[0]['message'] != 'done':
                    res0    = res0[0]['res']
                res_d[(table_type, ijson_c.get('grpid', ''))]   = res0
        missing_conn    = self.table_connection_validation(ijson, res_d, final_ar)
        print 'Missing Connection ', missing_conn
        for k,v in missing_conn.keys():
            table_err.append((k, '', 'Final Output',[{'e':"Connection missing %s-%s"%(k,v), 'info':[], 'xmls':[]}]))
            
                    
        for table_type, t_d in tc_d.items():
                table_grp   = {}            
                tids    = filter(lambda x:len(t_d[x].keys())> 1, t_d.keys())
                if tids:
                    print '\n========================================================='
                    print 'More than one column in group ', table_type
                    for tid in tids:
                        print '\t',tid, t_d[tid]
                        t, cid  = tid.split('-') 
                        table_grp.setdefault(t, {}).setdefault(cid, {}).update(t_d[tid])
                    for table_id, cinfo in table_grp.items():
                        table_id    = str(table_id)
                        ids         = txn_m.get('GV_'+table_id)
                        if not ids:continue
                        col_x   = {}
                        for c_id in ids.split('#'):
                            tid, r, c   = c_id.split('_')
                            r       = int(r)
                            c       = int(c)
                            x       = txn_m.get('XMLID_'+c_id)
                            col_x.setdefault(c, {})[x]  = 1
                        e_ar    = []
                        for c, grpinfo in cinfo.items():
                            e_ar.append({'e':'More than One group %s '%(grpinfo.values()), 'xmls':col_x[int(c)].keys(), 'info':grpinfo.values()})
                        table_err.append((table_type, table_id, 'Final Output',e_ar))
        return table_err



    def check_formula_specific(self, opr_ar, res_elm):
        values = opr_ar + [ res_elm ]
        from libgcomputation_cpp import compute as gcompute_cpp
        #values = [10, 20, 30, 60]
        pattern_file = '/mnt/eMB_db/GComp_txt_files/1/ExtnGCPatterns.txt'
        formula_file = '/mnt/eMB_db/GComp_txt_files/1/Extnadd_formula.txt'
        results = []
        gcompute_cpp(values, pattern_file, formula_file, results)
        k_ar = []
        for r in results:
            if r[2] == False:
               if r[1][-1] == len(opr_ar):
                  k_ar.append(r)
            elif r[2] == True:
               if r[1][0] == len(opr_ar):
                  k_ar.append(r)
        return k_ar

                            
                        
        
        

    def phcsv_validation(self, ijson):
        import validation.ph_csv as ph_csv
        ph_csv_obj  = ph_csv.PHCSV()
        res         = ph_csv_obj.validate_phcsv(ijson)
        return res

    def order_by_table_structure_db(self, f_taxo_d, table_ids):
        f_taxo_arr  = map(lambda x:{'t_l':[x], 'ks':map(lambda x1:(x1.split('_')[0],  x1), f_taxo_d[x].keys())}, f_taxo_d.keys())
        tmptable_ids   = map(lambda x:x[1], table_ids)
        table_match_d   = {}
        for ti, dd in enumerate(f_taxo_arr):
            ks      = dd['ks']
            for table_id, c_id in ks:
                table_match_d.setdefault(table_id, {}).setdefault(ti, {})[c_id] = 1
        tabl_ids    = table_match_d.keys()
        tabl_ids.sort(key=lambda x:tmptable_ids.index(x))
        #tabl_ids.sort(key=lambda x:len(table_match_d[x].keys()), reverse=True)
        final_arr   = []
        for table_id in tabl_ids:
            inds    = table_match_d[table_id].keys()
            inds.sort()
            #print '\n==========================================================='
            #print table_id, sorted(inds)
            inds.sort(key=lambda x:int(sorted(map(lambda x1:int(x1.split('_')[1]), table_match_d[table_id][x].keys()))[0]))
            #print 'Ordered ', inds
            #print map(lambda x:(x, table_match_d[table_id][x].keys()), inds)
            if not final_arr:
                final_arr   = inds
            else:
                m_d = list(sets.Set(final_arr).intersection(sets.Set(inds)))
                deletion    = {}
                tmp_arr     = []
                ftmp_arr     = []
                for t in inds:
                    if t in m_d:
                        ftmp_arr    = []
                        if tmp_arr:
                            deletion[t] = copy.deepcopy(tmp_arr[:])
                            #tmp_arr = []    
                        continue
                    tmp_arr.append(t)
                    ftmp_arr.append(t)
                done_d  = {}
                m_d.sort(key=lambda x:final_arr.index(x))
                
                for t in m_d:
                    if t in deletion:
                        tmp_arr = []
                        for t1 in deletion[t]:
                            if t1 not in done_d:
                                tmp_arr.append(t1)
                                done_d[t1]  = 1
                        index       = final_arr.index(t)
                        final_arr   = final_arr[:index]+tmp_arr+final_arr[index:]    
                
                if ftmp_arr:
                    final_arr   = final_arr+ftmp_arr
            
            #print 'FINAL ', final_arr
        missing = sets.Set(range(len(f_taxo_arr))) - sets.Set(final_arr)
        if len(final_arr) > len(f_taxo_arr):
            print 'Duplicate ', final_arr
            sys.exit()
        if len(missing):
            print 'missing ', list(missing)
            sys.exit()
        f_taxo_arr  = map(lambda x:f_taxo_arr[x], final_arr)
        order_d = {}
        for ii, rr in enumerate(f_taxo_arr):
            order_d[rr['t_l'][0]]   = ii+1
        return order_d


    def read_sig_db_output(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = '%s_%s'%(project_id, deal_id)

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn         = env.begin()

        lmdb_path   = '/var/www/html/Rajeev/BBOX/'+str(project_id)+'_'+str(deal_id)
        lmdb_path   = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        env1        = lmdb.open(lmdb_path, readonly=True)
        txn1        = env1.begin()

        doc_d1       = {}
        doc_d       = {}
        dphs        = {}
        path    = "/var/www/html/TASFundamentalsV2/tasfms/data/output/%s/%s/1_1/21/sdata/doc_map.txt"%(project_id, deal_id)
        if os.path.exists(path):
            fin = open(path, 'r')
            lines   = fin.readlines()
            fin.close()
        else:
            lines   = []
        doc_ph_d    = {}
        c_year      = self.get_cyear(lines)
        start_year  = c_year - int(ijson.get('year', 10))
        for line in lines[1:]:
            line    = line.split('\t')
            if len(line) < 8:continue
            line    = map(lambda x:x.strip(), line)
            try:
                year    = int(line[7])
            except:continue
            if not line[3]:continue
            ph      = line[3]+line[7]
            if ph: #and start_year<int(ph[-4:]):
                doc_id          = line[0]
                doc_d[doc_id]   = (ph, line[2])
                doc_d1[doc_id]   = (ph, line[2])
                dphs[line[3]+str(year)]        = 1
                doc_ph_d[doc_id]    = line[3]+str(year)


        db_error_path   = '/mnt/eMB_db/%s/%s/table_sign_errors'%(company_name, model_number)
        env         = lmdb.open(db_error_path, readonly=True)
        txn_e       = env.begin()
        key         = ijson['sigkey']
        print [key]
        tfinal_d     = eval(txn_e.get(str(key)))
        table_type  = ijson['table_type']
        m_tables, rev_m_tables, doc_m_d, table_type_md = self.get_main_table_info(company_name, model_number, [table_type])
        table_ph_d  = {}
        table_xml_d = {}
        tfinal_d    = tfinal_d.get(table_type, {})
        for k, v in tfinal_d.items():
            for c_id in v.keys():
                tid, rid, cid = c_id.split('_')
                table_id        = str(tid)
                table_ph_d[(doc_m_d[tid], tid)] = 1
                x   = txn_m.get('XMLID_'+c_id)
                x    = self.clean_xmls(x)
                if x:
                    x    = x.split(':@:')[0].split('#')[0]
                    table_xml_d.setdefault(table_id, {})[(x.split('_')[1], x.split('_')[0].strip('x'))]   = 1
        import data_builder.db_data as db_data
        obj = db_data.PYAPI()
        dphs_ar         = report_year_sort.year_sort(dphs.keys())
        dphs_ar.reverse()

        table_ids   = table_ph_d.keys()
        table_ids.sort(key=lambda x:(dphs_ar.index(doc_ph_d[x[0]]), sorted(table_xml_d[x[1]].keys())[0]))
        order_d     = self.order_by_table_structure_db(tfinal_d, table_ids)
        ks          = tfinal_d.keys()
        ks.sort(key=lambda x:order_d[x])

        ignore_col  = obj.get_ignore_cols(company_name, model_number, company_id, txn_m)

        

        table_d   = {}
        res         = []
        row_id      = 1
        for ii, k in enumerate(ks):
            taxo_id = ii+1
            order_id= ii+1
            taxonomy    = k
            user_taxonomy, missing_taxo, ph, ph_label, gcom, ngcom, m_rows, vgh_text, vgh_group   = '', '', '', '', '', '', '', '', ''
            
            for c_id in tfinal_d[k].keys():
                table_id    = c_id.split('_')[0]
                doc_id      = doc_m_d[table_id]
                table_d.setdefault(table_id, {})[int(c_id.split('_')[1])]   = 1
                gv_xml  = c_id
                xml_id  = txn_m.get('XMLID_'+c_id)
                res.append(( row_id, taxo_id, order_id, taxonomy, user_taxonomy, missing_taxo, table_id, gv_xml, ph, ph_label,gcom, ngcom, doc_id,m_rows, vgh_text, vgh_group, xml_id))
                
        r_ld, rp_ld, rc_ld  = obj.read_tinfo(txn_m, table_d, ignore_col, txn1)
        e_lbl_d = {}
        main_header = ''
        return_tc   = {}
        final_res   = obj.form_db_data(res, txn_m, txn1, txn, r_ld, rp_ld, ijson, dphs, doc_d, table_d, e_lbl_d, m_tables, rev_m_tables, table_type, main_header, doc_m_d, 'N', {}, {}, return_tc)
        final_res[0]['g_ar']    = obj.read_all_vgh_groups(table_type, company_name, model_number, doc_m_d, return_tc)
        g_ar                    = filter(lambda x:'HGHGROUP' not in x['grpid'], final_res[0]['g_ar'])
        final_res[0]['g_ar']    = g_ar
        if g_ar:
            final_res[0]['g_ar']    = [{'n':'ALL', 'phs':return_tc.get('ALL', {})}] + g_ar
        return final_res
        
        
        

    def table_grp_data(self, doc_m_d, doc_ph_map, company_name, model_number, company_id, txn_m, table_types, ijson, return_err_flg=None):
        if table_types and ijson.get('only_error') != 'Y':
            import model_view.phcsv_check as PhCsv_Validata
            obj = PhCsv_Validata.PhCsv_Validata()
            error_dct, ttype_ph_sig_d, t_err_d, t_symm_d, u_err_f = obj.validate_init_class(company_name, model_number, company_id, table_types)
            #print 'here'
            #sys.exit()
        else:
            import model_view.phcsv_check_new as PhCsv_Validata
            obj = PhCsv_Validata.PhCsv_Validata()
            #print company_name, model_number, company_id;sys.exit()
            #print '>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>'
            ph_flip_flg = 0
            if ijson.get("flip") == "Y":
                ph_flip_flg = 1
            error_dct, ttype_ph_sig_d, t_err_d, t_symm_d, u_err_f, triplet_issue_tables = obj.group_tables_by_sign(company_name, model_number, company_id, table_types, ph_flip_flg)
            #sys.exit()
            print 'SSSSSSSSS', len(triplet_issue_tables)
        import data_builder.db_data as db_data
        obj = db_data.PYAPI()
        ignore_col  = obj.get_ignore_cols(company_name, model_number, company_id, txn_m)
            
        pt_d    = {"FY":1,"H1":1,"H2":1,"Q1":1,"Q2":1,"Q3":1,"Q4":1,"M9":1,"CY":1,"FYESD":1,"H1ESD":1,"H2ESD":1,"Q1ESD":1,"Q2ESD":1,"Q3ESD":1,"Q4ESD":1,"M9ESD":1,"CYESD":1,"PY":1,"CUY":1}
        #{table_id:{error_msg:[xmls]}}
        db_error_path   = '/mnt/eMB_db/%s/%s/table_sign_errors'%(company_name, model_number)
        try:
            env         = lmdb.open(db_error_path, readonly=True)
            txn_e       = env.begin()
        except:
            txn_e       = {}

        lmdb_path    = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        env1    = lmdb.open(lmdb_path, readonly=True)
        txn1    = env1.begin()

        ph_sig_map_d    = {}
        table_grp_d     = {}
        table_error_d  = {}
        for k, v in ttype_ph_sig_d.items():
            print k, v
            grpid = 1
            #print k
            for tgrp in v[1]:
                #print '\t',tgrp
                for table_id in tgrp:
                    table_grp_d[table_id]   = grpid
                grpid   += 1
            for ptup, sig_ar in v[0].items():
                for (sig, table_id, doc_id, ph_ar, col_xml, col_flg) in sig_ar:
                    col_xml_ar  = []
                    for ii, x_ar in enumerate(col_xml):
                        xml_d   = {}
                        cols    = ''
                        for x in x_ar:
                            if cols == '':
                                tk      = self.get_quid(table_id+'_'+x)
                                c_id    = txn_m.get('XMLID_MAP_'+tk)
                                cols    = c_id.split('_')[2]
                            xml_d[x]   = {}
                        
                        col_xml_ar.append((xml_d, cols, col_flg[ii]))
                        
                    ph_sig_map_d[(k, tuple(sig), table_id)]    = (ph_ar, col_xml_ar)
                    #(['D0~FY', 'D1~FY'], '264', '10', [['FY2018'], ['FY2017']])
        #import table_match_new
        table_score_map = {} #table_match_new.table_wrapper(company_name, model_number, company_id)
        #import model_view.phcsv_check as PhCsv_Validata
        #pvObj  = PhCsv_Validata.PhCsv_Validata()
        table_err = []
        for table_type, sig_ar in error_dct.items():
            for grpid, sig_d in enumerate(sig_ar):
                table_ar    = []
                for sig, tids in sig_d.items():
                    ntids = tids
                    ntids.sort()
                    #sigkey = 'DB:'+self.get_quid(table_type+'^!!^'+'$:$'.join(sig))
                    sigkey = 'DB:'+self.get_quid('^!!^'.join(ntids))
                    #key = self.get_quid(table_type+'^!!^'+'$:$'.join(sig))

                    key = 'ERROR'+self.get_quid('^!!^'.join(ntids))
                    db_e_flg    = txn_e.get(str(key))

                    key = self.get_quid('^!!^'.join(ntids))
                    db_e    = txn_e.get(str(key))
                    #print (table_type, sig, key), db_e
                    taxo_xmls       = {}
                    taxo_err_xmls   ={}
                    max_col_d       = {'c':{}, 's':{}, 'vt':{}}
                    if db_e:# and ijson.get("DB") == 'Y':
                        db_e    = eval(db_e)
                        tmpdb_e = {}
                        for tx, em_d in db_e.items():
                            for emg , t_csvd in em_d.items():
                                c, s,vt = t_csvd
                                table_id, xml_id    = emg
                                for ii, cname in enumerate(['c', 's', 'vt']):
                                    comp                = cname+'~'+t_csvd[ii]
                                    taxo_err_xmls[comp]    = comp
                                    tmpdb_e.setdefault(table_id, {}).setdefault(comp, []).append(xml_id)
                                taxo_xmls.setdefault(tx, {'c':{}, 's':{}, 'vt':{}})['c'].setdefault(c, {})[table_id+':$$:'+xml_id]  = 1
                                taxo_xmls[tx]['s'].setdefault(s, {})[table_id+':$$:'+xml_id]  = 1
                                taxo_xmls[tx]['vt'].setdefault(vt, {})[table_id+':$$:'+xml_id]  = 1
                                max_col_d['c'][c]   = 1
                                max_col_d['s'][s]   = 1
                                max_col_d['vt'][vt]   = 1
                                #print (tx, emg, t_csvd)
                                #sys.exit()
                                #for comp, t_d   in t_csvd.items():
                                #    for table_id, xml_id in t_d:
                                #        taxo_xmls.setdefault(tx, {})[table_id+':$$:'+xml_id]  = 1
                                #        taxo_err_xmls[emg+' ('+str(comp)+')']    = comp
                                #        tmpdb_e.setdefault(table_id, {}).setdefault(emg+' ('+str(comp)+')', []).append(xml_id)
                        db_e    = tmpdb_e
                    else:
                        db_e    = {}
                    tinfo   = []
                    score_info = {}
                    score_threshold = 0.5
                    error_table_dct = {}
                    if table_score_map:
                        for table1 in tids:
                            for table2 in tids:
                                if table1 == table2:continue
                                get_scr = table_score_map.get((table1, table2), '')
                                flg = 'N'
                                if get_scr < score_threshold:
                                    flg = 'Y'
                                    error_table_dct[table1] = 'TE'
                                dct = {'scr':get_scr, 'sc_f':flg}
                                score_info['-'.join([table1, table2])] = dct
                    components_dct = {}
                    final_color_d   = {}
                    e_components_dct = {}
                    e_final_color_d   = {}

                    db_components_dct = {}
                    db_final_color_d   = {}

                    e_map_d = {}
                    sym_err = 0
                    csv_map_d   = {}
                    tgrps   = []
                    grp_d   = {}
                    for tid in tids:
                        grp_d.setdefault(table_grp_d.get(tid, ''), {})[tid] = 1
                    #print 'GGGGGGGGGGGGGGGGGGGGGGGGGGGGG', grp_d
                    for tgrpid, tids_d in grp_d.items():
                        tids    = tids_d.keys()
                        for tid in tids:
                            doc_id  = doc_m_d[tid]
                            print 'DDDDDDDDDDDDD', [doc_id, tid]
                            berr = self.bbox_xml_validation(tid, txn_m, txn1, ignore_col)   
                            if berr:
                                berr[0]['doc_ph']   = doc_ph_map[doc_id]
                                table_err.append((table_type, tid, 'BBOX', berr))
                            html_str, err_flg, data_dct = t_symm_d.get(tid, ('', 'ERROR', {}))#'', 'ERROR', {}
                            te = error_table_dct.get(tid, 'NE')
                            e_flg = 'N'
                            for xml, comp in data_dct.items():
                                comp    = tuple(comp) #'('+', '.join(comp)+')'
                                if comp not in components_dct:
                                    components_dct[comp]    = len(components_dct.keys())+1
                                comp_id = components_dct[comp]
                                final_color_d.setdefault(tid, {}).setdefault(xml, {})[comp_id]  = 1
                            if (te == 'TE') or (err_flg == 'E'):
                                e_flg = 'Y'
                            phinfo  = []
                            ph_error    = 'N'
                            for pi, ph in enumerate(ph_sig_map_d[(table_type, sig, tid)][0]):
                                cell_wise = 'N'
                                phs = '-'.join(ph)
                                xml_d, col_num, col_flg   = ph_sig_map_d[(table_type, sig, tid)][1][pi]
                                if col_flg:
                                    cell_wise   = 'Y'
                                    ph_error    = 'Y'
                                #print ';;;;;;;;;;;;', doc_id, doc_ph_map[doc_id]
                                #print doc_ph_map
                                phinfo.append({'em':phs, 'x':xml_d.keys(), 't':tid, 'd':doc_id, 'dp':'P', 'doc_ph':doc_ph_map[doc_id], 'e_flg':cell_wise, 'cln':col_num})
                            flip_sts    = 'N'
                            if txn_m.get('FLIP_'+str(tid)):
                                flip_sts    = txn_m.get('FLIP_'+str(tid))
                            #"CSV Symmetric Error"
                            e_flg = 'N'
                            f_error = 'N'
                            if err_flg: #CSV MAP
                                #t_err_d.setdefault(tid, {})["CSV Symmetric Error"] = err_flg
                                sym_err = 'Y'
                                e_flg = 'Y'
                                for x in err_flg:
                                    #f_error['CSV-MAP']  = 1
                                    f_error = 'Y'
                                    if ijson.get('deal_id', '') in ('110'):
                                        f_error = 'N'
                                    csv_map_d[data_dct[x]]   = 1
                            if u_err_f.get(tid, {}):
                                dst = u_err_f[tid]
                                for tpl, xml_arr in dst.iteritems():
                                    #print 'SASASASASA', [tid], tpl, xml_arr
                                    t_err_d.setdefault(tid, {})[str(tpl)] = xml_arr
                            
                            #if t_err_d.get(tid, {}): #CHK-MAP
                            #    e_flg = 'Y'
                            for emg, xml_d in t_err_d.get(tid, {}).items():
                                if isinstance(xml_d, list):
                                    if emg not in e_components_dct:
                                        e_components_dct[emg] = len(e_components_dct.keys())+1
                                    comp_id = e_components_dct[emg]
                                    for x in xml_d:
                                        e_final_color_d.setdefault(tid, {}).setdefault(x, {})[comp_id]  = 1
                                    
                                else:
                                    for comp, xml_ids in xml_d.items():
                                        emg = emg+' ('+str(comp)+')'
                                        e_map_d[emg]    = comp
                                        if emg not in e_components_dct:
                                            e_components_dct[emg] = len(e_components_dct.keys())+1
                                        comp_id = e_components_dct[emg]
                                        for x in xml_ids:
                                            e_final_color_d.setdefault(tid, {}).setdefault(x, {})[comp_id]  = 1

                            if db_e.get(tid, {}) and db_e_flg == '1': #DB-MAP
                                #f_error['DB-MAP']  = 1
                                #f_error = 'Y'
                                pass

                            for emg, xml_ids in db_e.get(tid, {}).items():
                                if emg not in db_components_dct:
                                    db_components_dct[emg] = len(db_components_dct.keys())+1
                                comp_id = db_components_dct[emg]
                                for x in xml_ids:
                                    db_final_color_d.setdefault(tid, {}).setdefault(x, {})[comp_id]  = 1
        
                            if ph_error == 'Y':
                                #f_error['PH-ERROR']  = 1
                                f_error = 'Y'
                                if ijson.get('deal_id', '') in ('110'):
                                        f_error = 'N'
    
                            if e_flg == 'Y': #f_error == 'Y':
                                table_error_d[str(tid)]  = 1

                        
                            #print 'HHHHHHHHHHHHHHHHHHHHHHHHHHHHHH', [doc_id, tid], '\n'    
                            tinfo.append({'t':tid, 'x':[], 'em':'', 'd':doc_id, 'dp':'P','info':phinfo, 'flip':flip_sts, 'err_flg':e_flg, 'doc_ph':doc_ph_map[doc_id],'tgrpid':tgrpid, 'ph_error':ph_error, 'f_error':f_error})
                        if len(grp_d.keys()) > 1:
                            tinfo.append({'t':'', 'x':[], 'em':'', 'd':'', 'dp':'P','info':'', 'flip':'', 'err_flg':'', 'doc_ph':''})
                    db_components_ar   = []
                    ks  = db_components_dct.keys()
                    ks.sort(key=lambda x:db_components_dct[x])
                    for comp in ks:
                        com_p   = taxo_err_xmls[comp]
                        db_components_ar.append({'n':comp, 'id':db_components_dct[comp]})
                        for ii, csv in enumerate(com_p[0].split('_')):
                            db_components_ar[-1][csv]  = com_p[1+ii]

                    e_components_ar   = []
                    ks  = e_components_dct.keys()
                    ks.sort(key=lambda x:e_components_dct[x])
                    for comp in ks:
                        e_components_ar.append({'n':comp, 'id':e_components_dct[comp]})
                        if comp in e_map_d:
                            com_p   = e_map_d[comp]
                            for ii, csv in enumerate(com_p[0].split('_')):
                                e_components_ar[-1][csv]  = com_p[1+ii]
                    components_ar   = []
                    ks  = components_dct.keys()
                    ks.sort(key=lambda x:components_dct[x])
                    components_ar_len   = 0
                    for comp in ks:
                        #[Currency, Scale, Value Type]
                        currency, scale, value_type = comp
                        comp_eflg   = 'N'
                        if comp in csv_map_d:
                            comp_eflg   = 1
                            components_ar_len   += 1
                        if not scale:
                            comp_eflg   = 'Y'
                        if value_type == 'Percentage' and scale != '1': 
                            comp_eflg   = 'Y'
                        if not value_type:
                            comp_eflg   = 'Y'
                        if value_type in ['MNUM'] and (not currency or not scale):
                            comp_eflg   = 'Y'
                        components_ar.append({'n':'('+', '.join(comp)+')', 'id':components_dct[comp], 'c':comp[0], 's':comp[1], 'vt':comp[2], 'e_flg':comp_eflg})
                    tmpmax_col_d    = {}
                    for ctyp, cvs in max_col_d.items():
                        tmpmax_col_d[ctyp]  = len(cvs.keys())
                    #print 'TTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT', tinfo
                    table_ar.append({'sig':'('+', '.join(sig)+')', 'info':tinfo, 'si':score_info, 'components':components_ar, 'color_d':final_color_d, 'e_components':e_components_ar, 'e_color_d':e_final_color_d, 'db_components':db_components_ar, 'db_color_d':db_final_color_d, 'taxo_xmls':taxo_xmls, 'sigkey':sigkey, 'components_len':components_ar_len, 'e_components_len':len(e_components_ar), 'db_components_len':len(db_components_ar) if db_e_flg == '1' else 0, 'max_col_d':tmpmax_col_d})
                #print 'MMMMMMMMMMMMMMMMMMMM', table_ar
                    
                table_err.append((table_type, '', 'TABLE-GRP', [{'e':'Group-%s'%(grpid+1), 'info':table_ar, 'xmls':[]}]))
        if return_err_flg == 'Y':
            return table_error_d
        error_type  = ['"TABLE-GRP"', '"BBOX"']
        #print 'HHHHHHHHHH', triplet_issue_tables
        return table_err ,error_type
        

    def multi_cls_validation(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = '%s_%s'%(project_id, deal_id)
        if ijson.get('PRINT') != 'Y':
            disableprint()

        path    = "%s/%s/%s/1_1/21/sdata/doc_map.txt"%(self.doc_path, project_id, deal_id)
        if os.path.exists(path):
            fin = open(path, 'r')
            lines   = fin.readlines()
            fin.close()
        else:
            lines   = []
        doc_ph_map  = {}
        for line in lines[1:]:
            line    = line.split('\t')
            if len(line) < 8:continue
            line    = map(lambda x:x.strip(), line)
            ph      = line[3]+line[7]
            doc_id  = line[0]
            doc_ph_map[doc_id]   = ph
        
        print 'FFFFFFF', 'SSSSSSSS', doc_ph_map


        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()
        m_tables, rev_m_tables, doc_m_d, table_type_md = self.get_main_table_info(company_name, model_number)
        table_err, error_type   = self.table_grp_data(doc_m_d, doc_ph_map, company_name, model_number, company_id, txn_m, ijson.get("table_types", []), ijson)
        enableprint()
        return table_err , ', '.join(error_type)


        import model_view.phcsv_check as PhCsv_Validata
        obj             = PhCsv_Validata.PhCsv_Validata()
        error_class_elms, ph_sign_types, er_ph, ph_suspect, ttype_ph_sig_d = obj.validate(company_name, model_number, company_id)
        #print error_class_elms
        #print ph_sign_types
        #print er_ph
        #print ttype_ph_sig_d
        #sys.exit()
        ph_sig_map_d    = {}
        for k, v in ttype_ph_sig_d.items():
            for ptup, sig_ar in v.items():
                for (sig, table_id, doc_id, ph_ar, col_xml) in sig_ar:
                    col_xml_ar  = []
                    for x_ar in col_xml:
                        xml_d   = {}
                        for x in x_ar:
                            xml_d[x]   = {}
                        col_xml_ar.append(xml_d)
                        
                    ph_sig_map_d[(k, tuple(sig), table_id)]    = (ph_ar, col_xml_ar)
                    #(['D0~FY', 'D1~FY'], '264', '10', [['FY2018'], ['FY2017']])

        import model_view.phcsv_check_new as PhCsv_Validata
        obj = PhCsv_Validata.PhCsv_Validata()
        #print company_name, model_number, company_id;sys.exit()
        error_phcsv_dct = obj.validate_csv(company_name, model_number, company_id)

        import model_view.table_tagging_v2 as TableTagging 
        Omobj = TableTagging.TableTagging()
        table_id_cell_dict = Omobj.all_user_selected_references(company_name, model_number, company_id)



        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn         = env.begin()

        
        ignore_col      = {}
        for table_id, xmls in table_id_cell_dict.items():
            table_id    = str(table_id)
            for xml_id in xmls:
                xml_id  = '#'.join(filter(lambda x:x, xml_id.split('#')))
                tk      = self.get_quid(table_id+'_'+xml_id)
                c_id    = txn_m.get('XMLID_MAP_'+tk)
                if not c_id:continue
                c       = int(c_id.split('_')[2])
                ignore_col[(table_id, c)]  = 1


        ph_csv_d    = {}
        for table_type, sig_d in ph_sign_types.items():
            table_ar    = []
            for sig, tids in sig_d.items():
                tinfo   = []
                for tid, doc_id in tids:
                    phinfo  = []
                    for pi, ph in enumerate(ph_sig_map_d[(table_type, sig, tid)][0]):
                        phs = '-'.join(ph)
                        xml_d   = ph_sig_map_d[(table_type, sig, tid)][1][pi]
                        phinfo.append({'em':phs, 'x':xml_d.keys(), 't':tid, 'd':doc_id, 'dp':'P', 'doc_ph':doc_ph_map[doc_id]})
                    
                    #phcsv, xml_d, csv_info   = self.read_phcsv(tid, txn_m, txn, ignore_col, ph_csv_d)
                    #cols    = phcsv.keys()
                    #cols.sort()
                    #phinfo  = []
                    #for c in cols:
                    #    if ('COL',c) not in csv_info:continue
                    #    phs = '-'.join(phcsv[c].keys())
                    #    phinfo.append({'em':phs, 'x':xml_d[c].keys(), 't':tid, 'd':doc_id, 'dp':'P', 'doc_ph':doc_ph_map[doc_id]})
                    tinfo.append({'t':tid, 'x':[], 'em':'', 'd':doc_id, 'dp':'P','info':phinfo})
                table_ar.append({'sig':'('+', '.join(sig)+')', 'info':tinfo})
            if len(table_ar) > 1:
                table_err.append((table_type, '', 'PH-Pattern', [{'e':'More than one PH Pattern found', 'info':table_ar, 'xmls':[]}]))
        #Error
        for (doc_id, table_id), xml_ar in er_ph.items():
            xmls    = []
            xml_d   = {}
            phcsv, xml_d, csv_info   = self.read_phcsv(table_id, txn_m, txn, ignore_col, ph_csv_d)
            rc_d    = {}
            for xr in xml_ar:
                txt, x, pt, p, pos_pt, pos_p    = xr
                if('ROW', x) not in csv_info:continue
                tk          = self.get_quid(table_id+'_'+x)
                c_id        = txn_m.get('XMLID_MAP_'+tk)
                if int(c_id.split('_')[2]) in ignore_col:continue
                xml_d[x]    = {'pt':pos_pt, 'p':pos_p}
                xmls.append(x)
            if xmls:
                table_type  = m_tables[str(table_id)]
                table_err.append((table_type, table_id, 'NULL-PH', [{'e':'Null Period & Period type', 'info':[xml_d], 'xmls':xmls,'dp':'P', 'doc_ph':doc_ph_map[doc_id]}]))

        for table_id, xml_d in error_phcsv_dct.items():
            doc_id = doc_m_d[table_id]
            xmls    = xml_d.keys()
            table_type  = m_tables[str(table_id)]
            table_err.append((table_type, table_id, 'NULL-CSV', [{'e':'CSV Error', 'info':[], 'xmls':xmls,'dp':'P', 'doc_ph':doc_ph_map[doc_id]}]))
        for (doc_id, table_id) in ph_suspect:
            phcsv, xml_d, csv_info   = self.read_phcsv(table_id, txn_m, txn, ignore_col, ph_csv_d)
            xmls    = []
            for c, xd in xml_d.items():
                xmls    += xd.keys()
                
            table_err.append((table_type, table_id, 'PH-Suspect', [{'e':'Suspicious table', 'info':[], 'xmls':xmls,'dp':'P', 'doc_ph':doc_ph_map[doc_id]}]))
            pass
        #sys.path.append('/root/databuilder_train_ui/tenkTraining/Data_Builder_Training/pysrc')

        for table_id, table_type in m_tables.items():
            doc_id = doc_m_d[table_id]
            phcsv, xml_d, csv_info   = self.read_phcsv(table_id, txn_m, txn, ignore_col, ph_csv_d)
            xmls    = []
            txmls    = []
            error_grp   = {}
            #print '\n==================='
            #print [table_id, table_type]
            for x, csv in csv_info.items():
                if isinstance(x, tuple):continue
                t   = csv_info[('TEXT', x)]
                if not t:continue
                period_type, period, currency, scale, value_type    = csv
                #print [table_id, x, csv]
                #****************************************************
                if not scale:# and value_type != 'Other':
                    error_grp.setdefault('Empty Scale', {})[x]  = {}
                    xmls.append(x)
                #****************************************************
                if value_type == 'Percentage':
                    if scale != '1':
                        error_grp.setdefault('Incorrect Scale for Value type Percentage', {})[x]  = {'s':['1']}
                t   = csv_info[('TEXT', x)]
                if '%' in t and value_type != 'Percentage':
                    error_grp.setdefault('Incorrect Value type for Percentage Text', {})[x]  = {'s':['1'],'vt':['Percentage']}
                #****************************************************
                if not value_type:
                    error_grp.setdefault('Empty Value Type', {})[x]  = {}
                #****************************************************
                if value_type in ['MNUM']:
                    if not currency and not scale:
                        error_grp.setdefault('Empty Currency & Scale for MNUM', {})[x]  = {}
                    elif not currency:
                        error_grp.setdefault('Empty Currency for MNUM', {})[x]  = {}
                    elif not scale:
                        error_grp.setdefault('Empty Scale for MNUM', {})[x]  = {}
            for em, x_d in error_grp.items():
                table_err.append((table_type, table_id, 'CSV-ERROR', [{'e':em, 'info':[x_d], 'xmls':x_d.keys(),'dp':'P', 'doc_ph':doc_ph_map[doc_id]}]))
        if 1:#ijson.get('gen') == 'Y':
            final_ar    = self.read_all_tinfo(ijson)
            for ii, ijson_c in enumerate(final_ar):
                ijson_c['gen_output'] = 'Y'
                table_type  = ijson_c['table_type']
                print 'Running ', ii, ' / ', len(final_ar), [ijson_c['table_type'], ijson_c['type'], ijson_c.get('data', []), ijson_c.get('grpid', '')]
                res                 = self.create_seq_across(ijson_c)
                for rr in res[0]['data']:
                    phcsv_d = {}
                    for ph in res[0]['phs']:
                        if 'phcsv' in rr.get(ph['k'], {}):
                            if not rr[ph['k']]['v'].strip():continue
                            phcsv   = rr[ph['k']]['phcsv']
                            if not phcsv:continue
                            table_id,   xml_id  = rr[ph['k']]['t'], rr[ph['k']]['x']
                            phcsv_d.setdefault('Currency', {}).setdefault(phcsv['c'], {}).setdefault((table_id, doc_id), {})[xml_id]  = {'c':[phcsv['c']]}
                            phcsv_d.setdefault('Scale', {}).setdefault(phcsv['s'], {}).setdefault((table_id, doc_id), {})[xml_id]     = {'s':[phcsv['s']]}
                            phcsv_d.setdefault('Value Type', {}).setdefault(phcsv['vt'], {}).setdefault((table_id, doc_id), {})[xml_id]   = {'vt':[phcsv['vt']]}
                    table_ar    = []
                    for ptype, pinfo in phcsv_d.items():
                        if len(pinfo.keys()) <=1:continue
                        t_d = {}
                        for sig, tinfo in pinfo.items():
                            phinfo  = []
                            for k, xml_d in tinfo.items():
                                t_d.setdefault(k, {}).setdefault(sig, {}).update(xml_d)
                        tinfo   = []
                        for (tid, doc_id), sig_d in t_d.items():
                            phinfo  = []
                            for sig, xml_d in sig_d.items():
                                phinfo.append({'em':sig, 'x':xml_d.keys(), 't':tid, 'd':doc_id, 'dp':'P', 'doc_ph':doc_ph_map[doc_id]})
                            tinfo.append({'t':tid, 'x':[], 'em':'', 'd':doc_id, 'dp':'P','info':phinfo})
                        table_ar.append({'sig':'More than one '+ptype+' ('+', '.join(pinfo.keys())+')', 'info':tinfo})
                    if table_ar:
                        table_err.append((table_type, '', 'Data-Builder', [{'e':'More than one CSV Pattern found ("%s") '%(rr['t_l']), 'info':table_ar, 'xmls':[], 'dp':'D', 'dbinfo':{'table_type':ijson_c['table_type'], 'data':ijson_c.get('data', []), 'grpid':ijson_c.get('grpid', ''), 'vids':ijson_c.get('vids', []), 'taxo_id':rr['t_id']}}]))
                        
                            
        enableprint()
        error_type  = ['"PH-Pattern"', '"NULL-PH"', '"NULL-CSV"', '"CSV-ERROR"', '"PH-Suspect"', '"TABLE-GRP"', '"Data-Builder"']
        return table_err , ', '.join(error_type)



    def read_phcsv(self,table_id, txn_m, txn, ignore_col, ph_csv_d):
        table_id    = str(table_id)
        if table_id in ph_csv_d:
            return ph_csv_d[table_id]
        ids         = txn_m.get('GV_'+table_id)
        if not ids:
            return {}, {}, {}
        phcsv   = {}
        xml_d   = {}
        t_phcsv   = {}
        r_xmls      = {}
        valid_row   = {}
        valid_col   = {}
        for c_id in ids.split('#'):
            tid, r, c   = c_id.split('_')
            r       = int(r)
            c       = int(c)
            x       = txn_m.get('XMLID_'+c_id)
            if (table_id, c) in ignore_col:continue
            t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
            xml_d.setdefault(c, {})[x]  = 1
            key     = table_id+'_'+self.get_quid(x)
            ph_map  = txn.get('PH_MAP_'+str(key))
            if ph_map:
                period_type, period, currency, scale, value_type    = ph_map.split('^')
            else:
                period_type, period, currency, scale, value_type    = '', '', '', '', ''
            r_xmls.setdefault(r, {})[x]      = (r, c)
            t_phcsv[('TEXT',x)]  = t
            if not t:continue 
            t_phcsv[x]   = (period_type, period, currency, scale, value_type)
            phcsv.setdefault(c, {})[period_type+period] = 1
            valid_row[r]   = {}
            valid_col[c]   = {}
        for r in valid_row.keys():
            for x, (r, c) in r_xmls[r].items():
                t_phcsv[('ROW',x)]  = (r, c)
        for c in valid_col.keys():
            t_phcsv[('COL',c)]  = 1
        ph_csv_d[table_id]  = (phcsv, xml_d, t_phcsv)
        return phcsv, xml_d, t_phcsv
                    
    def run_validation(self, ijson):
        table_err   = []
        t_et        = []
        #ttable_err, t_et   = self.multi_cls_validation(ijson)
        #table_err   += ttable_err
        if ijson.get('multi_cls') == 'Y':
            return []
        if ijson.get('PRINT', '') != 'Y':
            disableprint()
        final_err   = self.phcsv_validation(ijson)
        if ijson.get('PHCSV') == 'Y':
            return []
        err         = self.db_validation(ijson)
        enableprint()
        error_type  = ['"Final Output"', '"PH-CSV"', '"BBOX"', '"CSV"'] +t_et #,'"PH-Pattern"', '"NULL-PH"']
        return final_err+err+table_err , ', '.join(error_type)

    def update_formula_str_column(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        user_name       = ijson["user"]
        table_type      = ijson["table_type"]
        group_id        = ijson["grpid"]
        grp_lst         = ijson["grps"][0]
        formula         = grp_lst["f"]

        formula         = formula.replace('%3D', '=')
        ph              = grp_lst["ph"]
        formula_str     = '##'.join(map(str, grp_lst["ignore_taxo"]))
        #formula_str     = ijson["ignore_taxo"]
        #formula_type    = ijson["Ignore_taxo"]
        data_tup        = (formula, table_type, group_id)
        

        db_file = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn    = sqlite3.connect(db_file)
        cur     = conn.cursor()

        read_qry = 'select row_id, formula, table_type, group_id from ph_derivation where formula_type="Ignore_taxo"'
        cur.execute(read_qry)
        dt = cur.fetchall()
        table_data = {}
        #tuple(map(str, row[1:])):int(row[0]) for row in dt}
        for row in dt:
            table_data.setdefault(row[1:], []).append(str(row[0]))
        
        if data_tup in table_data:
            get_rowId = table_data[data_tup]
            #print 'UPDATE ph_derivation SET formula_str="%s" WHERE row_id in(%s)'%(formula_str, ', '.join(get_rowId))
            cur.execute('UPDATE ph_derivation SET formula_str="%s" WHERE row_id in(%s)'%(formula_str, ', '.join(get_rowId)))
        elif data_tup not in table_data and formula_str:
            data_row = (formula, table_type, group_id, ph, 'Ignore_taxo', formula_str, user_name)
            cur.execute('INSERT INTO ph_derivation(formula, table_type, group_id, ph, formula_type, formula_str, user_name) VALUES(?, ?, ?, ?, ?, ?, ?)', data_row)
        conn.commit()
        conn.close()
        return [{'message':'done'}] 

    def update_user_csv_row_rest(self, company_name, model_number, ijson, rc, xml_dict, lmdbPath=''):
        db_file     = os.path.join('/mnt/eMB_db/', company_name, str(model_number), 'tas_tagging.db')
        conn1     = sqlite3.connect(db_file)
        cur1      = conn1.cursor()
        crt_qry     = 'CREATE TABLE IF NOT EXISTS UsrTableCsvPhInfo(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_id VARCHAR(256), xml_id TEXT, period_type TEXT, period TEXT, currency TEXT, value_type TEXT, scale TEXT, month VARCHAR(20), review_flg INTEGER)'
        cur1.execute(crt_qry)

        data = []
        apply_xml_ids = []  
        apply_cell_dict = {}
        period_type, period, currency, scale, value_type, month = map(str, [ijson.get('pt', ''), ijson.get('p', ''), ijson.get('c', ''), ijson.get('s', ''), ijson.get('vt', ''), ijson.get('m', '')])

        review_flg = 1
        for table_id, xml_list in xml_dict.iteritems():
            db_file     = os.path.join('/mnt/eMB_db/', company_name, str(model_number), 'table_tagging_dbs', '%s.db'%table_id)
            conn  = sqlite3.connect(db_file)
            cur   = conn.cursor()
            xml_list = map(lambda x:str(x), xml_list[:])
            review_flg = 1
            if rc:
                apply_xml_ids = []
                if len(rc) == 2:
                    apply_xml_ids = xml_list
                    if not apply_xml_ids:
                        stmt = "select distinct(xml_id) from TableCsvPhInfo where cell_type='GV'"
                        cur.execute(stmt)
                        apply_xml_ids += map(lambda x:str(x[0]), cur.fetchall())    
                else:
                    xml_ids = ["'"+x+"'" for x in xml_list]
                    stmt = "select nrow, ncol from TableCsvPhInfo where xml_id in (%s)" %(', '.join(xml_ids))
                    cur.execute(stmt)
                    res = cur.fetchall()
                    for row in res:
                        sel_row, sel_col = row[0], row[1]
                        for rr in rc:
                            if rr == 'ROW':
                                stmt = "select distinct(xml_id) from TableCsvPhInfo where cell_type='GV' and nrow='%s'" %(sel_row)
                            elif rr == 'COL':
                                stmt = "select distinct(xml_id) from TableCsvPhInfo where cell_type='GV' and ncol='%s'" %(sel_col)
                            cur.execute(stmt)
                            apply_xml_ids += map(lambda x:str(x[0]), cur.fetchall())    
                ################################################################
                apply_xml_ids = list(set(apply_xml_ids))
                rem_xml_ids = []
                for new_xml_id in apply_xml_ids:
                    if new_xml_id in xml_list:
                        ttup = (period_type, period, currency, value_type, scale, month)
                        #review_flg = self.validate_cell_ph_info(ttup)
                        apply_cell_dict[new_xml_id] = review_flg
                        data.append((table_id, new_xml_id, period_type, period, currency, value_type, scale, month, review_flg))
                    else:
                        rem_xml_ids.append(new_xml_id)
                if xml_list:
                    apply_xml_ids = list(set(apply_xml_ids) - set(rem_xml_ids))
                else:
                    apply_xml_ids = list(set(apply_xml_ids))
                ################################################################   
            else:
                apply_xml_ids = xml_list
                ttup = (period_type, period, currency, value_type, scale, month)
                #review_flg = self.validate_cell_ph_info(ttup)
                data = []
                for xml_id in xml_list: 
                    apply_cell_dict[xml_id] = review_flg
                    data.append((table_id, xml_id, period_type, period, currency, value_type, scale, month, review_flg))
            ####### ADD TO USER DATA #################    
            app_xml_str = ', '.join(['"'+e+'"' for e in apply_xml_ids]) 
            table_name = 'UsrTableCsvPhInfo'
            stmt = "delete from UsrTableCsvPhInfo where xml_id in (%s) and table_id='%s'" %(app_xml_str, table_id)
            cur1.execute(stmt)
            #msg = self.qObj.insertIntoLite(conn1, cur1, '', table_name, column_tup, data)
            cur1.executemany('INSERT INTO UsrTableCsvPhInfo(table_id, xml_id, period_type, period, currency, value_type, scale, month, review_flg) VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?)', data)
            stmt = "update TableCsvPhInfo set period_type='%s', period='%s', currency='%s', value_type='%s', scale='%s', usr_value_type='%s' where xml_id in (%s)" %(period_type, period, currency, value_type, scale, month, app_xml_str) 
            cur.execute(stmt)
            conn.commit()
            conn.close()      

            if lmdbPath:
                lmdbdict = {}
                for xml_id in apply_xml_ids:
                    key = table_id+'_'+self.get_quid(xml_id)
                    pkey = 'PH_MAP_'+key
                    pval = '^'.join([period_type, period, currency, scale, value_type])
                    lmdbdict[pkey] = pval
                msg = self.get_update_lmdb(lmdbPath, lmdbdict) 
        conn1.commit()
        conn1.close()
        return 'done', apply_cell_dict

    def update_user_csv_row_by_sel_rest(self, company_name, model_number, ijson, rc, xml_dict, sh_list, lmdbPath=''):
        db_file     = os.path.join('/mnt/eMB_db/', company_name, str(model_number), 'tas_tagging.db')
        conn1, cur1   = self.get_connection(db_file)
        crt_qry     = 'CREATE TABLE IF NOT EXISTS UsrTableCsvPhInfo(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_id VARCHAR(256), xml_id TEXT, period_type TEXT, period TEXT, currency TEXT, value_type TEXT, scale TEXT, month VARCHAR(20), review_flg INTEGER)'
        cur1.execute(crt_qry)
        data = []
        apply_xml_ids = []
        apply_cell_dict = {}

        period_type, period, currency, scale, value_type, month = map(str, [ijson.get('pt', ''), ijson.get('p', ''), ijson.get('c', ''), ijson.get('s', ''), ijson.get('vt', ''), ijson.get('m', '')])

        td_map = {'pt':'period_type', 'p':'period', 'c':'currency', 'vt':'value_type', 's':'scale', 'm':'month'}
        no_data_li = [td_map[x] for x in ['pt', 'p', 'c', 'vt', 's', 'm'] if not ijson.get(x, '').strip()]

        for table_id, xml_list in xml_dict.iteritems():
            db_file     = os.path.join('/mnt/eMB_db/', company_name, str(model_number), 'table_tagging_dbs', '%s.db'%table_id)
            conn, cur   = self.get_connection(db_file)
            xml_list = map(lambda x:str(x), xml_list[:])
            user_sel_flg = 0 
            if 0:
                apply_xml_ids = []
                if len(rc) == 2:
                    apply_xml_ids = xml_list[:]
                    if not apply_xml_ids:
                        stmt = "select distinct(xml_id) from TableCsvPhInfo where cell_type='GV'"
                        cur.execute(stmt)
                        apply_xml_ids += map(lambda x:str(x[0]), cur.fetchall())    
                        user_sel_flg = 1
                else:
                    xml_ids = ["'"+e+"'" for e in xml_list]
                    stmt = "select nrow, ncol from TableCsvPhInfo where xml_id in (%s)" %(', '.join(xml_ids))
                    cur.execute(stmt)
                    res = cur.fetchall()
                    for row in res:
                        sel_row, sel_col = row[0], row[1]
                        for rr in rc:
                            if rr == 'ROW':
                                stmt = "select distinct(xml_id) from TableCsvPhInfo where cell_type='GV' and nrow='%s'" %(sel_row)
                            elif rr == 'COL':
                                stmt = "select distinct(xml_id) from TableCsvPhInfo where cell_type='GV' and ncol='%s'" %(sel_col)
                            cur.execute(stmt)
                            apply_xml_ids += map(lambda x:str(x[0]), cur.fetchall())    
                ################################################################
                if xml_list:
                    apply_xml_ids = [x for x in list(set(apply_xml_ids)) if x in xml_list]
                else:
                    apply_xml_ids = list(set(apply_xml_ids))
                ################################################################   
            else:
                apply_xml_ids = xml_list[:]
            ########################################################################
            app_xml_str = ', '.join(['"'+e+'"' for e in apply_xml_ids]) 
            stmt = "select org_row, org_col, nrow, ncol, cell_type, txt, xml_id, period_type, period, currency, value_type, scale, usr_value_type from TableCsvPhInfo where xml_id in (%s)" %(app_xml_str)
            cur.execute(stmt)
            res = cur.fetchall()
            mms = ['pt', 'p', 'c', 'vt', 's', 'm']
            sel_poss = []
            for x in sh_list:
                sel_poss.append((mms.index(x), str(ijson[x])))
            ################################################   
            new_ph_csv_data = [] 
            review_flg = 1
            xml_map_dict = {}
            for row in res:
                org_row, org_col, nrow, ncol, cell_type, txt, cell_ref, speriod_type, speriod, scurrency, svalue_type, sscale, smonth = row #map(str, row[:])
                mm = [speriod_type, speriod, scurrency, svalue_type, sscale, smonth, review_flg]   
                for (p, cc) in sel_poss:
                    mm[p] = cc
                ttup = tuple(mm[:-1])
                #review_flg = self.validate_cell_ph_info(ttup)
                mm[-1] = review_flg
                apply_cell_dict[cell_ref] = review_flg
                dtup = tuple([table_id, cell_ref] + mm[:]) 
                data.append(dtup)
                xml_map_dict[cell_ref] = map(str, [mm[0], mm[1], mm[2], mm[4], mm[3]])
                new_ph_csv_data.append((org_row, org_col, nrow, ncol, cell_type, txt, cell_ref, mm[0], '', mm[1], '', mm[2], '', mm[3], mm[5], mm[4], ''))
            ####### ADD TO USER DATA ###############################################    
            table_name = 'UsrTableCsvPhInfo'
            stmt = "delete from UsrTableCsvPhInfo where xml_id in (%s) and table_id='%s'" %(app_xml_str, table_id)
            cur1.execute(stmt)
            #msg = self.qObj.insertIntoLite(conn1, cur1, '', table_name, column_tup, data) 
            cur1.executemany('INSERT INTO UsrTableCsvPhInfo(table_id, xml_id, period_type, period, currency, value_type, scale, month, review_flg) VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?)', data)
            #####################ADD TO SYSTEM ######################################
            new_table_name = 'TableCsvPhInfo'
            stmt = "delete from TableCsvPhInfo where xml_id in (%s)" %(app_xml_str)
            cur.execute(stmt)
            cur.executemany('INSERT INTO TableCsvPhInfo(org_row, org_col, nrow, ncol, cell_type, txt, xml_id, period_type, usr_period_type, period, usr_period, currency, usr_currency, value_type, usr_value_type, scale, usr_scale) VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)', new_ph_csv_data)
            if lmdbPath:
                lmdbdict = {}
                for xml_id in apply_xml_ids:
                    key = table_id+'_'+self.get_quid(xml_id)
                    pkey = 'PH_MAP_'+ str(key)
                    period_type, period, currency, scale, value_type = xml_map_dict.get(xml_id, ['', '', '', '', ''])
                    pval = '^'.join([period_type, period, currency, scale, value_type])
                    lmdbdict[pkey] = pval
                #print lmdbdict
                msg = self.get_update_lmdb(lmdbPath, lmdbdict)
            conn.commit()
            conn.close()       
        conn1.commit()
        conn1.close()
        msg = 'done'
        if not xml_map_dict:
            msg = 'selected references are not exists Re-Run PHCSV'
        return msg, apply_cell_dict

    ## NEW ARUN
    def save_phcsv_rest_data(self, ijson):
        company_name      = ijson["company_name"]
        model_number      = ijson["model_number"]
        project_id        = ijson["project_id"]
        deal_id           = ijson["deal_id"]
        company_id        = '%s_%s'%(project_id, deal_id)
       
        lmdbPath  = "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)

        ph_comp_flg = int(ijson.get('ph_comp', '0'))
        self.ph_comp_flg = ph_comp_flg
 
        row = ijson['row']
        rc = ijson.get('rc', [])
        xml_dict = ijson.get('xml_dict', [])
        sh_list = ijson.get('sh_list', [])
        if not sh_list: 
            msg, apply_cell_dict = self.update_user_csv_row_rest(company_name, model_number, row, rc, xml_dict, project_id, deal_id, lmdbPath)
        else:
            msg, apply_cell_dict = self.update_user_csv_row_by_sel_rest(company_name, model_number, row, rc, xml_dict, sh_list, lmdbPath)
        for x, x1 in apply_cell_dict.items():
            if int(x1) == 0:
                apply_cell_dict[x] = 2
            else:
                apply_cell_dict[x] = 1
        return json.dumps([{'message':msg, 'cdict':apply_cell_dict}])

    def save_label_change_data(self, ijson):
        company_name      = ijson["company_name"]
        model_number      = ijson["model_number"]
        project_id        = ijson["project_id"]
        deal_id           = ijson["deal_id"]
        company_id        = '%s_%s'%(project_id, deal_id)
        
        db_path      = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn         = sqlite3.connect(db_path)
        cur          = conn.cursor()

        table_name   = 'label_changing_map'
        sql = "CREATE TABLE IF NOT EXISTS label_changing_map(row_id INTEGER PRIMARY KEY AUTOINCREMENT, taxo_id VARCHAR(100), table_type VARCHAR(100), table_id VARCHAR(100), xml_id TEXT, parent_xml_id TEXT, label TEXT, user_name TEXT, datetime TEXT);"
        cur.execute(sql)
        conn.commit()
        
        taxo_id     = ijson['t_id']
        data_dict   = ijson['data']
        table_type  = ijson['table_type']
        label       = ijson['text']
        del_row     = []
        i_row       = []
        dtime   = str(datetime.datetime.now()).split('.')[0]
        for table_id, xml_d in data_dict.items():
            for xml_id, parent_xml_id in xml_d.items():
                del_row.append((taxo_id, table_type, table_id, xml_id, parent_xml_id))
                i_row.append((label, taxo_id, table_type, table_id, xml_id, parent_xml_id, ijson['user'], dtime))
        cur.executemany('delete from label_changing_map where taxo_id = ? and table_type = ? and table_id = ? and xml_id = ? and parent_xml_id = ?', del_row)
        cur.executemany("insert into label_changing_map (label, taxo_id, table_type, table_id, xml_id, parent_xml_id, user_name, datetime) values(?, ?, ?, ?, ?,?, ?, ?)", i_row)
        conn.commit()
        conn.close()
        return [{'message':'done'}]

    def read_populate_status(self, ijson):
        import random
        db_path      = '/mnt/eMB_db/deal_wise_update_doc_population.db'
        conn         = sqlite3.connect(db_path)
        cur          = conn.cursor()
        sql         = "select deal_id, stage1, stage2, stage3, stage4, stage5, stage6, stage7,stage8,stage9,stage10 from deal_wise_update_doc_population" 
        try:
            cur.execute(sql)
            res = cur.fetchall()
        except:
            res = []
        conn.close()
        ijson   = self.read_company_info({})
        data        = []
        phs         = [
                        ]
        res         = []
        for cid, info in ijson.items():
                deal_id = cid
                stages  = ['N']*10
                for i in range(len(stages)):
                    rnd     = random.sample(range(0, 10), 3)
                    stages[rnd[0]] = 'N'        
                    stages[rnd[1]] = 'P'        
                    stages[rnd[2]] = 'Y'        
                res.append([deal_id]+stages)
                
        for rr in res:
            deal_id, stage1, stage2, stage3, stage4, stage5, stage6, stage7,stage8,stage9,stage10   = rr
            info    = ijson[int(deal_id)]
            dd  = {'name':info['org_company_name'], 'id':str(deal_id)}
            for key, stages in [('html', [stage1, stage2, stage3]), ('phcsv', [stage4, stage5, stage6]), ('triplet', [stage7,stage8,stage9,stage10])]:
                sts_d   = {}
                for ii, stage in enumerate(stages):
                    sts_d.setdefault(stage, {})[ii+1]   = 1
                done        = len(sts_d.get('Y', {}).keys())
                process     = len(sts_d.get('P', {}).keys())
                not_done    = len(sts_d.get('N', {}).keys())
                if done == len(stages): 
                    dd[key]  = {'sts':'Y', 'v':'Ok'}
                elif not_done == len(stages):
                    dd[key]  = {'sts':'N'}
                elif 'P' in sts_d:
                    dd[key]  = {'sts':'P', 'v':str(done)+' / '+str(len(stages))}
                else:
                    dd[key]  = {'sts':'N'}
            data.append(dd)
        phs = [
                {'g':"Populate","k":"html", 'n':'HTML'},
                {'g':"Populate","k":"phcsv", 'n':'PH-CSV'},
                {'g':"Populate","k":"triplet", 'n':'Triplet'},
            ]
        res = [{'data':data, 'phs':phs, 'message':'done'}]
        return res
            
        
        
    def cpread_doc_data_txt(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        txt_path =  "%s/%s/%s/1_1/21/sdata/doc_map.txt"%(self.doc_path, project_id, deal_id)
        abs_txt_path = os.path.join('/var/www/html/DB_Model/', company_name)
        if not os.path.exists(abs_txt_path):
            os.system('mkdir -p %s'%abs_txt_path) 
        abs_txt_path = abs_txt_path+'/doc_map.txt'
        wf = open(abs_txt_path, 'w')
        f = open(txt_path, 'r')
        file_data = f.readlines()
        for idx, line in enumerate(file_data):
            line_lst = line.strip().split('\t')
            if idx == 0:
                dt = line_lst + ['Absolute Path']
                add_abs_path_heading = '\t'.join(dt)
                wf.write(add_abs_path_heading + '\n')
            elif idx != 0:
                get_doc = line_lst[0]
                path_abs = "/var/www/html/TASFundamentalsV2/tasfms/data/output/%s_common/data/%s/output/%s/"%(project_id, deal_id, get_doc)
                dt = line_lst + [path_abs]
                add_abs_path = '\t'.join(dt)
                wf.write(add_abs_path + '\n')
        f.close()
        wf.close() 
        return 'done'

    def save_taxo_builder_frm_TEST(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type      = ijson['table_type']
        db_file         = '/mnt/eMB_db/%s/%s/taxo_data_builder.db'%(company_name, 'TEST')
        db_file_m       = db_file
        conn, cur       = conn_obj.sqlite_connection(db_file)
        sql             = "select taxo_id, order_id, table_type, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label, gcom, ngcom, isvisible, vgh_text, doc_id, prev_id, xml_id, user_name, datetime from mt_data_builder where table_type='%s'"%(table_type)
        cur.execute(sql)
        i_ar         = cur.fetchall()
        conn.close()
        db_file         = '/mnt/eMB_db/%s/%s/taxo_data_builder.db'%(company_name, model_number)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        tmp_i_arr       = []
        with conn:
            sql = "select seq from sqlite_sequence WHERE name = 'mt_data_builder'"
            cur.execute(sql)
            r       = cur.fetchone()
            if not r:
                g_id    = 1
            else:
                g_id    = int(r[0])+2
            sql     = "select max(taxo_id) from mt_data_builder" 
            cur.execute(sql)
            r       = cur.fetchone()
            try:
                tg_id    = int(r[0])+1
            except:
                tg_id    = 1
            g_id    = max(g_id, tg_id)
            prev_taxo_map   = {}
            for rr in i_ar:
                if int(rr[15]) == -10000:
                    rr  = list(rr)
                    if rr[0] in prev_taxo_map:
                        rr[0]   = prev_taxo_map[rr[0]]
                    else:
                        prev_taxo_map[rr[0]] = g_id
                        rr[0]   = g_id
                        g_id    += 1
                tmp_i_arr.append(tuple(rr))
        i_ar    = tmp_i_arr[:]
        all_table_types = {}
        all_table_types['"'+table_type+'"'] = 1
        sql = "delete from mt_data_builder where table_type in (%s)"%(','.join(all_table_types.keys()))
        cur.execute(sql)
        #print sql
        cur.executemany("insert into mt_data_builder(taxo_id, order_id, table_type, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label, gcom, ngcom, isvisible, vgh_text, doc_id, prev_id, xml_id, user_name, datetime)values(?,?, ?, ?, ?, ?, ?,?, ?, ?, ?, ?, ?,?, ?, ?, ?, ?, ?)", i_ar)
        conn.commit()
        conn.close()
        #os.system("rm -rf "+db_file_m)
        import pyapi
        pyapi_obj   = pyapi.PYAPI()
        ijson_c     = copy.deepcopy(ijson)
        if 'dtime' in ijson_c:# and (not n_taxos):
            del ijson_c['dtime']
        ijson_c['table_type'] = table_type
        ijson_c['update_db'] = 'Y'
        ijson_c['taxo_flg'] = 1
        #print ijson_c
        pyapi_obj.re_order_seq(ijson_c)

    def print_table_rc_string(self, ijson):
        company_name, model_number, deal_id, project_id, company_id = self.parse_ijson(ijson)
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn         = env.begin()

        final_d     = {}
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, [])    
        for table_id in ijson['table_ids']:
            if not table_id:continue
            table_d         = {}
            disp_rows       = {}
            max_col         = 0
            min_col         = ''
            consider_row    = {}
            rowspan = {}
            consider_row    = {}
            table_id            = str(table_id)
            doc_id              = doc_m_d[table_id]
            tid             = table_id
            cold            = {}
            for l in ijson.get('SHTYPES', ['GH', 'HGH', 'GV', 'VGH']):
                l = str(l)
                ids = txn_m.get(l+'_'+str(tid))
                if not ids:continue
                for c_id in ids.split('#'):
                    table_id, r, c  = c_id.split('_')
                    #if r not in consider_row:continue
                    r   = int(r)
                    c   = int(c)
                    cold[c]            = 1
                    t   = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                    t   = self.convert_html_entity(t)
                    x   = txn_m.get('XMLID_'+c_id)
                    rs  = int(txn_m.get('rowspan_'+c_id))
                    cs  = txn_m.get('colspan_'+c_id)
                    key         = table_id+'_'+self.get_quid(x)
                    ph_map      = txn.get('PH_MAP_'+str(key))
                    if ph_map:
                        period_type, period, currency, scale, value_type    = ph_map.split('^')
                    else:
                        period_type, period, currency, scale, value_type   = '', '', '', '', ''
                    tbox    = eval(txn_m.get('BBOX_'+c_id, '[]')) #self.get_bbox_frm_xml(txn1, table_id, x)
                    dd  = {'t_l':t, 'x':x, 'colspan':cs, 'rowspan':rs, 'sh':l, 'c':c, 'bbox':tbox, 'phcsv':{'p':period, 'pt':period_type, 's':scale, 'c':currency, 'vt':value_type}, 'd':doc_id} #, 'd':doc_m_d[tid]}
                    table_d.setdefault(r, {})[c]    = dd
            rows    = table_d.keys()
            rows.sort()
            final_ar    = []
            for row in rows:
                cols    = table_d[row].keys()
                cols.sort()
                cols    = map(lambda x:table_d[row][x], cols)
                final_ar.append({'r':row, 'cols':cols, 'cr':'Y' if row in consider_row else ''})
            if final_ar:
                final_ar[0]['num_c']    = range(len(cold.keys()))
            final_d[table_id]     = final_ar
        res = [{'message':'done','data':final_d}]
        return res

    def save_user_selected_table_column(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        user_name       = ijson["user"] 
        save_data       = ijson["save_data"]
        table_type      = ijson["table_type"]
        #sql = "CREATE TABLE IF NOT EXISTS f_vgh_group_info(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_type TEXT, group_txt TEXT, table_str TEXT, user_name VARCHAR(100), datetime TEXT);"
        #txn_m.get('COLTEXT_'+table_id+'_'+colid)
        #txn_m.get('COLXMLID_'+table_id+'_'+colid)
        comp_db_path = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn  = sqlite3.connect(comp_db_path)
        cur   = conn.cursor()
        crt_qry = "CREATE TABLE IF NOT EXISTS f_vgh_group_info(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_type TEXT, group_txt TEXT, table_str TEXT, user_name VARCHAR(100), datetime TEXT);"
        cur.execute(crt_qry)
        
        for row_dct in  save_data:
            row_id, group_txt, table_str = row_dct["rid"], row_dct["n"], '#'.join(row_dct["table_info"].keys())
            entry_time = str(datetime.datetime.now())
            if not row_id or row_id == 'new':
                cur.execute('INSERT INTO f_vgh_group_info(table_type, group_txt, table_str, user_name, datetime) VALUES("%s", "%s", "%s", "%s", "%s")'%(table_type, group_txt, table_str, user_name, entry_time))
                conn.commit()
            elif row_id:
                row_id = int(row_id)
                cur.execute('UPDATE f_vgh_group_info SET table_str="%s", group_txt="%s" WHERE row_id=%d'%(table_str, group_txt,row_id))
                conn.commit()
        conn.close()
        res_lst = self.read_user_saved_vgh_data(ijson)[0]['data'] 
        return [{'message':'done', 'data':res_lst}]
         
    def read_user_saved_vgh_data(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type      = ijson["table_type"]
        comp_db_path = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn  = sqlite3.connect(comp_db_path)
        cur   = conn.cursor()
        crt_qry = "CREATE TABLE IF NOT EXISTS f_vgh_group_info(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_type TEXT, group_txt TEXT, table_str TEXT, user_name VARCHAR(100), datetime TEXT);"
        cur.execute(crt_qry)
         
        read_qry = 'SELECT row_id, group_txt, table_str FROM f_vgh_group_info WHERE table_type="%s"'%(table_type)
        cur.execute(read_qry)
        table_data = cur.fetchall()
        conn.close()
        #txn_m.get('COLTEXT_'+table_id+'_'+colid)
        #txn_m.get('COLXMLID_'+table_id+'_'+colid)
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, table_type)    
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin() 

        lmdb_path   = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        env1        = lmdb.open(lmdb_path, readonly=True)
        txn1        = env1.begin()
        
        res_lst = []
        table_grp = {}
        for sn, row in enumerate(table_data):
            row_id, group_txt, table_str = row[:]
            group_dct = {'n':group_txt, 'txt_info':[], 'sn':sn, 'rid':int(row_id)}
            vgh_dct = {}
            for table_col in table_str.split('#'):
                if not table_col:continue
                table_id, colid = map(str, table_col.split('-'))
                if not doc_m_d.get(table_id, ''):continue
                if not txn_m.get('COLTEXT_'+table_id+'_'+colid):continue
                vgh_txt = binascii.a2b_hex(txn_m.get('COLTEXT_'+table_id+'_'+colid))
                table_vgh_xml = txn_m.get('COLXMLID_'+table_id+'_'+colid)
                bbox    = self.get_bbox_frm_xml(txn1, table_id, table_vgh_xml)
                vgh_dct.setdefault(vgh_txt, []).append({'x':table_vgh_xml.split(':@:'), 't':table_col, 'c':table_col, 'bbox':bbox, 'd':doc_m_d[table_id]})
                table_grp.setdefault(table_col, {})[table_col] = 1 
            for vgh, tab_xml_lst in vgh_dct.iteritems():
                dd  = {'txt':vgh, 'table_ids':tab_xml_lst}
                dd['d'] = tab_xml_lst[0]['d']
                dd['bbox'] = tab_xml_lst[0]['bbox']
                dd['t'] = tab_xml_lst[0]['t'].split('-')[0]
                dd['x'] =  tab_xml_lst[0]['x']
                group_dct['txt_info'].append(dd)
                dd['max_len'] = len(tab_xml_lst)
            res_lst.append(group_dct)
        if not res_lst:
            res_lst = [{'n': '', 'txt_info': [], 'new_group': 'Y', 'rid': 'new', 'txt_update': 'Y'}]
        return [{'message':'done', 'data':res_lst}]

    def read_user_saved_vgh_data_new(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type      = ijson["table_type"]
        comp_db_path = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn  = sqlite3.connect(comp_db_path)
        cur   = conn.cursor()
        crt_qry = "CREATE TABLE IF NOT EXISTS f_vgh_group_info(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_type TEXT, group_txt TEXT, table_str TEXT, user_name VARCHAR(100), datetime TEXT);"
        cur.execute(crt_qry)
         
        read_qry = 'SELECT row_id, group_txt, table_str FROM f_vgh_group_info WHERE table_type="%s"'%(table_type)
        cur.execute(read_qry)
        table_data = cur.fetchall()
        conn.close()
        #txn_m.get('COLTEXT_'+table_id+'_'+colid)
        #txn_m.get('COLXMLID_'+table_id+'_'+colid)
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, [table_type])    
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin() 

        lmdb_path   = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        env1        = lmdb.open(lmdb_path, readonly=True)
        txn1        = env1.begin()
        res_lst = []
        table_grp = {}
        for sn, row in enumerate(table_data):
            row_id, group_txt, table_str = row[:]
            row_id = str(row_id)
            for table_col in table_str.split('#'):
                if not table_col.strip():continue
                table_id, colid = map(str, table_col.split('-'))
                coltxt  = txn_m.get('COLTEXT_'+table_id+'_'+str(0))
                table_vch    = ''
                if coltxt:
                    table_vch =   binascii.a2b_hex(coltxt)#Aniket - VCH Search
                if table_id not in m_tables:continue
                if txn_m.get('COLTEXT_'+table_id+'_'+colid) == None:#continue
                    vgh_txt = 'No VGH for Column'
                    table_vgh_xml = 'No XML'
                    bbox = []

                if txn_m.get('COLTEXT_'+table_id+'_'+colid) != None:#continue
                    vgh_txt = binascii.a2b_hex(txn_m.get('COLTEXT_'+table_id+'_'+colid))
                    table_vgh_xml = txn_m.get('COLXMLID_'+table_id+'_'+colid)
                    bbox    = self.get_bbox_frm_xml(txn1, table_id, table_vgh_xml)
                    gh_txt = binascii.a2b_hex(txn_m.get('GHTEXT_'+table_id)) #Aniket - to add GH search
                    table_col = '-'.join([table_id, colid])
                    if  company_id in ['20_81', '20_25']: # ADD GH/First VGH 
                        vgh_txt = gh_txt+' - '+table_vch+' - '+vgh_txt
                table_grp.setdefault(table_col, set()).add(row_id)
        vgh_lst, get_max, index_d = self.deal_vgh_data(doc_m_d, txn_m, txn1, m_tables.keys(), table_grp, company_id, ijson)
        for sn, row in enumerate(table_data):
            row_id, group_txt, table_str = row[:]
            row_id = str(row_id)
            group_dct = {'grp':group_txt, 'txt_info':[], 'sn':sn, 'grpid':str(row_id), 'vids':{}, 'table_ids':{}}
            vgh_dct = {}
            for table_col in table_str.split('#'):
                if not table_col:continue
                table_id, colid = map(str, table_col.split('-'))
                if table_id not in m_tables:continue
                coltxt  = txn_m.get('COLTEXT_'+table_id+'_'+str(0))
                table_vch    = ''
                if coltxt:
                    table_vch =   binascii.a2b_hex(coltxt)#Aniket - VCH Search
                if txn_m.get('COLTEXT_'+table_id+'_'+colid) == None:#continue
                    vgh_txt = 'No VGH for Column'
                    table_vgh_xml = 'No XML'
                    bbox = []
                if txn_m.get('COLTEXT_'+table_id+'_'+colid) != None:#continue
                    vgh_txt = binascii.a2b_hex(txn_m.get('COLTEXT_'+table_id+'_'+colid))
                    table_vgh_xml = txn_m.get('COLXMLID_'+table_id+'_'+colid)
                    bbox    = self.get_bbox_frm_xml(txn1, table_id, table_vgh_xml)
                    gh_txt = binascii.a2b_hex(txn_m.get('GHTEXT_'+table_id)) #Aniket - to add GH search
                    table_col = '-'.join([table_id, colid])
                    if  company_id in ['20_81', '20_25']: # ADD GH/First VGH 
                        vgh_txt = gh_txt+' - '+table_vch+' - '+vgh_txt
                vid     = self.get_quid(vgh_txt)
                if vid not in index_d:continue
                group_dct['vids'][index_d[vid]] = vid
                group_dct['table_ids'][table_id+'-'+colid]  =   1
            res_lst.append(group_dct)
        if not res_lst:
            #res_lst = [{'n': '', 'txt_info': [], 'new_group': 'Y', 'rid': 'new', 'txt_update': 'Y'}]
            res_lst = [{'grp':'', 'vids':{}, 'grpid':'new'}]
        return [{'message':'done', 'groups':res_lst, 'data':vgh_lst, 'max_tlen':get_max}]

    def deal_vgh_data(self, doc_m_d, txn_m, txn1, all_tables, table_grp, company_id, ijson):
        if not all_tables:
            return []
        index_d = {}
        res_dct_vgh = {}
        table_str_dict = {}
        g_txt_data = {}
        done_dict = {}
        for row in all_tables:
            table_id = row[:]
            doc_id  = doc_m_d.get(table_id, '')
            if ijson.get('grp_doc_ids', {}) and str(doc_id) not in ijson['grp_doc_ids']:continue
            get_all_cids = txn_m.get('COLS_'+str(table_id))
            #print get_all_cids, table_id
            if not doc_m_d.get(table_id, ''):continue
            if get_all_cids == None:continue
            coltxt  = txn_m.get('COLTEXT_'+table_id+'_'+str(0))
            table_vch    = ''
            if coltxt:
                table_vch =   binascii.a2b_hex(coltxt)#Aniket - VCH Search
            for colid in get_all_cids.split('#'):
                #print colid, table_id
                if txn_m.get('COLTEXT_'+table_id+'_'+colid) == None:#continue
                    vgh_txt = 'No VGH for Column' 
                    table_vgh_xml = 'No XML'
                    bbox          = []
                    table_col = '-'.join([table_id, colid])
                    
                #print '>>>', txn_m.get('COLTEXT_'+table_id+'_'+colid)
                #print 
                if txn_m.get('COLTEXT_'+table_id+'_'+colid) != None:
                    vgh_txt = binascii.a2b_hex(txn_m.get('COLTEXT_'+table_id+'_'+colid))
                    table_vgh_xml = txn_m.get('COLXMLID_'+table_id+'_'+colid)
                    bbox    = self.get_bbox_frm_xml(txn1, table_id, table_vgh_xml)
                    gh_txt = binascii.a2b_hex(txn_m.get('GHTEXT_'+table_id)) #Aniket - to add GH search
                    table_col = '-'.join([table_id, colid])
                    if  company_id in ['20_81', '20_25']: # ADD GH/First VGH 
                        vgh_txt = gh_txt+' - '+table_vch+' - '+vgh_txt
                #res_dct_vgh.setdefault(vgh_txt, []).append({'x':table_vgh_xml, 't':table_col, 'c':table_col, 'bbox':bbox, 'd':doc_m_d.get(table_id, '')})
                res_dct_vgh.setdefault(vgh_txt, []).append({'x':table_vgh_xml, 't':table_col, 'c':table_col, 'bbox':bbox, 'd':doc_m_d.get(table_id, '')}) #Aniket - GH & VCH search
                table_str_dict.setdefault(vgh_txt, set()).add(table_col)
                g_txt_data.setdefault(vgh_txt, set()).update(table_grp.get(table_col, set()))
                #print '>>>>>', table_col 
                if table_col in table_grp:
                    get_table_col_set = table_grp[table_col]
                    done_dict[table_col]= dict.fromkeys(get_table_col_set, 1)
        #print '>>>>>>', res_dct_vgh
        #print '>>>>', done_dict
        res_lst = []
        max_len = 0
        i = 0
        for vt, tab_xml_lst in  res_dct_vgh.iteritems():
            dd  = {'txt':vt, 'table_ids':tab_xml_lst, 'table_str':', '.join(table_str_dict[vt]), 'g':list(g_txt_data[vt])} 
            dd['d'] = tab_xml_lst[0]['d']
            dd['bbox'] = tab_xml_lst[0]['bbox']
            dd['t'] = tab_xml_lst[0]['t'].split('-')[0]
            dd['x'] =  tab_xml_lst[0]['x']
            dd['i'] = i
            i += 1
            hashed = self.get_quid(vt)
            dd['v_id'] = hashed
            max_len = max(max_len, len(tab_xml_lst))
            dd_dict = {}
            for tb in table_str_dict[vt]:
                tb_dict = done_dict.get(tb, {})
                if not tb_dict:continue
                dd_dict[tb] = tb_dict 
            dd['done_tables'] = dd_dict
            index_d[hashed] = len(res_lst)
            res_lst.append(dd)
            #print 'dd>>>>>>>>', dd
            #print 'txt<<<<<', vt, hashed
            #print
        #print '>>>>>>', res_lst
        #print '<<<<<<', max_len
        #print '########', index_d
        return res_lst, max_len, index_d
    def delete_vgh_info_new(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        grpid           = int(ijson['grpid'])          
        #hashed         = map(str, ijson["rid"])
        comp_db_path = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn  = sqlite3.connect(comp_db_path)
        cur = conn.cursor()
        del_qry = 'DELETE FROM f_vgh_group_info WHERE row_id=%d'%(grpid)
        cur.execute(del_qry)
        conn.commit()
        conn.close()
        res_dat = self.read_user_saved_vgh_data_new(ijson)[0] 
        groups = res_dat['groups']
        dt = res_dat['data']
        mx_ln = res_dat['max_tlen'] 
        return [{'message':'done', 'data':dt, 'groups':groups, 'max_tlen':mx_ln}]

    def cpread_user_saved_vgh_data_new(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type      = ijson["table_type"]
        comp_db_path = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn  = sqlite3.connect(comp_db_path)
        cur   = conn.cursor()
        crt_qry = "CREATE TABLE IF NOT EXISTS f_vgh_group_info(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_type TEXT, group_txt TEXT, table_str TEXT, user_name VARCHAR(100), datetime TEXT);"
        cur.execute(crt_qry)
         
        read_qry = 'SELECT row_id, group_txt, table_str FROM f_vgh_group_info WHERE table_type="%s"'%(table_type)
        cur.execute(read_qry)
        table_data = cur.fetchall()
        conn.close()
        #txn_m.get('COLTEXT_'+table_id+'_'+colid)
        #txn_m.get('COLXMLID_'+table_id+'_'+colid)
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, table_type)    
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin() 

        lmdb_path   = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        env1        = lmdb.open(lmdb_path, readonly=True)
        txn1        = env1.begin()
        
        res_lst = []
        table_grp = {}
        for sn, row in enumerate(table_data):
            row_id, group_txt, table_str = row[:]
            group_dct = {'n':group_txt, 'txt_info':[], 'sn':sn, 'rid':int(row_id)}
            vgh_dct = {}
            for table_col in table_str.split('#'):
                table_id, colid = map(str, table_col.split('-'))
                vgh_txt = binascii.a2b_hex(txn_m.get('COLTEXT_'+table_id+'_'+colid))
                table_vgh_xml = txn_m.get('COLXMLID_'+table_id+'_'+colid)
                bbox    = self.get_bbox_frm_xml(txn1, table_id, table_vgh_xml)
                vgh_dct.setdefault(vgh_txt, []).append({'x':table_vgh_xml.split(':@:'), 't':table_col, 'c':table_col, 'bbox':bbox, 'd':doc_m_d[table_id]})
                table_grp.setdefault(table_col, {})[table_col] = 1 
            for vgh, tab_xml_lst in vgh_dct.iteritems():
                dd  = {'txt':vgh, 'table_ids':tab_xml_lst}
                dd['d'] = tab_xml_lst[0]['d']
                dd['bbox'] = tab_xml_lst[0]['bbox']
                dd['t'] = tab_xml_lst[0]['t'].split('-')[0]
                group_dct['txt_info'].append(dd)
                dd['max_len'] = len(tab_xml_lst)
            res_lst.append(group_dct)
        if not res_lst:
            res_lst = [{'n': '', 'txt_info': [], 'new_group': 'Y', 'rid': 'new', 'txt_update': 'Y'}]
        vgh_lst, get_max = self.deal_vgh_data(doc_m_d, txn_m, txn1, m_tables.keys(), ijson)
        return [{'message':'done', 'data':res_lst, 'vgh_data':vgh_lst, 'max_tlen':get_max}]

    def cpdeal_vgh_data(self, doc_m_d, txn_m, txn1, all_tables):
        if not all_tables:
            return []
        res_dct_vgh = {}
        for row in all_tables:
            table_id = row[:]
            get_all_cids = txn_m.get('COLS_'+str(table_id))
            if get_all_cids == None:continue
            for colid in get_all_cids.split('#'): 
                vgh_txt = binascii.a2b_hex(txn_m.get('COLTEXT_'+table_id+'_'+colid))
                table_vgh_xml = txn_m.get('COLXMLID_'+table_id+'_'+colid)
                bbox    = self.get_bbox_frm_xml(txn1, table_id, table_vgh_xml)
                table_col = '-'.join([table_id, colid])
                res_dct_vgh.setdefault(vgh_txt, []).append({'x':table_vgh_xml, 't':table_col, 'c':table_col, 'bbox':bbox, 'd':doc_m_d.get(table_id, '')})
        res_lst = []
        max_len = 0
        i = 0
        for vt, tab_xml_lst in  res_dct_vgh.iteritems():
            dd  = {'txt':vt, 'table_ids':tab_xml_lst} 
            dd['d'] = tab_xml_lst[0]['d']
            dd['bbox'] = tab_xml_lst[0]['bbox']
            dd['t'] = tab_xml_lst[0]['t'].split('-')[0]
            dd['x'] = tab_xml_lst[0]['x']
            dd['i'] = i
            i += 1
            v_id = self.quid(hashlib.md5(vt).hexdigest())
            dd['v_id'] = v_id
            max_len = max(max_len, len(tab_xml_lst))
            res_lst.append(dd)
        return res_lst, max_len

    def delete_vgh_info(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        row_ids         = map(int, ijson["rid"])
        comp_db_path = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn  = sqlite3.connect(comp_db_path)
        cur = conn.cursor()
        for row_id in row_ids:
            del_qry = 'DELETE FROM f_vgh_group_info WHERE row_id=%d'%(row_id)
            cur.execute(del_qry)
        conn.commit()
        conn.close()
        res_lst = self.read_user_saved_vgh_data(ijson)[0]['data'] 
        return [{'message':'done', 'data':res_lst}]
         

    def no_number_text(self, vgh_below_gvs):
        ar = [] 
        import common.unicodeconvert as unicodeconvert
        for vgh in vgh_below_gvs:
            txt = vgh[2].strip()
            #if not txt: continue
            pattern = unicodeconvert.unicodeconvert().convert(txt)  
            #print [ txt ], ' == ', [ pattern ] 
            num_flg = 0
            for pelm in pattern:
                if pelm in '0123456789%$':
                   num_flg = 1
                   break
            if num_flg==0:
               ar.append(vgh)
        return ar           


    def no_number_text(self, vgh_below_gvs):
        return  vgh_below_gvs
        ar = [] 
        import common.unicodeconvert as unicodeconvert
        for vgh in vgh_below_gvs:
            txt = vgh[2].strip()
            #if not txt: continue
            pattern = unicodeconvert.unicodeconvert().convert(txt)  
            #print [ txt ], ' == ', [ pattern ] 
            num_flg = 0
            for pelm in pattern:
                if pelm in '0123456789%$':
                   num_flg = 1
                   break
            if num_flg==0:
               ar.append(vgh)
        return ar
    
    def get_bds_comp_table_ids(self, company_name, model_number):
        db_path = '/mnt/eMB_db/%s/%s/tas_company.db'%(company_name, model_number)
        conn = sqlite3.connect(db_path)
        cur  = conn.cursor()
        read_qry = 'SELECT row_id, doc_id FROM table_dict_phcsv_info;'
        cur.execute(read_qry)
        td = cur.fetchall()
        conn.close()
        all_tables_dict = {}
        for row in td:
            table_id, doc_id = map(str, row)
            all_tables_dict[table_id] = doc_id
        return all_tables_dict
    
    def call_index_tag(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        #print lmdb_path1
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        lmdb_path   = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        env1        = lmdb.open(lmdb_path, readonly=True)
        txn1        = env1.begin()

        #m_tables, rev_m_tables, doc_m_d, table_type_md = self.get_main_table_info(company_name, model_number) 
        #res = self.generate_indextag_data(company_name, model_number, company_id, m_tables, txn_m, doc_m_d, txn1)
        if project_id != '50':
            import model_view.cp_company_docTablePh_details  as py
            cObj = py.Company_docTablePh_details()
            all_tables = cObj.getDistinct_tableIds(company_id)
            table_doc_map = cObj.get_docId_passing_tableId(company_id)
        elif project_id == '50':
            table_doc_map  = self.get_bds_comp_table_ids(company_name, model_number)
            all_tables = table_doc_map.keys()
        #print  cObj.get_tableList_passing_doc(company_id)
        res = self.generate_indextag_data(company_name, model_number, company_id, all_tables, txn_m, table_doc_map, txn1)
        #return res
        return res
    def generate_indextag_data(self, company_name, model_number, company_id, m_tables, txn_m, doc_m_d, txn1):
        txt_rev = {}
        freq_text = {} 
        index_txt_rev = {}       
        #for indx , table_doc_id in enumerate(tab_doc_pairs[:]):
        table_ids   = m_tables[:]
        res_dct = {}  
        for ii, table_id in enumerate(table_ids):
            tid     = str(table_id)
            doc_id  = doc_m_d[table_id]
               
            get_all_cids = txn_m.get('COLS_'+str(table_id))
                  
            if get_all_cids == "":
               print 'Continue ', ii, ' / ' , len(table_ids), ' == ', table_id, ' == ', doc_id 
               continue
            elif not get_all_cids:
               print 'Muthu code not run for the table ', ii, ' / ' , len(table_ids), ' == ', table_id, ' == ', doc_id 
               #sys.exit()
               continue 
            get_table_col_vgh_txt   = []
            for colid in get_all_cids.split('#'):
                ttt = txn_m.get('COLTEXT_'+tid+'_'+colid)
                if ttt == None:
                    print tid+'_'+colid
                    print 'Muthu code not run for the table '
                    continue
                    #sys.exit()
                vgh_txt = binascii.a2b_hex(ttt)
                xml_id = txn_m.get('COLXMLID_'+tid+'_'+colid) 
                bbox    = self.get_bbox_frm_xml(txn1, table_id, xml_id)
                get_table_col_vgh_txt.append((doc_id, tid+'-'+colid, vgh_txt, 'VGH', bbox, '', '', xml_id))
            vgh_below_gvs   = get_table_col_vgh_txt[:]
            filter_vgh_below_gvs = self.no_number_text(vgh_below_gvs)
            if filter_vgh_below_gvs:
               ftxt_ar = map(lambda x:x[2], filter_vgh_below_gvs[:])
               fref_ar = map(lambda x:x[7], filter_vgh_below_gvs[:])
               hkey_ar = map(lambda x:hashlib.md5(x).hexdigest(), ftxt_ar[:])
               for i, hkey in enumerate(hkey_ar):
                    if hkey not in freq_text:
                       freq_text[hkey] = 0
                    freq_text[hkey] += 1 
                    mod_txt = binascii.b2a_hex(ftxt_ar[i])
                    txt_rev[hkey] = mod_txt
                    if hkey not in index_txt_rev:
                      index_txt_rev[hkey] = []  
                    #for x in fref_ar[i].split(':@:'):
                    #    if x:
                    #        index_txt_rev[hkey].append((table_id, doc_id, x))
                    index_txt_rev[hkey].append((table_id, doc_id, fref_ar[i]))
                    tr      = filter_vgh_below_gvs[i]
                    txt     = tr[2]
                    xm      = tr[7]
                    tab_col = tr[1]
                    res_dct.setdefault(hkey, {'txt':txt, 'tinfo':[]})['tinfo'].append({'t':tab_col, 'd':doc_id, 'bbox':bbox, 'x':xm})
        
        import lmdb 
        lmdb_fname = os.path.join('/mnt/eMB_db', 'index_vgh_notestag', company_id)
        cmd = 'rm -rf %s' %(lmdb_fname)
        #print cmd 
        os.system(cmd) 
        cmd = 'mkdir -p %s' %(lmdb_fname)
        #print cmd 
        os.system(cmd)
        #print 'Writing in lmdb: ', lmdb_fname 
        ########################
        env = lmdb.open(lmdb_fname, map_size=20*1000*1000*1000)
        with env.begin(write=True) as txn:
            for k, v in index_txt_rev.items():
                v = str(v)
                txn.put('INDHASH:'+k, v)
            for k, v in txt_rev.items():
                txn.put('HASH:'+k, v)
            for k, v in freq_text.items():
                txn.put('FREQ:'+k, str(v))
            all_hashkeys = freq_text.keys()
            txn.put('ALLHASHKEYS', str(all_hashkeys))
        db_file = os.path.join('/mnt/eMB_db/', company_name, str(model_number), 'tas_tagging.db')

        #with sqlite3.connect(db_file) as conn:
        if 1:   
            conn, cur   = conn_obj.sqlite_connection(db_file)
            create_query = '''CREATE TABLE IF NOT EXISTS USER_SELECTED_TAG_HASH_KEYS(hkey TEXT PRIMARY KEY);'''
            cur.execute(create_query)

            select_query = ''' SELECT hkey from USER_SELECTED_TAG_HASH_KEYS;'''
            all_selected_hkeys = {str(each[0]):1 for each in cur.execute(select_query).fetchall()}
        mx_len = 0
        res_lst = []
        sn  = 1 
        for hkey, vinfo in res_dct.items():
            vgh_txt = vinfo['txt']
            data_lst    = vinfo['tinfo']
            sf = 0
            if hkey in all_selected_hkeys:  
                sf = 1
            data_dct = {'txt':vgh_txt, 'table_ids':data_lst, 'sn':sn, 'hkey':hkey, 'sf':sf}
            sn  += 1
            data_dct['d']  = data_lst[0]['d']
            data_dct['x']  = data_lst[0]['x']
            data_dct['bbox']  = data_lst[0]['bbox']
            data_dct['t']  = data_lst[0]['t'].split('-')[0]
            mx_len = max(len(data_lst), mx_len)
            res_lst.append(data_dct)
        return [{'message':'done', 'data':res_lst, 'max_len':mx_len}]    

    def index_tags(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        inp_hashkey = str(ijson['ihk'])
        flg = int(ijson['flg'])
        msg = self.save_user_selected_hashkey(company_name, model_number, company_id, inp_hashkey, flg)
        data = []
        return [{'message': msg, 'data': data}]
    def save_user_selected_hashkey(self, company_name, model_number, company_id, inp_hashkey, flg):

        db_file = os.path.join('/mnt/eMB_db', company_name, str(model_number), 'tas_tagging.db')
        msg = 'done'

        try:

            with sqlite3.connect(db_file) as conn:

                cur = conn.cursor()
                insert_query = ''' INSERT INTO USER_SELECTED_TAG_HASH_KEYS(hkey) VALUES('%s'); '''
                delete_query = ''' DELETE FROM USER_SELECTED_TAG_HASH_KEYS WHERE hkey='%s'; '''

                if flg:
                    res = cur.execute(insert_query %(inp_hashkey))
                else:
                    res = cur.execute(delete_query %(inp_hashkey))

        except Exception as e:
            print 'Error', e
            msg = 'Error'

        finally:
            return msg

    def get_vgh_info(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        given_table_type = ijson["table_type"]
        given_rid       = ijson["rid"] 
        import model_view.gen_vgh_group_pos as py
        obj = py.PhCsv_Validata()
        res_data = obj.get_vgh_pos(company_name, model_number, company_id, given_table_type, given_rid)
        return [{'message':'done', 'data':res_data}]


    def doc_map_ph_dct(self, ijson): 
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        path    = "%s/%s/%s/1_1/21/sdata/doc_map.txt"%(self.doc_path, project_id, deal_id)
        if os.path.exists(path):
            fin = open(path, 'r')
            lines   = fin.readlines()
            fin.close()
        else:
            lines   = []
        doc_ph_map  = {}
        for line in lines[1:]:
            line    = line.split('\t')
            if len(line) < 8:continue
            line    = map(lambda x:x.strip(), line)
            ph      = line[3]+line[7]
            doc_id  = line[0]
            doc_ph_map[doc_id]   = ph
        return doc_ph_map

    def get_hgh_error_data(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type      = ijson["table_type"]
        if str(project_id) == '40':
            return [{'message':'done', 'data':{}, 'error_class':{}}]
        if ijson.get('PRINT') != 'Y':
            disableprint()
            
    
        import data_builder.db_data as py
        pObj = py.PYAPI()
        
        
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        
        lmdb_path   = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        env1        = lmdb.open(lmdb_path, readonly=True)
        txn1        = env1.begin()
        
        db_path = '/mnt/eMB_db/%s/%s/taxo_data_builder.db'%(company_name, model_number)
        conn = sqlite3.connect(db_path)
        cur  = conn.cursor()
        
        table_err_flg = self.table_report_data(company_name, model_number)

        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, [table_type]) 
       
        # "gen":"N","err_class":"Y","only_error":"Y"
 
        extra_data_ijson = {"gen":"N", "err_class":"Y", "only_error":"Y"}
        ijson.update(extra_data_ijson)
        
        doc_ph_map = self.doc_map_ph_dct(ijson)    
        err_table_dct = self.table_grp_data(doc_m_d, doc_ph_map, company_name, model_number, company_id, txn_m, [table_type], ijson, 'Y')


        read_qry = 'SELECT table_id, xml_id, c_id FROM mt_data_builder WHERE table_type="%s"'%(table_type)
        cur.execute(read_qry)
        table_data = cur.fetchall()
        
        table_xml_lst = {}
        table_row_dct = {}
        for row in table_data:
            table_id, xml_id, c_id = map(str, row[:])
            table_xml_lst.setdefault(table_id, {})
            #print
            for x in xml_id.split(':@:')[:0]:
                tk          = self.get_quid(table_id+'_'+x)
                c_id        = txn_m.get('XMLID_MAP_'+tk)
                table_xml_lst.setdefault(table_id, {})[x]  = 1
                #print '\t', (table_id, x, c_id)
                if not c_id:continue
                table_row_dct.setdefault(table_id, {})[int(c_id.split('_')[1])] = 1  
            tk   = self.get_quid(table_id+'_'+xml_id)
            c_id        = txn_m.get('XMLID_MAP_'+tk)
            #print (table_id, xml_id, c_id)
            if not c_id:continue
            table_xml_lst.setdefault(table_id, {})[xml_id]  = 1
            table_row_dct.setdefault(table_id, {})[int(c_id.split('_')[1])] = 1  
        for table_id, r_d in table_row_dct.items():
            ids = txn_m.get('GV_'+str(table_id))
            if ids:
                for c_id in ids.split('#'):
                    table_row_dct.setdefault(table_id, {})[int(c_id.split('_')[1])] = 1  
            
        ignore_col = {}
             
        table_error_dct = {}
        error_class = {}
        r_ld, rp_ld, rc_ld  = pObj.read_tinfo(txn_m, table_row_dct, ignore_col, txn1)
        for t, r_d in table_xml_lst.items():
            #print t
            if (t in err_table_dct) and (t not in table_err_flg):
                error_class[t] = 1
            ids = txn_m.get('GV_'+str(t))
            if not ids:
                #print '\nNO GV'
                table_error_dct.setdefault(t, {})['NO_GV']  = 1
            else:
                for c_id in ids.split('#'):
                    tid, rid, cid = c_id.split('_')
                    xml_id  = txn_m.get('XMLID_'+c_id)
                    if 'x-' in xml_id:continue
                    c_ar    = rp_ld.get(('ALL', tid, int(rid)), [])
                    #print '\n\t', [c_id, c_ar]
                    if not c_ar:
                        table_error_dct.setdefault(tid, {})['NO_C_AR']  = 1
                    else:
                        f   = 0
                        xml_d       = {}
                        for (c, txt, x) in c_ar:
                            xml_d[x]    = 1
                            if x in table_xml_lst.get(tid, {}):
                                f   = 1
                                break
                        #print '\n\t FOUND ', [f]
                        if (f == 0):
                            table_error_dct.setdefault(tid, {}).update(xml_d)
        #print error_class
        enableprint()
        return [{'message':'done', 'data':table_error_dct, 'error_class':error_class}]
    def table_report_data(self, company_name, model_number):
        db_path = '/mnt/eMB_db/%s/%s/company_report.db'%(company_name, model_number)
        conn  = sqlite3.connect(db_path)
        cur   = conn.cursor()
        
        read_qry = 'SELECT table_id FROM Table_Report WHERE error_accepted="Y";'
        try:
            cur.execute(read_qry)
            table_data = cur.fetchall()
        except:
            table_data  = []
        conn.close()
        table_dct = {row[0]:1 for row in table_data}
        return table_dct

    def cpget_hgh_error_data(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type      = ijson["table_type"]
        if ijson.get('PRINT') != 'Y':
            disableprint()
            
    
        import data_builder.db_data as py
        pObj = py.PYAPI()
        
        
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        lmdb_path   = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        env1        = lmdb.open(lmdb_path, readonly=True)
        txn1        = env1.begin()
        
        db_path = '/mnt/eMB_db/%s/%s/taxo_data_builder.db'%(company_name, model_number)
        conn = sqlite3.connect(db_path)
        cur  = conn.cursor()
            
        read_qry = 'SELECT table_id, xml_id, c_id FROM mt_data_builder WHERE table_type="%s"'%(table_type)
        cur.execute(read_qry)
        table_data = cur.fetchall()
        
        table_xml_lst = {}
        table_row_dct = {}
        for row in table_data:
            table_id, xml_id, c_id = map(str, row[:])
            table_xml_lst.setdefault(table_id, {})
            #print
            for x in xml_id.split(':@:')[:0]:
                tk          = self.get_quid(table_id+'_'+x)
                c_id        = txn_m.get('XMLID_MAP_'+tk)
                table_xml_lst.setdefault(table_id, {})[x]  = 1
                #print '\t', (table_id, x, c_id)
                if not c_id:continue
                table_row_dct.setdefault(table_id, {})[int(c_id.split('_')[1])] = 1  
            tk   = self.get_quid(table_id+'_'+xml_id)
            c_id        = txn_m.get('XMLID_MAP_'+tk)
            #print (table_id, xml_id, c_id)
            if not c_id:continue
            table_xml_lst.setdefault(table_id, {})[xml_id]  = 1
            table_row_dct.setdefault(table_id, {})[int(c_id.split('_')[1])] = 1  
        for table_id, r_d in table_row_dct.items():
            ids = txn_m.get('GV_'+str(table_id))
            if ids:
                for c_id in ids.split('#'):
                    table_row_dct.setdefault(table_id, {})[int(c_id.split('_')[1])] = 1  
            
        ignore_col = {}
             
        table_error_dct = {}
        r_ld, rp_ld, rc_ld  = pObj.read_tinfo(txn_m, table_row_dct, ignore_col, txn1)
        for t, r_d in table_xml_lst.items():
            ids = txn_m.get('GV_'+str(t))
            if not ids:
                #print '\nNO GV'
                table_error_dct.setdefault(t, {})['NO_GV']  = 1
            else:
                for c_id in ids.split('#'):
                    tid, rid, cid = c_id.split('_')
                    xml_id  = txn_m.get('XMLID_'+c_id)
                    if 'x-' in xml_id:continue
                    c_ar    = rp_ld.get(('ALL', tid, int(rid)), [])
                    #print '\n\t', [c_id, c_ar]
                    if not c_ar:
                        table_error_dct.setdefault(tid, {})['NO_C_AR']  = 1
                    else:
                        f   = 0
                        xml_d       = {}
                        for (c, txt, x) in c_ar:
                            xml_d[x]    = 1
                            if x in table_xml_lst.get(tid, {}):
                                f   = 1
                                break
                        #print '\n\t FOUND ', [f]
                        if (f == 0):
                            table_error_dct.setdefault(tid, {}).update(xml_d)
        enableprint()
        return [{'message':'done', 'data':table_error_dct}]
    def save_start_end(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type      = ijson["table_type"]
        group_id        = ijson.get("grpid", '')
        save_data       = ijson["save_data"]
        user_name       = ijson["user"]
        
        formula_str_lst = []
        for dt_dct in save_data:
            b = str(dt_dct["b"])
            e = str(dt_dct["e"])
            st = '_'.join(['b', b])
            ed = '_'.join(['e', e])
            c_dt = '#'.join([st, ed])
            formula_str_lst.append(c_dt)
        formula_str = '$'.join(formula_str_lst)
        
        db_path = '/mnt/eMB_db/%s/%s/taxo_data_builder.db'%(company_name, model_number)
        conn = sqlite3.connect(db_path)
        cur  = conn.cursor()
        
        create_stmt = "CREATE TABLE IF NOT EXISTS ph_derivation(row_id INTEGER PRIMARY KEY AUTOINCREMENT, formula VARCHAR(256), table_type VARCHAR(100), group_id VARCHAR(50), ph VARCHAR(10), formula_type VARCHAR(20), formula_str VARCHAR(256), user_name VARCHAR(100));"
        cur.execute(create_stmt)

        read_qry = 'SELECT formula_type, group_id, table_type FROM ph_derivation WHERE formula_type="STEND" and table_type="%s"'%(table_type) 
        cur.execute(read_qry)
        table_data = cur.fetchall()
        check_data = {row:1 for row in table_data}
        if ('STEND', group_id, table_type) not in check_data: 
            cur.execute('INSERT INTO ph_derivation(formula, table_type, group_id, ph, formula_type, formula_str, user_name) VALUES("%s", "%s", "%s", "%s", "%s", "%s", "%s")'%('', table_type, group_id, '', 'STEND', formula_str, user_name))
        else:
            cur.execute('UPDATE ph_derivation SET formula_str="%s" WHERE table_type="%s" and formula_type="%s" and group_id="%s"'%(formula_str, table_type, 'STEND', group_id))
        conn.commit()
        conn.close()
        return [{'message':'done'}]

    def read_start_end(self, ijson, from_preview=None):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type      = ijson["table_type"]
        group_id        = ijson.get("grpid", '')
        user_name       = ijson.get("user", "")
        
        db_path = '/mnt/eMB_db/%s/%s/taxo_data_builder.db'%(company_name, model_number)
        conn = sqlite3.connect(db_path)
        cur  = conn.cursor()
        
        create_stmt = "CREATE TABLE IF NOT EXISTS ph_derivation(row_id INTEGER PRIMARY KEY AUTOINCREMENT, formula VARCHAR(256), table_type VARCHAR(100), group_id VARCHAR(50), ph VARCHAR(10), formula_type VARCHAR(20), formula_str VARCHAR(256), user_name VARCHAR(100));"
        cur.execute(create_stmt)

        read_qry = 'SELECT formula_str FROM ph_derivation WHERE formula_type="STEND" and table_type="%s" and group_id="%s"'%(table_type, group_id) 
        cur.execute(read_qry)
        table_data = cur.fetchall()
        tmptable_data   = []
        for row in table_data:
            formula_str = row[0]
            if not formula_str:continue
            tmptable_data.append(row)
        if not tmptable_data and (from_preview == 'Y'):
            read_qry = 'SELECT formula_str FROM ph_derivation WHERE formula_type="STEND" and table_type="%s"'%(table_type) 
            cur.execute(read_qry)
            table_data = cur.fetchall()
        conn.close()
        res_lst = []
        for row in table_data:
            formula_str = row[0]
            if not formula_str:continue
            st_ed = formula_str.split('$')
            for se_str in st_ed:
                se = se_str.split('#')
                skv = se[0].split('_')
                s_k = skv[0]
                s_v = skv[1]
                ekv = se[1].split('_')
                e_k = ekv[0]
                e_v = ekv[1]
                dt = {s_k:s_v, e_k:e_v}
                res_lst.append(dt)
        return [{'message':'done', 'data':res_lst}]

    def bbox_xml_validation(self, table_id, txn_m, txn1, ignore_col):
        table_id    = str(table_id)
        ids         = txn_m.get('GV_'+table_id)
        if not ids:
            return [], []
        bbox_not_found  = {}
        for c_id in ids.split('#'):
            tid, r, c   = c_id.split('_')
            r       = int(r)
            c       = int(c)
            x       = txn_m.get('XMLID_'+c_id)
            if (table_id, c) in ignore_col:continue
            t       = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
            if not t.strip():continue
            bbox        = self.get_bbox_frm_xml(txn1, table_id, x)
            
            if not bbox or not bbox[0]:
                bbox_not_found[x]  = 1
        ids         = txn_m.get('HGH_'+table_id)
        if not ids:
            ids = []
        else:
            ids = ids.split('#')
        for c_id in ids:
            tid, r, c   = c_id.split('_')
            r       = int(r)
            c       = int(c)
            x       = txn_m.get('XMLID_'+c_id)
            bbox        = self.get_bbox_frm_xml(txn1, table_id, x)
            if not bbox or not bbox[0]:
                bbox_not_found[x]  = 1
        err = []
        if bbox_not_found:
            err += [{'e':'BBOX NOT AVAILABLE', 'info':[], 'xmls':bbox_not_found.keys()}]
        return err



    def update_flip_table(self, ijson):
        if ijson.get('NM') == 'Y':
            return self.update_merge_table(ijson)
            
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type      = ijson["table_type"]
        if ijson.get('PRINT') != 'Y':
            disableprint()
            
    
        import data_builder.db_data as py
        pObj = py.PYAPI()
        
        
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        lmdb_path   = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        env1        = lmdb.open(lmdb_path, readonly=True)
        txn1        = env1.begin()
        
        db_path = '/mnt/eMB_db/%s/%s/taxo_data_builder.db'%(company_name, model_number)
        conn = sqlite3.connect(db_path)
        cur  = conn.cursor()
            
        read_qry = 'SELECT table_id, xml_id, c_id FROM mt_data_builder WHERE table_type="%s" and isvisible="Y"'%(table_type)
        cur.execute(read_qry)
        table_data = cur.fetchall()
        error_d = self.get_hgh_error_data(ijson)[0]['data']
        table_xml_d = {}
        xml_split_d = {}
        for row in table_data:
            table_id, xml_id, c_id = map(str, row[:])
            tk          = self.get_quid(table_id+'_'+xml_id)
            c_id        = txn_m.get('XMLID_MAP_'+tk)
            if not c_id and txn_m.get('FLIP_'+str(table_id)) in ['Y'] and str(table_id) in error_d :
                table_xml_d.setdefault(str(table_id), {})[xml_id] = row[2]
                for x in xml_id.split(':@:'):
                    xml_split_d.setdefault((table_id, x), {})[xml_id]   = 1
        u_ar    = []
        for table_id, xml_d in table_xml_d.items():
            hgh_xmlids  = {}
            ids     = txn_m.get('HGH_'+str(table_id))
            txml_d  = {}
            for c_id in ids.split('#'):
                x   = txn_m.get('XMLID_'+c_id) 
                hgh_xmlids[x]   = c_id  
                txml_d[c_id]  = x
            for xml_id, c_id in xml_d.items():
                print
                print (table_id, xml_id, c_id)
                nxml    = txml_d[c_id]
                print '\tnxml', [nxml]
                c_xml   = sets.Set(xml_id.split(':@:')).intersection(sets.Set(nxml.split(':@:')))
                if not c_xml:
                    print 'ERROR Common XML '
                    sys.exit()
                rem_xml = sets.Set(xml_id.split(':@:')) - c_xml
                print '\tCXML ', c_xml
                r_xmls  = {}
                xml_id_sp   = xml_id.split(':@:')
                for xml in c_xml:
                    if txn_m.get('FLIP_'+str(table_id)) in ['Y']:
                        r_xmls.update(xml_split_d.get((table_id, xml), {}))
                    else:
                        for tmpx in xml_split_d.get((table_id, xml), {}).keys():
                            tmpx_sp = tmpx.split(':@:')
                            if tmpx_sp[0] == xml_id_sp[0] and tmpx_sp[1] == xml_id_sp[1]:
                                r_xmls[tmpx]    = 1
                if len(r_xmls.keys()) != 1 or r_xmls.keys()[0] != xml_id:
                    print 'Error XML ', r_xmls
                    sys.exit()
                print '\tREM ', rem_xml
                print 'FINAl ', r_xmls
                u_ar.append((nxml, table_id, xml_id))
        if u_ar and ijson.get('UPDATE') == 'Y':
            print 'UPDATE'
            cur.executemany("update mt_data_builder set xml_id=? where table_id=? and xml_id=?", u_ar)
            conn.commit()
        conn.close()

    def update_merge_table(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        table_type      = ijson["table_type"]
        if ijson.get('PRINT') != 'Y':
            disableprint()
            
    
        import data_builder.db_data as py
        pObj = py.PYAPI()
        
        
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        lmdb_path   = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        env1        = lmdb.open(lmdb_path, readonly=True)
        txn1        = env1.begin()
        
        db_path = '/mnt/eMB_db/%s/%s/taxo_data_builder.db'%(company_name, model_number)
        conn = sqlite3.connect(db_path)
        cur  = conn.cursor()
            
        read_qry = 'SELECT table_id, xml_id, c_id FROM mt_data_builder WHERE table_type="%s" and isvisible="Y"'%(table_type)
        cur.execute(read_qry)
        table_data = cur.fetchall()
        error_d = self.get_hgh_error_data(ijson)[0]['data']
        table_xml_d = {}
        xml_split_d = {}
        for row in table_data:
            table_id, xml_id, c_id = map(str, row[:])
            tk          = self.get_quid(table_id+'_'+xml_id)
            c_id        = txn_m.get('XMLID_MAP_'+tk)
            if not c_id and txn_m.get('FLIP_'+str(table_id)) in ['NM'] and str(table_id) in error_d :
                table_xml_d.setdefault(str(table_id), {})[xml_id] = row[2]
                for x in xml_id.split(':@:'):
                    xml_split_d.setdefault((table_id, x), {})[xml_id]   = 1
        u_ar    = []
        for table_id, xml_d in table_xml_d.items():
            hgh_xmlids  = {}
            ids     = txn_m.get('HGH_'+str(table_id))
            txml_d  = {}
            for c_id in ids.split('#'):
                x   = txn_m.get('XMLID_'+c_id) 
                hgh_xmlids[x]   = c_id  
                txml_d[c_id]  = x
            for xml_id, c_id in xml_d.items():
                print
                print (table_id, xml_id, c_id)
                nxml    = txml_d[c_id]
                print '\tnxml', [nxml]
                c_xml   = sets.Set(xml_id.split(':@:')).intersection(sets.Set(nxml.split(':@:')))
                if not c_xml:
                    print 'ERROR Common XML '
                    sys.exit()
                
                if xml_id.split(':@:')[0] != nxml.split(':@:')[0]:
                    print 'Error XML ', [nxml]
                    sys.exit()
                u_ar.append((nxml, table_id, xml_id))
        if u_ar and ijson.get('UPDATE') == 'Y':
            print 'UPDATE ', len(u_ar)
            for ii, rr in enumerate(u_ar):
                print 'Running ', ii,'/',len(u_ar)
                cur.executemany("update mt_data_builder set xml_id=? where table_id=? and xml_id=?", [rr])
            conn.commit()
        print 'DONE'
        conn.close()


    def clean_response(self, res):
        data    = res[0]['data']
        phs     = res[0]['phs']
        desc_ks = ['t_id', 't_l', 'f', 'th_flg']
        common  = ['d', 'x', 'bbox', 't']
        cell_ks = ['f_col', 'f_col_org', 'f_ph_str', 'fv', 'v', 'c_s']
        tmp_ar  = []
        for rr in data:
            row = {}
            for k in desc_ks+common:
                if rr.get(k):
                    row[k]  = rr[k]
                row['t_l']  = rr['t_l']
                for ph in phs:
                    if ph['k'] not in rr:continue
                    v_d = {}
                    for k in cell_ks+common:
                        if rr[ph['k']].get(k):
                            if k in ['f_col', 'f_col_org']:
                                tcl_ar  = []
                                #{"xml_id": "x1949_93", "description": "DILUTED EARNINGS PER SHARE (CENTS)", "currency": "CENTS", "clean_value": "27.9", "value_type": "MNUM", "bbox": [[1529.0, 1992.0, 51.0, 24.0]], "pno": "93", "x": "x1949_93", "operator": "+", "taxo_id": 56, "scale": "1", "d": "5", "actual_value": "27.9", "t": "636", "table_id": "636", "ph": "FY2012", "gv_ph": "2012FY"}
                                for ft_ar in rr[ph['k']][k]:
                                    tar = []
                                    for ft in ft_ar:
                                        dd  = {}
                                        for k1 in ["description", "operator", "taxo_id", "ph","pno","clean_value", "actual_value"]:
                                            if ft.get(k1):
                                                dd[k1]  = ft[k1]
                                        dd['operator']  = ft.get('operator', '')
                                        tar.append(dd)
                                    tcl_ar.append(tar)
                                v_d[k]  = tcl_ar
                                        
                            else:
                                v_d[k]  = rr[ph['k']][k]
                    row[ph['k']]    = v_d
            tmp_ar.append(row)
        res = [{'message':'done', 'data':tmp_ar, 'phs':phs, 'table_ar':res[0]['table_ar'][:1], 'disp_name':res[0].get('disp_name')}]
        return res

        
    def read_DB_table_info(self, ijson, ret_flg=None):
        self.dbname = "tfms_urlid"
        dbconnstr = "172.16.20.229#root#tas123"
        self.ip_addr, self.uname, self.passwd = dbconnstr.split('#') 
        import MySQLdb

        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        db_file         = os.path.join(self.model_db, company_name, str(model_number), 'company_report.db')
        conn, cur       = conn_obj.sqlite_connection(db_file)
        self.alter_table_coldef(conn, cur, 'Table_Report', ['populate', 'identification'])
        if ijson.get('table_ids', []):
            sql             = "select table_id, doc_id, classification, normalization, error_accepted, db_status, populate, identification from Table_Report where table_id in (%s)"%(', '.join(map(lambda x:str(x), ijson['table_ids'])))
        else:
            sql             = "select table_id, doc_id, classification, normalization, error_accepted, db_status, populate, identification from Table_Report"
        try:
            cur.execute(sql)
            res = cur.fetchall()
        except:
            res = []
        if ret_flg == 'Y':
            if not res:
                return {}



        db=MySQLdb.connect(self.ip_addr,self.uname,self.passwd,self.dbname+"_"+str(project_id)+"_"+str(deal_id)+"")
        cursor=db.cursor()
        sql = "select d.resid,n.norm_resid,n.docid,n.pageno from norm_data_mgmt n,data_mgmt d where n.process_status= 'Y' and n.active_status= 'Y' and n.review_flag='Y' and d.process_status='Y' and d.active_status='Y' and d.resid=n.ref_resid and d.pageno=n.pageno and d.docid=n.docid and d.taxoname not in ('Grid_Index','Grid@~@Grid Header','Grid@~@Parent Grid Header','Grid@~@Grid Footer','Grid@~@Parent Vertical Header','Grid@~@Vertical Grid Header','Non_Financial_Grid')"
        cursor.execute(sql)
        results = cursor.fetchall()
        db.close()
        norm_resid_d = {}
        for r in results:
            resid = str(r[0])
            norm_resid = str(r[1])
            docid = str(r[2])
            pageno = str(r[3])
            norm_resid_d[str(norm_resid)] = (str(docid), pageno)

        path    = "%s/%s/%s/1_1/21/sdata/doc_map.txt"%(self.doc_path, project_id, deal_id)
        if os.path.exists(path):
            fin = open(path, 'r')
            lines   = fin.readlines()
            fin.close()
        else:
            lines   = []
        doc_d   = {}
        for line in lines[1:]:
            line    = line.split('\t')
            if len(line) < 8:continue
            line    = map(lambda x:x.strip(), line)
            ph      = line[3]+line[7]
            doc_id  = line[0]
            doc_d[doc_id]   = (ph, line[2])
        if ret_flg == 'Y':
            count_d      = {'ph-csv':{}, 'c_t':{}, 'inc':{}, 'e_c':{}, 'db_status':{}, 'total':0, 'ind':{}}
            for rr in res:
                table_id, doc_id, classification, normalization, error_accepted, db_status, populate, identification = map(lambda x:'' if x == None or x == 'N' else x, rr )
                if str(table_id) not in norm_resid_d:continue
                dd   = {'c_t':classification, 'inc':normalization, 'e_c':error_accepted, 'db_status':db_status, 'ph-csv':populate, 'pno':norm_resid_d[str(table_id)][1], 'ind':identification}
                for flg in [('EC', 'e_c'), ('INC', 'inc'), ('IND', 'ind'), ('POPULATE', 'ph-csv'), ("DB", 'db_status')]:
                    tc  = dd[flg[1]]
                    count_d[flg[1]][tc]   = count_d[flg[1]].get(tc, 0) + 1
                if classification:
                    count_d['c_t']['Y']   = count_d['c_t'].get('Y', 0) + 1
                        
                count_d['total']    += 1
            return count_d
                
            
        lmdb_path   = os.path.join(self.model_db, company_name, str(model_number), 'Table_Status')
        env1        = lmdb.open(lmdb_path, max_dbs=1)
        db_name1    = env1.open_db('tableinfo', dupsort=True)
        txn1        = env1.begin(write=False, db=db_name1)
        cursor      = txn1.cursor()
        f_d         = {}
        done_d      = {'ph-csv':0, 'c_t':0, 'inc':0, 'e_c':0, 'db_status':0, 'total':0}
        count_d     = {'ph-csv':{}, 'c_t':{}, 'inc':{}, 'e_c':{}, 'db_status':{}, 'total':0, 'ind':{}}
        for rr in res:
            table_id, doc_id, classification, normalization, error_accepted, db_status, populate, identification = map(lambda x:'' if x == None or x == 'N' else x, rr )
            if str(table_id) not in norm_resid_d:continue
            f_d[table_id]   = {'d':str(doc_id)+'-'+'_'.join(doc_d.get(str(doc_id), [])), 'c_t':classification, 'inc':normalization, 'e_c':error_accepted, 'db_status':db_status, 'ph-csv':populate, 'pno':norm_resid_d[str(table_id)][1], 'ind':identification, 'did': norm_resid_d[str(table_id)][0]}
            uinfo_d = {}
            dd              = f_d[table_id]
            for flg in [('EC', 'e_c'), ('INC', 'inc'), ('CLS', 'c_t'), ('IND', 'ind'), ('POPULATE', 'ph-csv'), ('DB', 'db_status')]:
                if not f_d[table_id][flg[1]]:continue
                if flg[1] != 'c_t':
                    tc  = dd[flg[1]]
                    count_d[flg[1]][tc]   = count_d[flg[1]].get(tc, 0) + 1
                key = flg[0]+'_'+str(table_id)
                error_class_ar  = []
                if cursor.set_key(key):
                    for idx, data in enumerate(cursor.iternext_dup()):
                        data_sp = data.split('@')
                        error_class_ar.append({'u':data_sp[0], 'dtime':data_sp[1]})
                if error_class_ar:
                    error_class_ar.sort(key=lambda x:self.sort_by_date(x))
                    error_class_ar  = error_class_ar[-10:]
                    f_d[table_id][flg[1]+'_uinfo']  = error_class_ar  
                    f_d[table_id][flg[1]+'_user']  = error_class_ar[-1]
                if classification:
                    count_d['c_t']['Y']   = count_d['c_t'].get('Y', 0) + 1
        count_d.update(ijson)
        res = [{'message':'done', 'data':f_d, 'deal_id':str(deal_id), 'stats_d':done_d, 'company_info':{deal_id:count_d}}]
        return res

    def sort_by_date(self, x):
        if x['dtime']:
            try:
                return datetime.datetime.strptime(x['dtime'], "%Y-%m-%d %H:%M:%S")
            except:
                return 1
        return -1

    def read_DB_table_info_company(self, ijson):
        if ijson.get('deal_ids'):
            cinfo_d   = self.read_company_info({"cids":ijson['deal_ids']})
        else:
            cinfo_d   = self.read_company_info({})
        f_d = {}
        for deal_id, cinfo in cinfo_d.items():
            count_d = self.read_DB_table_info(cinfo, 'Y')
            f_d[deal_id] = cinfo
            f_d[deal_id].update(count_d)
        res = [{'message':'done', 'data':f_d, 'CINFO':'Y'}]
        return res

    def populate_scales(self, ijson):
        db_file = '/mnt/eMB_db/unit_conversion/conversion_factor.db'
        conn = sqlite3.connect(db_file)
        cur  = conn.cursor()
        sql = "CREATE TABLE IF NOT EXISTS scale_info(row_id INTEGER PRIMARY KEY AUTOINCREMENT, scale TEXT, disp_name TEXT, value TEXT, user_name VARCHAR(100), datetime TEXT);"
        cur.execute(sql)
        sql = "select scale, disp_name, value from scale_info"
        cur.execute(sql)
        res = cur.fetchall()
        exists_d    = {}
        for rr in res:
            scale, disp_name, value = rr
            exists_d[scale] = (disp_name, value)
        s_ar    = ijson.get('s_ar', [])
        if not s_ar:
            s_ar    = sconvert_obj.scale_ar
        i_ar    = []
        u_ar    = []
        dtime   = str(datetime.datetime.now()).split('.')[0]
        for rr in s_ar:
            if rr['s'] in exists_d:
                u_ar.append((rr['s_label'], rr['v'], ijson.get('user', 'SYSTEM'), dtime, rr['s']))
            else:
                i_ar.append((rr['s_label'], rr['v'], rr['s'], ijson.get('user', 'SYSTEM'), dtime))
        if u_ar:
            cur.executemany('update scale_info set disp_name=?, value=?, user_name=?, datetime=? where scale=?', u_ar)
        if i_ar:
            cur.executemany('insert into scale_info(disp_name, value, scale,  user_name, datetime)values(?,?,?,?,?)', i_ar)
        conn.commit()
        res = [{"message":"done"}]
        return res

            
        
    def populate_factor(self, ijson):
        db_file = '/mnt/eMB_db/unit_conversion/conversion_factor.db'
        conn = sqlite3.connect(db_file)
        cur  = conn.cursor()
        sql = "CREATE TABLE IF NOT EXISTS conversion_factor(row_id INTEGER PRIMARY KEY AUTOINCREMENT, from_scale TEXT, to_scale TEXT, value TEXT, user_name VARCHAR(100), datetime TEXT);"
        cur.execute(sql)
        sql = "select from_scale, to_scale, value from conversion_factor"
        cur.execute(sql)
        res = cur.fetchall()
        exists_d    = {}
        for rr in res:
            from_scale, to_scale, value = rr
            exists_d[(from_scale, to_scale)] = value
        con_factor  = ijson.get('factor', [])
        if not con_factor:
            con_ar  = [
                {'s': '1', 's_label': '1', 'v':1},
                {'s': 'TH/KILO', 's_label': 'TH/KILO', 'v':1000},
                {'s': 'TH', 's_label': 'TH', 'v':1000},
                {'s': 'KILO', 's_label': 'KILO', 'v':1},
                {'s': 'TENTHOUSAND', 's_label': 'TENTHOUSAND', 'v':10000},
                {'s': 'Mn/Ton', 's_label': 'Mn/Ton', 'v':1000000},
                {'s': 'Ton', 's_label': 'Ton', 'v':1000},
                {'s': 'Lakhs', 's_label': 'Lakhs', 'v':100000},
                {'s': 'Mn', 's_label': 'Mn', 'v':1000000},
                {'s': 'Bn', 's_label': 'Bn', 'v':1000000000},
                {'s': 'Tn', 's_label': 'Tn', 'v':1000},
            ]
            for t1 in con_ar:
                for t2 in con_ar:
                    if t1['s'] == t2['s']:continue
                    frm_val = t1['v']
                    to_val  = t2['v']
                    div_val = float(frm_val)/float(to_val)
                    div_val = sconvert_obj.convert_floating_point(div_val).replace(',', '')
                    con_factor.append([t1['s'], t2['s'], div_val])
        u_ar    = []
        i_ar    = []
        dtime   = str(datetime.datetime.now()).split('.')[0]
        for rr in con_factor:
            from_scale, to_scale, value = rr
            if (from_scale, to_scale) in exists_d:
                u_ar.append((value, ijson.get('user', 'SYSTEM'), dtime, from_scale, to_scale))
            else:
                i_ar.append((value, ijson.get('user', 'SYSTEM'), dtime, from_scale, to_scale))
        if u_ar:
            cur.executemany('update conversion_factor set value=?, user_name=?, datetime=? where from_scale=? and to_scale=?', u_ar)
        if i_ar:
            cur.executemany('insert into conversion_factor(value,user_name, datetime, from_scale, to_scale)values(?,?,?,?,?)', i_ar)
        conn.commit()
        res = [{"message":"done"}]
        return res



    def gen_final_output_all(self, ijson, ret_flg=None):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        db_file         = self.get_db_path(ijson)
        #ijson['only_index'] = 'Y'
        #ijson['run_flip'] = 'Y'
        if ijson.get('PRINT', '') != 'Y':
            disableprint()
        #self.run_ph_csv(ijson)
        #enableprint()
        conn, cur   = conn_obj.sqlite_connection(db_file)

        import data_builder.db_data as db_data
        obj = db_data.PYAPI()

        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, [])
        doc_d1       = {}
        doc_d       = {}
        dphs        = {}
        path    = "/var/www/html/TASFundamentalsV2/tasfms/data/output/%s/%s/1_1/21/sdata/doc_map.txt"%(project_id, deal_id)
        if os.path.exists(path):
            fin = open(path, 'r')
            lines   = fin.readlines()
            fin.close()
        else:
            lines   = []
        doc_ph_d    = {}
        c_year      = self.get_cyear(lines)
        start_year  = c_year - int(ijson.get('year', 10))
        for line in lines[1:]:
            line    = line.split('\t')
            if len(line) < 8:continue
            line    = map(lambda x:x.strip(), line)
            try:
                year    = int(line[7])
            except:continue
            if not line[3]:continue
            ph      = line[3]+line[7]
            if ph:# and start_year<int(ph[-4:]):
                doc_id          = line[0]
                doc_d[doc_id]   = (line[3], year)
                doc_d1[doc_id]   = (ph, line[2])
                dphs[line[3]+str(year)]        = 1
                doc_ph_d[doc_id]    = line[3]+str(year)
        g_d = {}
        table_grp_d = {}
        for table_type in rev_m_tables.keys():
            if not table_type:continue
            ijson['table_type'] = table_type
            g_ar    = obj.read_all_vgh_groups(table_type, company_name, model_number, doc_m_d, {})
            for rr in g_ar:
                table_grp_d.setdefault(table_type, {})[rr['grpid']] = 1
                g_d[(table_type, rr['grpid'])] =  rr
        sql = "select group_id, table_type, group_txt from vgh_group_info where table_type like 'HGHGROUP%'"
        try:
            cur.execute(sql)
            res = cur.fetchall()
        except:
            res = []
        for rr in res:
            group_id, table_type, group_txt = rr
            group_txt = group_txt.replace('\t', '')
            g_d[(table_type, group_id)] = group_txt
            table_grp_d.setdefault(table_type, {})[group_id] = 1
        final_ar    = []
        rev_m_tables['PassengerTransportation-Airline'] = {}
        
        for table_type in rev_m_tables.keys():
            #if table_type != 'KPI':continue
            grpd    = table_grp_d.get(table_type)
            if not grpd:
                grpd    = {'':1}
            for group_text in grpd.keys():
                group_text   = str(group_text)
                ijson_c   = copy.deepcopy(ijson)
                ijson_c['table_type']   = table_type
                ijson_c['type']         = 'display'
                ijson_c['row_id']       = [table_type, 'display', group_text]
                ijson_c['infotup']       = (table_type, group_text)
                ijson_c['norm_scale']       = 'Y'
                #ijson_c['print']       = 'Y'
                if group_text:
                    #print (table_type, group_text), (table_type, group_text) not in vgh_grp
                    if (table_type, group_text) not in g_d:continue
                    ijson_c['data']     = [g_d[(table_type, group_text)]['n']]
                    ijson_c['grpid']    = group_text
                    ijson_c['vids']     = g_d[(table_type, group_text)]['vids']
                    ijson_c['infotup']       = (table_type,g_d[(table_type, group_text)]['n'])
                #else:continue
                #print ijson_c
                if table_type == 'PassengerTransportation-Airline':
                    ijson_c['template_id']  = '1'
                final_ar.append(copy.deepcopy(ijson_c))
        if ijson.get('PRINT', '') != 'Y':
            disableprint()
        for ii, ijson_c in enumerate(final_ar):
            print 'Running ', ii, ' / ', len(final_ar), [ijson_c['table_type'], ijson_c['type'], ijson_c.get('data', []), ijson_c.get('grpid', '')]
            #print json.dumps(ijson_c)
            tijson_c    = copy.deepcopy(ijson_c)
            tijson_c['gen_output'] = 'Y'
            mres                 = [] #self.create_seq_across(tijson_c)
            tijson_c    = copy.deepcopy(ijson_c)
            tijson_c['DB_DATA']  = copy.deepcopy(mres)
            tijson_c['norm_scale']  = 'Y'
            res1 = self.create_final_output_with_ph(tijson_c, 'P', 'N')
        for k, info_d in sconvert_obj.no_scale_factor_map.items():
            enableprint()
            print '\n======================'
            print k
            for infotup in info_d.keys():
                if not infotup:continue
                print '\t', infotup
                
            
    def meta_info_txt_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        
        #check_data = {'Q1':1, 'Q2':1, 'Q3':1, 'Q4':1, 'H1':1, 'H2':1, 'FY':1, 'M9':1}
        check_data = ['Q1', 'Q2', 'H1', 'Q3', 'M9', 'Q4', 'H2', 'FY']
        txt_path = '/mnt/eMB_db/%s/%s/company_meta_info.txt'%(company_name, model_number)
        f = open(txt_path)
        txt_data = f.readlines() 
        f.close()
        if len(txt_data) > 1:
            header = txt_data[0].split('\t')
            meta   = txt_data[1].split('\t')
        else:
            header  = []
            meta    = []
        ''' 
        pt_dict = {} 
        for idx, hdr in enumerate(header):
            if hdr not in check_data:continue
            pt_dict[hdr] = meta[idx]
        res_lst = []
        for pt in check_data:
            if pt not in pt_dict:continue
            dct = {'pt':pt, 'yr':pt_dict[pt]} 
            res_lst.append(dct)
        '''
        res_lst_indx = [] 
        for pt in check_data:
            if pt not in header:continue
            get_idx = header.index(pt)
            dct = {'pt':pt, 'yr':meta[get_idx]}
            res_lst_indx.append(dct)
        return [{'message':'done', 'data':res_lst_indx}]

    def form_gp_name(self, sub_k):
        sub_k_li = list(sub_k)
        sub_k_li = list(sets.Set(sub_k_li))
        new_ar = []
        for elm in sub_k_li:
            if not elm.strip(): continue
            if elm not in new_ar:
               new_ar.append(elm)
        new_ar.sort()
        return '~'.join(new_ar)    
         
                          


    def read_ph_derivation_preview_poss(self, ijson):
        import pyapi_db_avi
        p_obj   = pyapi_db_avi.PYAPI()
        preview_data, output, all_clean_value_dict        = p_obj.get_db_preview_data_builder_exhastive_intf(ijson)
        rev_map = {}
        for ti, rr in enumerate(preview_data['data']):
            rev_map[str(rr['t_id'])]    = ti

        avi_ddict = {}

        tmpoutput   = []
        t_ex_d      = {}
        all_taxo_d  = {}
        index_d     = {}
        expr_d  = {}
        for ii, rr_elm in enumerate(output):
            rr = rr_elm[0]
            sub_k = rr_elm[1]
           
            #print rr 
            #print ' == ', sub_k
            sub_k_sign = self.form_gp_name(sub_k)
            print '     sub_key: ', sub_k_sign
  
            if sub_k_sign not in avi_ddict:
               avi_ddict[sub_k_sign] = []  
            avi_ddict[sub_k_sign].append(rr)

            t_d     = {}
            tids    = []
            ex_ar   = []
            ph_d    = {}
            for tr in rr['taxo_arr']:
                tids.append(tr['t_id'])
                ex_ar.append(tr['ex'])
                if tr.get('ph_info'):
                    tmp_phinfo  = {}
                    for ph, vi in tr.get('ph_info', {}).items():
                        tmp_phinfo[ph]  = (tr.get('ex', ''), vi)
                ph_d.update(tr.get('ph_info', {}))
                tr['ph_info']   = tmp_phinfo
                if tr['ex']:
                    t_d[tr['t_id']] = (tr['ex'], tr['ph_info'])
            rr['phs']   = ph_d.keys()
            index_d[ii] = t_d
            expr_d.setdefault(sub_k_sign, {})[ii]  = 1
            all_taxo_d.setdefault(tuple(tids), {})[ii]  = 1
            t_ex_d.setdefault(tuple(tids), {}).setdefault(tuple(ex_ar), {})[ii] = 1


        if 0:
            print ' EXPECTED DISPLAY'
            for k, vs in avi_ddict.items():
                print 'k : ', k
                for i, v in enumerate(vs):
                    print '  --Group: ', i
                    for elm in v['taxo_arr']:
                        print '       ********* ', elm
                       

            sys.exit()         

        tmpoutput   = []
        if 0:
            for tids, idxs in all_taxo_d.items():
                grp_ar  = []
                ex_d    = t_ex_d[tids]
                ex_ar   = ex_d.keys()
                ex_ar.sort(key=lambda x:len(filter(lambda x1:x1, x)), reverse=True)
                grp_d   = {}
                #print '\n', tids
                for extup in ex_ar:
                    #print '\t', extup
                    fnd_grp = {}
                    for grp in grp_d.keys():
                        f_al    = 1
                        for i, g in enumerate(grp):
                            if g and extup[i] and g != extup[i]:
                                f_al    = 0
                                break
                        if f_al == 1:
                            fnd_grp[grp]    = 1
                    if not fnd_grp:
                        fnd_grp[extup]  = 1
                    if extup not in fnd_grp:
                        fnd_grp[extup]  = 1
                    tex_ar  = fnd_grp.keys()
                    tex_ar.sort(key=lambda x:len(filter(lambda x1:x1, x)), reverse=True)
                    fgrp    = tex_ar[0]
                    #print '\t\tfnd_grp : ', fnd_grp
                    #print '\n\t\tFINAL : ', fgrp
                    grp_d[fgrp] = {}
                    for k in fnd_grp:
                        grp_d[fgrp].update(ex_d[k])
                for k, v in grp_d.items():
                    #print '\t', [k, v]
                    t_d = {}
                    for i in v.keys():
                        for tid, vinfo in index_d[i].items():
                            ex, ph_info = vinfo
                            if ex:
                                for ph, vi in ph_info.items():
                                    t_d.setdefault(tid, {})[ph] = (ex, vi)
                    ar  = []
                    for tid in tids:
                        dd  = {'t_id':tid, 'ph_info':t_d.get(tid, {})}
                        ar.append(dd)
                    tmpoutput.append({'taxo_arr':ar})
            output  = tmpoutput
            
            
            
            
        #if not output:
        #    return [{"message":"No Poss"}]
        company_name, model_number, company_id, table_type = ijson['company_name'], ijson['model_number'], ijson['project_id'] + '_' + ijson['deal_id'], ijson['table_type']
        gid             = str(ijson['grpid'])
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        group_id        = gid
        ftype           = 'PH'
        if group_id:
            sql         = "select row_id, ph, formula, formula_type, formula_str, group_id from ph_derivation where formula_type='%s' and table_type='%s' and group_id = '%s'"%(ftype, table_type, group_id)
        else:
            sql         = "select row_id, ph, formula, formula_type, formula_str, group_id from ph_derivation where formula_type='%s' and table_type='%s'"%(ftype, table_type)
        cur.execute(sql)
        res         = cur.fetchall()
        exists_formla_d = {}
        f_formula       = 0
        for rr in res:
            row_id, ph, formula, formula_type, formula_str, tgroup_id    = rr
            if not group_id and tgroup_id:continue
            exists_formla_d[('ROWID', formula)] = row_id
            if formula_str and formula and '=' in formula:
                pt  = formula.split('=')[0]
                if pt != ijson['period_type']:continue
                for t_k in formula_str.split('##'):
                    c_sp    = t_k.split('$')
                    taxo_id = c_sp[0]
                    for cell_id in c_sp[1:]:
                        cell_id = cell_id.split('^^')[0]
                        f_formula       = 1
                        exists_formla_d.setdefault(formula, {}).setdefault(taxo_id, {})[cell_id]    = 1
         
        #if not output and not f_formula:
        #    return [{"message":"No Poss"}]
        for formula, tinfo in exists_formla_d.items():
            if isinstance(formula, str) or  isinstance(formula, unicode):
                grp_ar  = []
                #print '\n========================='
                #print formula
                ph_d    = {}
                formula_d   = {}
                for tid, phinfo in tinfo.items():
                    if tid in rev_map:
                        ph_info = {}
                        for pk in phinfo.keys():
                            #print '\t', [tid, preview_data['data'][rev_map[tid]]['t_l'], pk]
                            if pk in preview_data['data'][rev_map[tid]]:
                                ph_info[pk] = (formula, preview_data['data'][rev_map[tid]][pk]['v'])
                                formula_d[formula]   = 1
                        if ph_info:
                            ph_d.update(ph_info)
                            grp_ar.append({'t_id':tid, 'ex':formula, 'ph_info':ph_info})
                if grp_ar:
                    grp_ar.sort(key=lambda x:rev_map[str(x['t_id'])])
                    output.append(({'taxo_arr':grp_ar, 'done_grp':'Y', 'phs':ph_d.keys()}, formula_d.keys()))
        output.sort(key=lambda x:len(x[0]['phs']), reverse=True)
                
        g_id    = 0
        eq_d    = {}
        feq_d   = {}
        done_grps   = {}
        done_taxos  = {}
        tmpoutput   = []
        for rr_elm in output:
            rr = rr_elm[0]
            sub_k = rr_elm[1]
            rr['g_id']  = g_id
            g_id        += 1
           
            #print rr 
            #print ' == ', sub_k
            sub_k_sign = self.form_gp_name(sub_k)
            #continue
            done_d      = {}
            eqs         = {}
            tids        = []
            r_phs       = {}
            for tr in rr['taxo_arr']:
                done_taxos[str(tr['t_id'])]  = 1
                tids.append(tr['t_id'])
                done    = 0
                for ph in tr['ph_info'].keys():
                    r_phs[ph]       = 1
                    #eqs[tr['ph_info'][ph][0]]         = 1
                    #print [ph, tr['ph_info'][ph]]
                    if ph in exists_formla_d.get(tr['ph_info'][ph][0], {}).get(str(tr['t_id']), {}):
                        done    = 1
                if done == 1:
                    #tr['done']  = 'Y'
                    done_d[tr['t_id']]  = 1
            if len(done_d.keys()) == len(rr['taxo_arr']):
                done_grps[rr['g_id']]   = 1
                rr['done']  = 'Y'
            else:
                for tr in rr['taxo_arr']:
                    if tr['t_id'] in done_d:
                        tr['prtly_done']    = 'Y'
            rr['ph_info']   = r_phs
            feq_d.update(done_d)
            #eqs = eqs.keys()
            #eqs.sort()
            eqs         = sub_k_sign.split('~')
            if rr.get('done_grp') == 'Y':
                eqs = ['DONE']+eqs
            eq_d.setdefault(tuple(eqs), {})[rr['g_id']]    = 1
            #tids.sort()
            #eq_d.setdefault(tuple(eqs), {}).setdefault(tuple(tids), {})[rr['g_id']]    = 1
            tmpoutput.append(rr)
        output  = tmpoutput

        grps    = eq_d.keys()
        grps.sort(key=lambda x:len(eq_d[x].keys()), reverse=True)
        grp_ar  = []
        for grp in grps:
            tgids    = eq_d[grp]
            done_all    = 'Y'
            #for ttup, tgids in eq_d[grp].items():
            gids    = {}
            for gid in tgids.keys():
                gids[gid]   = 1
                if gid not in done_grps:
                    done_all    = 'N'
            grp_ar.append({'n':'( '+str(len(eq_d[grp].keys()))+') '+' ~ '.join(grp), 'gids':gids, 'done':done_all, 'l':len(eq_d[grp].keys())})
        grp_ar.sort(key=lambda x:({'Y':1}.get(x['done'], 0), 999999 - x['l']))

        remaing_taxo    = list(sets.Set(rev_map.keys()) - sets.Set(done_taxos.keys()))
        del preview_data['clean_value_dict']
        if remaing_taxo:# and ijson.get("PRINT") == 'Y':
            eq_d        = {}
            derive_cols = []
            rev_map_d   = {}
            for ph in preview_data['phs']:
                if ph.get('derived') == 'Y' and ph['n'][:-4] == ijson['period_type']:
                    derive_cols.append(ph['k'])
                rev_map_d[ph['n']]   = ph['k']
            derive_cols.reverse()
            form_d  = {}
            for t_id in remaing_taxo:
                for ph in derive_cols:
                    if ph in preview_data['data'][rev_map[t_id]]:continue
                    if (int(t_id), ph) in all_clean_value_dict:
                        for val in all_clean_value_dict[(int(t_id), ph)]:
                            if val[1] == 'G':continue
                            form_d.setdefault(val[1], {}).setdefault(t_id, {})[ph]  = (val[1], val[0])
            for formula, tinfo in form_d.items():
                for tid, ph_info in tinfo.items():
                    r_phs   = {}
                    for ph in ph_info.keys():
                        r_phs[ph]   = 1
                    tgrp_ar  = []
                    if ph_info:
                        tgrp_ar.append({'t_id':tid, 'ex':formula, 'ph_info':ph_info})
                    if tgrp_ar:
                        eq_d.setdefault(('Without-G', formula), {})[tid]    = {g_id:1}
                        rr  = {'taxo_arr':tgrp_ar, 'remain_grp':'Y', 'g_id':g_id, 'ph_info':r_phs}
                        g_id    += 1
                        output.append(rr)
            grps    = eq_d.keys()
            grps.sort(key=lambda x:len(eq_d[x].keys()), reverse=True)
            tgrp_ar  = []
            for grp in grps:
                gids    = {}
                done_all    = 'Y'
                for ttup, tgids in eq_d[grp].items():
                    for gid in tgids.keys():
                        gids[gid]   = 1
                        if gid not in done_grps:
                            done_all    = 'N'
                tgrp_ar.append({'n':'( '+str(len(eq_d[grp].keys()))+') '+' ~ '.join(grp), 'gids':gids, 'done':done_all, 'l':len(eq_d[grp].keys())})
            tgrp_ar.sort(key=lambda x:({'Y':1}.get(x['done'], 0), 999999 - x['l']))
            grp_ar  = grp_ar+tgrp_ar
        return [{'message':'done', 'data':output, 'g_ar':grp_ar, 'feq_d':feq_d, 'res':preview_data}]

    def get_document_info(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        import model_view.cp_company_docTablePh_details as py
        obj = py.Company_docTablePh_details()
        table_doc_page = obj.docTablePage_tup(company_id)

        db_path = '/mnt/eMB_db/%s/%s/tas_company.db'%(company_name, model_number)
        conn = sqlite3.connect(db_path)
        cur  = conn.cursor()
    
        read_qry = 'SELECT doc_id, doc_name from company_meta_info;'
        cur.execute(read_qry)
        table_data  = cur.fetchall()
        conn.close()
        doc_id_name_map = {str(row[0]):row[1] for row in table_data}
        #print doc_id_name_map;sys.exit()
       
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()
 
        res_dct = {}
        for dc_tup in table_doc_page:
            doc_id, page_no, table_id = dc_tup
            get_table_gv_cids = txn_m.get('GV_'+str(table_id))
            #print dc_tup, '>>>', get_table_gv_cids
            if not get_table_gv_cids:continue
            row = {}
            col = {}
            for tab_r_c in get_table_gv_cids.split('#'):
                sd = tab_r_c.split('_')
                r, c = sd[1], sd[2]
                row[r] = 'row'
                col[c] = 'col'
            row_cnt = len(row.keys())
            col_cnt = len(col.keys())
            dc_name = doc_id_name_map.get(doc_id, '')
            res_dct.setdefault((doc_id, dc_name), []).append({'table_id':table_id, 'page_no':page_no, 'no_rows':row_cnt, 'no_cols':col_cnt})
        #print res_dct;sys.exit()
        res_lst = []
        for dn_tup, tab_lst in res_dct.iteritems():
            dct = {'doc_id':dn_tup[0], 'doc_name':dn_tup[1], 'table_info':tab_lst}
            res_lst.append(dct)
        return [{'message':'done', 'data':res_lst}]

    def taxo_ph_value_save_mt_data(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        save_data       = ijson["save_data"]                    

        db_path = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn = sqlite3.connect(db_path)
        cur  = conn.cursor()
        
        insert_rows = []
        update_rows = []
        for row_dct in save_data:
            table_type, taxonomy, ph, value, tmprow_id   = row_dct["tt"], row_dct["tx"], row_dct["ph"], row_dct["v"], row_dct.get('sid', '')
            rd_qry  = 'SELECT row_id FROM mt_data_builder WHERE table_type="%s" and taxonomy="%s" and ph="%s"'%(table_type, taxonomy, ph)
            cur.execute(rd_qry)
            try:
                row_id = cur.fetchone()[0]
            except:row_id = []
            if not row_id:
                insert_rows.append((table_type, taxonomy, ph, value))
            elif row_id:
                update_rows.append((table_type, taxonomy, ph, value, int(row_id)))
        if insert_rows:
            cur.executemany('INSERT INTO mt_data_builder(table_type, taxonomy, ph, c_id) VALUES(?, ?, ?, ?)', insert_rows)
        if update_rows:
            cur.executemany('UPDATE mt_data_builder SET table_type=?, taxonomy=?, ph=?, c_id=? WHERE row_id=?', update_rows)
        conn.commit()
        conn.close()
        return [{'message':'done'}]

    def read_project_preview_lbl_data(self, project_name, deal_ids=[]):
        cinfo = self.read_company_info({"cids":deal_ids})
        final_ar    = []
        for deal_id, ijson_c in cinfo.items():
            company_name    = ijson_c['company_name']
            model_number    = ijson_c['model_number']
            db_file         = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
            if not os.path.exists(db_file):continue
            conn, cur       = conn_obj.sqlite_connection(db_file)
            sql             = "select project_name from company_config where project_name like '%s'"%(project_name)
            try:
                cur.execute(sql)
                res             = cur.fetchone()    
            except:
                res = []
            conn.close()
            if res and res[0]:
                ijson_c["project_name"] = project_name
                ijson_c["norm_scale"]   = 'Y'
                ijson_c["PRINT"]   = 'Y'
                ijson_c["user"]         = 'captain'
                final_ar.append(ijson_c)
        final_d = {}
        phfinal_d = {}
        for ii, ijson_c in enumerate(final_ar):
            ijson_c = self.get_year(ijson_c)
            ktup    = (ijson_c['deal_id'], ijson_c['company_name'])
            final_d.setdefault(ktup, {})
            res_ar  = self.gen_final_output(ijson_c, 'Y')
            print '\n========================='
            print 'Running ', ii, '/', len(final_ar), ktup
            for tmpjson in res_ar:
                tmpjson["PRINT"]   = 'Y'
                tmpjson["norm_scale"]   = 'Y'
                tmpjson["type"]   = 'display'
                print '\tTable Type ', (tmpjson['table_type'], tmpjson.get('grpid'))
                #res = self.create_seq_across(tmpjson)
                res = self.create_final_output_with_ph(tmpjson, 'P', 'Y', {})
                table_type  = tmpjson['table_type']
                grpid       = tmpjson.get('grpid', '')
                if res and res[0]['message'] == 'done':
                    final_d[ktup].setdefault((table_type, grpid), {})
                    phfinal_d[(ktup, (table_type, grpid))] = map(lambda x:x['n'], res[0]['phs'])
                    for rr in res[0]['data']:
                        ldata   = map(lambda x:x['txt'], rr.get('ldata', []))
                        if not ldata:
                            ldata   = [rr['t_l']]
                        for ph in res[0]['phs']:
                            if ph['k'] in rr:
                                rr[ph['n']] = copy.deepcopy(rr[ph['k']])
                                del rr[ph['k']]
                        #rr['lc_ar'] = ldata
                        #if (rr.get('f', '') == 'Y'):
                        #    print 'Res: ', rr
                        #    sys.exit()
                        final_d[ktup][(table_type, grpid)][rr['t_id']]    = rr
                        
        return final_d, phfinal_d

    def read_project_preview_data(self, ijson_c):
        enableprint()
        data    = self.read_project_preview_lbl_data(ijson_c['project_name'])
        for ctup, ld in data.items():
            for (table_type, grpid), taxo_d in ld.items():
                print '\n====================================='
                print ctup, (table_type, grpid)
                for tid, l_ar in taxo_d.items():
                    print '\t', [tid, l_ar]

    def get_merge_taxo_pos(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        project_name    = ''.join(ijson['project_name'].split())
        output_path     = '/mnt/eMB_db/mresults/%s/'%(project_name)
        env             = lmdb.open(output_path, readonly=True)
        txn           = env.begin()
        to_deal         = ijson['to_deal']
        to_company_name = ijson['to_company']
        tid             = str(ijson['tid'])
        table_type      = ijson['table_type'].split('@MERGE')[0]
        grpid           = ijson['grpid']
        if grpid == '':
            grpid   = str(grpid)
        key             = 'MATCH:'+str(((str(deal_id), company_name), (str(table_type), grpid)))+':'+tid+':'+str((str(to_deal), to_company_name)) 
        pos_data        = []
        #print output_path
        #for k, v in txn.cursor():
        #    print k, v
        #print key, txn.get(key)
        try:
            pos_data    = eval(txn.get(key))
        except:pass
        cinfo           = self.read_company_info({"cids":[to_deal]})
        ijson_c         = cinfo[int(to_deal)]
        #[(0, 21090, ('PSH', ''))]
        table_grpd      = {}
        for rr in pos_data:
            score, taxo_id, (table_type, grpid) = rr
            if grpid:
                table_grpd.setdefault(table_type, {})[grpid]    = 1
        import data_builder.db_data as db_data
        obj = db_data.PYAPI()
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(to_company_name, model_number, [])
        for table_type in table_grpd.keys():
            g_ar    = obj.read_all_vgh_groups(table_type, company_name, model_number, doc_m_d, {})
            for rr in g_ar:
                table_grpd[table_type][rr['grpid']] = rr
        grpjson = []
        for rr in pos_data:
            score, taxo_id, (table_type, grpid) = rr
            tmpijson    = {} #copy.deepcopy(ijson_c)
            tmpijson['table_type']  = table_type
            tmpijson['grpid']       = grpid
            tmpijson['c_tids']       = [str(taxo_id)]
            tmpijson['n']           = table_type
            tmpijson['k']           = table_type
            if grpid:
                    tmpijson.update(table_grpd[table_type][grpid])
            tmpijson['l']           = tmpijson['n']
            grpjson.append(tmpijson)
        res = [{"message":"done", "data":grpjson, 'key':key}]
        return res


    def update_taxo_map(self, map_d):
        for cinfo, tinfo in  map_d.items():
            dealijson   = self.read_company_info({"cids":[cinfo[0]]})
            deal_id     = cinfo[0]
            ijson_c     = copy.deepcopy(dealijson[int(deal_id)])
            for ttinfo, taxo_d in tinfo.items():
                (table_type, grpid) = ttinfo
                ijson_c['table_type']   = table_type+'@MERGE'
                ijson_c['grpid']        = grpid
                ph_formula_d                            = self.read_ph_user_formula(ijson_c, grpid, {}, {})
                rmids                   = {}
                ks_d                    = {}
                for ktup, v_d in ph_formula_d.items():
                    if isinstance(ktup, tuple) and isinstance(v_d, dict):
                        if ktup[0] in ['ALL_F', 'ALL_SYS F', 'ALL_CELL F', 'ALL_PTYPE F', 'ALL_USER F'] and int(ktup[1]) in taxo_d:
                            for k, vtup in v_d.items():
                                ks_d[vtup[0].split('-')[1]] = 1
                db_file     = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(ijson_c['company_name'], ijson_c['model_number'])
                conn, cur   = conn_obj.sqlite_connection(db_file)
                sql         ="select row_id from ph_derivation where row_id in (%s)"%(', '.join(ks_d.keys()))
                cur.execute(sql)
                conn.commit()
                conn.close()
                form_grps   = []
                for taxo_id, mapinfo in taxo_d.items():
                    dd  = self.form_formula_ar(mapinfo, table_type+'@MERGE', grpid, str(taxo_id))
                    form_grps.append(dd)
                ijson_c['df']           = 0
                ijson_c["grps"]         = form_grps
                ijson_c["type"]         = "USERFORMULA"
                ijson_c["user"]         = "SYSTEM"
                self.save_ph_derivation_data(ijson_c)
        res = [{'message':'done'}]
        return res

    def form_formula_ar(self, dealinfo, table_type, grpid, r):
        
        ar1  = ["@@".join([r, '=', table_type, grpid, 't', '', '', '', '', ''])]
        for tmpinfo in dealinfo:
            (deal_id, company_name), (org_tt, org_gid), map_taxo   = tmpinfo[:3]
            op  = '+'
            if len(dealinfo) == 4:
                op  = tmpinfo[4]
            ar2  = "@@".join([str(map_taxo), op, org_tt, org_gid, 't', '', '', '', '', str(deal_id)])
            ar1.append(ar2)
            
        dd  = {"f":"$$".join(ar1),"ph":"","tids":{r:{"FY":[]}},"rid":"new"}
        return dd

    def create_merge_view(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        project_name    = ''.join(ijson['project_name'].split())
        output_path     = '/mnt/eMB_db/mresults/%s/'%(project_name)
        print output_path
        env             = lmdb.open(output_path, readonly=True)
        txn             = env.begin()
        try:
            #grp_m           = eval(txn.get('TOTALMATCH'))
            grp_m           = eval(txn.get('ALLMATCHES'))
        except:
            grp_m   = {}
        merge_deal_id   = txn.get('MERGEEDCOMPID')
        print 'merge_deal_id ',merge_deal_id
        frm_comp    = (str(deal_id), company_name)
        #print frm_comp, grp_m.get(frm_comp, {})
        #print grp_m
        other_cinfo = {}
        grp_m_d = {}
        for cinfo in grp_m:
            tmp_d   = {}
            deal_tup    = {}
            for ctup, taxo_ar in cinfo.items():
                dealinfo, table_grp, sign = ctup
                if merge_deal_id != str(deal_id) and str(deal_id) != dealinfo[0]:continue
                deal_tup[dealinfo]    = (table_grp, sign)
                tmp_d[ctup]   = taxo_ar
                other_cinfo.setdefault(dealinfo[0], {}).setdefault(table_grp[0], {})[table_grp[1]]    = 1
            if frm_comp in deal_tup:
                table_grp, sign  = deal_tup[frm_comp]
                grp_m_d.setdefault(table_grp, []).append((sign, tmp_d))
        dealijson  = self.read_company_info({"cids":other_cinfo.keys()})
        import data_builder.db_data as db_data
        obj = db_data.PYAPI()
        ttype_d = {}
        ph_d    = {}
        phs_d   = {}
        all_phs = {}
        display_name_d  = {}
        for to_dealid, tinfo in other_cinfo.items():
         to_dealid  = int(to_dealid)
         tmpjson    = copy.deepcopy(dealijson[int(to_dealid)])
         tmpjson['project_name']    = ijson['project_name']
         tmpjson['model_number']    = ijson['model_number']
         tmpjson                    = self.get_year(tmpjson)
         m_tables, rev_m_tables, doc_m_d, table_type_m = self.get_main_table_info(tmpjson['company_name'], tmpjson['model_number'])
         for ttable_type, gid in tinfo.items():
            g_ar    = obj.read_all_vgh_groups(ttable_type, tmpjson['company_name'], tmpjson['model_number'], doc_m_d, {})
            g_d     = {}
            for rr in g_ar:
                g_d[rr['grpid']] =  rr
            for grpid, tids in gid.items():
                print (to_dealid, ttable_type, grpid)
                ijson_c = copy.deepcopy(tmpjson)
                ijson_c['table_type'] = ttable_type
                ijson_c['template_id'] = ''
                res = []
                if grpid == '':
                    ijson_c['table_type']   = ttable_type
                    ijson_c['type']         = 'display'
                    if ijson_c.get('grpid'):
                        del ijson_c['grpid']
                    #print json.dumps(ijson_c)
                    res = self.create_final_output(ijson_c, {})
                    if ijson.get("PRINT") != "Y":
                        disableprint()
                elif grpid in g_d:
                    ijson_c['table_type']   = ttable_type
                    ijson_c['grpid']        = grpid
                    ijson_c['vids']         = g_d[grpid]['vids']
                    ijson_c['type']         = 'display'
                    res = self.create_final_output(ijson_c, {})
                    if ijson.get("PRINT") != "Y":
                        disableprint()
                else:
                    print '\tNOT AVAILABLE ',(grpid, g_d.keys())
                if res:
                    if res[0]['message'] != 'done':
                        print '\t', (ttable_type, grpid, res[0]['message'] )
                        if res[0]['message'] == 'Error PHS not available':continue
                        if 'res' not in res:
                            return res
                        res = res[0]['res']
                    if res  and 'data' in res[0]:
                        res = self.clean_response(res)
                        print '\tDONE'
                        display_name_d[(to_dealid, ttable_type, grpid)]    = res[0]['disp_name']
                        for rr in res[0]['data']:
                            for ph in res[0]['phs']:
                                if ph['k'] in rr:
                                    rr[ph['n']] = copy.deepcopy(rr[ph['k']])
                                    del rr[ph['k']]
                            ttype_d.setdefault((to_dealid, ttable_type, grpid), {})[str(rr['t_id'])]   = rr
                        for ph in res[0]['phs']:
                            #phs_d.setdefault((to_dealid, ttable_type, grpid), {})[ph['n']] = ph['k']
                            all_phs[ph['n']] = 1
                else:
                    print '\tERROR'
        phs = report_year_sort.year_sort(all_phs.keys())
        phs = map(lambda x:{'k':x, 'n':x,'g':x,  'dph':(x[:-4], int(x[-4:]))}, phs)
        tphs    = map(lambda x:x['n'], phs)
        f_ar    = []
        sn      = 1
        rid     = 1
        for table_grp, ar in grp_m_d.items():
            if len(f_ar) and 0:
                row     = {'desc':{'v':'', 'rid':rid}, '$treeLevel':0, 'level_id':0, 'sn':sn, 'clen':1, 'hflg':'H'}
                sn      += 1
                rid     += 1
                f_ar.append(row)
            ctup    = (int(deal_id), table_grp[0], table_grp[1])
            row     = {'desc':{'v':display_name_d[ctup], 'rid':rid}, '$treeLevel':0, 'level_id':0, 'sn':sn, 'clen':1, 'hflg':'H', 'ct':"Y"}
            sn      += 1
            rid     += 1
            f_ar.append(row)
            print '\n======================================================'
            print table_grp, tphs
            for (sign, cinfo_d) in ar:
                #print
                re_tup      = {}
                tmp_vald    = {}
                tform_d     = {}
                form_ar     = []
                all_taxos   = {}
                for ctup, taxo_ar in cinfo_d.items():
                    dealinfo, table_grp, sign = ctup
                    re_tup[dealinfo]  = (table_grp, taxo_ar[0])
                    if dealinfo == frm_comp:continue
                    t_row_f     = self.form_merge_formula(taxo_ar, table_grp, int(dealinfo[0]))
                    form_ar     += t_row_f    
                for ctup, taxo_ar in cinfo_d.items():
                    dealinfo, table_grp, sign = ctup
                    if dealinfo != frm_comp:continue
                    t_row_f     = self.form_merge_formula(taxo_ar, table_grp, int(dealinfo[0]))
                    form_ar     += t_row_f    
                    #print '\t', taxo_ar
                table_grp, tid  = re_tup[frm_comp]
                table_type, grpid = table_grp
                dd      = {'vt': '', 'rid': 'RID-0', 'c': '', 'g_id': grpid, 't_type': table_type, 'txid': str(tid), 's': '', 'type': u't', 'op': u'=', 'to_dealid':int(deal_id)}
                form_ar = [dd]+form_ar
                rr  = ttype_d[(int(deal_id), table_grp[0], table_grp[1])][str(tid)]
                for k in ['t_id', 't_l', 'f', 'th_flg', 'd', 'x', 'bbox', 't']:
                    if k in rr:
                        tmp_vald[k] = rr[k]
                t_row_f = ('RID-0', form_ar)
                val_dict, form_d, flip_sign, mismatch_scale, mismatch_vt, taxo_grp = self.get_formula_evaluation_across(tmp_vald, t_row_f, ttype_d, tphs, phs_d, rr.get('m_scale', ''), '')
                update_pk   = {}
                f_value = 0
                for k, v in val_dict.get(str(tid), {}).items():
                    #print '\t\t', [k,v]
                    update_pk[k]   = 1
                    tmp_vald[k]   = v
                #print '\tval_dict', val_dict
                #sys.exit()
                res_row = self.read_sys_form_across(t_row_f, tmp_vald, phs, ttype_d, table_type, {}, phs_d, ijson, display_name_d, update_pk, form_d, dealijson)
                rr  = tmp_vald
                row     = {'desc':{'v':rr['t_l'], 'f':rr.get('f', ''), 'rid':rid}, '$treeLevel':1, 'level_id':1, 'sn':sn, 'clen':0}
                sn      += 1
                rid     += 1
                #print '\t', rr['t_l']
                for pk in tphs:
                    if pk in rr:
                        #print pk, rr[pk]
                        row[pk] = rr[pk]
                        if row[pk].get('f_col'):
                            row['desc']['f_col'] = row[pk]['f_col'][0][1:]
                            row['desc']['f_col'][0]['show_cinfo']        = 'Y'
                            row['desc']['f'] = 'Y'
                            f_col   = filter(lambda x:x['clean_value'], row[pk]['f_col'][0][1:])
                            row[pk]['f_col']    = f_col
                            if f_col:
                                row[pk]['f']        = 'Y'
                                row[pk]['f_col'][0]['show_cinfo']        = 'Y'
                        row[pk]['rid'] = rid
                        rid += 1
                f_ar.append(row)
                
        phs = [{'n':"Description", 'k':'desc'}]+phs
        bbox_dict   = {}
        for deal_id in other_cinfo.keys():
            tmpijson    = dealijson[int(deal_id)]
            page_coordinate_path = os.path.join('/var/www/html/DB_Model/%s/'%(tmpijson['company_name']), 'page_coord.txt')
            pc_data = {}
            with open(page_coordinate_path, 'r') as f:
                pc_data_json = f.read()
                pc_data_json = pc_data_json.strip()
                pc_data     = eval(pc_data_json)        
            bbox_dict[deal_id]  = pc_data_json
        cinfo   = map(lambda x:{'deal_id':x, 'company_name':dealijson[int(x)]['org_company_name']}, other_cinfo.keys())
        res = [{"message":'done', 'phs':phs, 'data':f_ar, 'deal_ids':map(lambda x:int(x), other_cinfo.keys()), 'bbox':bbox_dict, 'cinfo':cinfo}]
        txt_file_path = '/var/www/html/demo_data/V1/%s/%s/Merge/'%(company_name, project_name)
        os.system("mkdir -p '%s'"%(txt_file_path))
        txt_file_path   = txt_file_path+'/ALL.txt'
        f = open(txt_file_path, 'w')
        f.write(json.dumps(res))
        f.close()
        return [{'message':'done', 'data':txt_file_path}]

    def form_merge_formula(self, taxo_ar, table_grp, to_dealid):
        table_type, grpid   = table_grp
        f_ar    = []
        for tid in taxo_ar:
            dd      = {'vt': '', 'rid': 'RID-0', 'c': '', 'g_id': grpid, 't_type': table_type, 'txid': str(tid), 's': '', 'type': u't', 'op': u'+', 'to_dealid':to_dealid}
            f_ar.append(dd)
        return f_ar



    def update_old_to_new(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        db_file         = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn, cur   = conn_obj.sqlite_connection(db_file)
        sql = "select group_id, table_type, group_txt from vgh_group_info"
        try:
            cur.execute(sql)
            res = cur.fetchall()
        except:
            res = []
        grp_info    = {}
        for rr in res:
            group_id, table_type, group_txt = rr
            group_txt = group_txt.replace('\t', '')
            grp_info.setdefault(table_type, {})[group_txt] = str(group_id)
        sql = "select taxo_id, table_type, table_id, xml_id, isvisible from mt_data_builder"
        cur.execute(sql)
        res = cur.fetchall()
        taxo_xml_id1    = {}
        ttypes          = {}
        for rr in res:
            taxo_id, table_type, table_id, xml_id, isvisible    = rr
            if isvisible != 'N' and table_type and  'HGHGROUP' not in table_type:
                ttypes[table_type]          = 1
                taxo_xml_id1[(table_type, str( table_id), xml_id)]   = taxo_id
        import data_builder.db_data as db_data
        obj = db_data.PYAPI()
        path1   = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        path2   = '/mnt/eMB_db/%s/%s/mt_data_builder_%s.db'%(company_name, model_number, datetime.datetime.now().strftime('%Y-%m-%d-%H_%M_%S'))
        path3   = '/mnt/eMB_db/%s/%s/mt_data_builder_%s_VGH.db'%(company_name, model_number, datetime.datetime.now().strftime('%Y-%m-%d-%H_%M_%S'))
        if ijson.get('update_VGH') == 'Y':
            cmd = "cp %s %s"%(path1, path3)
            os.system(cmd)
            obj.populate_old_vgh_grps(ijson)
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, [])
        #obj.populate_old_vgh_grps(ijson)
        ijson['year']   = 50
       # m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, [])
        old_new_map = {}
        hgh_taxo    = {}
        for table_type in ttypes.keys():
            print '\n========================='
            print table_type
            ijson['table_type'] = table_type
            sql = "select group_id, table_type, group_txt from vgh_group_info where table_type like '%s'"%('HGHGROUP-'+table_type+'%')
            cur.execute(sql)
            res = cur.fetchall()
            for rr in res:
                group_id, ttable_type, group_txt = rr
                group_txt = group_txt.replace('\t', '')
                tids    = []
                for tid in  ttable_type.split('HGHGROUP-'+table_type+'-')[1].split('-')[:-1]:
                    tids.append(tid)    
                if tids:
                    hgh_taxo.setdefault(table_type, {})[tuple(tids)]   = 1
                grp_info.setdefault(table_type, {})[group_txt]  = str(group_id)
            tg_ar    = [] #self.create_group_ar(ijson, txn_m)
            g_ar    = obj.read_all_vgh_groups(table_type, company_name, model_number, doc_m_d, {})
            grpd    = {}
            for rr in tg_ar:
                grpd[rr['n']]    = rr['grpid']
            for rr in g_ar:
                if table_type not in grp_info:continue
                if 'HGHGROUP' not in rr['grpid']:
                    old_new_map.setdefault('GRP', {}).setdefault(table_type, {})[grp_info[table_type][rr['n']]] = rr['grpid']
            res = self.create_seq_across(ijson)
            for rr in res[0]['data']:
                m_taxo_ids  = {}
                f_missing   = {}
                for ph in res[0]['phs']:
                    if ph['k'] in  rr:
                        ktup    = (table_type, str(rr[ph['k']]['t']), rr[ph['k']]['x'])
                        if ktup in taxo_xml_id1:
                            m_taxo_ids[taxo_xml_id1[ktup]]  = 1
                        else:
                            f_missing[ph['g']]  = ph['n']
                if len(m_taxo_ids.keys()) != 1 :
                    print 'More than one taxo ids ', [rr['t_id'], rr['t_l'], m_taxo_ids]
                    #sys.exit()
                #if f_missing:
                #    print 'Found Missing cell ', [rr['t_id'], rr['t_l'], f_missing]
                    #sys.exit()
                if m_taxo_ids:
                    #print [table_type, str(m_taxo_ids.keys()[0]), str(rr['t_id'])]
                    old_new_map.setdefault(table_type, {})[str(m_taxo_ids.keys()[0])] = str(rr['t_id'])
                    
        if ijson.get('update_DB') == 'Y':
            cmd = "cp %s %s"%(path1, path2)
            os.system(cmd)
        for table_type, tid_d in hgh_taxo.items():
            ftaxo   = []
            for ttup in tid_d.keys():
                ntaxo   = {}
                for tid in ttup:
                    if tid in old_new_map.get(table_type, {}):
                        ntaxo[old_new_map[table_type][tid]]  = 1
                    else:
                        ntaxo   = {}
                        break
                if ntaxo:
                    ftaxo.append(ntaxo.keys())
                else:
                    print 'Missing HGH taxo ', [table_type, ttup]
            if ftaxo:
                ijson_c             = copy.deepcopy(ijson)
                ijson_c['table_type'] = table_type
                res = self.create_seq_across(ijson_c)
                t_d = {}
                for rr in res[0]['data']:
                    t_d[rr['t_id']] = rr['t_l']
                for ntaxo in ftaxo:
                    print '\t', [table_type, ntaxo]
                    tmpd    = {}
                    for tid in ntaxo:
                        tmpd[int(tid)]    = t_d[int(tid)]
                    print [table_type, tmpd]
                    if ijson.get('update_DB') == 'Y':
                        ijson_c['t_ids']    = tmpd
                        self.create_HGH_group(ijson_c)
                        pass
                    
                    
                    
                    
            
                
        ijson['table_types']    = ttypes.keys()+['ALL']
        import create_table_seq
        tmpobj  = create_table_seq.TableSeq()
        ph_formula_d    = tmpobj.update_ph_user_formula(db_file, old_new_map, ijson)
        tmpobj.update_project_config(ijson, old_new_map, db_file)
        res = [{'message':'done'}]
        return res
                    
            
        
        

        
            
                    
            
                    
                    
            

                
                
            

    def populate_currency(self, ijson):
        db_file = '/mnt/eMB_db/unit_conversion/conversion_factor.db'
        conn = sqlite3.connect(db_file)
        cur  = conn.cursor()
        sql = "CREATE TABLE IF NOT EXISTS currency_info(row_id INTEGER PRIMARY KEY AUTOINCREMENT, currency TEXT, disp_name TEXT, value TEXT, user_name VARCHAR(100), datetime TEXT);"
        cur.execute(sql)
        sql = "select currency, disp_name, value from currency_info"
        cur.execute(sql)
        res = cur.fetchall()
        exists_d    = {}
        for rr in res:
            scale, disp_name, value = rr
            exists_d[scale] = (disp_name, value)
        s_ar    = ijson.get('s_ar', [])
        i_ar    = []
        u_ar    = []
        dtime   = str(datetime.datetime.now()).split('.')[0]
        for rr in s_ar:
            if rr['s'] in exists_d:
                u_ar.append((rr['s_label'], rr['v'], ijson.get('user', 'SYSTEM'), dtime, rr['s']))
            else:
                i_ar.append((rr['s_label'], rr['v'], rr['s'], ijson.get('user', 'SYSTEM'), dtime))
        if u_ar:
            cur.executemany('update currency_info set disp_name=?, value=?, user_name=?, datetime=? where currency=?', u_ar)
        if i_ar:
            cur.executemany('insert into currency_info(disp_name, value, currency,  user_name, datetime)values(?,?,?,?,?)', i_ar)
        conn.commit()
        res = [{"message":"done"}]
        return res

    def populate_currency_factor(self, ijson):
        db_file = '/mnt/eMB_db/unit_conversion/conversion_factor.db'
        conn = sqlite3.connect(db_file)
        cur  = conn.cursor()
        sql = "CREATE TABLE IF NOT EXISTS currency_conversion_factor(row_id INTEGER PRIMARY KEY AUTOINCREMENT, from_currency TEXT, to_currency TEXT, value TEXT, user_name VARCHAR(100), datetime TEXT);"
        cur.execute(sql)
        sql = "select from_currency, to_currency, value from currency_conversion_factor"
        cur.execute(sql)
        res = cur.fetchall()
        exists_d    = {}
        for rr in res:
            from_scale, to_scale, value = rr
            exists_d[(from_scale, to_scale)] = value
        con_factor  = ijson.get('factor', [])
        u_ar    = []
        i_ar    = []
        dtime   = str(datetime.datetime.now()).split('.')[0]
        for rr in con_factor:
            from_scale, to_scale, value = rr
            if (from_scale, to_scale) in exists_d:
                u_ar.append((value, ijson.get('user', 'SYSTEM'), dtime, from_scale, to_scale))
            else:
                i_ar.append((value, ijson.get('user', 'SYSTEM'), dtime, from_scale, to_scale))
        if u_ar:
            cur.executemany('update currency_conversion_factor set value=?, user_name=?, datetime=? where from_currency=? and to_currency=?', u_ar)
        if i_ar:
            cur.executemany('insert into currency_conversion_factor(value,user_name, datetime, from_currency, to_currency)values(?,?,?,?,?)', i_ar)
        conn.commit()
        res = [{"message":"done"}]
        return res

    def remove_taxo_builder(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        db_file         = '/mnt/eMB_db/%s/%s/taxo_data_builder.db'%(company_name, model_number)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        table_types     = map(lambda x:"'"+x+"'", ijson['table_types'])
        if not table_types:
            res = [{"message":"Nothing to do"}]
            return res
        sql             = "delete from mt_data_builder where table_type in (%s)"%(', '.join(table_types))
        ans = raw_input(sql+' Y/n :')
        if ans != 'Y':
            res = [{"message":"Nothing to do"}]
            return res
        cur.execute(sql)
        conn.commit()
        conn.close()
        res = [{"message":"done","sql":sql}]
        return res

    def remove_taxo_builder_by_tableid(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        db_file         = '/mnt/eMB_db/%s/%s/taxo_data_builder.db'%(company_name, model_number)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        table_ids       = map(lambda x:"'"+x+"'", ijson['table_ids'])
        table_type      = ijson['table_type']
        sql             = "delete from mt_data_builder where table_type in ('%s') and table_id in (%s)"%(table_type, ', '.join(table_ids))
        #ans = raw_input(sql+' Y/n :')
        #if ans != 'Y':
        #    res = [{"message":"Nothing to do"}]
        #    return res
        cur.execute(sql)
        conn.commit()
        conn.close()
        res = [{"message":"done","sql":sql}]
        return res

    def move_cell_from_taxo(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        db_file         = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        sql             = "CREATE TABLE IF NOT EXISTS taxo_column_map(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_type TEXT, group_id TEXT, from_taxo_id TEXT, from_column TEXT, to_taxo_id TEXT, to_column TEXT, user_name VARCHAR(100), datetime TEXT);"
        cur.execute(sql)
        table_type      = ijson['table_type']
        grpid           = ijson.get('grpid', '')
        ftaxo           = ijson['data']['from_taxo']
        ttaxo           = ijson['data']['to_taxo']
        from_xml        = ijson['data']['from_xml']
        to_xml          = ijson['data'].get('to_xml', '')
        dtime   = str(datetime.datetime.now()).split('.')[0]
        cur.executemany("insert into taxo_column_map(table_type, group_id,from_taxo_id, from_column, to_taxo_id,to_column, user_name, datetime) values(?, ?, ?, ?, ?,?, ?, ?)", [(table_type, grpid, ftaxo, from_xml, ttaxo, to_xml, ijson['user'], dtime)])
        conn.commit()
        conn.close()
        res = [{"message":"done"}]
        return res

    def revert_move_cell_from_taxo(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        db_file         = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        table_type      = ijson['table_type']
        grpid           = ijson.get('grpid', '')
        t_id            = ijson['taxo_id']
        xml             = ijson['xml']
        sql             = "select row_id, from_taxo_id from taxo_column_map where table_type='%s' and  to_taxo_id='%s' and from_column='%s'"%(table_type, t_id, xml)
        cur.execute(sql)
        row_id, from_taxo_id    = cur.fetchone()
        sql             = "delete from taxo_column_map where row_id=%s"%(row_id)
        cur.execute(sql)
        conn.commit()
        conn.close()
        res = [{"message":"done", 'from_taxo':from_taxo_id}]
        return res

    def revert_dnc_cell_from_taxo(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        db_file         = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        table_type      = ijson['table_type']
        grpid           = ijson.get('grpid', '')
        t_id            = ijson['taxo_id']
        xml             = ijson['xml']
        sql             = "select row_id, from_taxo_id from taxo_column_map where table_type='%s' and group_id='%s' and from_taxo_id='%s' and from_column='%s'"%(table_type, grpid, t_id, xml)
        cur.execute(sql)
        row_id, from_taxo_id    = cur.fetchone()
        sql             = "delete from taxo_column_map where row_id=%s"%(row_id)
        cur.execute(sql)
        conn.commit()
        conn.close()
        res = [{"message":"done", 'from_taxo':from_taxo_id}]
        return res


    def get_labels(self, ph, oper, ttype_d, phs_d, all_phs):
        op_txid    = oper['taxo_id']
        op         = oper['operator']
        op_type    = oper['type']
        ttype    = oper['tt']
        grpid    = oper['grpid']
        to_dealid= oper['to_dealid']
        tph         = ph
        if oper.get('k', ''):
            tph  = oper['k']
        tph         = phs_d.get((to_dealid, ttype, grpid), {}).get(tph, tph)
        data_row    = ttype_d.get((to_dealid, ttype, grpid), {}).get(op_txid, {}) 
        cell_d      = data_row.get(tph, {})
        lbls        = [data_row['t_l']]+map(lambda x:x['txt'], data_row.get('ldata', []))
        l_d         = {}
        for txt in lbls:
            txt = ' '.join(txt.split(' ')).strip()
            l_d[txt]    = 1
        return l_d
        
        

    def index_kpi_mapping(self, ijson):
        dealijson   = self.read_company_info({"industry_type":ijson['industry_type']})
        formula_d   = {}
        taxo_map_d  = {}
        final_lb_d  = {}
        index_d = {}
        fi   = 0
        l   = len(dealijson.keys())
        for deal_id, cinfo in dealijson.items():
            if '_' not in str(deal_id):continue
            project_id, deal_id = deal_id.split('_')
            #if project_id != ijson['project_id']:continue
            fi   += 1
            #if str(deal_id) not in '61 34 51 28 67 69 76 30 71 32'.split():continue
            ijson_c = copy.deepcopy(ijson)
            ijson_c.update(cinfo)
            company_name    = ijson_c['company_name']
            mnumber         = ijson_c['model_number']
            model_number    = mnumber
            deal_id         = ijson_c['deal_id']
            project_id      = ijson_c['project_id']
            company_id      = "%s_%s"%(project_id, deal_id)
            db_file         = self.get_db_path(ijson_c)
            if not os.path.exists(db_file):continue
            conn, cur   = conn_obj.sqlite_connection(db_file)
            table_type  = ijson_c['table_type']
            sql         = 'select row_id, ph, formula, formula_type, formula_str, group_id from ph_derivation where table_type="%s"'%(table_type)
            try:
                cur.execute(sql)
                res         = cur.fetchall()
            except:
                res  = []
            cur.close()
            conn.close()
            if not res:continue
            print 'Running ', fi, ' / ', l
            print '\n\t=========================================================='
            print '\t\t', deal_id, cinfo
            print '\t\tRead KPI....'
            res, (display_name_d, ttype_d, phs_d, all_phs) = self.read_kpi_data(ijson_c, 'Y')
            print '\t\tGot'
            for i, rr in enumerate(res[0]['data']):
                taxo_map_d[rr['t_id']]  = (i, rr['t_l'])
                #print '\n\t---------------------------------------------------------------------------'
                #print '\t', [rr['t_l']]
                for ph in res[0]['phs']:
                    if ph['k'] in rr:
                        #print '\t\t', ph['n']
                        f_ar    = []
                        ftype   = ''
                        found_missing_op    = 0
                        found_op    = 0
                        label_map_d = {}
                        for ft in rr[ph['k']]['f_col'][0]:
                            #{'taxo_id': u'WeightrelatedkeyfiguresCargoandmailCarried', 'to_dealid': 50, 'operator': u'=', 'rid': '247', 'disp_name': u'KPIs(Airline Industry)', 'phcsv': {'p': '2010', 's': u'Tonne', 'vt': u'Measurement', 'c': '', 'pt': 'FY'}, 'to_company': u'DeutscheLufthansaAG', 'tt': u'PassengerTransportation-Airline', 't': '162', 'grpid': u'', 'v_ar': [2023.0], 'ph': 'FY2010', 'type': 't', 'expr_str': "+Freight and mail thousand tonnes ['+2023.0']", 'company_info': {'deal_id': 50, 'company_name': u'DeutscheLufthansaAG'}, 'i_f_type': u'FORMULA', 'description': u'Cargo and mail Carried', 'fv': 'Y', 'clean_value': '2023.00', 'R': 'Y', 'ftype': 'G', 'bbox': [[1453.0, 1141.0, 92.0, 29.0]], 'd': '4', 'i': 0, 'k': 'FY2010', 'conv_value': {}, 'expr_val': '+2023.0', 'v': '2023.00', 'x': 'x1000140_78#x1000141_78#x1000149_78#x1426_78'}
                            if ft['i_f_type']:
                                ftype   = ft['i_f_type']
                            if ft['type'] != 'v':
                                if ft['description'] == 'Not Exists': 
                                    found_missing_op    = 1
                                    continue
                                if ft['operator'] != '=':
                                    found_op    = 1
                            else:
                                if isinstance(ft['description'], unicode):
                                    ft['description']   = ft['description'].encode('utf-8')
                                    
                            if ft['type'] == 'v':
                                f_ar.append(('', '',ft['v'], '', ft['operator'], ft['type'], ''))    
                            elif ft['tt'] == table_type:
                                #ft['disp_name'] = ' '.join(ft['disp_name'].split()).lower()
                                if ftype in ['PTYPEFORMULA', 'CELLFORMULA']:
                                    f_ar.append((ft['tt'], ft['disp_name'], ft['taxo_id'], ft['ph'], ft['operator'], 'WITHIN', ''))    
                                else:
                                    f_ar.append((ft['tt'], ft['disp_name'], ft['taxo_id'], '', ft['operator'], 'WITHIN', ''))    
                            else:
                                #ft['disp_name']     = ' '.join(ft['disp_name'].split()).lower()
                                #ft['description']   = ' '.join(ft['description'].split()).lower()
                                trow    = ttype_d.get((ft['to_dealid'], ft['tt'], ft['grpid']), {}).get(ft['taxo_id'], {})
                                tindex  = trow['tindex']
                                tl      = trow['t_l']
                                if isinstance(tl, unicode):
                                    tl  = tl.encode('utf-8')
                                tl  = ' '.join(tl.split()).lower()
                                tmptids = ttype_d[(ft['to_dealid'], ft['tt'], ft['grpid'])][('L', tl)]
                                if len(tmptids.keys()) <= 1:
                                    tindex  = ''
                                if ftype in ['PTYPEFORMULA', 'CELLFORMULA']:
                                    label_map_d[len(f_ar)]  = self.get_labels(ph['n'], ft, ttype_d, phs_d, all_phs)
                                    f_ar.append((ft['tt'], ft['disp_name'], ft['description'], ft['ph'], ft['operator'], 'OTHER', tindex))    
                                else:
                                    label_map_d[len(f_ar)]  = self.get_labels(ph['n'], ft, ttype_d, phs_d, all_phs)
                                    f_ar.append((ft['tt'], ft['disp_name'], ft['description'], '', ft['operator'], 'OTHER', tindex))    
                            
                            #print '\t\t\t', ft
                        if f_ar and ftype and found_missing_op == 0 and found_op == 1:
                            for ttup in f_ar:
                                if ttup[4] != '=' and ttup[5] in ['WITHIN', 'OTHER']:
                                    index_d.setdefault(rr['t_id'], {}).setdefault((ftype, ttup), {}).setdefault(len(f_ar), {})[tuple(f_ar)]   = 1
                            formula_d.setdefault(rr['t_id'], {}).setdefault(ftype, {}).setdefault(tuple(f_ar), {})[deal_id] = 1
                            final_lb_d.setdefault(rr['t_id'], {}).setdefault(ftype, {}).setdefault(tuple(f_ar), {})
                            for k, v in label_map_d.items():
                                final_lb_d[rr['t_id']][ftype][tuple(f_ar)].setdefault(k, {}).update(v)
        ks  = taxo_map_d.keys()
        ks.sort(key=lambda x:x[0])
        if 1:
            lc_changes  = []
            for k, vd in index_d.items():
                for ftup, vlen_d in vd.items():
                    len_ar   = vlen_d.keys()
                    len_ar.sort(reverse=True)
                    grp_ar  = []
                    for l in len_ar:
                        v_d = vlen_d[l]
                        if len(v_d.keys()) < 2:continue
                        grp_ar.append(v_d.keys())
                    if grp_ar:
                        print '\n\t++++++++++++++++++++++++++++++++++++++++++++++++++++++++++='
                        print '\tFTYPE ', k, ftup
                        for f_ars in grp_ar:
                            change_d    = {}
                            for f_ar in f_ars:
                                print
                                for i, ft in enumerate(f_ar):
                                    print '\t\t\t\t', ft
                                    change_d.setdefault(i, {}).setdefault(ft[0], {})
                                    if ft[5] in ['WITHIN', 'OTHER']:
                                        change_d[i][ft[0]][ft[2]]    = 1
                            for i in range(len(f_ars[0])):
                                for table_type, desc in change_d[i].items():
                                    if len(desc.keys()) > 1:    
                                        print 'Lchanges '
                                        tmpar   = []
                                        for lc in desc.keys():
                                            print '\t', [lc]        
                                            try:
                                                lc  = lc.decode('utf-8')
                                            except:pass
                                            tmpar.append(lc)
                                        lc_changes.append({'new':'Y', 'data':tmpar, 'table_type':table_type})
                            
            self.add_kpi_lc({'texts':lc_changes})
            #sys.exit()
            

        path        = '/mnt/eMB_db/KPI_FORMULA/%s'%(ijson['table_type'])
        env         = lmdb.open(path,map_size=20*1024*1024*1024*1024)
        with env.begin(write=True) as txn:
            for k in ks:
                if k not in formula_d:continue
                print '\n=========================================='
                print k, [taxo_map_d[k][1]]
                print 'POS_'+str(k)
                txn.put('POS_'+str(k), binascii.b2a_hex(str(formula_d[k])))
                for ftype, f_d in formula_d[k].items():
                    print '\n\t++++++++++++++++++++++++++++++++++++++++++++++++++++++++++='
                    print '\tFTYPE ', [ftype]
                    f_ars   = f_d.keys()
                    f_ars.sort(key=lambda x:len(f_d[x].keys()), reverse=True)
                    for f_ar in f_ars:
                        print   
                        print '\t\tNo.Occ ', len(f_d[f_ar].keys()), f_d[f_ar].keys()
                        for ft in f_ar:
                            print '\t\t\t', ft
            
            
        pass



    def add_kpi_lc(self, ijson):
        texts           = ijson['texts']
        db_file         = '/mnt/eMB_db/KPI_LC.db'
        user            = ijson.get('user', 'SYSTEM')
        dtime               = str(datetime.datetime.now()).split('.')[0]
        conn, cur       = conn_obj.sqlite_connection(db_file)
        sql = "CREATE TABLE IF NOT EXISTS label_changes(row_id INTEGER PRIMARY KEY AUTOINCREMENT, group_id INTEGER,  label TEXT, table_type VARCHAR(100), user_name VARCHAR(100), datetime TEXT);"
        cur.execute(sql)
        u_ar    = []
        i_ar    = []
        with conn:
            #sql = "select max(taxo_id) from kpi_input"
            sql = "select seq from sqlite_sequence WHERE name = 'label_changes'"
            cur.execute(sql)
            r       = cur.fetchone()
            g_id    = 1
            if r and r[0]:
                g_id    = int(r[0])+1
            
            sql = "select max(group_id) from label_changes"
            cur.execute(sql)
            r       = cur.fetchone()
            tg_id   = 1
            if r and r[0]:
                tg_id    = int(r[0])+1
            g_id     = max(g_id, tg_id)
            for dd in texts:
                if dd.get('new') == 'Y' or not dd.get('grpid'):
                    grpid   = g_id
                    g_id    += 1
                else:
                    grpid   = dd['grpid']
                for t in dd['data']:
                    i_ar.append((grpid, t, dd['table_type'], user, dtime))
        cur.executemany("insert into label_changes(group_id, label, table_type, user_name, datetime)values(?,?,?,?, ?)", i_ar)
        conn.commit()
        cur.close()
        conn.close()
        res = [{"message":"done"}]
        return
        

        
    def read_pos_mapping(self, ijson):

        def get_row_dict(ttype, dd, ftype, desc, ph, to_dealid, ft, grpid, taxo_id, not_exists=None):
            dd['i_f_type']      = ftype
            dd['type']          = ttype
            dd['clean_value']   = ''
            if desc == 'Not Exists' or not_exists == 'not_exists':
                dd['description']   = ft[2]
                dd['not_exists']   = 'Y'
            else:
                dd['description']   = desc
            if ft[6] != '':
                dd['description']   = dd['description']+' ('+str(ft[6]+1)+')'
                
            dd['ph']            = ph
            dd['to_dealid']     = to_dealid
            dd['taxo_id']       = taxo_id
            dd['tt']            = ft[0]
            dd['grpid']         = grpid
            dd['operator']      = ft[4]
            dd['disp_name']     = ft[1]
            return dd
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        db_file         = self.get_db_path(ijson)

        conn, cur   = conn_obj.sqlite_connection(db_file)
        crt_qry = 'CREATE TABLE IF NOT EXISTS tt_group_display_name_info(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_type TEXT, group_id TEXT, group_name TEXT, display_name TEXT)'
        cur.execute(crt_qry)
        conn.commit()
        read_qry = 'SELECT table_type, group_id, display_name FROM tt_group_display_name_info;'
        cur.execute(read_qry)
        table_data = cur.fetchall()
        cur.close()
        conn.close()

        tt_group_disp_nm_info = {}
        for row in table_data:
            table_type, group_id, display_name = row[:]
            if display_name:
                if group_id:
                    tt_group_disp_nm_info.setdefault(table_type, {})[str(group_id)] = display_name
                else:
                    tt_group_disp_nm_info.setdefault(table_type, {})[''] = display_name

        db_file         = '/mnt/eMB_db/KPI_LC.db'
        conn, cur       = conn_obj.sqlite_connection(db_file)
        sql             = "select group_id, label, table_type from label_changes"
        cur.execute(sql)
        res = cur.fetchall()
        lc_d    = {}
        for rr in res:
            group_id, label, table_type = rr
            label   = ' '.join(label.split()).strip().lower()
            lc_d.setdefault(group_id, {})[label]    = 1
            lc_d.setdefault((table_type, label), {})[group_id]    = 1
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, [])
        import data_builder.db_data as db_data
        obj = db_data.PYAPI()
        g_d = {}
        for table_type in rev_m_tables.keys():
            #ijson['table_type'] = table_type
            g_ar    = obj.read_all_vgh_groups(table_type, company_name, model_number, doc_m_d, {})
            tdisp_name   = table_type_m.get(table_type, table_type)
            if tt_group_disp_nm_info.get(table_type, {}).get(''):
                tdisp_name   = tt_group_disp_nm_info[table_type]['']
            g_d.setdefault(table_type, {}).setdefault(tdisp_name, {})['']    = 1
            for rr in g_ar:
                disp_name   = tdisp_name+' - '+rr['n']
                if tt_group_disp_nm_info.get(table_type, {}).get(rr['grpid']):
                    disp_name   = tt_group_disp_nm_info[table_type][rr['grpid']]
                g_d.setdefault(table_type, {}).setdefault(tdisp_name, {})[rr['grpid']]    = 1
            

        industry_id = ijson['template_id']
        db_file     = "/mnt/eMB_db/template_info.db"
        conn, cur   = conn_obj.sqlite_connection(db_file)
        sql = "select time_series, display_name from meta_info where row_id=%s"%(industry_id)
        cur.execute(sql)
        tmpres  = cur.fetchone()
        display_name_d  = {}
        display_name_d[(int(deal_id), ijson['table_type'], '')]   = tmpres[1]
        taxo_map_d  = {}
        sql         = "select taxo_id, prev_id, parent_id, taxonomy, taxo_label, scale, value_type, client_taxo, yoy, editable, formula_str, sign from industry_kpi_template where industry_id=%s"%(industry_id)
        cur.execute(sql)
        res         = cur.fetchall()
        cur.close()
        conn.close()
        for rr in res:
            taxo_id, prev_id, parent_id, taxonomy, taxo_label, scale, value_type, client_taxo, yoy, editable, formula_str, sign   = rr
            taxo_map_d[taxonomy]    = taxo_label


        if ijson.get('gen_output') != 'Y':
            path        = '/mnt/eMB_db/KPI_FORMULA/%s'%(ijson['table_type'])
            env         = lmdb.open(path, readonly=True)
            txn         = env.begin()
        else:
            txn         = {}
        tids        = taxo_map_d.keys() if ijson['t_id'] == 'ALL' else [ijson['t_id']]
        color_map_d = {
                    'G':2,
                    'O':1,
                    'R':0,
                }
        taxo_pos_d  = {}
        ttype_d, phs_d, all_phs = {}, {}, {}
        tinfo   = {}
        #tinfo.setdefault(ijson['table_type'], {})['']  = 1
        for tid in tids:
            pos         = txn.get('POS_'+str(tid))
            #print 'POS_'+str(ijson['t_id'])
            final_d     = {}
            to_dealid   = int(deal_id)
            if not pos:continue
            pos =  eval(binascii.a2b_hex(pos))
            for ftype, f_d in pos.items():
                #print '\n\t++++++++++++++++++++++++++++++++++++++++++++++++++++++++++='
                #print '\tFTYPE ', [ftype]
                f_ars   = f_d.keys()
                f_ars.sort(key=lambda x:len(f_d[x].keys()), reverse=True)
                for f_ar in f_ars:
                    #print   
                    #print '\t\tNo.Occ ', len(f_d[f_ar].keys()), f_d[f_ar].keys()
                    for ft in f_ar:
                        #print '\t\t\t', ft
                        if ft[5] == 'OTHER':
                            if ft[0] not in g_d:continue
                            m_names = g_d[ft[0]].keys()
                            for name in m_names:
                                grpids  = g_d[ft[0]][name].keys()
                                grpid   = grpids[0]
                                if len(grpids) > 1:
                                    grpid   = filter(lambda x:x, grpids)[0]
                                tinfo.setdefault(ft[0], {})[grpid]  = 1
                                #all_table_types.setdefault(ft[0], {})[ft[1]]    = grpid 
        tmpjson = copy.deepcopy(ijson)
        if ijson.get('INPUT_DB_DATA'):
            (display_name_d, ttype_d, phs_d, all_phs)   = ijson['INPUT_DB_DATA']
        else:
            self.get_table_type_data(tinfo, tmpjson, company_name, model_number, doc_m_d, obj, ijson, to_dealid, display_name_d, ttype_d, phs_d, all_phs)
            tinfo   = {}
            tinfo.setdefault(ijson['table_type'], {})['']  = 1
            tmpijson    = copy.deepcopy(ijson)
            tmpijson['INPUT_DB_DATA']   = (display_name_d, ttype_d, phs_d, all_phs)
            print 'READ KPI...'
            res = self.read_kpi_data(tmpijson)
            if res  and 'data' in res[0]:
                #display_name_d[(to_dealid, ttable_type, grpid)]    = res[0]['disp_name']
                ttable_type, grpid  = ijson['table_type'], ''
                for rr in res[0]['data']:
                    ttype_d.setdefault((to_dealid, ttable_type, grpid), {})[str(rr['t_id'])]   = rr
                    tl  = rr['t_l']
                    if isinstance(tl, unicode):
                        tl  = tl.encode('utf-8')
                    tl  = ' '.join(tl.split()).lower()
                    #print (ttable_type, grpid, ('L', tl), rr['t_id'])
                    ttype_d.setdefault((to_dealid, ttable_type, grpid), {}).setdefault(('L', tl), {})[str(rr['t_id'])]   = 1
                for ph in res[0]['phs']:
                    phs_d.setdefault((to_dealid, ttable_type, grpid), {})[ph['n']] = ph['k']
                    all_phs[ph['n']] = 1
        for tid in tids:
            pos         = txn.get('POS_'+str(tid))
            #print 'POS_'+str(tid)
            final_d     = {}
            to_dealid   = int(deal_id)
            if not pos:continue
            pos =  eval(binascii.a2b_hex(pos))
            all_table_types = {}
            for ftype, f_d in pos.items():
                #print '\n\t++++++++++++++++++++++++++++++++++++++++++++++++++++++++++='
                #print '\tFTYPE ', [ftype]
                f_ars   = f_d.keys()
                f_ars.sort(key=lambda x:len(f_d[x].keys()), reverse=True)
                for f_ar in f_ars:
                    #print   
                    #print '\t\tNo.Occ ', len(f_d[f_ar].keys()), f_d[f_ar].keys()
                    for ft in f_ar:
                        #print '\t\t\t', ft
                        if ft[5] == 'OTHER':
                            if ft[0] not in g_d:continue
                            m_names = g_d[ft[0]].keys()
                            for name in m_names:
                                grpids  = g_d[ft[0]][name].keys()
                                grpid   = grpids[0]
                                if len(grpids) > 1:
                                    grpid   = filter(lambda x:x, grpids)[0]
                                tinfo.setdefault(ft[0], {})[grpid]  = 1
                                all_table_types.setdefault(ft[0], {})[ft[1]]    = grpid 
        
            #display_name_d.update(tdisplay_name_d)
            for ftype, f_d in pos.items():
                f_ars   = f_d.keys()
                f_ars.sort(key=lambda x:len(f_d[x].keys()), reverse=True)
                for f_ar in f_ars:
                    frm_ar      = []
                    frm_tup_ar  = []
                    found_al    = 1
                    found_op    = 0
                    gtmph       = ''
                    deal_ids    = f_d[f_ar].keys()
                    deal_ids.sort()
                    ftype_d    = {}
                    for ft in f_ar:
                        dd  = {}
                        if ft[5] == 'v':
                            dd  = {}
                            dd  = get_row_dict('v', {}, ftype, '', '', to_dealid, ft, '', ft[2])
                            dd['v']             = ft[2]
                            dd['label']         = ''
                            frm_tup_ar.append((ft[2], ft[4]))
                        elif ft[5] == 'OTHER':
                            if ft[4] != '==':
                                ftype_d[ft[0]]  = ftype_d.get(ft[0], 0)+1
                            grpid = all_table_types.get(ft[0], {}).get(ft[1])
                            if grpid == None:
                                dd  = get_row_dict('t', {}, ftype, 'Not Exists', '', to_dealid, ft, '', '')
                                frm_ar.append(dd)
                                continue
                                #print 'GRP NOT FOUND ', ft
                                frm_ar      = []
                                found_al = 0
                                break
                            tl  = ft[2]
                            if isinstance(tl, unicode):
                                tl  = tl.encode('utf-8')
                            tl      = ' '.join(tl.split()).lower()
                            if tl in lc_d:
                                labels  = []
                                for grpid in lc_d[(ft[0], tl)].keys():
                                    labels  += lc_d[grpid].keys()
                            else:
                                labels  = [tl]
                            tids    = []
                            for label in labels:
                                tids_d    = ttype_d.get((to_dealid, ft[0], grpid), {}).get(('L', tl), {})
                                if ft[6] != '':
                                    tids    = filter(lambda x:tids_d[x] == ft[6], tids_d.keys())
                                else:
                                    tids    = filter(lambda x:tids_d[x] == 0, tids_d.keys())
                                if tids:
                                    break
                            if not tids:
                                #print 'TAXO NOT FOUND ', tl, (to_dealid, ft[0], grpid)
                                dd  = get_row_dict('t', {}, ftype, 'Not Exists', '', to_dealid, ft, grpid, '')
                                frm_ar.append(dd)
                                continue
                            val_d   = {}
                            tmph    = ''
                            for ph in all_phs.keys():
                                pk = phs_d.get((to_dealid, ft[0], grpid), {}).get(ph)   
                                if pk and pk in ttype_d[(to_dealid, ft[0], grpid)][tids[0]]:
                                    for k in ['v', 'd', 'x', 'phcsv', 'bbox', 't']:
                                        val_d[k]    = ttype_d[(to_dealid, ft[0], grpid)][tids[0]][pk][k]
                                    tmph    = ph
                                    break
                            #print ft, (to_dealid, ft[0], grpid), tids, val_d
                            if tmph and not gtmph:
                                gtmph   = tmph
                            dd  = copy.deepcopy(val_d)
                            dd  = get_row_dict('t', dd, ftype, ttype_d[(to_dealid, ft[0], grpid)][tids[0]]['t_l'], tmph, to_dealid, ft, grpid, tids[0])
                            dd['clean_value']   = val_d.get('v', '')
                            found_op    = 1
                            frm_tup_ar.append((ft[0], grpid, tids[0]))
                        elif ft[5] == 'WITHIN':
                            if ft[4] != '=':
                                ftype_d[ft[0]]  = ftype_d.get(ft[0], 0)+1
                            if ft[2] not in taxo_map_d:
                                #frm_ar      = []
                                #found_al = 0
                                dd  = get_row_dict({}, ftype, 'Not Exists', '', to_dealid, ft, '', '')
                                frm_ar.append(dd)
                                continue
                                #break
                            tids    = [ft[2]]
                            if ft[4] != '=':
                                val_d   = {}
                                tmph    = ''
                                grpid   = ''
                                for ph in all_phs.keys():
                                    pk = phs_d.get((to_dealid, ft[0], grpid), {}).get(ph)   
                                    if pk and pk in ttype_d[(to_dealid, ft[0], grpid)][tids[0]]:
                                        for k in ['v', 'd', 'x', 'phcsv', 'bbox', 't']:
                                          if k in ttype_d[(to_dealid, ft[0], grpid)][tids[0]][pk]: #error for deal 28
                                            val_d[k]    = ttype_d[(to_dealid, ft[0], grpid)][tids[0]][pk][k]
                                        tmph    = ph
                                        break
                                if not val_d:
                                    dd  = get_row_dict('t', {}, ftype, taxo_map_d[ft[2]], '', to_dealid, ft, '', ft[2], '')
                                    frm_ar.append(dd)
                                    continue
                            dd  = get_row_dict('t', {}, ftype, taxo_map_d[ft[2]], '', to_dealid, ft, '', ft[2])
                            found_op    = 1
                            frm_tup_ar.append((ft[0], '', ft[2]))
                        frm_ar.append(dd)
                        pass
                    if frm_ar and found_op:
                        ftup    = tuple(frm_tup_ar)
                        if ftup in final_d:continue
                        if gtmph:
                            for dd in frm_ar:
                                if dd['type'] != 't' and not dd['ph']:
                                    dd['ph']    = gtmph
                        if 0:
                            dd  = {}
                            dd['i_f_type']      = ftype
                            dd['type']          = 't'
                            dd['clean_value']   = gtmph
                            dd['description']   = taxo_map_d[tid]
                            dd['ph']            = ''
                            dd['to_dealid']     = to_dealid
                            dd['taxo_id']       = ijson[tid]
                            dd['tt']            = ijson['table_type']
                            dd['grpid']         = ''
                            dd['operator']      = '='
                            dd['disp_name']     = display_name_d[(to_dealid, ijson['table_type'], '')]
                            frm_ar  = [dd]+frm_ar
                        m_p = int((len(frm_tup_ar)/float(len(f_ar))) * 100)
                        color   = 'R'
                        if m_p == 100:
                            color   = 'G'
                        elif m_p >= 50:
                            color   = 'O'
                        mftype  = 'WITHIN'
                        if len(ftype_d.keys()) > 1:
                            mftype  = 'ACROSS'
                        elif len(ftype_d.keys()) == 1 and ijson['table_type'] not in ftype_d:
                            mftype  = 'DIRECT'
                        final_d[ftup]  = (m_p, color, len(frm_tup_ar), frm_ar, m_p, deal_ids, ftype,mftype)
                    else:
                        #print 'NO MATCH'
                        #for ft in f_ar:
                        #    print '\t\t\t', ft
                        pass
            all_frm = final_d.keys()
            all_frm.sort(key=lambda x:(color_map_d[final_d[x][1]], final_d[x][2]), reverse=True)
            f_ar    = []
            for i, k in enumerate(all_frm):
                dd  = {'n':'F-'+str(i+1), 'data':final_d[k][3], 'c':final_d[k][1], 'mp':final_d[k][0], 'ftype':final_d[k][7], 'deal_ids':final_d[k][5]}
                f_ar.append(dd)
            if f_ar:
                taxo_pos_d[tid]  = f_ar
        if ijson.get('t_id') == 'ALL':
            if ijson.get('type') == 'all':
                res = [{"message":"done", "data":taxo_pos_d}]
                return res
            tmpd    = {}
            for k, v in taxo_pos_d.items():
                s_d = {}
                for v1 in v:
                    s_d[v1['c']] = s_d.get(v1['c'], 0)+1
                tmpd[k] = s_d
            #res = [{"message":"done", "data":taxo_pos_d}]
            res = [{"message":"done", "data":tmpd}]
        else:
            res = [{"message":"done", "data":taxo_pos_d.get(str(ijson['t_id']), [])}]
        return res

    def gen_table_type_data(self, tinfo, tmpjson, company_name, model_number, doc_m_d, obj, ijson, to_dealid, display_name_d, ttype_d, phs_d, all_phs, rev_form_d):
        reported    = 'N'
        if ijson.get('reported') == 'Y':
            reported    = 'Y'
        #display_name_d  = {}
        #ttype_d         = {}
        #phs_d           = {}
        #all_phs         = {}
        for ttable_type, gid in tinfo.items():
            #g_ar    = self.create_group_ar(ijson_c, txn_m)
            if 'HGHGROUP' in ttable_type:
                ttable_type = '-'.join(ttable_type.split('-')[1:-2])
            g_ar    = obj.read_all_vgh_groups(ttable_type, company_name, model_number, doc_m_d, {})
            g_d     = {}
            for rr in g_ar:
                g_d[rr['grpid']] =  rr
                #print rr['grpid'], rr['n']
            for grpid, tids in gid.items():
                print '\n============================================='
                print 'DB ',(to_dealid, ttable_type, grpid)
                ijson_c = copy.deepcopy(tmpjson)
                ijson_c['table_type'] = ttable_type
                ijson_c['template_id'] = ''
                ijson_c['norm_scale'] = 'Y'
                ijson_c['store_flg'] = 'Y'
                ijson_c['gen_output'] = 'Y'
                ijson_c['from_template_gen'] = 'Y'
                ijson_c['rev_form_d'] = rev_form_d
                res = []
                if grpid == '':
                    ijson_c['type'] = 'all'
                    print '\tGEN DATA',(ttable_type, grpid)
                    ijson_c['table_type']   = ttable_type
                    ijson_c['store_flg']   = 'Y'
                    #if ijson.get('gen_output') != 'Y':
                    #    ijson_c['c_tids']       = tids.keys()
                    if ijson_c.get('grpid'):
                        del ijson_c['grpid']
                    #print json.dumps(ijson_c)
                    res = self.create_final_output(ijson_c, {})
                elif grpid in g_d:
                    ijson_c['type'] = 'group'
                    print '\tGEN DATA ',(ttable_type, grpid)
                    ijson_c['table_type']   = ttable_type
                    ijson_c['store_flg']   = 'Y'
                    #if ijson.get('gen_output') != 'Y':
                    #    ijson_c['c_tids']       = tids.keys()
                    ijson_c['grpid']        = grpid
                    ijson_c['vids']         = g_d[grpid]['vids']
                    res = self.create_final_output(ijson_c, {})

    def get_table_type_data(self, tinfo, tmpjson, company_name, model_number, doc_m_d, obj, ijson, to_dealid, display_name_d, ttype_d, phs_d, all_phs):
        reported    = 'N'
        if ijson.get('reported') == 'Y':
            reported    = 'Y'
        #display_name_d  = {}
        #ttype_d         = {}
        #phs_d           = {}
        #all_phs         = {}
        for ttable_type, gid in tinfo.items():
            #g_ar    = self.create_group_ar(ijson_c, txn_m)
            if 'HGHGROUP' in ttable_type:
                ttable_type = '-'.join(ttable_type.split('-')[1:-2])
            g_ar    = obj.read_all_vgh_groups(ttable_type, company_name, model_number, doc_m_d, {})
            g_d     = {}
            for rr in g_ar:
                g_d[rr['grpid']] =  rr
                #print rr['grpid'], rr['n']
            for grpid, tids in gid.items():
                if (to_dealid, ttable_type, grpid) in display_name_d:continue
                print '\n============================================='
                print 'DB ',(to_dealid, ttable_type, grpid)
                #sys.exit()
                ijson_c = copy.deepcopy(tmpjson)
                ijson_c['table_type'] = ttable_type
                ijson_c['template_id'] = ''
                ijson_c['norm_scale'] = 'Y'
                ijson_c['store_flg'] = 'Y'
                res = []
                if grpid == '':
                    #if g_ar:
                    #    print 'Error Mapped in Main ', (ijson_c['deal_id'], ijson_c['company_name'], ttable_type)
                    #    sys.exit()
                    key             = ''.join(ttable_type)+'_'+''.join(str(grpid).split())+'_'+''.join(ijson.get('project_name', '').split())+'_'+reported
                    path            = '/mnt/eMB_db/%s/%s/PREVIEW_OUTPUT/%s/'%(company_name, model_number, key)
                    print path
                    #os.system("mkdir -p "+path)
                    tdata   = []
                    if ijson.get('store_flg') != 'Y' and os.path.exists(path):
                        try:
                            env1            = lmdb.open(path, readonly=True)
                            txn             = env1.begin()
                        except:
                            txn = {}
                        tdata           = txn.get('res')
                        if tdata:
                            res = eval(tdata)
                            print 'disp_name ', (ttable_type, grpid), res[0].get('disp_name')
                    if not tdata or 'disp_name' not in res[0]:
                        print '\tREAD DATA',(ttable_type, grpid)
                        ijson_c['table_type']   = ttable_type
                        ijson_c['store_flg']   = 'Y'
                        #if ijson.get('gen_output') != 'Y':
                        #    ijson_c['c_tids']       = tids.keys()
                        ijson_c['type']         = 'display'
                        if ijson_c.get('grpid'):
                            del ijson_c['grpid']
                        #print json.dumps(ijson_c)
                        res = self.create_final_output(ijson_c, {})
                        if ijson.get("PRINT") != "Y":
                            disableprint()
                elif grpid in g_d:
                    key             = ''.join(ttable_type)+'_'+''.join(str(grpid).split())+'_'+''.join(ijson.get('project_name', '').split())+'_'+reported
                    path            = '/mnt/eMB_db/%s/%s/PREVIEW_OUTPUT/%s/'%(company_name, model_number, key)
                    print path
                    #os.system("mkdir -p "+path) 
                    tdata   = []
                    if ijson.get('store_flg') != 'Y' and os.path.exists(path):
                        try:
                            env1            = lmdb.open(path, readonly=True)
                            txn             = env1.begin()
                        except:
                            txn = {}
                        tdata           = txn.get('res')
                        if tdata:
                            res = eval(tdata)
                            print 'disp_name ', (ttable_type, grpid), res[0].get('disp_name')
                    if not tdata or 'disp_name' not in res[0]:
                        print '\tREAD DATA ',(ttable_type, grpid)
                        ijson_c['table_type']   = ttable_type
                        ijson_c['store_flg']   = 'Y'
                        #if ijson.get('gen_output') != 'Y':
                        #    ijson_c['c_tids']       = tids.keys()
                        ijson_c['grpid']        = grpid
                        ijson_c['vids']         = g_d[grpid]['vids']
                        ijson_c['type']         = 'display'
                        res = self.create_final_output(ijson_c, {})
                        if ijson.get("PRINT") != "Y":
                            disableprint()
                if res:
                    if res[0]['message'] != 'done':
                        print '\tNODATA', (ttable_type, grpid, res[0]['message'] )
                        if res[0]['message'] == 'Error PHS not available':continue
                        if 'res' not in res:
                            return res
                        res = res[0]['res']
                    if res  and 'data' in res[0]:
                        for tmpt, tmpv_d in res[0].get('t_col_grp_d', {}).items():
                            #print tmpt, tmpv_d
                            ttype_d.setdefault('t_col_grp_d', {}).setdefault(tmpt, {}).update(tmpv_d)
                        #sys.exit()
                        display_name_d[(to_dealid, ttable_type, grpid)]    = res[0]['disp_name']
                        lable_index = {}
                        for ii, rr in enumerate(res[0]['data']):
                            ttype_d.setdefault((to_dealid, ttable_type, grpid), {})[str(rr['t_id'])]   = rr
                            tl  = rr['t_l']
                            if isinstance(tl, unicode):
                                tl  = tl.encode('utf-8')
                            tl  = ' '.join(tl.split()).lower()
                            rr['tindex']    = lable_index.get(tl, -1) + 1
                            #if 'HGHGROUP' in grpid:#(u'FA', u'24', ('L', 'total'), 170846) == (ttable_type, grpid, ('L', tl), rr['t_id']):
                            #    print (ttable_type, grpid, ('L', tl), rr['t_id'], rr['t_l'], rr.get('Q32019'))
                            
                            ttype_d.setdefault((to_dealid, ttable_type, grpid), {}).setdefault(('L', tl), {})[str(rr['t_id'])]   = rr['tindex']
                            lable_index[tl] = lable_index.get(tl, -1)+1
                        for ph in res[0]['phs']:
                            #if ph['k'] in rr:
                            #    print (ttable_type, grpid,  rr['t_id'], rr['t_l'], rr[ph['k']].get('o_v'],  rr[ph['k']].get('re_stated'))
                            phs_d.setdefault((to_dealid, ttable_type, grpid), {})[ph['n']] = ph['k']
                            all_phs[ph['n']] = 1
        return display_name_d, ttype_d, phs_d, all_phs
        
        
    def form_auto_databuilder_struct(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        m_table_type      = ijson["table_type"]
       
        import read_taxo_data as read_taxo_data
        eObj = read_taxo_data.DB()
        model_scr, all_model_results    = eObj.applicator_builddb(m_table_type, company_id)        
        #model_scr.sort(key=lambda x:x[0], reverse=True)
        #print model_scr, all_model_results.keys()
        #model_scr.sort()
        res_drop_lst = []
        for scr_model in model_scr:
            score  = scr_model[0]
            tt_str  = scr_model[1]      #= model_scr[0][1]
            res_data_dct = {'key':tt_str, 'n':tt_str, 's':score}
            res_drop_lst.append(res_data_dct)
            rows_model =  all_model_results[tt_str].keys() 
            rows_model.sort()
            lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
            env         = lmdb.open(lmdb_path1, readonly=True)
            txn_m       = env.begin()
            res_lst = []
            all_auto_table_xmls = {}
            for row in rows_model:
                row_data_st = []
                for elm in  all_model_results[tt_str][row]:
                    # '658', ('non-currentliabilities', '0f8f03275213fa65c028ff5448a89037', ((16, 0), 'x967_137#x1000202_137', 'Non-current liabilities')))
                    table_id = elm[0]
                    xml_id  = elm[1][2][1]
                    #print elm
                    #print xml_id, table_id
                    if xml_id:
                        tk          = self.get_quid(table_id+'_'+xml_id)
                        c_id        = txn_m.get('XMLID_MAP_'+tk)
                        all_auto_table_xmls.setdefault(table_id, {})[xml_id] = 1
                        #print c_id
                        if c_id:
                            tab_tup = (table_id, c_id, 'Label', '', '')                      
                            row_data_st.append(tab_tup)
                dat_arr = {'t_l':'', 'ks':row_data_st}
                res_lst.append(dat_arr)
            if 0:
                missing_taxo_data = []
                for tb, xm_dct in all_auto_table_xmls.iteritems():
                    get_cid = txn_m.get('HGH_'+tb)
                    if not get_cid:continue
                    #missed_table_xm = []
                    for cd in get_cid.split('#'):
                        get_xm_cid = txn_m.get('XMLID_'+cd)
                        if get_xm_cid and (get_xm_cid not in xm_dct):
                            missed_xm = (tb, cd, 'Label', '', '')
                            #missed_table_xm.append(missed_xm)
                            ks_data = {'t_l':'', 'ks':[missed_xm]}
                            missing_taxo_data.append(ks_data)
                res_lst += missing_taxo_data
            m_tables, rev_m_tables, doc_m_d, table_type_m = self.get_main_table_info(company_name, model_number)
            tmpres = self.populate_taxo_data(company_name, model_number, {m_table_type:res_lst}, ijson, txn_m, tt_str, doc_m_d, m_table_type)
            #break
        #res_drop_lst.sort(key=lambda x:x['s'], reverse=True)
        #print 'here'
        return [{'message':'done', 'data':res_drop_lst}] 

    def populate_taxo_data(self, company_name, model_number, table_type_d, ijson, txn_m, n_taxos, doc_m_d, m_table_type):
        #model_number    = '1'
        #ijson['model_number']   = '1'
        db_file         = '/mnt/eMB_db/%s/%s/'%(company_name, model_number)
        os.system("mkdir -p "+db_file)
        db_file         = '/mnt/eMB_db/%s/%s/taxo_data_builder.db'%(company_name, model_number)
        mconn, mcur   = conn_obj.sqlite_connection(db_file)
        db_file         = '/mnt/eMB_db/%s/%s/taxo_data_builder_%s.db'%(company_name, model_number, n_taxos)
        #print db_file
        conn, cur   = conn_obj.sqlite_connection(db_file)
        sql     = "CREATE TABLE IF NOT EXISTS vgh_group_map(row_id INTEGER PRIMARY KEY AUTOINCREMENT, vgh_id TEXT, group_txt TEXT, table_type TEXT, user_name TEXT, datetime TEXT)"
        cur.execute(sql)
        sql     = "CREATE TABLE IF NOT EXISTS vgh_info(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_type TEXT, vgh_id TEXT, vgh TEXT, user_name TEXT, datetime TEXT)"
        cur.execute(sql)
        sql     = "CREATE TABLE IF NOT EXISTS mt_data_builder(row_id INTEGER PRIMARY KEY AUTOINCREMENT, taxo_id INTEGER, prev_id INTEGER,order_id INTEGER, table_type, taxonomy TEXT, user_taxonomy TEXT, missing_taxo varchar(1), table_id TEXT, c_id TEXT, gcom TEXT, ngcom TEXT, ph TEXT, ph_label TEXT, user_name TEXT, datetime TEXT, isvisible varchar(1), m_rows TEXT, vgh_text TEXT, vgh_group TEXT, doc_id INTEGER, xml_id TEXT, period VARCHAR(50), period_type VARCHAR(50), scale VARCHAR(50), currency VARCHAR(50), value_type VARCHAR(50), user_label TEXT)"
        cur.execute(sql)
        mcur.execute(sql)
        dtime   = str(datetime.datetime.now()).split('.')[0]
        i_ar    = []
        new_taxo    = {}
        with mconn:
            sql = "select seq from sqlite_sequence WHERE name = 'mt_data_builder'"
            mcur.execute(sql)
            r       = mcur.fetchone()
            if not r:
                g_id    = 1
            else:
                g_id    = int(r[0])+2
            sql     = "select max(taxo_id) from mt_data_builder" 
            mcur.execute(sql)
            r       = mcur.fetchone()
            try:
                tg_id    = int(r[0])+1
            except:
                tg_id    = 1
            g_id    = max(g_id, tg_id)
            for table_type, f_taxo_arr in table_type_d.items():
                prev_id = -1
                for ii, dd in enumerate(f_taxo_arr):
                    ks      = dd['ks']
                    taxos   = dd['t_l']
                    e_taxo  = dd.get('o_tinfo', '')
                    taxo    = ' / '.join(list(sets.Set(taxos))) 
                    missing = dd.get('missing', '')
                    taxo_l  = ''
                    if e_taxo:
                        taxo    = e_taxo[1]
                        taxo_l  = e_taxo[2]
                    try:
                        taxo    = taxo.decode('utf-8')
                    except:pass

                    try:
                        taxo_l    = taxo_l.decode('utf-8')
                    except:pass
                    #print '\n========================================================'
                    #print [taxo, e_taxo]
            
                    for (table_id, c_id, ph, tlabel, vgh_id) in ks:
                        #print '\t', c_id
                        #if table_id == '77':continue
                        tid, rid, cid   = c_id.split('_')
                        te_taxo = ''
                        if not ijson.get('dtime', ''):
                            #te_taxo  = table_i_d.get(tid, {}).get(rid, '')
                            pass
                        doc_id  = doc_m_d[table_id]
                        x       = txn_m.get('XMLID_'+c_id)
                        gcom    = ''
                        ngcom   = ''
                        if c_id in dd.get('gcom', {}):
                            gcom    = 'Y'
                        if c_id in dd.get('ngcom', {}):
                            ngcom    = 'Y'
                        try:
                            tlabel    = tlabel.decode('utf-8')
                        except:pass
                        #print (g_id, table_id, c_id, ph, e_taxo, taxo)
                        if te_taxo:
                            taxo_l  = ''
                            if te_taxo:
                                taxo    = te_taxo[1]
                                taxo_l  = te_taxo[2]
                            try:
                                taxo    = taxo.decode('utf-8')
                            except:pass

                            try:
                                taxo_l    = taxo_l.decode('utf-8')
                            except:pass
                            i_ar.append((te_taxo[0], ii+1, table_type, taxo, taxo_l, missing, table_id, c_id, ph, tlabel, gcom, ngcom, 'Y', vgh_id, doc_id, prev_id, x, 'SYSTEM', dtime))
                        elif e_taxo:
                            i_ar.append((e_taxo[0], ii+1, table_type, taxo, taxo_l, missing, table_id, c_id, ph, tlabel, gcom, ngcom, 'Y', vgh_id, doc_id, prev_id, x, 'SYSTEM', dtime))
                        else:
                            new_taxo[g_id]    = 1
                            prev_id = -10000
                            i_ar.append((g_id, ii+1, table_type, taxo, '', missing, table_id, c_id, ph, tlabel, gcom, ngcom, 'Y', vgh_id, doc_id, prev_id, x, 'SYSTEM', dtime))
                        #print i_ar[-1][:6]
                    prev_id = g_id
                    g_id    += 1
        mconn.close()
        #for r in i_ar:
        #    print r
        #sys.exit()
        sql = "delete from mt_data_builder where table_type in ('%s')"%(table_type_d.keys()[0])
        cur.execute(sql)
        #print sql, [n_taxos]
        cur.executemany("insert into mt_data_builder(taxo_id, order_id, table_type, taxonomy, user_taxonomy, missing_taxo, table_id, c_id, ph, ph_label, gcom, ngcom, isvisible, vgh_text, doc_id, prev_id, xml_id, user_name, datetime)values(?,?, ?, ?, ?, ?, ?,?, ?, ?, ?, ?, ?,?, ?, ?, ?, ?, ?)", i_ar)
        conn.commit()
        conn.close()
        #sys.exit()
        #import pyapi
        table_type = m_table_type
        pyapi_obj   = self #.PYAPI()
        ijson_c     = copy.deepcopy(ijson)
        ijson_c['dtime']    = n_taxos
        ijson_c['table_type'] = m_table_type
        ijson_c['update_db'] = 'Y'
        ijson_c['taxo_flg'] = 1
        #print ijson_c
        pyapi_obj.re_order_seq(ijson_c)

        t_ijson = copy.deepcopy(ijson)
        t_ijson['dtime']    = n_taxos
        t_ijson['auto_DB']    = 'Y'
        #t_ijson['table_i_d'] = {table_type:1}
        t_ijson['table_types'] = [table_type]
        #import data_builder.taxo_group_update as py
        #tObj = py.TaxoGroup()
        #tObj.create_taxo_grp(t_ijson)
        #ijson_c['c_tids']    = table_i_d.keys()
        #tmpres  = pyapi_obj.create_seq_across(ijson_c)
        #print tmpres
        return []


    def read_table_bbox(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path   = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        #print lmdb_path
        env1        = lmdb.open(lmdb_path, readonly=True)
        txn1        = env1.begin()

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()
        table_ids    = ijson['table_ids']
        final_d     = {}
        for table_id in table_ids:
            table_bbox_d    = {}
            pd  = {}
            for l in ijson.get('SHTYPES', ['GH', 'HGH', 'GV', 'VGH']):
                l = str(l)
                ids = txn_m.get(l+'_'+str(table_id))
                if not ids:continue
                for c_id in ids.split('#'):
                    table_id, r, c  = c_id.split('_')
                    r   = int(r)
                    c   = int(c)
                    x   = txn_m.get('XMLID_'+c_id)
                    x_c = self.clean_xmls(x)
                    if x_c:
                        x_c    = x_c.split(':@:')[0].split('#')[0]
                        pd[int(x_c.split('_')[1].split('@')[0])]   = 1
                    rs  = int(txn_m.get('rowspan_'+c_id))
                    cs  = txn_m.get('colspan_'+c_id)
                    tbox    = self.get_bbox_frm_xml(txn1, table_id, x)
                    table_bbox_d.setdefault(r, {})[c]   = {'bbox':tbox, 'x':x, 'type':l}
            rows = table_bbox_d.keys()
            rows.sort()
            f_ar    = []
            for r in rows:
                cols    = table_bbox_d[r].keys()
                cols.sort()
                tmpcells    = []
                for c in cols:
                    tmpcells.append(table_bbox_d[r][c])
                dd  = {'r':r, 'cols':tmpcells}
                f_ar.append(dd)
            pnos    = pd.keys()
            final_d.setdefault(pnos[0], {})[table_id]   = f_ar
        res = [{'message':'done', 'data':final_d}]
        return res
        
   



    def get_availability_label(self, INPUT_DB_DATA, v_d, table_type, ph, to_dealid, final_d, fdone_d):
        #for k in final_d.keys():
        #    for k1 in final_d[k].keys():
        #        print '\t\tB4 FINAL', k1
        #print [table_type]
        map_d       = {}
        #final_d     = {}
        for findex, frm_d in enumerate(v_d['pos_formula']):
            taxo_grp    = {}
            tmpd    = {}
            #print
            for ft in frm_d['data']:
                #print ft
                if ft['type'] == 't':
                    if 'txid' not in ft and 'taxo_id' in ft:
                        ft['txid']  = ft['taxo_id']
                    val_d   = {}
                    if INPUT_DB_DATA and ft['txid']:
                        pk = INPUT_DB_DATA[2].get((to_dealid, ft['tt'], ft['grpid']), {}).get(ph['ph'])   
                        if pk and pk in INPUT_DB_DATA[1][(to_dealid, ft['tt'], ft['grpid'])][ft['txid']]:
                            for k in ['v', 'd', 'x', 'phcsv', 'bbox', 't']:
                              if k in INPUT_DB_DATA[1][(to_dealid, ft['tt'], ft['grpid'])][ft['txid']][pk]: #error for deal 28
                                val_d[k]    = INPUT_DB_DATA[1][(to_dealid, ft['tt'], ft['grpid'])][ft['txid']][pk][k]
                    new_ft  = val_d
                    for kkk in ['i_f_type', 'type', 'description', 'to_dealid', 'taxo_id', 'tt', 'grpid', 'disp_name']:
                        new_ft[kkk] = ft.get(kkk, '')
                    new_ft['operator']  = ''
                    if ft['operator'] != '=':
                        if 'txid' in ft and ft['txid']:
                            taxo_grp[(to_dealid, ft['tt'], ft['grpid'], ft['txid'])]    = 1
                        else:
                            taxo_grp[(to_dealid, ft['tt'], ft['grpid'], ft['description'])] = 1
                    if not val_d.get('v'):
                        new_ft['not_exists']    = 'Y'
                    if ft.get('i_f_type') != 'PTYPEFORMULA':
                        new_ft['ph']            = ph['ph']
                    else:
                        new_ft['ph']            = ft.get('ph', '')
                    if ft['tt'] != table_type:
                        if ft['txid']:
                            tmpd[(to_dealid, ft['tt'], ft['grpid'], ft.get('txid', ft['description']), new_ft['ph'])]    = new_ft
                        else:
                            tmpd[(to_dealid, ft['tt'], ft['grpid'], ft['description'], new_ft['ph'])]    = new_ft
                    elif 'txid' in ft and ft['txid'] and ft['operator'] != '=':
                        if (to_dealid, ft['tt'], ft['grpid'], ft.get('txid', ft['description'])) not in fdone_d:# and INPUT_DB_DATA:
                            fdone_d[(to_dealid, ft['tt'], ft['grpid'], ft.get('txid', ft['description']))]  = 1
                            pk = INPUT_DB_DATA[2].get((to_dealid, ft['tt'], ft['grpid']), {}).get(ph['ph'])   
                            #print '\t\t\t', [pk, pk in INPUT_DB_DATA[1][(to_dealid, ft['tt'], ft['grpid'])][ft['txid']]]
                            if pk and pk in INPUT_DB_DATA[1][(to_dealid, ft['tt'], ft['grpid'])][ft['txid']]:
                                v_d = INPUT_DB_DATA[1][(to_dealid, ft['tt'], ft['grpid'])][ft['txid']][pk] #.get('pos_formula', [])
                                if v_d.get('pos_formula', []):
                                    tmpfrm_ar   = self.get_availability_label(INPUT_DB_DATA, v_d, table_type, ph, to_dealid, final_d, fdone_d)
            fttype  = 'Contributing Line Items'
            if len(taxo_grp.keys()) == 1:
                fttype  = 'Direct Line Items'
            if tmpd:
                #for k in tmpd.items():
                #    print '\t\tFINAL', k
                final_d.setdefault(fttype, {}).update(tmpd)
        return final_d

    def read_doc_table_info(self, ijson):
        self.dbname = "tfms_urlid"
        dbconnstr = "172.16.20.229#root#tas123"
        self.ip_addr, self.uname, self.passwd = dbconnstr.split('#') 
        import MySQLdb

        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        db=MySQLdb.connect(self.ip_addr,self.uname,self.passwd,self.dbname+"_"+str(project_id)+"_"+str(deal_id)+"")
        cursor=db.cursor()
        if ijson.get('docids'):
            sql = "select d.resid,n.norm_resid,n.docid,n.pageno from norm_data_mgmt n,data_mgmt d where n.process_status= 'Y' and n.active_status= 'Y' and n.review_flag='Y' and d.process_status='Y' and d.active_status='Y' and d.resid=n.ref_resid and d.pageno=n.pageno and d.docid=n.docid and d.taxoname not in ('Grid_Index','Grid@~@Grid Header','Grid@~@Parent Grid Header','Grid@~@Grid Footer','Grid@~@Parent Vertical Header','Grid@~@Vertical Grid Header','Non_Financial_Grid') and n.docid in (%s)"%(', '.join(map(lambda x:str(x), ijson.get('docids'))))
        else:
            sql = "select d.resid,n.norm_resid,n.docid,n.pageno from norm_data_mgmt n,data_mgmt d where n.process_status= 'Y' and n.active_status= 'Y' and n.review_flag='Y' and d.process_status='Y' and d.active_status='Y' and d.resid=n.ref_resid and d.pageno=n.pageno and d.docid=n.docid and d.taxoname not in ('Grid_Index','Grid@~@Grid Header','Grid@~@Parent Grid Header','Grid@~@Grid Footer','Grid@~@Parent Vertical Header','Grid@~@Vertical Grid Header','Non_Financial_Grid')"
        cursor.execute(sql)
        results = cursor.fetchall()
        db.close()
        norm_resid_d = {}
        for r in results:
            resid = str(r[0])
            norm_resid = str(r[1])
            docid = str(r[2])
            pageno = str(r[3])
            norm_resid_d[str(norm_resid)] = (str(docid), pageno)
        table_ids = norm_resid_d.keys()
        table_ids.sort(key=lambda x:(norm_resid_d[x][0], int(norm_resid_d[x][1])))
        f_ar    = []

        lmdb_path   = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        #print lmdb_path
        env1        = lmdb.open(lmdb_path, readonly=True)
        txn1        = env1.begin()
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()
        bbox_d  = {}
        for table_id in table_ids:
            docid, pageno   = norm_resid_d[table_id]
            table_bbox_d    = {}
            for l in ['GH', 'HGH', 'GV', 'VGH']:
                l = str(l)
                ids = txn_m.get(l+'_'+str(table_id))
                if not ids:continue
                for c_id in ids.split('#'):
                    table_id, r, c  = c_id.split('_')
                    x   = txn_m.get('XMLID_'+c_id)
                    tbox    = self.get_bbox_frm_xml(txn1, table_id, x)
                    table_bbox_d[x]   = {'bbox':tbox, 'x':x, 'type':l}
            bbox_d[table_id]  = table_bbox_d
            dd  = {'t':table_id, 'd':docid, 'pno':pageno}
            f_ar.append(dd)
        res = [{"message":"done","data":f_ar, 'bbox':bbox_d}]
        return res
    
    def get_all_active_docs(self, ijson):
        self.dbname = "tfms_urlid"
        dbconnstr = "172.16.20.229#root#tas123"
        self.ip_addr, self.uname, self.passwd = dbconnstr.split('#') 
        import MySQLdb

        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)

        db=MySQLdb.connect(self.ip_addr,self.uname,self.passwd,self.dbname+"_"+str(project_id)+"_"+str(deal_id)+"")
        cursor=db.cursor()
        read_qry = """select document_name,document_id from ir_document_master where active_status='Y';"""
        cursor.execute(read_qry)
        table_data = cursor.fetchall()
        db.close()
        res_lst = []
        for idx, row in enumerate(table_data, 1):
            dt = {'sn':idx, 'd':row[1], 'dn':row[0]}
            res_lst.append(dt)
        res = [{"message":"done","data":res_lst}]
        return res

    def print_table_rc_string_html(self, ijson):
        company_name, model_number, deal_id, project_id, company_id = self.parse_ijson(ijson)
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn         = env.begin()

        final_d     = {}
        #m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, [])    
        final_html  = ''
        width       = {'HGH':'width:200px', 'GV':'width:70px'}
        for table_id in ijson['table_ids']:
            table_str   = txn_m.get('TABLE_STR_'+str(table_id))
            row         = txn_m.get('ROWS_'+str(table_id))
            col         = txn_m.get('COLS_'+str(table_id))
            if table_str:
                table_str   = binascii.a2b_hex(table_str)
                row         = len(row.split('#'))     
                col         = len(col.split('#'))     
                final_d[table_id]   = {'html':table_str, 'rows':row, 't':table_id, 'cols':col}
        table_ids   = final_d.keys()
        table_ids.sort(key=lambda x:(final_d[x]['rows'], final_d[x]['cols']), reverse=True)
        ar  = []
        for table_id in table_ids:
            ar.append(final_d[table_id])
        res = [{'message':'done','data':ar}]
        return res
        


    def populate_review_meta_data(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        m_tables, rev_m_tables, doc_m_d, table_type_m = self.get_main_table_info(company_name, model_number)
        self.dbname = "tfms_urlid"
        dbconnstr = "172.16.20.229#root#tas123"
        self.ip_addr, self.uname, self.passwd = dbconnstr.split('#') 
        import MySQLdb
        db=MySQLdb.connect(self.ip_addr,self.uname,self.passwd,self.dbname+"_"+str(project_id)+"_"+str(deal_id)+"")
        cursor=db.cursor()
        sql = "select d.resid,n.norm_resid,n.docid,n.pageno from norm_data_mgmt n,data_mgmt d where n.process_status= 'Y' and n.active_status= 'Y' and n.review_flag='Y' and d.process_status='Y' and d.active_status='Y' and d.resid=n.ref_resid and d.pageno=n.pageno and d.docid=n.docid and d.taxoname not in ('Grid_Index','Grid@~@Grid Header','Grid@~@Parent Grid Header','Grid@~@Grid Footer','Grid@~@Parent Vertical Header','Grid@~@Vertical Grid Header','Non_Financial_Grid')"
        cursor.execute(sql)
        results = cursor.fetchall()
        db.close()
        norm_resid_d = {}
        for r in results:
            resid = str(r[0])
            norm_resid = str(r[1])
            docid = str(r[2])
            pageno = str(r[3])
            norm_resid_d[str(norm_resid)] = (str(docid), pageno)

        db_file = '/mnt/eMB_db/%s/%s/tas_company.db'%(company_name, model_number)
        sheet_id_map = self.sheet_id_map()
        conn, cur   = conn_obj.sqlite_connection(db_file)
        
        doc_d   = {}
        sql     = 'select doc_id, table_id, sheet_id from table_group_mapping'
        cur.execute(sql)
        res = cur.fetchall()
        table_types = {}
        for r in res:   
            doc_id, table_id, sheet_id  = r
            main_header, sub_header, node_name, description = sheet_id_map.get(str(sheet_id), ['','','',''])
            if table_id:
                for tid in table_id.split('^!!^'):
                    if tid:
                        doc_d.setdefault(str(doc_id), {'tids':{},'tt':{}})['tt'].setdefault(node_name, []).append({'t':tid})
                        doc_d[str(doc_id)]['tids'][tid]  = 1
        sql     = "select doc_id, doc_name, document_type, filing_type, period, reporting_year from company_meta_info"
        cur.execute(sql)
        res     = cur.fetchall()
        cur.close()
        conn.close()
        f_ar    = []
        sn  = 1
        res  = list(res)
        phs = {}
        for r in res:
            doc_id, doc_name, document_type, filing_type, period, reporting_year    = r
            phs[period+str(reporting_year)] = 1
            tinfo       = doc_d.get(str(doc_id), {}).get('tt', {})
            tables      = len(doc_d.get(str(doc_id), {}).get('tids', {}).keys())
            for tt, tids in tinfo.items():
                table_types[tt] = max(table_types.get(tt, 0), len(tids))
            dd          = {'d':doc_id, 'n':doc_name, 'type':document_type, 'ph':period+str(reporting_year), 'tinfo':tinfo, 'c':tables, 'period':period, 'reporting_year':reporting_year, 'filing_type':filing_type}
            #print dd
            f_ar.append(dd)

        

        db_file = '/mnt/rMB_db/%s/%s/tas_company.db'%(company_name, model_number)
        os.system("mkdir -p /mnt/rMB_db/%s/%s/"%(company_name, model_number))
        conn, cur   = conn_obj.sqlite_connection(db_file)
        sql = "DROP TABLE IF EXISTS document_mgmt"
        cur.execute(sql)
        sql = "CREATE TABLE document_mgmt (row_id INTEGER PRIMARY KEY AUTOINCREMENT, doc_id INTEGER, doc_name TEXT, period varchar(256), year INTEGER, filing_type TEXT, doc_type TEXT)"
        cur.execute(sql)

        sql = "DROP TABLE IF EXISTS table_mgmt"
        cur.execute(sql)
        sql = "CREATE TABLE table_mgmt (row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_id INTEGER, doc_id INTEGER, pageno INTEGER, classification TEXT)"
        cur.execute(sql)

        di_ar    = []
        ti_ar    = []
        for r in f_ar:
            doc_name    = r['n']
            doc_type    = r['type']
            doc_id      = r['d']
            period      = r['period']
            year        = r['reporting_year']
            filing_type = r['filing_type']
            di_ar.append((doc_id, doc_name, doc_type, period, year, filing_type))
            for tt, tinfo in r['tinfo'].items():
                for tr in tinfo:
                    table_id    = tr['t']
                    page_no = norm_resid_d[table_id][1]
                    ti_ar.append((table_id, doc_id, page_no, tt))
        cur.executemany("insert into document_mgmt(doc_id, doc_name, doc_type, period, year, filing_type)values(?, ?, ?, ?, ?,?)", di_ar)
        cur.executemany("insert into table_mgmt(table_id, doc_id, pageno, classification)values(?, ?, ?, ?)", ti_ar)
        conn.commit()
        conn.close()

    def cread_error_types(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        fltr_key        = ijson['key']
        db_file = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        if fltr_key == 'GROUP':
            sql = "select row_id, error_type, error_msg, table_type, group_id, group_name, color from error_class where error_type in ('Group','Spike')"
        elif fltr_key == 'ALL':
            sql = sql = "select row_id, error_type, error_msg, table_type, group_id, group_name, color from error_class where error_type not in ('Group', 'Spike')"
        cur.execute(sql)
        res1 = cur.fetchall()
        cur.close()
        conn.close()    
        ar  = []
        ar  = []
        e_d = {}
        for r in res1:
            row_id, error_type, error_msg, table_type, group_id, group_name, color   = r
            e_d.setdefault((error_type, error_msg), {}).setdefault(table_type, []).append((row_id, group_id, group_name, color))
        rid = 1
        for etup, ttypes in e_d.items():
            dd  = {'n':etup[1], 'k':etup[0]}
            sl  = []        
            for t, grpinfo in ttypes.items():
                tmpdata = []
                for (row_id, group_id, group_name, clr) in grpinfo:
                    if clr == 'G':
                        clr = 'grn'
                    elif clr == 'O':
                        clr = 'org'
                    elif clr == 'R':
                        clr == 'red'
                    tdd  = {'n':group_name, 'k':rid,'idata':str(row_id), 'table_type':t,'grpid':group_id, 'clr':clr}
                    rid += 1
                    tmpdata.append(tdd)
                tdd = {'n':t, 'k':rid,'idata':tmpdata}
                rid += 1
                sl.append(tdd)
            dd['sl']    = sl
            ar.append(dd)
        orderd = {
                    'WPH'   : 1, 
                    'DBDL'  : 2, 
                    'LPOSS' : 3,
                    'Spike' : 4,
                    'R_CHECKSUM' : 5,
                    'Group' : 9999,
                }
        ar.sort(key=lambda x:orderd.get(x['k'], 9999))
        if ar:
            ar[0]['f']  = 1
        res = [{"message":'done', 'data':ar}]
        return res

    def read_error_types(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        fltr_key        = ijson['key']
        db_file = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        self.alter_table_coldef(conn, cur, 'error_class', ['done_status', 'update_flg'])
        if fltr_key == 'GROUP':
            sql = "select row_id, error_type, error_msg, table_type, group_id, group_name, color, done_status, update_flg from error_class where error_type in ('Group','Spike', 'Line-Item-Group') and error_type not like 'TALLY-%' "
        elif fltr_key == 'ALL':
            sql = "select row_id, error_type, error_msg, table_type, group_id, group_name, color, done_status, update_flg from error_class where error_type not in ('Group', 'Spike', 'Line-Item-Group') and error_type not like 'TALLY-%' "
        elif fltr_key == 'TALLY':
            sql = "select row_id, error_type, error_msg, table_type, group_id, group_name, color, done_status, update_flg from error_class where error_type like 'TALLY-%'"
        #print sql
        cur.execute(sql)
        res1 = cur.fetchall()
        cur.close()
        conn.close()    
        if ijson.get('from_demo') == 'Y' and fltr_key == 'TALLY':
            res1    = [('TALLY-ALL', 'TALLY-ALL', 'TALLY-ALL', 'TALLY-All', '', 'TALLY-All', '', '','')]
        ar  = []
        ar  = []
        e_d = {}
        for r in res1:
            row_id, error_type, error_msg, table_type, group_id, group_name, color, done_status, update_flg   = r
            if not update_flg:
                update_flg  = 'N'
            if not done_status:
                done_status = 'N'
            e_d.setdefault((error_type, error_msg), {}).setdefault(table_type, []).append((row_id, group_id, group_name, color, done_status, update_flg))
        rid = 1
        for etup, ttypes in e_d.items():
            dd  = {'n':etup[1], 'k':etup[0]}
            sl  = []        
            f_flg   = {}
            d_flg   = {}
            for t, grpinfo in ttypes.items():
                tmpdata = []
                for (row_id, group_id, group_name, clr, dn_st, update_flg) in grpinfo:
                    d_flg[dn_st]    = 1
                    f_flg[update_flg]   = 1
                    if clr == 'G':
                        clr = 'grn'
                    elif clr == 'O':
                        clr = 'org'
                    elif clr == 'R':
                        clr == 'red'
                    if not dn_st:
                        dn_st = 'N'
                    tdd  = {'n':group_name, 'k':rid,'idata':str(row_id), 'table_type':t,'grpid':group_id, 'clr':clr, 'dn_st':dn_st, 'status_flg':update_flg}
                    rid += 1
                    tmpdata.append(tdd)
                tdd = {'n':t, 'k':rid,'idata':tmpdata}
                rid += 1
                sl.append(tdd)
            if 'U' in f_flg:
                dd['status_flg']    = 'U'
            else:
                dd['status_flg']    = 'N'
            if 'N' not in d_flg:
                dd['dn_st']    = 'Y'
            dd['sl']    = sl
            ar.append(dd)
        order_ar    = [
                        'FORMULA_OVERLAP',
                        'WPH',      
                        'WTPH',      
                        'NOT-REPORTED',
                        'RESTATEMENT-BEFORE_REPORTED',
                        'FPOSS',
                        'DBDL',
                        'WMerge',
                        'WMergeAcross',
                        'LPOSS',
                        'R_CHECKSUM',
                        'CSV',
                        'Sign_Change',
                        'Sign_Change_Pattern',
                        'VGHPATTERN',
                        'Spike',
                        'Group',
                        ]
        ar.sort(key=lambda x:order_ar.index(x['k']) if x['k'] in order_ar else 9999)
        if ar:
            ar[0]['f']  = 1
        res = [{"message":'done', 'data':ar}]
        return res
        
    def done_status_update(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        row_id          = ijson["idata"]
        db_file = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        update_qry = """UPDATE error_class SET done_status='Y' WHERE row_id='%s'"""%(row_id)
        conn.commit()
        conn.close()
        return [{"message":'done'}]

    def store_error_update_info(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        user_name       = ijson['user'] 
        save_data       = ijson['data']
        table_type      = ijson['table_type']
        group_id        = ijson['grpid']
        source_row_id   = ijson['idata']
        error_type      = ijson['etype']
        action          = ijson['action']
        dt_time         = str(datetime.datetime.now())
        data_prep       = (source_row_id, error_type, table_type, group_id, save_data, user_name, dt_time, action)
        db_file = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        crt_qry = """CREATE TABLE IF NOT EXISTS user_action(row_id INTEGER PRIMARY KEY AUTOINCREMENT, source_row_id TEXT, error_type TEXT, table_type TEXT, group_id TEXT, info BLOB, user_name TEXT, date_time TEXT)"""
        cur.execute(crt_qry)
        self.alter_table_coldef(conn, cur, 'user_action', ['action'])
        #del_stmt = """DELETE FROM user_action WHERE source_row_id='%s'"""%(source_row_id)
        #cur.execute(del_stmt)
        insert_stmt = """INSERT INTO user_action(source_row_id, error_type, table_type, group_id, info, user_name, date_time, action) VALUES('%s', '%s', '%s', '%s', "%s", '%s', '%s', '%s')"""%(data_prep)
        cur.execute(insert_stmt)
        update_stmt = """UPDATE error_class SET edit_flag='Y' WHERE row_id='%s'"""%(source_row_id)
        cur.execute(update_stmt)
        conn.commit()
        conn.close()
        return [{'message':'done'}]

    def read_data_builder_data_loss_info_update(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        user_name       = ijson['user'] 
        table_type      = ijson['table_type']
        group_id        = ijson['grpid']
        source_row_id   = ijson['idata']
        db_file = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        crt_qry = """CREATE TABLE IF NOT EXISTS user_action(row_id INTEGER PRIMARY KEY AUTOINCREMENT, source_row_id TEXT, error_type TEXT, table_type TEXT, group_id TEXT, info BLOB, user_name TEXT, date_time TEXT)"""
        cur.execute(crt_qry)
        conn.commit() 
        read_qry = """SELECT info FROM user_action WHERE source_row_id='%s'"""%(source_row_id)
        cur.execute(read_qry)
        
        t_data = cur.fetchone()
        if not t_data:  
            info = ""
        else:
            info = eval(t_data[0])
        conn.close()
        return [{'message':'done', 'data':info}]

    def read_data_builder_data_loss_info(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        user_name       = ijson['user'] 
        table_type      = ijson['table_type']
        if ijson.get('idata') == 'TALLY-ALL':[{'message':'done', 'data':[]}]
        group_id        = ijson['grpid']
        source_row_id   = ijson['idata']
        db_file = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        #crt_qry = """CREATE TABLE IF NOT EXISTS user_action(row_id INTEGER PRIMARY KEY AUTOINCREMENT, source_row_id TEXT, error_type TEXT, table_type TEXT, group_id TEXT, info BLOB, user_name TEXT, date_time TEXT)"""
        #cur.execute(crt_qry)
        #conn.commit() 
        read_qry = """SELECT info FROM user_action WHERE source_row_id='%s'"""%(source_row_id)
        
        try:
            cur.execute(read_qry)
            t_data = cur.fetchall()
        except:
            t_data  = []
        if not t_data:  
            info = ""
        else:
            info = []
            for rw in t_data:
                info.append(eval(rw[0]))
        conn.close()
        return [{'message':'done', 'data':info}]

    def read_user_corrected_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        #db_file = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        #conn, cur       = conn_obj.sqlite_connection(db_file)
        #self.alter_table_coldef(conn, cur, 'error_class', ['edit_flg'])
        #sql = "select row_id, error_type, error_msg, table_type, group_id, group_name, data from error_class where row_id=%s and edit_flg='Y'"%(ijson['idata'])
        #cur.execute(sql)
        #res1 = cur.fetchone()
        #cur.close()
        #conn.close()    
        #if not res1:return [{"message":""}]
        #row_id, error_type, error_msg, table_type, group_id, group_name, data   = res1
        #ijson['idata']  = [table_type, group_id, eval(data)]
        if ijson['etype'] == 'Group':
            return self.read_data_builder_data_loss_info(ijson)
            #return [{'message':'done', 'data':''}]
        elif ijson['etype'] == 'WPH':
            return self.read_data_builder_data_loss_info(ijson)
            #return [{'message':'done', 'data':''}]
        elif ijson['etype'] == 'DBDL':
            #return self.read_user_selected_col_map_info(ijson)
            return self.read_data_builder_data_loss_info(ijson)
        elif ijson['etype'] == 'Spike':
            return self.read_data_builder_data_loss_info(ijson)
            #return [{'message':'done', 'data':''}]
        elif ijson['etype'] == 'LPOSS':
            return self.read_data_builder_data_loss_info(ijson)
        elif ijson['etype'] == 'R_CHECKSUM':
            return self.read_data_builder_data_loss_info(ijson)
            #return [{'message':'done', 'data':''}]
        elif ijson['etype'] == 'FORMULA_OVERLAP':
            return self.read_data_builder_data_loss_info(ijson)
            #return [{'message':'done', 'data':''}]
        elif ijson['etype'] == 'Sign_Change':
            return self.read_data_builder_data_loss_info(ijson)
            #return [{'message':'done', 'data':''}]


    def read_error_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        ijson['irowid'] = '%s^^%s'%(company_id, ijson['idata'])
        db_file = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        if ijson['idata'] == 'TALLY-ALL':
            return self.read_tally_all_data(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        read_qry = "SELECT info FROM user_action WHERE source_row_id='%s'"%(ijson['idata'])
        try:
            cur.execute(read_qry)
            t_data = cur.fetchall()
        except:
            t_data  = []
        if not t_data:  
            info = []
        else:
            info = []
            for rw in t_data:
                info.append(eval(rw[0]))
        sql = "select row_id, error_type, error_msg, table_type, group_id, group_name, data from error_class where row_id=%s"%(ijson['idata'])
        cur.execute(sql)
        res1 = cur.fetchone()
        cur.close()
        conn.close()    
        if not res1:return [{"message":"No data"}]
        row_id, error_type, error_msg, table_type, group_id, group_name, data   = res1
        ijson['idata']  = [table_type, group_id, eval(data)]
        ijson['user_data']  =  info
        ijson['etype']  =  error_type
        if ijson['etype'] == 'Group':
            return self.read_group_data(ijson)
        elif ijson['etype'] == 'WPH':
            return self.read_ph_error_data(ijson)
        elif ijson['etype'] == 'DBDL':
            return self.read_ph_data_loss(ijson)
        elif ijson['etype'] == 'Spike':
            return self.read_spike_error_data(ijson)
        elif ijson['etype'] == 'LPOSS':
            return self.read_lpos_error_data(ijson)
        elif ijson['etype'] == 'R_CHECKSUM':
            return self.read_r_checksum_error_data(ijson)
        elif ijson['etype'] == 'CSV':
            return self.read_csv_error_data(ijson)
        elif ijson['etype'] == 'FORMULA_OVERLAP':
            return self.read_frm_overlap_error_data(ijson)
        elif ijson['etype'] == 'Sign_Change':
            return self.read_sign_change_error_data(ijson)
        elif ijson['etype'] == 'Sign_Change_Pattern':
            return self.read_sign_change_error_data(ijson)
        elif ijson['etype'] == 'WMerge':
            return self.read_wrong_merge_error_data(ijson)
        elif ijson['etype'] == 'WMergeAcross':
            return self.read_wrong_merge_across_error_data(ijson)
        elif ijson['etype'] == 'VGHPATTERN':
            return self.read_vgh_pattern_error_data(ijson)
        elif ijson['etype'] == 'NOT-REPORTED':
            return self.read_reported_ph_error_data(ijson)
        elif ijson['etype'] == 'RESTATEMENT-BEFORE_REPORTED':
            return self.read_rs_b4_reported_ph_error_data(ijson)
        elif ijson['etype'] == 'FPOSS':
            return self.read_formula_poss_error_data(ijson)
        elif ijson['etype'] == 'Line-Item-Group':
            return self.read_group_error_data(ijson)
        elif ijson['etype'] == 'WFPOSS':
            return self.read_wrong_usr_formula_poss_error_data(ijson)
        elif ijson['etype'] == 'WCleanup':
            return self.read_wrong_cleanup_error_data(ijson)
        elif ijson['etype'] == 'WTPH':
            return self.read_table_ph_error_data(ijson)
        elif ijson['etype'] in ('TALLY-EQUAL', 'TALLY-CF-EQUAL'):
            return self.read_tally_equal_data(ijson)
        elif ijson['etype'] in ('TALLY-SCHEDULE', 'TALLY-SCHEDULE-MUL', 'TALLY-GCOMP2', 'TALLY-GCOMP3'):
            return self.read_tally_multi_data(ijson)
        elif ijson['etype'] == 'RWR':
            return self.read_RWR_error_data(ijson)
        elif ijson['etype'] == 'TALLY-ALL':
            return self.read_tally_all_data(ijson)

    def read_RWR_error_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        table_type, grpid, kstr = ijson['idata']
        table_type  = str(table_type)
        grpid       = str(grpid)
        phs         = eval(txn.get('PHS:%s^^%s'%((table_type, grpid))))
        tids        = eval(txn.get('TAXOIDS:%s^^%s'%((table_type, grpid))))
        allpk       = {}
        p_d         = {}
        for k in kstr:
            allpk[k]   = 1
        data    = []
        ph_d    = {}
        ptypes = {}
        for ph in phs:
            ph_d[ph['k']]   = ph
            ptypes[ph['n']] = 1
        sn  = 1
        for tid in tids:
            tkstr    = '%s^^%s^^%s'%(table_type, grpid, tid)
            row     = eval(txn.get(str(tkstr)) )
            fkey    = 0
            for ph in phs:
                if ph['k'] in row and ph['k'] not in allpk:
                    del row[ph['k']]
                if ph['k'] in allpk:
                    fkey    = 1
            if fkey == 0:continue       
            row['sn']   = sn
            sn  += 1
            data.append(row)
        phs = filter(lambda x:x['k'] in allpk, phs)
        self.add_num_group(data, phs, txn)
        res = [{"message":"done", "data":data, 'phs':phs}]
        if ijson.get('from_review') == 'Y':
            ref_data_d  = {}
            rid = 0
            for rr in data:
                for k in rr.keys():
                    if ':$$:' in str(k):
                        del rr[k]
                for ph in phs:
                    if ph['k'] in rr:
                        tmpdd   = {}
                        for k1 in self.priority_ks:
                            if k1 in rr[ph['k']]:
                                tmpdd[k1] = rr[ph['k']][k1]
                        if rr[ph['k']].get('f_col'):
                            tmpdd['f']  = 'Y'
                        key = 'REF_DATA_%s^^%s'%(ijson['irowid'], rid)
                        tmpdd['r_key']    = key
                        rid += 1
                        ref_data_d[key] = rr[ph['k']]
                        rr[ph['k']] = tmpdd
            return res, ref_data_d
        return res

    def read_tally_equal_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        table_type, grpid, kstr = ijson['idata']
        grpid    = eval(grpid)
        (table_type, grpid), gname1, (ttype, gid), grpname2 = grpid
        phs         = eval(txn.get('PHS:%s^^%s'%((table_type, grpid))))
        #kstr    = {taxoId^pk: $$.join(['@@'.join([taxoid, ttype, grpid, ftype, c,s, v, pk, ph])), }
        #tkstr    = '%s^^%s^^%s'%(table_type, grpid, tid)
        #row     = eval(txn.get(str(tkstr)) )
        res_lst = []
        dd  = {}
        n_d = {}
        for resultant, opr_info in kstr.iteritems():
            rtx_id, rpk, rph, res_flg = resultant.split('^')
            op_lst  = []
            pklist  = []
            for opr in opr_info.split('$$'):
                #otx_id, ac_opr, ttype, gid, ftype, c,s, v, pk, ph = opr.split('@@')
                dt = opr.split('@@')
                if len(dt) == 10:
                    otx_id, ac_opr, ttype, gid, ftype, c,s, v, pk, ph = dt
                elif len(dt) == 11:
                    otx_id, ac_opr, ttype, gid, ftype, c,s, v, pk, ph, flg = dt
                pklist.append((pk, ph))
                op_lst.append((ttype, gid, otx_id, ac_opr))
            n_d.setdefault(rtx_id, {}).setdefault(tuple(op_lst), {})[rpk]   = pklist
        pi_d    = {}
        for i, ph in enumerate(phs):
            pi_d[ph['k']]    = (i, ph['n'])
            
        all_k   = {}
        sn  = 1
        for rtx_id, opr_info in n_d.iteritems():
            res_tkstr    = '%s^^%s^^%s'%(table_type, grpid, rtx_id)
            #print '\nres_tkstr :', res_tkstr
            res_row     = eval(txn.get(str(res_tkstr)))
            for op, mop_d  in  opr_info.items():
                op_d    = {}
                op_rd   = {}
                for i, top in enumerate(op):
                    ttype, gid, otx_id, ac_opr  = top 
                    opr_tkstr    = '%s^^%s^^%s'%(ttype, gid, otx_id)
                    if opr_tkstr not in dd:
                        opr_row      = eval(txn.get(str(opr_tkstr)))
                        dd[opr_tkstr]   = opr_row
                    else:
                        opr_row     = dd[opr_tkstr]
                    op_rd[i]    = opr_row
                    op_d[i]    = {'t_l':'%s - %s'%(ttype, opr_row['t_l']), 't_id':opr_row['t_id']}
                row1    = {'t_l':'%s-%s'%(table_type, res_row['t_l']), 't_id':res_row['t_id'], 'f':'Y'}
                rpks    = mop_d.keys()
                rpks.sort(key=lambda x:pi_d[x][0])
                for ii, rpk in enumerate(rpks): 
                    pklist  = mop_d[rpk]
                    all_k[ii]   = 1
                    row1['C_%s'%(ii)]   = {'v':res_row[rpk]['v'], 'x':res_row[rpk]['x'], 't':res_row[rpk]['t'], 'd':res_row[rpk]['d'], 'bbox':res_row[rpk]['bbox']}   #res_row[rpk] #v, x, t, d, bbox, 
                    row1['CPH_%s'%(ii)] = {'v':'(%s) %s'%('=', pi_d[rpk][1]), 'f':'Y'}
                    f = 'Y'
                    f_col = []
                    f_col.append({'txid':str(row1['t_id']), 'taxo_id':str(row1['t_id']), 'type':'t','t_type':table_type, 'operator':'=', 'ph':'', 'k':rpk,  'yd':0, 'i_f_type':'PTYPEFORMULA', 'tt':table_type, 'grpid':grpid, 'clean_value': res_row[rpk]['v'], 'description':row1['t_l']})
                    for i, opk in enumerate(pklist):
                        ttype, gid, otx_id, ac_opr  = op[i]
                        opk, oph    = opk
                        f_col.append({'txid':otx_id, 'taxo_id':str(otx_id), 'type':'t','t_type':ttype, 'operator':ac_opr, 'ph':oph, 'k':opk,  'yd':0, 'i_f_type':'PTYPEFORMULA', 'tt':ttype, 'grpid':gid, 'clean_value': op_rd[i][opk]['v'], 'description':row1['t_l']})
                        op_d[i]['C_%s'%(ii)]    = {'v':op_rd[i][opk]['v'], 'x':op_rd[i][opk]['x'], 't':op_rd[i][opk]['t'], 'd':op_rd[i][opk]['d'], 'bbox':op_rd[i][opk]['bbox']}
                        op_d[i]['CPH_%s'%(ii)] = {'v':'(%s) %s'%(ac_opr, oph)}
                    row1['C_%s'%(ii)]['C_%s'%(ii)]  = [f_col]
                    row1['C_%s'%(ii)]['f']  = 'Y'
                grp_ar  = []
                oplist  = []
                for i, top in enumerate(op):
                    ttype, gid, otx_id, ac_opr  = top 
                    oplist.append((ttype, gid, otx_id, ac_opr))
                    row = op_d[i]
                    row['sn']   = sn
                    sn  += 1
                    #grp_ar.append({'t_l':'%s-%s'%(ttype, gid)})
                    grp_ar.append(row)
                row1['sn']  = sn
                row1['opinfo']  = oplist
                row1['rpinfo']  = [table_type, grpid, rtx_id]
                row1['']  = oplist
                sn  += 1
                #grp_ar.append({'t_l':'%s-%s'%(table_type, grpid)})
                grp_ar.append(row1)
                res_lst.append((len(rpks), grp_ar))
        f_ar    = []
        sn  = 1
        res_lst.sort(key=lambda x:x[0], reverse=True)
        for ix, grp in enumerate(res_lst, 1):
            for r in grp[1]:
                r['sn'] = sn
                r['t_id'] = '%s-%s'%(r['t_id'], str(ix))
                sn  += 1
                f_ar.append(r)
            f_ar.append({'sn':sn, 'empty_row':"Y"})
            sn  += 1
        phs = []
        ks  = all_k.keys()
        ks.sort()
        for k in ks:
            dd = {'k':'C_%s'%(k), 'n':'COL-%s'%(k), 'g':''}
            phs.append(dd)
            dd = {'k':'CPH_%s'%(k), 'n':'COL PH-%s'%(k), 'g':''}
            phs.append(dd)
            phs.append({'n':' ', 'k':'EMPTY', 'g':' '})
        data    = f_ar
        res = [{'message':'done', 'data':data, 'phs':phs}]     
        if ijson.get('from_review') == 'Y':
            ref_data_d  = {}
            rid = 0
            for rr in data:
                for k in rr.keys():
                    if ':$$:' in str(k):
                        del rr[k]
                for ph in phs:
                    if ph['k'] in rr:
                        tmpdd   = {}
                        for k1 in self.priority_ks:
                            if k1 in rr[ph['k']]:
                                tmpdd[k1] = rr[ph['k']][k1]
                        if rr[ph['k']].get('f_col'):
                            tmpdd['f']  = 'Y'
                        key = 'REF_DATA_%s^^%s'%(ijson['irowid'], rid)
                        tmpdd['r_key']    = key
                        rid += 1
                        ref_data_d[key] = rr[ph['k']]
                        rr[ph['k']] = tmpdd
            return res, ref_data_d
        return res

    def read_tally_all_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        f_path = '/var/www/html/prashant/tally_info_%s.txt'%(company_id)
        try:
                return open(f_path, 'r').read()
        except:pass
        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        db_path = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
        conn = sqlite3.connect(db_path)
        cur  = conn.cursor()
        
        sql = "select row_id, error_type, error_msg, table_type, group_id, group_name, data from error_class where error_type like 'TALLY-%'"
        cur.execute(sql)
        res1 = cur.fetchall()
        cur.close()
        conn.close()
        if not res1:return [{"message":"No data"}]
        all_k   = {}
        res_lst = []
        for r in res1:
            row_id, error_type, error_msg, table_type, group_id, group_name, data   = r
            kstr = eval(data)   
            if error_type in ['TALLY-EQUAL', 'TALLY-CF-EQUAL']:
                grpid    = eval(group_id)
                (table_type, group_id), gname1, (ttype, gid), grpname2 = grpid
            
            table_type, grpid = map(str, [table_type, group_id])
            phs         = eval(txn.get('PHS:%s^^%s'%((table_type, grpid))))
            #kstr    = {taxoId^pk: $$.join(['@@'.join([taxoid, ttype, grpid, ftype, c,s, v, pk, ph])), }
            #tkstr    = '%s^^%s^^%s'%(table_type, grpid, tid)
            #row     = eval(txn.get(str(tkstr)) )
            dd  = {}
            n_d = {}
            #print (error_type, table_type, grpid, group_name)
            for resultant, opr_info_lst in kstr.iteritems():
                #print resultant.split('^')
                rtx_id, rpk, rph = resultant.split('^')[:3]
                for opr_info in [opr_info_lst]:   
                    op_lst  = []
                    pklist  = []
                    for opr in opr_info.split('$$'):
                        #otx_id, ac_opr, ttype, gid, ftype, c,s, v, pk, ph = opr.split('@@')
                        dt = opr.split('@@')
                        otx_id, ac_opr, ttype, gid, ftype, c,s, v, pk, ph = dt[:10]
                        pklist.append((pk, ph))
                        op_lst.append((ttype, gid, otx_id, ac_opr))
                    n_d.setdefault(rtx_id, {}).setdefault(tuple(op_lst), {})[rpk]   = pklist
            #sys.exit() #{'': [('', '146-1-')]}
            pi_d    = {}
            for i, ph in enumerate(phs):
                pi_d[ph['k']]    = (i, ph['n'])
                
            sn  = 1
            for rtx_id, opr_info in n_d.iteritems():
                res_tkstr    = '%s^^%s^^%s'%(table_type, grpid, rtx_id)
                #print '\nres_tkstr :', res_tkstr
                res_row     = eval(txn.get(str(res_tkstr)))
                for op, mop_d  in  opr_info.items():
                    op_d    = {}
                    op_rd   = {}
                    for i, top in enumerate(op):
                        ttype, gid, otx_id, ac_opr  = top 
                        opr_tkstr    = '%s^^%s^^%s'%(ttype, gid, otx_id)
                        if opr_tkstr not in dd:
                            opr_row      = eval(txn.get(str(opr_tkstr)))
                            dd[opr_tkstr]   = opr_row
                        else:
                            opr_row     = dd[opr_tkstr]
                        op_rd[i]    = opr_row
                        op_d[i]    = {'t_l':'%s - %s'%(ttype, opr_row['t_l']), 't_id':opr_row['t_id']}
                    row1    = {'t_l':'%s-%s'%(table_type, res_row['t_l']), 't_id':res_row['t_id'], 'f':'Y'}
                    #print  mop_d.keys()
                    rpks    = filter(lambda x:x in pi_d, mop_d.keys())
                    #if not rpks:
                    #
                    #    sys.exit()
                    rpks.sort(key=lambda x:pi_d.get(x, [9999])[0])
                    for ii, rpk in enumerate(rpks): 
                        pklist  = mop_d[rpk]
                        all_k[ii]   = 1
                        row1['C_%s'%(ii)]   = {'v':res_row[rpk]['v'], 'x':res_row[rpk]['x'], 't':res_row[rpk]['t'], 'd':res_row[rpk]['d'], 'bbox':res_row[rpk]['bbox']}   #res_row[rpk] #v, x, t, d, bbox, 
                        row1['CPH_%s'%(ii)] = {'v':'(%s) %s'%('=', pi_d[rpk][1]), 'f':'Y'}
                        f = 'Y'
                        f_col = []
                        f_col.append({'txid':str(row1['t_id']), 'taxo_id':str(row1['t_id']), 'type':'t','t_type':table_type, 'operator':'=', 'ph':'', 'k':rpk,  'yd':0, 'i_f_type':'PTYPEFORMULA', 'tt':table_type, 'grpid':grpid, 'clean_value': res_row[rpk]['v'], 'description':row1['t_l']})
                        for i, opk in enumerate(pklist):
                            ttype, gid, otx_id, ac_opr  = op[i]
                            opk, oph    = opk
                            #print 'ss', [opk, '>>', i, '>>>', op_rd]
                            f_col.append({'txid':otx_id, 'taxo_id':str(otx_id), 'type':'t','t_type':ttype, 'operator':ac_opr, 'ph':oph, 'k':opk,  'yd':0, 'i_f_type':'PTYPEFORMULA', 'tt':ttype, 'grpid':gid, 'clean_value': op_rd[i][opk]['v'], 'description':row1['t_l']})
                            #except:print 'ns',[opk, '>>', i, '>>>', op_rd]
                            
                            op_d[i]['C_%s'%(ii)]    = {'v':op_rd[i][opk]['v'], 'x':op_rd[i][opk]['x'], 't':op_rd[i][opk]['t'], 'd':op_rd[i][opk]['d'], 'bbox':op_rd[i][opk]['bbox']}
                            op_d[i]['CPH_%s'%(ii)] = {'v':'(%s) %s'%(ac_opr, oph)}
                        row1['C_%s'%(ii)]['C_%s'%(ii)]  = [f_col]
                        row1['C_%s'%(ii)]['f']  = 'Y'
                    grp_ar  = []
                    for i, top in enumerate(op):
                        ttype, gid, otx_id, ac_opr  = top 
                        row = op_d[i]
                        row['sn']   = sn
                        sn  += 1
                        #grp_ar.append({'t_l':'%s-%s'%(ttype, gid)})
                        grp_ar.append(row)
                    row1['sn']  = sn
                    sn  += 1
                    #grp_ar.append({'t_l':'%s-%s'%(table_type, grpid)})
                    grp_ar.append(row1)
                    res_lst.append((len(rpks), grp_ar))
        f_ar    = []
        sn  = 1
        res_lst.sort(key=lambda x:x[0], reverse=True)
        for grp in res_lst[:500]:
            for r in grp[1]:
                r['sn'] = sn
                sn  += 1
                f_ar.append(r)
            f_ar.append({'sn':sn, 'empty_row':"Y"})
        phs = []
        ks  = all_k.keys()
        ks.sort()
        for k in ks:
            dd = {'k':'C_%s'%(k), 'n':'COL-%s'%(k), 'g':''}
            phs.append(dd)
            dd = {'k':'CPH_%s'%(k), 'n':'COL PH-%s'%(k), 'g':''}
            phs.append(dd)
            phs.append({'n':' ', 'k':'EMPTY', 'g':' '})
        data    = f_ar
        res = [{'message':'done', 'data':data, 'phs':phs}]     
        if ijson.get('from_review') == 'Y':
            ref_data_d  = {}
            rid = 0
            for rr in data:
                for k in rr.keys():
                    if ':$$:' in str(k):
                        del rr[k]
                for ph in phs:
                    if ph['k'] in rr:
                        tmpdd   = {}
                        for k1 in self.priority_ks:
                            if k1 in rr[ph['k']]:
                                tmpdd[k1] = rr[ph['k']][k1]
                        if rr[ph['k']].get('f_col'):
                            tmpdd['f']  = 'Y'
                        key = 'REF_DATA_%s^^%s'%(ijson['irowid'], rid)
                        tmpdd['r_key']    = key
                        rid += 1
                        ref_data_d[key] = rr[ph['k']]
                        rr[ph['k']] = tmpdd
            #print 'HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH'
            #return res, ref_data_d
        f_path = '/var/www/html/prashant/tally_info_%s.txt'%(company_id)
        f = open(f_path, 'w')
        f.write(json.dumps(res))
        f.close()
        #print 'SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS'
        return res
    
    def read_taxo_wise_tt(self, company_name, model_number):    
        db_path = '/mnt/eMB_db/%s/%s/taxo_data_builder.db'%(company_name, model_number)
        conn = sqlite3.connect(db_path) 
        cur  = conn.cursor()
        read_qry = """  SELECT taxo_id, table_type FROM mt_data_builder """
        cur.execute(read_qry)       
        t_data = cur.fetchall()
        conn.close()
        res_dct = {}    
        for row in t_data:
            res_dct[str(row[0])] = str(row[1])
        return res_dct

    def read_tally_multi_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        table_type, grpid, kstr = ijson['idata']
        table_type, grpid = map(str, [table_type, grpid])
        phs         = eval(txn.get('PHS:%s^^%s'%((table_type, grpid))))
        #kstr    = {taxoId^pk: $$.join(['@@'.join([taxoid, ttype, grpid, ftype, c,s, v, pk, ph])), }
        #tkstr    = '%s^^%s^^%s'%(table_type, grpid, tid)
        #row     = eval(txn.get(str(tkstr)) )
    
        r_dct = self.read_taxo_wise_tt(company_name, model_number)

        res_lst = []
        dd  = {}
        n_d = {}
        for resultant, opr_info in kstr.iteritems():
            rtx_id, rpk, rph = resultant.split('^')
            op_lst  = []
            pklist  = []
            for opr in opr_info.split('$$'):
                #otx_id, ac_opr, ttype, gid, ftype, c,s, v, pk, ph = opr.split('@@')
                dt = opr.split('@@')
                if len(dt) == 10:
                    otx_id, ac_opr, ttype, gid, ftype, c,s, v, pk, ph = dt
                pklist.append((pk, ph))
                ttype = r_dct[otx_id]
                op_lst.append((ttype, gid, otx_id, ac_opr))
            n_d.setdefault(rtx_id, {}).setdefault(tuple(op_lst), {})[rpk]   = pklist
        pi_d    = {}
        for i, ph in enumerate(phs):
            pi_d[ph['k']]    = (i, ph['n'])
            
        all_k   = {}
        sn  = 1
        for rtx_id, opr_info in n_d.iteritems():
            res_tkstr    = '%s^^%s^^%s'%(table_type, grpid, rtx_id)
            #print '\nres_tkstr :', res_tkstr
            res_row     = eval(txn.get(str(res_tkstr)))
            for op, mop_d  in  opr_info.items():
                op_d    = {}
                op_rd   = {}
                for i, top in enumerate(op):
                    ttype, gid, otx_id, ac_opr  = top 
                    opr_tkstr    = '%s^^%s^^%s'%(ttype, gid, otx_id)
                    if opr_tkstr not in dd:
                        opr_row      = eval(txn.get(str(opr_tkstr)))
                        dd[opr_tkstr]   = opr_row
                    else:
                        opr_row     = dd[opr_tkstr]
                    op_rd[i]    = opr_row
                    op_d[i]    = {'t_l':'%s - %s'%(ttype, opr_row['t_l']), 't_id':opr_row['t_id']}
                row1    = {'t_l':'%s-%s'%(table_type, res_row['t_l']), 't_id':res_row['t_id'], 'f':'Y'}
                rpks    = mop_d.keys()
                rpks.sort(key=lambda x:pi_d[x][0])
                for ii, rpk in enumerate(rpks): 
                    pklist  = mop_d[rpk]
                    all_k[ii]   = 1
                    row1['C_%s'%(ii)]   = {'v':res_row[rpk]['v'], 'x':res_row[rpk]['x'], 't':res_row[rpk]['t'], 'd':res_row[rpk]['d'], 'bbox':res_row[rpk]['bbox']}   #res_row[rpk] #v, x, t, d, bbox, 
                    row1['CPH_%s'%(ii)] = {'v':'(%s) %s'%('=', pi_d[rpk][1]), 'f':'Y'}
                    f = 'Y'
                    f_col = []
                    f_col.append({'txid':str(row1['t_id']), 'taxo_id':str(row1['t_id']), 'type':'t','t_type':table_type, 'operator':'=', 'ph':'', 'k':rpk,  'yd':0, 'i_f_type':'PTYPEFORMULA', 'tt':table_type, 'grpid':grpid, 'clean_value': res_row[rpk]['v'], 'description':row1['t_l']})
                    for i, opk in enumerate(pklist):
                        ttype, gid, otx_id, ac_opr  = op[i]
                        opk, oph    = opk
                        f_col.append({'txid':otx_id, 'taxo_id':str(otx_id), 'type':'t','t_type':ttype, 'operator':ac_opr, 'ph':oph, 'k':opk,  'yd':0, 'i_f_type':'PTYPEFORMULA', 'tt':ttype, 'grpid':gid, 'clean_value': op_rd[i][opk]['v'], 'description':row1['t_l']})
                        op_d[i]['C_%s'%(ii)]    = {'v':op_rd[i][opk]['v'], 'x':op_rd[i][opk]['x'], 't':op_rd[i][opk]['t'], 'd':op_rd[i][opk]['d'], 'bbox':op_rd[i][opk]['bbox']}
                        op_d[i]['CPH_%s'%(ii)] = {'v':'(%s) %s'%(ac_opr, oph)}
                    row1['C_%s'%(ii)]['C_%s'%(ii)]  = [f_col]
                    row1['C_%s'%(ii)]['f']  = 'Y'
                grp_ar  = []
                for i, top in enumerate(op):
                    ttype, gid, otx_id, ac_opr  = top 
                    row = op_d[i]
                    row['sn']   = sn
                    sn  += 1
                    #grp_ar.append({'t_l':'%s-%s'%(ttype, gid)})
                    grp_ar.append(row)
                row1['sn']  = sn
                sn  += 1
                #grp_ar.append({'t_l':'%s-%s'%(table_type, grpid)})
                grp_ar.append(row1)
                res_lst.append((len(rpks), grp_ar))
        f_ar    = []
        sn  = 1
        res_lst.sort(key=lambda x:x[0], reverse=True)
        for grp in res_lst:
            for r in grp[1]:
                r['sn'] = sn
                sn  += 1
                f_ar.append(r)
            f_ar.append({'sn':sn, 'empty_row':"Y"})
        phs = []
        ks  = all_k.keys()
        ks.sort()
        for k in ks:
            dd = {'k':'C_%s'%(k), 'n':'COL-%s'%(k), 'g':''}
            phs.append(dd)
            dd = {'k':'CPH_%s'%(k), 'n':'COL PH-%s'%(k), 'g':''}
            phs.append(dd)
            phs.append({'n':' ', 'k':'EMPTY', 'g':' '})
        data    = f_ar
        res = [{'message':'done', 'data':data, 'phs':phs}]     
        if ijson.get('from_review') == 'Y':
            ref_data_d  = {}
            rid = 0
            for rr in data:
                for k in rr.keys():
                    if ':$$:' in str(k):
                        del rr[k]
                for ph in phs:
                    if ph['k'] in rr:
                        tmpdd   = {}
                        for k1 in self.priority_ks:
                            if k1 in rr[ph['k']]:
                                tmpdd[k1] = rr[ph['k']][k1]
                        if rr[ph['k']].get('f_col'):
                            tmpdd['f']  = 'Y'
                        key = 'REF_DATA_%s^^%s'%(ijson['irowid'], rid)
                        tmpdd['r_key']    = key
                        rid += 1
                        ref_data_d[key] = rr[ph['k']]
                        rr[ph['k']] = tmpdd
            return res, ref_data_d
        return res
        

    def read_table_ph_error_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        table_type, grpid, kstr = ijson['idata']
        table_type  = str(table_type)
        grpid       = str(grpid)
        phs         = eval(txn.get('PHS:%s^^%s'%((table_type, grpid))))
        tids        = eval(txn.get('TAXOIDS:%s^^%s'%((table_type, grpid))))
        allpk       = {}
        p_d         = {}
        clr_map_d   = {
                        0:51,
                        1:52,
                        2:53,
                        }
        tid_d   = {}
        for i, k in enumerate(kstr[:]):
            for k1 in k:
                allpk[k1]   = clr_map_d[i]
        data    = []
        ph_d    = {}
        ptypes = {}
        for ph in phs:
            if ph['k'] in allpk:# and allpk[ph['k']] in [51, 53]:
                tid_d[ph['g']] = 1
        for ph in phs:
            ph_d[ph['k']]   = ph
            if ph['k'] in allpk:
                ph['clr']   = allpk[ph['k']]
            if ph['g'] in tid_d:
                allpk[ph['k']]  = ''
            ptypes[ph['n']] = 1
        sn  = 1
        for tid in tids:
            tkstr    = '%s^^%s^^%s'%(table_type, grpid, tid)
            row     = eval(txn.get(str(tkstr)) )
            fkey    = 0
            for ph in phs:
                if ph['k'] in row and ph['k'] not in allpk:
                    del row[ph['k']]
                if ph['k'] in allpk:
                    fkey    = 1
            if fkey == 0:continue       
            row['sn']   = sn
            sn  += 1
            data.append(row)
        phs = filter(lambda x:x['k'] in allpk, phs)
        prev_grp    = ''
        tmpphs  = []
        for ph in phs:
            tmpphs.append(ph)
            if prev_grp and prev_grp != ph['g']:
                tmpphs.append({'n':' ', 'k':'EMPTY', 'g':' '})
            prev_grp    = ph['g']
        phs = tmpphs  
                
        self.add_num_group(data, phs, txn)
        res = [{"message":"done", "data":data, 'phs':phs}]
        if ijson.get('from_review') == 'Y':
            ref_data_d  = {}
            rid = 0
            for rr in data:
                for k in rr.keys():
                    if ':$$:' in str(k):
                        del rr[k]
                for ph in phs:
                    if ph['k'] in rr:
                        tmpdd   = {}
                        for k1 in self.priority_ks:
                            if k1 in rr[ph['k']]:
                                tmpdd[k1] = rr[ph['k']][k1]
                        if rr[ph['k']].get('f_col'):
                            tmpdd['f']  = 'Y'
                        key = 'REF_DATA_%s^^%s'%(ijson['irowid'], rid)
                        tmpdd['r_key']    = key
                        rid += 1
                        ref_data_d[key] = rr[ph['k']]
                        rr[ph['k']] = tmpdd
            return res, ref_data_d
        return res

    def read_wrong_cleanup_error_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        table_type, grpid, kstr = ijson['idata']
        table_type  = str(table_type)
        grpid       = str(grpid)
        phs         = eval(txn.get('PHS:%s^^%s'%((table_type, grpid))))
        tids        = eval(txn.get('TAXOIDS:%s^^%s'%((table_type, grpid))))
        allpk       = {}
        tids    = kstr.keys()
        tids.sort(key=lambda x:tids.index(x) if x in tids else 9999999)
        f_pat   = 0
        sn  = 1
        data    = []
        ph_d    = {}
        for tid in tids:
            tkstr    = '%s^^%s^^%s'%(table_type, grpid, tid)
            row     = eval(txn.get(str(tkstr)) )
            fkey    = 0
            for ph in phs:
                if ph['k'] in row:
                    if ph['k'] not in kstr[tid]:
                        del row[ph['k']]
                    else:
                        ph_d[ph['k']]    = 1
                        row[ph['k']+'_clean']   = {'v':kstr[tid][ph['k']], 'clr':'R'}
                        fkey    = 1
            if fkey == 0:continue       
            row['sn']   = sn
            sn  += 1
            data.append(row)
        tmpph   = []
        for ph in phs:
            if ph['k'] not in ph_d:continue
            tmpph.append(ph)
            tmpph.append({'n':'Clean Value', 'g':ph['g'], 'k':ph['k']+'_clean'})
            tmpph.append({'n':' ', 'k':'EMPTY', 'g':' '})
        phs = tmpph
        self.add_num_group(data, phs, txn)
        res = [{"message":"done", "data":data, 'phs':phs}]
        if ijson.get('from_review') == 'Y':
            ref_data_d  = {}
            rid = 0
            for rr in data:
                for k in rr.keys():
                    if ':$$:' in str(k):
                        del rr[k]
                for ph in phs:
                    if ph['k'] in rr:
                        tmpdd   = {}
                        for k1 in self.priority_ks:
                            if k1 in rr[ph['k']]:
                                tmpdd[k1] = rr[ph['k']][k1]
                        if rr[ph['k']].get('f_col'):
                            tmpdd['f']  = 'Y'
                        key = 'REF_DATA_%s^^%s'%(ijson['irowid'], rid)
                        tmpdd['r_key']    = key
                        rid += 1
                        ref_data_d[key] = rr[ph['k']]
                        rr[ph['k']] = tmpdd
            return res, ref_data_d
        return res


    def read_wrong_usr_formula_poss_error_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        table_type, grpid, kstr = ijson['idata']
        table_type  = str(table_type)
        grpid       = str(grpid)
        phs         = eval(txn.get('PHS:%s^^%s'%((table_type, grpid))))
        tids       = eval(txn.get('TAXOIDS:%s^^%s'%((table_type, grpid))))
        order_d = {}
        for i, tid in enumerate(tids):
            order_d[tid]    = i
        data    = []
        sn      = 1
        ph_d    = {}
        grpd    = {}
        pk_grpd    = {}
        tgrps   = kstr
        for i, tids in enumerate(tgrps):
            tids, pk_d = tids
            for k, tids in pk_d.items():
                ph_d[k]   = 1
                grpd[k] = grpd.get(k, len(grpd.keys())+1)
                done_d  = {}
                for tgrp in tids:
                    if tuple(tgrp) in done_d:continue
                    done_d[tuple(tgrp)]    = 1
                    for tid in tgrp:
                        pk_grpd[(str(tid), k)]= grpd[k]
                
        for i, tids in enumerate(tgrps):
            tids, pk_d = tids
            tids.sort(key=lambda x:order_d[x])
            if i != 0:
                data.append({'sn':sn, 'empty_row':"Y"})
                sn  +=1
            for tid in tids:
                tkstr    = '%s^^%s^^%s'%(table_type, grpid, tid)
                row     = eval(txn.get(str(tkstr)) )
                row['sn']   = sn
                for ph in phs:
                    if ph['k'] in row:
                        if (tid, ph['k']) in pk_grpd:
                            row[ph['k']]['clr'] = pk_grpd[(tid, ph['k'])]
                sn  += 1
                data.append(row)
        phs = filter(lambda x:x['k'] in ph_d, phs)
        self.add_num_group(data, phs, txn)
        res = [{"message":"done", "data":data, 'phs':phs}]
        if ijson.get('from_review') == 'Y':
            ref_data_d  = {}
            rid = 0
            for rr in data:
                for k in rr.keys():
                    if ':$$:' in str(k):
                        del rr[k]
                for ph in phs:
                    if ph['k'] in rr:
                        tmpdd   = {}
                        for k1 in self.priority_ks:
                            if k1 in rr[ph['k']]:
                                tmpdd[k1] = rr[ph['k']][k1]
                        if rr[ph['k']].get('f_col'):
                            tmpdd['f']  = 'Y'
                        key = 'REF_DATA_%s^^%s'%(ijson['irowid'], rid)
                        tmpdd['r_key']    = key
                        rid += 1
                        ref_data_d[key] = rr[ph['k']]
                        rr[ph['k']] = tmpdd
            return res, ref_data_d
        return res




    def read_formula_poss_error_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        to_dealid  = int(deal_id)

        table_type, grpid, kstr = ijson['idata']
        table_type  = str(table_type)
        grpid       = str(grpid)
        phs         = eval(txn.get('PHS:%s^^%s'%((table_type, grpid))))
        tids       = eval(txn.get('TAXOIDS:%s^^%s'%((table_type, grpid))))
        reported_phs         = eval(txn.get('REPORTED_COL:%s^^%s'%((table_type, grpid))))
        rp_ks   = {}
        for ph in phs:
            rp_ks[ph['k']]   = ph
        ijson_c = copy.deepcopy(ijson)
        ijson_c['from_backend']   = 'Y'
        ijson_c['iph']            = reported_phs.keys()
        data    = []
        order_d = {}
        for ii, tid in enumerate(tids):
            order_d[str(tid)] = ii
            tkstr   = '%s^^%s^^%s'%(table_type, grpid, tid)
            rr      = eval(txn.get(str(tkstr)) )
            data.append(rr)
        chk_sum, final_d    = self.calculate_preview_col(ijson_c, data, [], reported_phs, table_type, grpid, txn)
        #print final_d.keys()
        new_data_d  = {}
        fphs    = {}
        for rr in data:
            row = {'t_l':rr['t_l'], 't_id':rr['t_id']}
            fval    = 0
            for ph in reported_phs.keys():
                k   = 'FINAL_'+ph
                if k in final_d.get(rr['t_id'], {}):
                    fphs[ph]    = 1
                    row[ph]  = final_d[rr['t_id']][k]
                    for k1 in ['f_col', 'c_s', 'f', 'expr_str', 'expr_val']:
                        if k1 in row[ph]:
                            del row[ph][k1] 
                    fval    = 1
                    for tk in reported_phs[ph]:
                        if tk in final_d.get(rr['t_id'], {}):
                            row[tk]  = final_d[rr['t_id']][tk]
                            for k1 in ['f_col', 'c_s', 'f', 'expr_str', 'expr_val']:
                                if k1 in row[tk]:
                                    del row[tk][k1] 
                        
            #if fval == 1:
            #print '\n', [rr['t_id'], rr['t_l'], fval, row.keys()]
            new_data_d[str(rr['t_id'])]  = row
        ph_order    = report_year_sort.year_sort(fphs)
        ph_order.reverse()
        phs = []
        for ph in ph_order:
            for tk in reported_phs[ph]:
                dd  = {'k':tk, 'g':ph, 'n':ph}
                phs.append(dd)
            dd  = {'k':ph, 'g':ph, 'n':ph, 'review':'Y'}
            phs.append(dd)
        phs_d       = {}
        fdata      = []
        pk_d        = {}
        f_checksum  = {}
        tsn         = 1
        ttype       = table_type
        ijson['table_type'] = table_type
        ijson['grpid'] = grpid
        dealijson   = self.read_company_info({"cids":[str(deal_id)]})
        f_phs   = []
        for ph in phs:
            ph['ph']    = ph['k']
            f_phs.append(ph)
        final_ar    = []
        for (tids, i_ks) in kstr:
            ttype_d = {}
            data    = []
            for tid in tids:
                tr  = copy.deepcopy(new_data_d[tid])
                ttype_d.setdefault((to_dealid, table_type, grpid), {})[str(tid)]    = tr
                data.append(tr)
            rr      = ttype_d[(to_dealid, table_type, grpid)][tids[-1]]
            ks  = {}
            for ph in phs:
                if rr.get(ph['k'], {}).get('v'):
                    ks[ph['k']]  = ph['n']
            if ks:
                k   = ks.keys()[0]
                ph = {'k':k, 'n':ks[k]}
                f_ar    = []
                dd  = {'txid':tids[-1], 'type':'t', 't_type':ttype, 'g_id':grpid, 'op':'=', 'c':'', 's':'','vt':'', 'rid':'RID-0', 'to_dealid':to_dealid, 'i_f_type':'FORMULA', 'ph':ph['n']}
                f_ar.append(dd)
                for tid in tids[:-1]:
                    dd  = {'txid':tid, 'type':'t', 't_type':ttype, 'g_id':grpid, 'op':'+', 'c':'', 's':'','vt':'', 'rid':'RID-0', 'to_dealid':to_dealid, 'i_f_type':'FORMULA', 'ph':ph['n']}
                    f_ar.append(dd)
                ph_formula_d    = {('F', tids[-1]):('RID-0', f_ar), ('ALL_F', tids[-1]):{tuple(tids):('RID-0', f_ar)}}
                xml_row_map     = {}
                txn1             = {}
                taxo_id_dict    = {}
                taxo_value_grp  = {}
                clean_v_d       = {}
                f_k_d           = {ph['k']:{}}
                key_map_rev, key_map_rev_t, disp_name, row_ind_map, csv_d, update_form,display_name_d, ph_afftected_tids  = {}, {}, table_type, {}, {}, 'F', {}, {}
                t_ids   = {}
                xml_col_map = {}
                t_op_d, flip_sign_d, done_form_d, f_pc_d = self.formula_derivations(data, [{'data':data, 'phs':phs, 'dont_derive':'Y'}], f_phs, ph_formula_d, xml_row_map, xml_col_map, txn1, t_ids, taxo_id_dict, taxo_value_grp, clean_v_d, f_k_d, key_map_rev, key_map_rev_t, disp_name, row_ind_map, csv_d, update_form,phs_d, ttype_d, ijson, display_name_d, dealijson, ph_afftected_tids, {} )
            
                f_c = {}
                #print
                i_ks    = []
                d_ks    = []
                for ph in phs:
                    #print '\t', ph, ph['k'] in ks, [rr.get(ph['k'], {}).get('v'), rr.get(ph['k'], {}).keys()]
                    if ph['k'] in rr and ph.get('review') == 'Y':# and rr[ph['k']]['v'] and rr[ph['k']].get('f_col'):
                        #print '\tCHK ', [rr[ph['k']].get('c_s')]
                        rr[ph['k']]['f']    = 'Y'
                        rr['f']    = 'Y'
                        if rr[ph['k']].get('c_s'):
                            
                            f_c.setdefault('Y', {})[ph['k']]    = 1
                            pk_d[ph['k']]   = 0
                        elif ph['k'] not in pk_d:
                            i_ks.append(ph['k'])
                            f_c.setdefault('N', {})[ph['k']]    = 1
                            pk_d[ph['k']]   = 1
                        else:
                            i_ks.append(ph['k'])
                            f_c.setdefault('N', {})[ph['k']]    = 1
                    if ph['k'] not in rr or not rr[ph['k']].get('v'):
                        d_ks.append(ph['k'])
                    #    del rr[ph['k']]
                #print f_c.keys()
                color   = 'G'
                if 'Y' in f_c and 'N' in f_c:
                    color   = 'O'
                elif 'Y' in f_c:
                    color   = 'R'
                elif not f_c:
                    continue
                    color   = 'R'
                f_checksum[color] = 1
                t_ar    = []    
                tmplst  = tids[:-1]
                tmplst.sort(key=lambda x:order_d[x])
                for tid in tmplst+[tids[-1]]:
                    tr  = ttype_d[(to_dealid, table_type, grpid)][tid]
                    tr['clr']   = color
                    for pk in f_checksum.get('Y', {}).keys():
                        if pk in tr:
                            tr[pk]['clr']   = 'R'   
                    for ik in i_ks:
                        if ik in tr:
                            tr[ik]['clr']   = 1   
                    for ik in d_ks:
                        if ik in tr:
                            del tr[ik]
                    t_ar.append(tr)
                #fdata.append({'sn':tsn, 'empty_row':"Y"})
                #tsn += 1
                final_ar.append((color,  ph['k'], t_ar))
        if ijson.get('from_backend') == 'Y':
            return  f_checksum
        final_ar.sort(key=lambda x:({'G':2, 'O':1, 'R':0}[x[0]], len(x[1])), reverse=True)
        sn  = 1
        for ii, tup in enumerate(final_ar):
            for tr in tup[2]:
                #ks  = filter(lambda x:':$$:' in x, tr.keys())
                #for k in ks:
                #    del tr[k]
                tr['sn']    = sn
                tr['t_grpid']    = ii+1
                sn  += 1
                fdata.append(tr)
            fdata.append({'sn':sn, 'empty_row':"Y"})
            sn += 1
        tmpphs  = []
        for ph in phs:
            if pk_d.get(ph['k']) == 0:
                ph['c_s']   = 'Y'
            if ph['k'] in rp_ks:
                ph['n'] = rp_ks[ph['k']]['g']
            tmpphs.append(ph)
            if ph['n'] == ph['k']:
                ph['n'] = 'Review'
                tmpphs.append({'n':' ', 'k':'EMPTY', 'g':' '})
        self.add_num_group(fdata, tmpphs, txn)
        res = [{"message":"done","data":fdata,"phs":tmpphs}]
        if ijson.get('from_review') == 'Y':
            ref_data_d  = {}
            rid = 0
            for rr in fdata:
                for k in rr.keys():
                    if ':$$:' in str(k):
                        del rr[k]
                for ph in tmpphs:
                    if ph['k'] in rr:
                        tmpdd   = {}
                        for k1 in self.priority_ks:
                            if k1 in rr[ph['k']]:
                                tmpdd[k1] = rr[ph['k']][k1]
                        if rr[ph['k']].get('f_col'):
                            tmpdd['f']  = 'Y'
                        key = 'REF_DATA_%s^^%s'%(ijson['irowid'], rid)
                        tmpdd['r_key']    = key
                        rid += 1
                        ref_data_d[key] = rr[ph['k']]
                        rr[ph['k']] = tmpdd
            return res, ref_data_d
        return res
            
                
            

    def read_rs_b4_reported_ph_error_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        table_type, grpid, kstr = ijson['idata']
        table_type  = str(table_type)
        grpid       = str(grpid)
        phs         = eval(txn.get('PHS:%s^^%s'%((table_type, grpid))))
        tids        = eval(txn.get('TAXOIDS:%s^^%s'%((table_type, grpid))))
        reported_phs         = eval(txn.get('REPORTED_COL:%s^^%s'%((table_type, grpid))))
        allpk   = {}
        ph_d    = {}
        dph_d    = {}
        for ph in phs:
            ph_d.setdefault(ph['n'], {})[ph['k']]   = 1
            dph_d.setdefault('%s%s'%ph['dph'], {})[ph['k']]   = 1
        for phtup in kstr:
            for ph in phtup[0] :
                for pk in reported_phs.get(ph[1], []):
                    allpk[pk]   = 1
            for pk in phtup[1] :
                    allpk[pk]   = 0
        sn  = 1
        data    = []
        found_value = {}
        for tid in tids:
            tkstr    = '%s^^%s^^%s'%(table_type, grpid, tid)
            row     = eval(txn.get(str(tkstr)) )
            fkey    = 0
            for ph in phs:
                if ph['k'] in row:
                    if ph['k'] not in allpk:
                        del row[ph['k']]
                    else:#if tid in allpk.get(ph['k'], {}):
                        if row[ph['k']]['v']:
                            found_value[ph['k']]    = 1
                            fkey    = 1
            if fkey == 0:continue       
            row['sn']   = sn
            sn  += 1
            data.append(row)
        tmpphs  = []
        
        grp_d   = {}
        for ph in phs:
            if ph['k'] not in found_value:continue
            if ph['k'] not in allpk:continue
            if allpk[ph['k']]   == 0:
                ph['c_s']   = 'Y'
            if ph['n']:
                grp_d.setdefault(ph['n'], []).append(ph)
        ph_order    = report_year_sort.year_sort(grp_d.keys())
        ph_order.reverse()
        for tph in ph_order:
            for ph in grp_d[tph]:
                g   = ph['g']
                ph['g']     = ph['n']
                ph['n']     = g
                tmpphs.append(ph)
            tmpphs.append({'n':' ', 'k':'EMPTY', 'g':' '})
        self.add_num_group(data, tmpphs, txn)
            
            
        res = [{"message":"done", "data":data, 'phs':tmpphs}]
        if ijson.get('from_review') == 'Y':
            ref_data_d  = {}
            rid = 0
            for rr in data:
                for k in rr.keys():
                    if ':$$:' in str(k):
                        del rr[k]
                for ph in tmpphs:
                    if ph['k'] in rr:
                        tmpdd   = {}
                        for k1 in self.priority_ks:
                            if k1 in rr[ph['k']]:
                                tmpdd[k1] = rr[ph['k']][k1]
                        if rr[ph['k']].get('f_col'):
                            tmpdd['f']  = 'Y'
                        key = 'REF_DATA_%s^^%s'%(ijson['irowid'], rid)
                        tmpdd['r_key']    = key
                        rid += 1
                        ref_data_d[key] = rr[ph['k']]
                        rr[ph['k']] = tmpdd
            return res, ref_data_d
        return res

    def read_reported_ph_error_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        table_type, grpid, kstr = ijson['idata']
        table_type  = str(table_type)
        grpid       = str(grpid)
        phs         = eval(txn.get('PHS:%s^^%s'%((table_type, grpid))))
        tids        = eval(txn.get('TAXOIDS:%s^^%s'%((table_type, grpid))))
        reported_phs         = eval(txn.get('REPORTED_COL:%s^^%s'%((table_type, grpid))))
        allpk   = {}
        ph_d    = {}
        dph_d    = {}
        for ph in phs:
            ph_d.setdefault(ph['n'], {})[ph['k']]   = 1
            dph_d.setdefault('%s%s'%ph['dph'], {})[ph['k']]   = 1
        for phtup in kstr:
            for ph in phtup[0] :
                for pk in dph_d.get(ph, []):
                    allpk[pk]   = 1
        sn  = 1
        data    = []
        found_value = {}
        for tid in tids:
            tkstr    = '%s^^%s^^%s'%(table_type, grpid, tid)
            row     = eval(txn.get(str(tkstr)) )
            fkey    = 0
            for ph in phs:
                if ph['k'] in row:
                    if ph['k'] not in allpk:
                        del row[ph['k']]
                    else:#if tid in allpk.get(ph['k'], {}):
                        if row[ph['k']]['v']:
                            found_value[ph['k']]    = 1
                            fkey    = 1
            if fkey == 0:continue       
            row['sn']   = sn
            sn  += 1
            data.append(row)
        tmpphs  = []
        
        grp_d   = {}
        for ph in phs:
            if ph['k'] not in found_value:continue
            if ph['k'] not in allpk:continue
            if allpk[ph['k']]   == 0:
                ph['c_s']   = 'Y'
            if '%s%s'%ph['dph']:
                grp_d.setdefault('%s%s'%ph['dph'], []).append(ph)
        ph_order    = report_year_sort.year_sort(grp_d.keys())
        ph_order.reverse()
        for tph in ph_order:
            for ph in grp_d[tph]:
                tmpphs.append(ph)
            tmpphs.append({'n':tph, 'k':'FINAL_'+tph, 'g':tmpphs[-1]['g'], 'c_s':'Y'})
            tmpphs.append({'n':' ', 'k':'EMPTY', 'g':' '})
            
            
        self.add_num_group(data, tmpphs, txn)
        res = [{"message":"done", "data":data, 'phs':tmpphs}]
        if ijson.get('from_review') == 'Y':
            ref_data_d  = {}
            rid = 0
            for rr in data:
                for k in rr.keys():
                    if ':$$:' in str(k):
                        del rr[k]
                for ph in tmpphs:
                    if ph['k'] in rr:
                        tmpdd   = {}
                        for k1 in self.priority_ks:
                            if k1 in rr[ph['k']]:
                                tmpdd[k1] = rr[ph['k']][k1]
                        if rr[ph['k']].get('f_col'):
                            tmpdd['f']  = 'Y'
                        key = 'REF_DATA_%s^^%s'%(ijson['irowid'], rid)
                        tmpdd['r_key']    = key
                        rid += 1
                        ref_data_d[key] = rr[ph['k']]
                        rr[ph['k']] = tmpdd
            return res, ref_data_d
        return res

    def read_vgh_pattern_error_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)


        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()

        lmdb_path   = '/var/www/html/Rajeev/BBOX/'+str(project_id)+'_'+str(deal_id)
        lmdb_path    = os.path.join(self.bbox_path, company_id, 'XML_BBOX')
        #print lmdb_path
        env1    = lmdb.open(lmdb_path, readonly=True)
        txn1    = env1.begin()

        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/ph_csv_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn         = env.begin()

        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        table_type, grpid, kstr = ijson['idata']
        import data_builder.db_data as db_data
        obj = db_data.PYAPI()
        m_tables, rev_m_tables, doc_m_d,table_type_m = self.get_main_table_info(company_name, model_number, [table_type])
        ignore_col  = obj.get_ignore_cols(company_name, model_number, company_id, txn_m)
        g_ar            = obj.read_all_vgh_groups(table_type, company_name, model_number, doc_m_d)
        grp_map_d   = {}
        grp_info    = {}
        for rr in g_ar:
            if 'HGHGROUP' in rr['n']:continue
            grp_info[('GRP', rr['grpid'])] = rr['n']
            for table_id, col_d  in rr['vids'][0].items():
                for col in col_d.keys():
                    key = str(table_id)+'-'+str(col)
                    grp_map_d[key]  = rr['grpid']
        r_grpd      = {}
        fdata       = []
        data        = []
        
        tsn         = 1
        final_grpd  = {}
        colids  = {}
        for pat, table_ids in kstr.items():
            for table_id in table_ids:
                if not doc_m_d.get(table_id, ''):continue
                get_all_cids = txn_m.get('COLS_'+str(table_id))
                if get_all_cids == None:continue
                tdd = {}
                cid = 1
                coluids = map(lambda x:int(x), get_all_cids.split('#'))
                coluids.sort()
                for colid in coluids:
                    colid   = str(colid)
                
                    #print table_id, colid, int(colid) in ignore_col.get(str(table_id), {})
                    if int(colid) in ignore_col.get(str(table_id), {}):continue
                    
                    vgh_txt = txn_m.get('COLTEXT_'+table_id+'_'+colid)
                    if vgh_txt == None:#continue
                        vgh_txt = 'No VGH for Column' 
                        table_vgh_xml = 'No XML'
                        bbox          = []
                        table_col = '-'.join([table_id, colid])
                    else:
                        vgh_txt = binascii.a2b_hex(vgh_txt)
                        table_vgh_xml = txn_m.get('COLXMLID_'+table_id+'_'+colid)
                        bbox    = self.get_bbox_frm_xml(txn1, table_id, table_vgh_xml)
                        table_col = '-'.join([table_id, colid])
                    if table_col in grp_map_d:
                        ktup    = grp_map_d[table_col]
                        r_grpd['User']    = 1
                    else:
                        ngrp    = self.get_quid(' '.join(vgh_txt.lower().split()).strip())
                        ktup    = 'NEW_'+ngrp
                        grp_info[('GRP', ktup)] = vgh_txt
                        r_grpd['System']    = 1
                    if 'NEW_' not in ktup:
                        color   = final_grpd.get(ktup, len(final_grpd.keys())+1)
                    else:
                        color   = 'R'
                    final_grpd[ktup]   = color
                    #print '\t', ktup
                    colids[cid]  = 1
                    tdd[cid]  = {'x':table_vgh_xml, 't':table_id, 'v':table_col, 'c':table_col, 'bbox':bbox, 'd':doc_m_d.get(table_id, ''), 'clr':color, 'grpid':ktup}
                    cid += 1
                if tdd:
                    dd  = {'t_l':'T: %s, D: %s'%(table_id, doc_m_d[table_id]), 't_id':table_id, 'sn':tsn}
                    dd.update(tdd)
                    tsn += 1
                    data.append(dd)
            data.append({'sn':tsn, 'empty_row':"Y"})
            tsn += 1
        grps    = colids.keys()
        grps.sort()
        phs     = []
        for grp in grps:
            dd  = {'k':str(grp), 'n':'Coulmn-%s'%(grp), 'g':''}
            phs.append(dd)
        if ijson.get('from_backend') == 'Y':
            return r_grpd
        grps    = []
        ks  = final_grpd.keys()
        ks.sort(key=lambda x:(0 if 'NEW' in x else 1))
        sn  = 1
        for k in ks:
            dd=  {'t_l':grp_info[('GRP', k)], 'clr':final_grpd[k], 'sn':sn, 't_id':k}
            sn  += 1
            grps.append(dd)
        res = [{"message":"done", "data":data, 'phs':phs, 'grps':grps}]
        if ijson.get('from_review') == 'Y':
            ref_data_d  = {}
            rid = 0
            for rr in data:
                for k in rr.keys():
                    if ':$$:' in str(k):
                        del rr[k]
                for ph in phs:
                    if ph['k'] in rr:
                        tmpdd   = {}
                        for k1 in self.priority_ks:
                            if k1 in rr[ph['k']]:
                                tmpdd[k1] = rr[ph['k']][k1]
                        if rr[ph['k']].get('f_col'):
                            tmpdd['f']  = 'Y'
                        key = 'REF_DATA_%s^^%s'%(ijson['irowid'], rid)
                        tmpdd['r_key']    = key
                        rid += 1
                        ref_data_d[key] = rr[ph['k']]
                        rr[ph['k']] = tmpdd
            return res, ref_data_d
        return res
        

    def read_wrong_merge_across_error_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        table_type, grpid, kstr = ijson['idata']
        table_type  = str(table_type)
        grpid       = str(grpid)
        phs         = eval(txn.get('PHS:%s^^%s'%((table_type, grpid))))
        tids        = eval(txn.get('TAXOIDS:%s^^%s'%((table_type, grpid))))
        sn  = 1
        ph_d    = {}
        for ph in phs:
            ph_d[ph['k']]   = ph
        data    = []
        for tid in tids:
            if str(tid) not in kstr:continue
            tkstr    = '%s^^%s^^%s'%(table_type, grpid, tid)
            trow     = eval(txn.get(str(tkstr)) )
            grp_ar = kstr[str(tid)]['m'] #self.check_spike(trow, phs, kstr[str(tid)])
            if not grp_ar:continue
            table_lc_d  = {}
            row     = {'t_id':trow['t_id'], 't_l':trow['t_l']}
            tsn = 1
            for ii, pd in enumerate(grp_ar):
                pd      = map(lambda x:x[1], pd)
                if kstr[str(tid)]['nm']:
                    nm_ar   =  map(lambda x:x[1], kstr[str(tid)]['nm'][ii])
                else:
                    nm_ar   =  []
                tphs    = []
                data_d  = {'t_id':tid, 'sn':tsn}
                tsn +=1
                for pk in pd:
                    if pk in trow:
                        v_d         = trow[pk]
                        data_d[pk]  = v_d
                        t           = v_d['t']
                        x           = v_d['x']
                        lc_d        = trow.get(t+':$$:'+x, {'v':''})
                        vtxt    = lc_d['v']
                        if not vtxt:
                            vtxt = ' '
                        table_lc_d.setdefault(pk, {})[vtxt] = 1
                        if pk in nm_ar:
                            v_d['clr']  = ii+1
                found_ph    = {}
                for ph in phs:
                    if ph['k'] in data_d:
                        ph  = copy.deepcopy(ph)
                        #ph['org_g'] = ph['g']
                        #ph['g'] = ' - '.join(table_lc_d[pk].keys())
                        tphs.append(ph)
                self.add_num_group([data_d], tphs, txn)
                row.setdefault('grp', []).append({'data':data_d, 'phs':tphs})
            if 0:
                grpdata = row['grp']
                fdata   = []
                rid     =  0
                for dd in grpdata:
                    data_d  = dd['data']
                    phs     = dd['phs']
                    tmplst  = [[], [], []]
                    prev_grp    = None
                    for ph in phs:
                        if prev_grp == None or prev_grp != ph['g']:
                            tmplst[0].append({'v':ph['g'], 'colspan':1, 't':str(rid), 'x':str(rid),'ctype':"PH"})
                            rid += 1
                            prev_grp    = ph['g']
                        elif prev_grp == ph['g']:
                            tmplst[0][-1]['colspan']    += 1
                        tmplst[1].append({'v':ph['n'], 'k':ph['k'], 'g':ph['g'], 't':str(rid), 'x':str(rid),'ctype':"PH"})
                        rid += 1
                        if ph['k'] in data_d:
                            tmplst[2].append(data_d[ph['k']])
                        else:
                            tmplst[2].append({'v':'', 't':str(rid), 'x':str(rid)})
                            rid += 1
                    fdata   += tmplst
                row['grp']  = fdata
            row['sn']   = sn
            sn  += 1
            data.append(row)
        res = [{"message":"done", "data":data}]
        if ijson.get('from_review') == 'Y':
            ref_data_d  = {}
            rid = 0
            for gr in data:
                for grpd in gr.get('grp', []):
                    for rr in [grpd['data']]:
                        for k in rr.keys():
                            if ':$$:' in str(k):
                                del rr[k]
                        for ph in grpd['phs']:
                            if ph['k'] in rr:
                                tmpdd   = {}
                                for k1 in self.priority_ks:
                                    if k1 in rr[ph['k']]:
                                        tmpdd[k1] = rr[ph['k']][k1]
                                if rr[ph['k']].get('f_col'):
                                    tmpdd['f']  = 'Y'
                                key = 'REF_DATA_%s^^%s'%(ijson['irowid'], rid)
                                tmpdd['r_key']    = key
                                rid += 1
                                ref_data_d[key] = rr[ph['k']]
                                rr[ph['k']] = tmpdd
            return res, ref_data_d
        return res

    def read_wrong_merge_error_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        table_type, grpid, kstr = ijson['idata']
        table_type  = str(table_type)
        grpid       = str(grpid)
        phs         = eval(txn.get('PHS:%s^^%s'%((table_type, grpid))))
        tids        = eval(txn.get('TAXOIDS:%s^^%s'%((table_type, grpid))))
        allpk       = {}
        p_d         = {}
        for tid, k in kstr['m'].items():
            for k1 in k:#.split('^^'):
                allpk.setdefault(k1, {})[tid]   = 1
        data    = []
        ph_d    = {}
        ptypes = {}
        ptypes_doc = {}
        for ph in phs:
            if ph['k'] in allpk:
                ph_d[ph['k']]   = ph
                ptypes.setdefault('%s%s'%ph['dph'], {})[ph['g'].split('-')[0]]  = 1
                ptypes_doc.setdefault(ph['g'].split('-')[0], []).append(ph)
        sn  = 1
        for tid in kstr['m'].keys():
            tkstr    = '%s^^%s^^%s'%(table_type, grpid, tid)
            row     = eval(txn.get(str(tkstr)) )
            fkey    = 0
            for ph in phs:
                if ph['k'] in row and tid not in allpk.get(ph['k'], {}):
                    del row[ph['k']]
                else:
                    fkey    = 1
            if fkey == 0:continue       
            row['sn']   = sn
            sn  += 1
            data.append(row)
        if 1:
            ph_order    = report_year_sort.year_sort(ptypes.keys())
            ph_order.reverse()
            phs = []
            for ph in ph_order:
                for docid in ptypes[ph].keys():
                    color_d    = {}
                    ks  = ptypes_doc[docid]
                    for tph in ks:
                        g   = tph['g']
                        tph['org_g']    = g
                        tph['n']    = '('+tph['n']+') '+g.split()[0].split('-')[1]
                        tph['clr']    = color_d.get(g.split()[0], len(color_d.keys())+1)
                        color_d[g.split()[0]]   = tph['clr']
                        tph['g']    = docid+' - '+' '.join(g.split()[1:])
                        phs.append(tph)
                    phs.append({'n':' ', 'k':'EMPTY', 'g':' '})
        else:
            phs = filter(lambda x:x['k'] in allpk, phs)
        for r in data:
            nmatch_ks   = kstr['nm'].get(str(r['t_id']), [])
            for ph in phs:
                if ph['k'] in r:
                    if ph['k'] in nmatch_ks:
                        r[ph['k']]['clr']   = ph['clr']
                    t   = r[ph['k']]['t']
                    x   = r[ph['k']]['x']
                    rowid   = txn.get('XML_GRP_MAP_'+self.get_quid('%s@@%s'%(t, x)))
                    if rowid:
                        r[ph['k']]['eid']   = rowid
        self.add_num_group(data, phs, txn)
        res = [{"message":"done", "data":data, 'phs':phs}]
        if ijson.get('from_review') == 'Y':
            ref_data_d  = {}
            rid = 0
            for rr in data:
                for k in rr.keys():
                    if ':$$:' in str(k):
                        del rr[k]
                for ph in phs:
                    if ph['k'] in rr:
                        tmpdd   = {}
                        for k1 in self.priority_ks:
                            if k1 in rr[ph['k']]:
                                tmpdd[k1] = rr[ph['k']][k1]
                        if rr[ph['k']].get('f_col'):
                            tmpdd['f']  = 'Y'
                        key = 'REF_DATA_%s^^%s'%(ijson['irowid'], rid)
                        tmpdd['r_key']    = key
                        rid += 1
                        ref_data_d[key] = rr[ph['k']]
                        rr[ph['k']] = tmpdd
            return res, ref_data_d
        return res

    def read_sign_change_error_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        table_type, grpid, kstr = ijson['idata']
        table_type  = str(table_type)
        grpid       = str(grpid)
        phs         = eval(txn.get('PHS:%s^^%s'%((table_type, grpid))))
        tids        = eval(txn.get('TAXOIDS:%s^^%s'%((table_type, grpid))))
        allpk       = {}
        p_d         = {}
        for tid, k in kstr.items():
            for k1 in k['ks']:#.split('^^'):
                allpk.setdefault(k1, {})[tid]   = 1
        data    = []
        ph_d    = {}
        ptypes = {}
        for ph in phs:
            if ph['k'] in allpk:
                ph_d[ph['k']]   = ph
                ptypes.setdefault(ph['n'], []).append(ph)
        sn  = 1
        tids    = kstr.keys()
        tids.sort(key=lambda x:(len(kstr[x]['pat']), len(kstr[x]['ks'])))
        f_pat   = 0
        for tid in tids:
            tkstr    = '%s^^%s^^%s'%(table_type, grpid, tid)
            row     = eval(txn.get(str(tkstr)) )
            fkey    = 0
            for ph in phs:
                if ph['k'] in row:
                    if tid not in allpk.get(ph['k'], {}):
                        del row[ph['k']]
                    else:#if tid in allpk.get(ph['k'], {}):
                        fkey    = 1
            if fkey == 0:continue       
            row['sn']   = sn
            sn  += 1
            if kstr[tid]['pat']:
                f_pat   = 1
                row['pattern']  = {'v': '( '+'  '.join(kstr[tid]['pat'])+' )', 'f_col':[[]]}
            data.append(row)
        empty_row   = []
        iph = ptypes.keys()
        if '' in ptypes:
            iph.remove('')
            empty_row   = ['']
        ph_order    = report_year_sort.year_sort(iph)
        ph_order.reverse()
        ph_order    += empty_row
        phs = []
        for ph in ph_order:
            ks  = ptypes[ph]
            for tph in ks:
                g   = tph['g']
                tph['n']    = g
                tph['g']    = ph
                phs.append(tph)
            phs.append({'n':' ', 'k':'EMPTY', 'g':' '})
        for r in data:
            for ph in phs:
                if ph['k'] in r:
                    t   = r[ph['k']]['t']
                    x   = r[ph['k']]['x']
                    rowid   = txn.get('XML_GRP_MAP_'+self.get_quid('%s@@%s'%(t, x)))
                    if rowid:
                        r[ph['k']]['eid']   = rowid
        if f_pat == 1:
            phs = [{'k':'pattern', 'n':'Sign', 'g':'Sign', 'pin_left':'Y'}]+phs
        self.add_num_group(data, phs, txn)
        res = [{"message":"done", "data":data, 'phs':phs}]
        if ijson.get('from_review') == 'Y':
            ref_data_d  = {}
            rid = 0
            for rr in data:
                for k in rr.keys():
                    if ':$$:' in str(k):
                        del rr[k]
                for ph in phs:
                    if ph['k'] in rr:
                        tmpdd   = {}
                        for k1 in self.priority_ks:
                            if k1 in rr[ph['k']]:
                                tmpdd[k1] = rr[ph['k']][k1]
                        if rr[ph['k']].get('f_col'):
                            tmpdd['f']  = 'Y'
                        key = 'REF_DATA_%s^^%s'%(ijson['irowid'], rid)
                        tmpdd['r_key']    = key
                        rid += 1
                        ref_data_d[key] = rr[ph['k']]
                        rr[ph['k']] = tmpdd
            return res, ref_data_d
        return res

    def read_frm_overlap_error_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        table_type, grpid, kstr = ijson['idata']
        table_type  = str(table_type)
        grpid       = str(grpid)
        phs         = eval(txn.get('PHS:%s^^%s'%((table_type, grpid))))
        all_taxo_ids    = {}
        all_grps    = {}
        for grp, overlap_grp in kstr.items():
            all_taxo_ids[grp[0]] = 1
            for tgrp in overlap_grp.keys():
                all_taxo_ids[tgrp[0]] = 1
                all_grps.setdefault(tgrp, {})[grp]  = 1

        taxo_d  = {}
        for tid in all_taxo_ids.keys():
            tkstr   = '%s^^%s^^%s'%(table_type, grpid, tid)
            row     = eval(txn.get(str(tkstr)) )
            trow    = {'t_id':tid, 't_l':row['t_l']}
            for ph in phs:
                if row.get(ph['k'], {}).get('f_col'):
                    trow['f_col']   = row.get(ph['k'], {}).get('f_col')
                    break
            taxo_d[tid] = trow
        f_ar    = []
        for grp, overlap in all_grps.items():
            dd  = {'taxo_ids':grp[0], 'overlap':map(lambda x:x[0], overlap.keys())}
            f_ar.append(dd) 
        res = [{"message":"done", "data":taxo_d, "grp":f_ar}]
        if ijson.get('from_review') == 'Y':
            return res, {}
        return res
            


    def read_csv_error_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        table_type, grpid, kstr = ijson['idata']
        table_type  = str(table_type)
        grpid       = str(grpid)
        phs         = eval(txn.get('PHS:%s^^%s'%((table_type, grpid))))
        tids        = eval(txn.get('TAXOIDS:%s^^%s'%((table_type, grpid))))
        data        = []
        ph_d        = {}
        sn          = 1
        table_d = {}
        for tid in tids:
            tkstr    = '%s^^%s^^%s'%(table_type, grpid, tid)
            row     = eval(txn.get(str(tkstr)) )
            trow    = {'t_id':row['t_id'], 't_l':row['t_l']}
            f   = 0
            for ph in phs:
                if ph['k'] in row:  
                    t   = row[ph['k']]['t']
                    x   = row[ph['k']]['x']
                    if kstr['err'].get(t, {}).get(x) != None:
                        table_d.setdefault(kstr['err'][t][x], {})['%s:$$:%s'%(t, x)]    = 1
                        f   = 1
                        ph_d[ph['k']]   = 1
                        trow[ph['k']]   = row[ph['k']]
                        trow[ph['k']]['clr'] = kstr['err'][t][x]

            if f == 1:
                trow['sn']   = sn
                sn  += 1
                data.append(trow)
        csv_ar  = []
        sn  = 1
        for csvtup, idx in kstr['csv'].items():
            csv_ar.append({'t_l':'('+', '.join(csvtup)+')', 't_id':sn, 'clr':idx, 'sn':sn, 'c':csvtup[0], 's':csvtup[1], 'vt':csvtup[2], 'data':table_d[idx]})
            sn  += 1
            
        phs = filter(lambda x:x['k'] in ph_d, phs)
        res = [{"message":"done", "data":data, 'phs':phs, 'grps':csv_ar}]
        if ijson.get('from_review') == 'Y':
            ref_data_d  = {}
            rid = 0
            for rr in data:
                for k in rr.keys():
                    if ':$$:' in str(k):
                        del rr[k]
                for ph in phs:
                    if ph['k'] in rr:
                        tmpdd   = {}
                        for k1 in self.priority_ks:
                            if k1 in rr[ph['k']]:
                                tmpdd[k1] = rr[ph['k']][k1]
                        if rr[ph['k']].get('f_col'):
                            tmpdd['f']  = 'Y'
                        key = 'REF_DATA_%s^^%s'%(ijson['irowid'], rid)
                        tmpdd['r_key']    = key
                        rid += 1
                        ref_data_d[key] = rr[ph['k']]
                        rr[ph['k']] = tmpdd
            return res, ref_data_d
        return res

    def read_r_checksum_error_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        table_type, grpid, kstr = ijson['idata']
        table_type  = str(table_type)
        grpid       = str(grpid)
        phs         = eval(txn.get('PHS:%s^^%s'%((table_type, grpid))))
        reported_phs         = eval(txn.get('REPORTED_COL:%s^^%s'%((table_type, grpid))))
        data        = []
        tgrps       = kstr.keys()
        tgrps.sort(key=lambda x:kstr[x]['o'])
        sn  = 1
        ph_d    = {}
        reported_grp    = {}
        ireported_grp    = {}
        pk_d    = {}
        al_phs     = {}
        for ph in phs:
            pk_d[ph['k']]    = ph
            if ph['n'] in reported_phs:
                al_phs[ph['n']]     = 1
                reported_grp[tuple(reported_phs[ph['n']])]  = ph['n']
                for k in reported_phs[ph['n']]:
                    ireported_grp[('GRP', k)]    = tuple(reported_phs[ph['n']])
        all_ks  = {}
        for i, tids in enumerate(tgrps):
            if i != 0:
                data.append({'sn':sn, 'empty_row':"Y"})
                sn  +=1
            all_ks.update(kstr[tids]['k'])
            for tid in tids:
                tkstr    = '%s^^%s^^%s'%(table_type, grpid, tid)
                row     = eval(txn.get(str(tkstr)) )
                row['sn']   = sn
                row['tids']   = tids
                sn  += 1
                data.append(row)
        phs = []
        ph_order    = report_year_sort.year_sort(al_phs.keys())
        ph_order.reverse()
        ks  = reported_grp.keys()
        ks.sort(key=lambda x:ph_order.index(reported_grp[x]), reverse=True)
        final_ph_d  = {}
        ph_k    = {'EMPTY':1}
        all_phs = {}
        for grp in ks:
            n   = reported_grp[grp]
            f   = 0
            tmp_phs = []
            for k in grp:
                ph  = copy.deepcopy(pk_d[k])
                ph['n'] = ph['g']
                ph['g'] = n
                if k in all_ks:
                    f   = 1
                tmp_phs.append(ph)
            if f == 1:
                if phs:
                    phs.append({'n':' ', 'k':'EMPTY', 'g':' '})
                final_ph_d['FINAL_'+n]  = grp
                all_phs[n] = 1
                ph_k['FINAL_'+n]    = 1
                tmp_phs.append({'n':'Final', 'k':'FINAL_'+n, 'g':n})
                phs += tmp_phs
        #print 'final_ph_d ', final_ph_d
        for r in data:
            for ph in phs:
                if ph['k'] in r:
                    t   = r[ph['k']]['t']
                    x   = r[ph['k']]['x']
                    rowid   = txn.get('XML_GRP_MAP_'+self.get_quid('%s@@%s'%(t, x)))
                    if rowid:
                        r[ph['k']]['eid']   =rowid
        ijson_c = copy.deepcopy(ijson)
        ijson_c['from_backend']   = 'Y'
        ijson_c['iph']            = all_phs.keys()
        chk_sum, final_d    = self.calculate_preview_col(ijson_c, data, [], reported_phs, table_type, grpid, txn)
        if ijson.get('from_backend', '') == 'Y':
            return chk_sum
        for r in data:
            if 't_id' not in r:continue
            if r['t_id'] not in final_d:continue
            for ph in all_phs.keys():
                k   = 'FINAL_'+ph
                if k in final_d[r['t_id']]:
                    r[k]    = final_d[r['t_id']][k]
        chk_cols    = {}
        for k, v in chk_sum.items():
            if k == '':continue
            chk_cols.update(v)
        for ph in phs:
            if ph['k'] in chk_cols:
                ph['c_s']   = 'Y'
        for i, r in enumerate(data):
            for k, v in final_ph_d.items():
                for k1 in v:
                    #print '\t', [k1, r]
                    if k1 in r:
                        ph_k[k1]    = 1
        if 0:
            ttype_d  = {}
            for i, r in enumerate(data):
                if 't_id' in r:
                    ttype_d.setdefault((int(deal_id), table_type, grpid), {})[str(r['t_id'])]  = r
                    for k, v in final_ph_d.items():
                        #print '\n', [k, v]
                        v_d = {}
                        for k1 in v:
                            #print '\t', [k1, r]
                            if k1 in r:
                                ph_k[k1]    = 1
                            if r.get(k1, {}).get('v', '').strip():
                                v_d = copy.deepcopy(r[k1])
                                break
                        if v_d:
                            ph_k[k]    = 1
                            v_d['f']    = 'Y'
                            r[k]    = v_d
            check_sum_d = {}
            for tids in tgrps:
                r_id    = kstr[tids]['r']
                #print '\n===================================='
                #print tids, [r_id]
                for grp in ks:
                    n   = reported_grp[grp]
                    #print '\t', grp, n
                    rr  = ttype_d[(int(deal_id), table_type, grpid)][str(r_id)]
                    if 'FINAL_'+n in rr:
                        phs_d   = {}
                        phs_d.setdefault((int(deal_id), table_type, grpid), {})[n] = 'FINAL_'+n
                        f_ar    = rr['FINAL_'+n]['f_col'][0]
                        for ft in f_ar:
                            ft['to_dealid'] = int(deal_id)
                            ft['op']        = ft['operator']
                            ft['t_type']    = table_type
                            ft['g_id']      = grpid
                            if 'k' in ft:
                                del ft['k']
                            ft['c'] = ''
                            ft['s'] = ''
                            ft['vt'] = ''
                            #ft['k']         = 'FINAL_'+n
                            if ft.get('taxo_id'):
                                ft['txid']  = ft['taxo_id']
                        val_dict, form_d, flip_sign, mismatch_scale, mismatch_vt, taxo_grp = self.get_formula_evaluation_across(rr, (f_ar[0]['rid'], f_ar), ttype_d, [n], phs_d, rr.get('m_scale', ''), rr.get('value_type', ''), 'Y', {}, {}, 'Y')
                        for k_org, v in val_dict.get(str(rr['t_id']), {}).items():
                            #print 'k_org ', k_org, v.get('v')
                            k   = 'FINAL_'+k_org
                            if rr.get(k, {}).get('v', None) != None and rr.get(k, {}).get('v', '') != '' and  rr.get(k, {}).get('FORM_VALUE') != 'Y':
                                #print 'INN'
                                v_d = rr.get(k, {})
                                try:
                                    clean_value = numbercleanup_obj.get_value_cleanup(v_d['v'])
                                except:
                                    clean_value = '0'
                                    pass
                                if clean_value == '':
                                    clean_value = '0'
                                clean_value = float(clean_value)
                                sum_val     = sum(v['v_ar'])
                                if v['ftype'] == 'NG':
                                    sum_val  = v['v'].replace(',', '')
                                n_value     = abs(abs(clean_value) - abs(float(sum_val))) #v['v'].replace(',', '')) 
                                n_value     = self.convert_floating_point(n_value).replace(',', '')
                                #print '\tCHK ', k, [clean_value, sum_val, n_value]
                                if n_value not in ['0', '0.00', '0.0']:# and ftype == 'WITHIN':
                                    rr[k]['c_s']    = n_value
                                else:
                                    rr[k]['c_s']    = ''
                                check_sum_d[rr[k]['c_s']] = 1
                                rr[k]['expr_str']    = v['expr_val']
                                continue
                            
                    
                    
                
        if ijson.get('from_backend', '') == 'Y':
            return check_sum_d
        phs = filter(lambda x:x['k'] in ph_k, phs)
        self.add_num_group(data, phs, txn)
        res = [{"message":"done", "data":data, 'phs':phs}]
        if ijson.get('from_review') == 'Y':
            ref_data_d  = {}
            rid = 0
            for rr in data:
                for k in rr.keys():
                    if ':$$:' in str(k):
                        del rr[k]
                for ph in phs:
                    if ph['k'] in rr:
                        tmpdd   = {}
                        for k1 in self.priority_ks:
                            if k1 in rr[ph['k']]:
                                tmpdd[k1] = rr[ph['k']][k1]
                        if rr[ph['k']].get('f_col'):
                            tmpdd['f']  = 'Y'
                        key = 'REF_DATA_%s^^%s'%(ijson['irowid'], rid)
                        tmpdd['r_key']    = key
                        rid += 1
                        ref_data_d[key] = rr[ph['k']]
                        rr[ph['k']] = tmpdd
            return res, ref_data_d
        return res

    def read_lpos_error_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        table_type, grpid, kstr = ijson['idata']
        table_type  = str(table_type)
        grpid       = str(grpid)
        phs         = eval(txn.get('PHS:%s^^%s'%((table_type, grpid))))
        data    = []
        tgrps   = kstr.keys()
        tgrps.sort(key=lambda x:(1 if 'FROM_POS' in kstr[x][1] else 0, kstr[x][0]), reverse=True)
        sn  = 1
        ph_d    = {}
        for i, tids in enumerate(tgrps):
            p_ar, pk_d = kstr[tids]
            if i != 0:
                data.append({'sn':sn, 'empty_row':"Y"})
                sn  +=1
            for tid in tids:
                tkstr    = '%s^^%s^^%s'%(table_type, grpid, tid)
                row     = eval(txn.get(str(tkstr)) )
                row['sn']   = sn
                if 'FROM_POS' in pk_d:
                    row['clr']  = 'O' if pk_d['FROM_POS'] == 'without-formula' else 1
                for ph in phs:
                    if ph['k'] in row:
                        ph_d[ph['k']]   = 1
                        if ph['k'] in pk_d:
                            row[ph['k']]['clr'] = 'G'
                sn  += 1
                data.append(row)
        phs = filter(lambda x:x['k'] in ph_d, phs)
        self.add_num_group(data, phs, txn)
        res = [{"message":"done", "data":data, 'phs':phs}]
        if ijson.get('from_review') == 'Y':
            ref_data_d  = {}
            rid = 0
            for rr in data:
                for k in rr.keys():
                    if ':$$:' in str(k):
                        del rr[k]
                for ph in phs:
                    if ph['k'] in rr:
                        tmpdd   = {}
                        for k1 in self.priority_ks:
                            if k1 in rr[ph['k']]:
                                tmpdd[k1] = rr[ph['k']][k1]
                        if rr[ph['k']].get('f_col'):
                            tmpdd['f']  = 'Y'
                        key = 'REF_DATA_%s^^%s'%(ijson['irowid'], rid)
                        tmpdd['r_key']    = key
                        rid += 1
                        ref_data_d[key] = rr[ph['k']]
                        rr[ph['k']] = tmpdd
            return res, ref_data_d
        return res


    def read_spike_error_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        table_type, grpid, kstr = ijson['idata']
        table_type  = str(table_type)
        grpid       = str(grpid)
        phs         = eval(txn.get('PHS:%s^^%s'%((table_type, grpid))))
        tids        = eval(txn.get('TAXOIDS:%s^^%s'%((table_type, grpid))))
        sn  = 1
        ph_d    = {}
        for ph in phs:
            ph_d[ph['k']]   = ph
        data    = []
        for tid in tids:
            if str(tid) not in kstr:continue
            tkstr    = '%s^^%s^^%s'%(table_type, grpid, tid)
            trow     = eval(txn.get(str(tkstr)) )
            spike_d = kstr[str(tid)] #self.check_spike(trow, phs, kstr[str(tid)])
            if not spike_d:continue
            row     = {'t_id':trow['t_id'], 't_l':trow['t_l']}
            for ptype, pd in spike_d.items():
                tphs    = []
                data_d  = {}
                for pk, clean_value in pd.items():
                    v_d         = trow[pk]
                    v_d['v_org']    = v_d['v']
                    v_d['v']    = self.convert_floating_point(clean_value.keys()[0])
                    data_d[pk]  = v_d
                found_ph    = {}
                for ph in phs:
                    if ph['k'] in data_d:
                        ph  = copy.deepcopy(ph)
                        if ph['n'] in found_ph:
                            cnt = found_ph[ph['n']]
                            ph['n'] = ph['n'][:-4]+'-'+str(cnt)+'-'+ph['n'][-4:]
                            cnt += 1
                            found_ph[ph['n']]   = cnt
                        else:
                            found_ph[ph['n']]   = 1
                        tphs.append(ph)
                row.setdefault('ptype', {})[ptype]   = {'data':data_d, 'phs':tphs}
            row['sn']   = sn
            sn  += 1
            data.append(row)
        self.add_num_group(data, phs, txn)
        res = [{"message":"done", "data":data, 'phs':phs}]
        if ijson.get('from_review') == 'Y':
            ref_data_d  = {}
            rid = 0
            for rr in data:
                for k in rr.keys():
                    if ':$$:' in str(k):
                        del rr[k]
                for ph in phs:
                    if ph['k'] in rr:
                        tmpdd   = {}
                        for k1 in self.priority_ks:
                            if k1 in rr[ph['k']]:
                                tmpdd[k1] = rr[ph['k']][k1]
                        if rr[ph['k']].get('f_col'):
                            tmpdd['f']  = 'Y'
                        key = 'REF_DATA_%s^^%s'%(ijson['irowid'], rid)
                        tmpdd['r_key']    = key
                        rid += 1
                        ref_data_d[key] = rr[ph['k']]
                        rr[ph['k']] = tmpdd
            return res, ref_data_d
        return res

    def read_ph_data_loss(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        table_type, grpid, kstr = ijson['idata']
        table_type  = str(table_type)
        grpid       = str(grpid)
        phs         = eval(txn.get('PHS:%s^^%s'%((table_type, grpid))))
        reported_phs         = eval(txn.get('REPORTED_COL:%s^^%s'%((table_type, grpid))))
        ph_ind  = {}
        tids        = eval(txn.get('TAXOIDS:%s^^%s'%((table_type, grpid))))
        allpk       = {}
        p_d         = {}
        for k in kstr:
            for k1 in k[0].split('^^'):
                allpk[k1]   = k[1]
        data    = []
        ph_d    = {}
        ptypes = {}
        for ii, ph in enumerate(phs):
            ph_d[ph['k']]   = ph
            ph_ind[ph['k']]  = ii
            ptypes[ph['n']] = 1
        sn  = 1
        for tid in tids:
            tkstr    = '%s^^%s^^%s'%(table_type, grpid, tid)
            row     = eval(txn.get(str(tkstr)) )
            fkey    = 0
            for ph in phs:
                if (ph['k'] in row) and (ph['k'] not in allpk):
                    del row[ph['k']]
                if (ph['k'] in allpk) and (ph['k'] in row):
                    fkey    = 1
                    if str(row['t_id']) in  allpk[ ph['k']]:
                        row[ph['k']]['data_overlap']   = 'Y'
            if fkey == 0:continue       
            row['sn']   = sn
            sn  += 1
            data.append(row)
        empty_ph    = []
        if '' in ptypes:
            del ptypes['']
            empty_ph.append('')
        ph_order    = report_year_sort.year_sort(ptypes.keys())
        ph_order.reverse()
        ph_order    += empty_ph
        kstr.sort(key=lambda x:tuple(map(lambda x1:ph_order.index(ph_d[x1]['n']), x[0].split('^^'))))
        phs = []
        all_phs = {}
        for ii, k in enumerate(kstr):
            tmpks   = k[0].split('^^')
            tmpks.sort(key=lambda x:ph_ind[x]) 
            for k1 in tmpks:
                all_phs[ph_d[k1]['n']] = 1
                phs.append(ph_d[k1])
            if kstr[-1] != k:
                phs.append({'n':' ', 'k':'EMPTY', 'g':' '})
        for r in data:
            for ph in phs:
                if ph['k'] in r:
                    t   = r[ph['k']]['t']
                    x   = r[ph['k']]['x']
                    rowid   = txn.get('XML_GRP_MAP_'+self.get_quid('%s@@%s'%(t, x)))
                    if rowid:
                        r[ph['k']]['eid']   =rowid
        ijson_c = copy.deepcopy(ijson)
        ijson_c['from_backend']   = 'Y'
        ijson_c['iph']            = all_phs.keys()
        chk_sum, final_d    = self.calculate_preview_col(ijson_c, data, phs, reported_phs, table_type, grpid, txn)
        if ijson.get('from_backend', '') == 'Y':
            return chk_sum, final_d
        for r in data:
            if r['t_id'] not in final_d:continue
            for ph in all_phs.keys():
                k   = 'FINAL_'+ph
                if k in final_d[r['t_id']]:
                    r[k]    = final_d[r['t_id']][k]
        chk_cols    = {}
        for k, v in chk_sum.items():
            if k == '':continue
            chk_cols.update(v)
        tmphs   = []
        for i, ph in enumerate(phs):
            if ph['k'] == 'EMPTY':
                tmphs.append({'k':'FINAL_'+phs[i-1]['n'], 'n':'Final', 'g':phs[i-1]['n'], 're_compute':"Y", 'c_s':chk_cols.get('FINAL_'+phs[i-1]['n'], 'N')})
            tmphs.append(ph)
        if phs:
            i   = len(phs)
            tmphs.append({'k':'FINAL_'+phs[i-1]['n'], 'n':'Final', 'g':phs[i-1]['n'], 're_compute':"Y", 'c_s':chk_cols.get('FINAL_'+phs[i-1]['n'], 'N')})
        phs = tmphs
                
        
        self.add_num_group(data, phs, txn)
        res = [{"message":"done", "data":data, 'phs':phs}]
        if ijson.get('from_review') == 'Y':
            ref_data_d  = {}
            rid = 0
            for rr in data:
                for k in rr.keys():
                    if ':$$:' in str(k):
                        del rr[k]
                for ph in phs:
                    if ph['k'] in rr:
                        tmpdd   = {}
                        for k1 in self.priority_ks:
                            if k1 in rr[ph['k']]:
                                tmpdd[k1] = rr[ph['k']][k1]
                        if rr[ph['k']].get('f_col'):
                            tmpdd['f']  = 'Y'
                        key = 'REF_DATA_%s^^%s'%(ijson['irowid'], rid)
                        tmpdd['r_key']    = key
                        rid += 1
                        ref_data_d[key] = rr[ph['k']]
                        rr[ph['k']] = tmpdd
            return res, ref_data_d
        return res


    def read_ph_error_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        table_type, grpid, kstr = ijson['idata']
        table_type  = str(table_type)
        grpid       = str(grpid)
        phs         = eval(txn.get('PHS:%s^^%s'%((table_type, grpid))))
        tids        = eval(txn.get('TAXOIDS:%s^^%s'%((table_type, grpid))))
        allpk       = {}
        p_d         = {}
        for k in kstr:
            for k1 in k.split('^^'):
                allpk[k1]   = 1
        data    = []
        ph_d    = {}
        ptypes = {}
        for ph in phs:
            ph_d[ph['k']]   = ph
            ptypes[ph['n']] = 1
        sn  = 1
        for tid in tids:
            tkstr    = '%s^^%s^^%s'%(table_type, grpid, tid)
            row     = eval(txn.get(str(tkstr)) )
            fkey    = 0
            for ph in phs:
                if ph['k'] in row and ph['k'] not in allpk:
                    del row[ph['k']]
                if ph['k'] in allpk:
                    fkey    = 1
            if fkey == 0:continue       
            row['sn']   = sn
            sn  += 1
            data.append(row)
        empty_ph    = []
        if '' in ptypes:
            del ptypes['']
            empty_ph.append('')
        ph_order    = report_year_sort.year_sort(ptypes.keys())
        ph_order.reverse()
        ph_order    += empty_ph
        kstr.sort(key=lambda x:tuple(map(lambda x1:ph_order.index(ph_d[x1]['n']), x.split('^^'))))
        phs = []
        for ii, k in enumerate(kstr):
            for k1 in k.split('^^'):
                if ph_d[k1]['n'] == '':
                    ph_d[k1]['n']   = ' '
                phs.append(ph_d[k1])
            if kstr[-1] != k:
                phs.append({'n':' ', 'k':'EMPTY', 'g':' '})
        self.add_num_group(data, phs, txn)
        res = [{"message":"done", "data":data, 'phs':phs}]
        if ijson.get('from_review') == 'Y':
            ref_data_d  = {}
            rid = 0
            for rr in data:
                for k in rr.keys():
                    if ':$$:' in str(k):
                        del rr[k]
                for ph in phs:
                    if ph['k'] in rr:
                        tmpdd   = {}
                        for k1 in self.priority_ks:
                            if k1 in rr[ph['k']]:
                                tmpdd[k1] = rr[ph['k']][k1]
                        if rr[ph['k']].get('f_col'):
                            tmpdd['f']  = 'Y'
                        key = 'REF_DATA_%s^^%s'%(ijson['irowid'], rid)
                        tmpdd['r_key']    = key
                        rid += 1
                        ref_data_d[key] = rr[ph['k']]
                        rr[ph['k']] = tmpdd
            return res, ref_data_d
        return res

    def add_num_group(self, data, phs, txn):
        for r in data:
            for ph in phs:
                if ph['k'] in r:
                    t   = r[ph['k']].get('t', '')
                    x   = r[ph['k']].get('x', '')
                    rowid   = txn.get('XML_GRP_MAP_'+self.get_quid('%s@@%s'%(t, x)))
                    if rowid:
                        r[ph['k']]['eid']   = rowid

    def read_group_error_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        table_type, grpid, kstr = ijson['idata']
        table_type  = str(table_type)
        grpid       = str(grpid)
        ttype_phs   = {}
        phs         = []
        ttype_d     = {}
        def index_value(ttype1, grpid1, tid1):
            if (ttype1, grpid1) not in ttype_phs:
                reported_phs        = eval(txn.get('REPORTED_COL:%s^^%s'%((ttype1, grpid1))))
                phs                 = eval(txn.get('PHS:%s^^%s'%((ttype1, grpid1))))
                ttype_phs[(ttype1, grpid1)]   = (reported_phs, phs)
            kstr    = '%s^^%s^^%s'%(ttype1, grpid1, tid1)
            row     = eval(txn.get(kstr) )
            ttype_d[(ttype1, grpid1, tid1)]     = row
            val_d   = {}
            for ph, k_ar in ttype_phs[(ttype1, grpid1)][0].items():
                v_d = {}
                for k in k_ar:
                    if row.get(k, {}).get('v'):
                        v_d = copy.deepcopy(row[k])
                        break
                if v_d:
                    row[ph] = v_d
                    clean_value = numbercleanup_obj.get_value_cleanup(v_d['v'])
                    if clean_value:
                        clean_value = float(clean_value)
                        if clean_value:
                            s   = v_d.get('phcsv', {}).get('s', '')
                            if s:
                                tv, factor  = sconvert_obj.convert_frm_to_1(s, '1', clean_value)
                                if factor:
                                    clean_value = float(tv.replace(',', ''))
                            clean_value = self.convert_floating_point(clean_value)
                            ktup    = (ph, clean_value, v_d.get('phcsv', {}).get('vt', ''), v_d.get('phcsv', {}).get('c', ''))
                            #ph_val_d.setdefault(ktup, {})[(ttype1, grpid1, tid1)]   = 1
                            val_d[ktup] = 1
            return val_d

        sn      = 1
        color_d = {}
        grps    = []
        done_d  = {}
        for tid,  m_ar in kstr:
            tid = str(tid)
            val_d   = index_value(table_type, grpid, tid)
            m_phs   = {}
            tm_d    = {}
            pd      = {}
            for ttup in m_ar:
                if ttup == (table_type, grpid, tid):continue
                ttype1, grpid1, tid1   = map(lambda x:str(x), ttup)
                val_d1  = index_value(ttype1, grpid1, tid1)
                m_d     = {}
                nm_d    = {}
                for k in val_d1.keys():
                    if k in val_d:
                        m_d[k]     = 1
                    else:
                        nm_d[k]     = 1
                nc      = len(val_d1.keys())
                m_c     = len(m_d.keys())
                nm_c    = len(nm_d.keys())
                p       = int((float(m_c)/float(nc))*100)
                if nc > 6 and p > 50:
                    pd[p]      = 1
                    tm_d[ttup]  = (map(lambda x:x[0], val_d1.keys()), map(lambda x:x[0], m_d.keys()), p)
                    for k in m_d.keys():
                        m_phs[k[0]] = 1
            if tm_d:
                done_d[tid]    = 1
                tm_d[(table_type, grpid, tid)]  = (map(lambda x:x[0], val_d.keys()), m_phs, 'G' if len(pd.keys()) == 1 and pd.keys()[0] == 100 else 'O')
                m_ar    = [(table_type, grpid, tid)]+tm_d.keys()
                data    = []
                color   = 'G'
                dd_t    = {}
                for ttup in m_ar:
                    if ttup in dd_t:continue
                    dd_t[ttup]  = 1
                    ttype1, grpid1, tid1   = ttup
                    trow    = copy.deepcopy(ttype_d[ttup])
                    for ph, k_ar in ttype_phs[(ttype1, grpid1)][0].items():
                        if ttup == m_ar[0]:
                            trow['f']   = 'Y'
                            if ph in trow:
                                trow[ph]['f_col']   = [[]]
                        if ph in trow:
                            if ph not in tm_d[ttup][0]:
                                del trow[ph]
                            elif ph in tm_d[ttup][1]:
                                trow[ph]['clr'] = 'G'
                            else:
                                trow[ph]['clr'] = '25'
                    if tm_d[ttup][2] == 100:
                        trow['clr']   = 'G'
                    else:
                        color   = 'O'
                        trow['clr']   = 'O'
                    if grpid1:
                        trow['t_l']     = ttype1+' - '+trow['t_l']+' ( '+grpid1+' )'
                    else:
                        trow['t_l']     = ttype1+' - '+trow['t_l']
                    trow['sn']    = sn
                    sn  += 1
                    data.append(trow)
                ph_order    = report_year_sort.year_sort(ttype_phs[(table_type, grpid)][0].keys())
                ph_order.reverse()
                phs = []
                prev_year   = ''
                for ph in ph_order:
                    if prev_year and prev_year != ph[-4:]:
                        phs.append({'n':' ', 'k':'EMPTY', 'g':' '})
                    dd  = {'n':ph, 'g':ph,'k':ph}
                    prev_year   = ph[-4:]
                    phs.append(dd)
                color_d[color]  = 1
                grps.append((color, {'data':data, 'phs':phs}))
        if ijson.get('from_backend') == 'Y':
            return color_d, done_d
        grps.sort(key=lambda x: {'G':0, 'O':1}[x[0]])
        final_grps  = []
        #sn  = 1
        for ttup in grps:
            data    = ttup[1]['data']
            #for r in data:
            #    r['sn'] = sn
            #    sn  += 1
            final_grps.append({'data':data, 'phs':ttup[1]['phs']})
            #final_grps.append({'sn':sn, 'empty_row':"Y"})
            #sn  += 1
        res = [{"message":"done", "data":final_grps}]
        if ijson.get('from_review') == 'Y':
            ref_data_d  = {}
            rid = 0
            for grpd in final_grps:
                for rr in grpd['data']:
                    for k in rr.keys():
                        if ':$$:' in str(k):
                            del rr[k]
                    for ph in grpd['phs']:
                        if ph['k'] in rr:
                            tmpdd   = {}
                            for k1 in self.priority_ks:
                                if k1 in rr[ph['k']]:
                                    tmpdd[k1] = rr[ph['k']][k1]
                            if rr[ph['k']].get('f_col'):
                                tmpdd['f']  = 'Y'
                            key = 'REF_DATA_%s^^%s'%(ijson['irowid'], rid)
                            tmpdd['r_key']    = key
                            rid += 1
                            ref_data_d[key] = rr[ph['k']]
                            rr[ph['k']] = tmpdd
            return res, ref_data_d
        return res
            
    def read_group_data(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
        env         = lmdb.open(lmdb_path, readonly=True)
        txn         = env.begin()
        taxo_key, table_xml = ijson['idata'][2]
        ttype_phs   = {}
        phs         = []
        ttype_d     = {}
        xml_ph_map  = {}    
        row_ks      = {}
        all_pkd     = {}
        ptypes  = {}
        for tk in taxo_key.split('^^'):
            ttype, grpid, tid   = map(lambda x:str(x), tk.split('$$'))
            if (ttype, grpid) not in ttype_phs:
                ttype_phs[(ttype, grpid)]   = eval(txn.get('PHS:%s^^%s'%((ttype, grpid))))
            kstr    = '%s^^%s^^%s'%(ttype, grpid, tid)
            row     = eval(txn.get(kstr) )
            for ph in ttype_phs[(ttype, grpid)]:
                if ph['k'] in row and ph['n']:
                    ptypes[ph['n']]  = {}
                    v_d = row[ph['k']]
                    t   = v_d['t']
                    x   = v_d['x']
                    row_ks.setdefault(kstr, {})[ph['k']]   = ph
                    all_pkd[(kstr, ph['k'])]     = ph
                    xml_ph_map.setdefault((t, x), {})[(kstr, ph['k'])]  = ph['n']
                    
            ttype_d[kstr]     = row
            dd  = {'n':ttype+' - '+row['t_l'], 'k':kstr}
            phs.append(dd)
        data    = []
        sn  = 1
        for xks in table_xml.split('^^'):
            ks  = {}
            ph  = 'Group-%s'%(sn)
            for xk in xks.split('$$'):
                t, x    = xk.split('@@')
                mphs    = xml_ph_map[(t, x)]
                for k, v in mphs.items():
                    ks[k]   = 1
                    ph  = v
            dd  = {'t_l':ph, 'sn':sn, 'match':'Y'}
            for k in ks.keys():
                dd[k[0]]    = ttype_d[k[0]][k[1]]
                if k in all_pkd:
                    del all_pkd[k]
            data.append(dd)
            sn  += 1
        ph_order    = report_year_sort.year_sort(ptypes.keys())
        ks  = all_pkd.keys()
        ks.sort(key=lambda x:ph_order.index(all_pkd[x]['n']), reverse=True)
        for k in ks:
            v   =all_pkd[k]
            dd  = {'t_l':v['n'], 'sn':sn}
            sn  += 1
            dd[k[0]]    = ttype_d[k[0]][k[1]]
            data.append(dd)
        res = [{"message":"done", "data":data, "phs":phs}]
        return res

    def calculate_preview_col(self, ijson, data=[], phs=[], reported_phs=[], table_type=None, grpid='', txn=None, iphs=[]):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        if txn == None:
            lmdb_path = '/mnt/eMB_db/%s/%s/DB_RAW_DATA/'%(company_name, model_number)
            env         = lmdb.open(lmdb_path, readonly=True)
            txn         = env.begin()
        if table_type == None:
            table_type, grpid   = ijson['table_type'], ijson['grpid']
        ijson['table_type'] = table_type
        ijson['grpid'] = grpid
        if not ijson.get('data'):
            ucol_m              = self.read_user_selected_col_map_info(ijson)
        else:
            ucol_m          = ijson.get('data')
        if not iphs:
            iphs        = ijson['iph']
        table_type  = str(table_type)
        grpid       = str(grpid)
        if not phs:
            phs         = eval(txn.get('PHS:%s^^%s'%((table_type, grpid))))
        if not reported_phs:
            reported_phs         = eval(txn.get('REPORTED_COL:%s^^%s'%((table_type, grpid))))
        for k, v in reported_phs.items():
            if ucol_m.get(k, {}).get('phs'):
                tmplst  = filter(lambda x:x.strip(), list(sets.Set(ucol_m.get(k, {}).get('phs'))))
                if tmplst:
                    reported_phs[k] = tmplst
        sn  = 1
        ph_d    = {}
        reported_grp    = {}
        ireported_grp    = {}
        pk_d    = {}
        al_phs     = {}
        for ph in phs:
            pk_d[ph['k']]    = ph
            if ph['n'] in reported_phs and ph['n'] in iphs:
                al_phs[ph['n']]     = 1
                reported_grp[tuple(reported_phs[ph['n']])]  = ph['n']
                for k in reported_phs[ph['n']]:
                    ireported_grp[('GRP', k)]    = tuple(reported_phs[ph['n']])
        all_ks  = {}
        for iph in iphs:
            for k in reported_phs[iph]:
                all_ks[k]   = 1
        tids        = eval(txn.get('TAXOIDS:%s^^%s'%((table_type, grpid))))
        if not data:
            for tid in tids:
                tkstr    = '%s^^%s^^%s'%(table_type, grpid, tid)
                row     = eval(txn.get(str(tkstr)) )
                row['sn']   = sn
                sn  += 1
                data.append(row)
        phs = []
        ph_order    = report_year_sort.year_sort(al_phs.keys())
        ph_order.reverse()
        ks  = reported_grp.keys()
        ks.sort(key=lambda x:ph_order.index(reported_grp[x]), reverse=True)
        final_ph_d  = {}
        for grp in ks:
            n   = reported_grp[grp]
            f   = 0
            tmp_phs = []
            for k in grp:
                ph  = copy.deepcopy(pk_d[k])
                ph['n'] = ph['g']
                ph['g'] = n
                if k in all_ks:
                    f   = 1
                tmp_phs.append(ph)
            if f == 1:
                if phs:
                    phs.append({'n':' ', 'k':'EMPTY', 'g':' '})
                final_ph_d['FINAL_'+n]  = grp
                tmp_phs.append({'n':'Final', 'k':'FINAL_'+n, 'g':n})
                phs += tmp_phs
        #print 'final_ph_d ', final_ph_d
        ph_k    = {'EMPTY':1}
        ttype_d  = {}
        for i, r in enumerate(data):
            if 't_id' in r:
                ttype_d.setdefault((int(deal_id), table_type, grpid), {})[str(r['t_id'])]  = r
                for k, v in final_ph_d.items():
                    #print '\n', [k, v]
                    v_d = {}
                    u_pk    = ucol_m.get(k.strip('FINAL_'), {}).get('tids', {}).get(str(r['t_id']))
                    if u_pk and u_pk in v and u_pk in r:
                        ph_k[u_pk]    = 1
                        v_d = copy.deepcopy(r[u_pk])
                    else:
                        for k1 in v:
                            #print '\t', [k1, r]
                            if k1 in r:
                                ph_k[k1]    = 1
                            if r.get(k1, {}).get('v', '').strip():
                                v_d = copy.deepcopy(r[k1])
                                break
                    if v_d:
                        ph_k[k]    = 1
                        v_d['f']    = 'Y'
                        r[k]    = v_d
        check_sum_d = {}
        for tid in tids:
            r_id    = tid
            if str(r_id) not in ttype_d[(int(deal_id), table_type, grpid)]:continue
            #print '\n===================================='
            #print tid, [r_id]
            for grp in ks:
                n   = reported_grp[grp]
                #print '\t', grp, n
                rr  = ttype_d[(int(deal_id), table_type, grpid)][str(r_id)]
                if 'FINAL_'+n in rr and rr['FINAL_'+n].get('f_col'):
                    phs_d   = {}
                    phs_d.setdefault((int(deal_id), table_type, grpid), {})[n] = 'FINAL_'+n
                    f_ar    = rr['FINAL_'+n]['f_col'][0]
                    for ft in f_ar:
                        ft['to_dealid'] = int(deal_id)
                        ft['op']        = ft['operator']
                        ft['t_type']    = table_type
                        ft['g_id']      = grpid
                        if 'k' in ft:
                            del ft['k']
                        ft['c'] = ''
                        ft['s'] = ''
                        ft['vt'] = ''
                        #ft['k']         = 'FINAL_'+n
                        if ft.get('taxo_id'):
                            ft['txid']  = ft['taxo_id']
                    val_dict, form_d, flip_sign, mismatch_scale, mismatch_vt, taxo_grp = self.get_formula_evaluation_across(rr, (f_ar[0]['rid'], f_ar), ttype_d, [n], phs_d, rr.get('m_scale', ''), rr.get('value_type', ''), 'Y', {}, {}, 'Y')
                    for k_org, v in val_dict.get(str(rr['t_id']), {}).items():
                        #print 'k_org ', k_org, v.get('v')
                        k   = 'FINAL_'+k_org
                        if rr.get(k, {}).get('v', None) != None and rr.get(k, {}).get('v', '') != '' and  rr.get(k, {}).get('FORM_VALUE') != 'Y':
                            #print 'INN'
                            v_d = rr.get(k, {})
                            try:
                                clean_value = numbercleanup_obj.get_value_cleanup(v_d['v'])
                            except:
                                clean_value = '0'
                                pass
                            if clean_value == '':
                                clean_value = '0'
                            clean_value = float(clean_value)
                            sum_val     = sum(v['v_ar'])
                            if v['ftype'] == 'NG':
                                sum_val  = v['v'].replace(',', '')
                            n_value     = abs(abs(clean_value) - abs(float(sum_val))) #v['v'].replace(',', '')) 
                            n_value     = self.convert_floating_point(n_value).replace(',', '')
                            #print '\tCHK ', k, [clean_value, sum_val, n_value]
                            if n_value not in ['0', '0.00', '0.0']:# and ftype == 'WITHIN':
                                rr[k]['c_s']    = n_value
                            else:
                                rr[k]['c_s']    = ''
                            check_sum_d.setdefault(rr[k]['c_s'], {})[k] = 'Y'
                            rr[k]['expr_str']    = v['expr_val']
                            continue
                        
        final_d = {}
        for r in data:
            for iph in iphs:
                k   = 'FINAL_'+iph
                if k in r:
                    final_d.setdefault(r['t_id'], {})[k]  = r[k]
                    for k1 in reported_phs[iph]:
                        if k1 in r:
                            final_d.setdefault(r['t_id'], {})[k1]  = r[k1]
        if ijson.get('from_backend', '') == 'Y':
            return check_sum_d, final_d
        res = [{"message":"done", "data":final_d}]
        return res


    def read_redis_data(self, ijson):
        import redis
        redis_host = "127.0.0.1"
        redis_port = 6379
        r = redis.StrictRedis(host=redis_host, port=redis_port, decode_responses=True)
        f_d = {}
        for k in ijson['keys']:
            v   = r.get(k)
            if not v:
                v   = '{}'
            v   = eval(v)
            f_d[k] = v
        res    = [{"message":"done","data":f_d}]
        return res

    def update_all_taxo_index_wise(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        updated_taxos   = ijson['taxos']
        tx_str   = ', '.join(['"'+str(e)+'"' for e in ijson['taxos']])
        table_type      = ijson['table_type']
        db_path = '/mnt/eMB_db/%s/%s/taxo_data_builder.db'%(company_name, model_number)
        conn = sqlite3.connect(db_path)
        cur  = conn.cursor()
        read_qry = """ SELECT order_id from mt_data_builder where table_type='%s' and taxo_id in (%s); """%(table_type, tx_str)
        cur.execute(read_qry)
        t_data = cur.fetchall()
        r_dct = {}
        for r in t_data:
            r_dct[r[0]] = 1
        ts = sorted(r_dct.keys())
        dd = {}
        for x, s in enumerate(ts):
            dd[x] = s
        
        for idx, txid in enumerate(updated_taxos):
            get_or_id = dd[idx]
            update_stmt = """update mt_data_builder set order_id='%s' where taxo_id='%s' and table_type='%s' """%(get_or_id, txid, table_type)
            cur.execute(update_stmt)
            
        conn.commit()
        conn.close()
        return [{'message':'done'}] 

    def auto_table_classification(self, ijson):
        docids  = ijson['doc_ids']
        import read_taxo_data
        obj = read_taxo_data.DB()
        d, rev_d = obj.form_gps_tables('20_0', [], docids, ijson['i_doc_ids'])
        final_d = {}
        count_d = {}
        for k, v in d.items():
            for ttype in v:
                if ttype not in rev_d:continue
                for tid in k:
                    tid     = tid.split('-')
                    docid   = tid[0]
                    final_d.setdefault(docid, {}).setdefault(ttype, [])
                    count_d.setdefault(ttype, {'count':0})
                    final_d[docid][ttype].append({'t':tid, 'data':'default', 'user':'default', 'n':'N'})
        for ttype, k in rev_d.items():
            for tid in k.keys():
                tid     = tid.split('-')
                docid   = tid[0]
                final_d.setdefault(docid, {}).setdefault(ttype, [])
                count_d.setdefault(ttype, {'count':0})
                final_d[docid][ttype].append({'t':tid, 'data':'default', 'user':'default', 'n':"Y"})
        docs    = final_d.keys()
        far = []
        docs.sort()
        for d in docs:
            dd  = final_d[d]
            for k, v in dd.items():
                count_d[k]['count']  = max(count_d[k]['count'], len(v))
            dd['doc_id']    = d
            far.append(dd)
        res = [{"message":"done","data":far, 'count':count_d}]
        return res

    def get_stats(self, ijson_c):
        project_id  = ijson_c['project_id']
        cids    = []
        if ijson_c.get('deal_ids'):
            cids    = map(lambda x:x.split('_')[1] if '_' in x else x, ijson_c['deal_ids'])
        
        cinfo_d     = self.read_company_info({"cids":cids})
        dd_s  = {}
        dd_s['total_comp']  = 0
        dd_s['total_table']  = 0
        dd_s['total_classified']  = 0
        dd_s['total_not_classified']  = 0
        dd_s['total_ph_csv']  = 0
        dd_s['total_ph']  =0
        dd_s['total_csv']  =0
        dd_s['total_db']  =0
        for k, v in cinfo_d.items():
            if ijson_c.get("deal_ids") and k not in ijson_c['deal_ids']:continue
            try:
                ijson = v
                company_name    = ijson['company_name']
                mnumber         = ijson['model_number']
                model_number    = mnumber
                deal_id         = ijson['deal_id']
                project_id      = ijson['project_id']
                company_id      = "%s_%s"%(project_id, deal_id)
                print (company_id, company_name)

                sheet_id_map    = self.get_sheet_id_map()

                d_path = '/mnt/eMB_db/%s/%s/tas_tagging.db'%(company_name, model_number)
                cn = sqlite3.connect(d_path)
                cr = cn.cursor()
                crt_tbl = """CREATE TABLE IF NOT EXISTS UsrTableCsvPhInfo(row_id INTEGER PRIMARY KEY AUTOINCREMENT, table_id VARCHAR(256), xml_id TEXT, period_type TEXT, period TEXT, currency TEXT, value_type TEXT, scale TEXT, month VARCHAR(20), review_flg INTEGER)"""
                cr.execute(crt_tbl)
                read_qry = """SELECT table_id, xml_id, period, period_type, currency, value_type, scale FROM UsrTableCsvPhInfo """
                cr.execute(read_qry)
                tdt = cr.fetchall()
                cn.close()
                chk_table_xml = {}
                for rw in tdt:
                    tbid, xl_id, prd, prd_type, crncy, vl_type, scl = rw
                    tbid    = str(tbid)
                    if prd or prd_type:
                        chk_table_xml.setdefault(tbid, {'all':{}, 'ph':{}, 'csv':{}})
                        chk_table_xml[tbid]['ph'][xl_id]    = 1
                        chk_table_xml[tbid]['all'][xl_id]    = 1
                    if crncy or vl_type or scl:
                        chk_table_xml.setdefault(tbid, {'all':{}, 'ph':{}, 'csv':{}})
                        chk_table_xml[tbid]['csv'][xl_id]    = 1
                        chk_table_xml[tbid]['all'][xl_id]    = 1


                db_file     = '/mnt/eMB_db/%s/%s/tas_company.db'%(company_name, model_number)
                conn, cur   = conn_obj.sqlite_connection(db_file)

                sql   = "select row_id, sheet_id, doc_id, doc_name, table_id from table_group_mapping;"
                try:
                    cur.execute(sql)
                    tres        = cur.fetchall()
                except:
                    tres    = []
                all_valid_documents = self.get_distinct_docids(conn, cur)
                dbf = '/mnt/eMB_db/%s/%s/mt_data_builder.db'%(company_name, model_number)
                conn1 = sqlite3.connect(dbf)
                cur1 = conn1.cursor()
                actual_tables = self.clean_actual_value_tableid_data(conn1, cur1) 
                conn1.commit()
                conn1.close()
                conn.close()
                db_file     = '/mnt/eMB_db/%s/%s/taxo_data_builder.db'%(company_name, model_number)
                conn, cur   = conn_obj.sqlite_connection(db_file)
                sql = "select table_id from mt_data_builder"
                cur.execute(sql)
                tmpres    = cur.fetchall()
                db_done = {}
                for r in tmpres:
                    db_done[str(r[0])] = 1
                    
                source_table_data = {}
                if project_id in ('20', ):
                   source_table_data =  self.source_table_information(company_id)    
                    
                data_rows = []
                classified_set = {}
                for row in tres:
                    row = map(str, row)
                    row_id, sheet_id, doc_id, doc_name, table_id_str = row
                    if doc_id not in all_valid_documents:continue
                    table_id_li = table_id_str.split('^!!^')
                    for tid in table_id_li:
                        if not tid.strip():continue
                        tup = tuple([tid, sheet_id_map.get(sheet_id, '')])
                        ac = 'N'
                        if tup in actual_tables:
                            ac = 'Y'
                        classified_set[tid] = doc_id
                        source_info = source_table_data.get(tid, {})
                        data_rows.append({'t':tid, 'd':doc_id, 'tt':sheet_id_map.get(sheet_id, ''), 'dn':doc_name, 'act_f':ac, 'source_info':source_info, 'rid':row_id})
                
                get_remaining = self.get_remaining_tables(company_id, classified_set, source_table_data)
                data_rows += get_remaining
                total_tables    = len(data_rows)
                classified      = 0
                ph_csv          = 0
                ph              = 0
                csv             = 0
                db_data         = 0
                for r in data_rows:
                    if r['tt']  != 'Not Classified':
                        classified  += 1
                    if chk_table_xml.get(r['t'], {}).get('all'):
                        ph_csv  += 1
                    if chk_table_xml.get(r['t'], {}).get('ph'):
                        ph  += 1
                    
                    if chk_table_xml.get(r['t'], {}).get('csv'):
                        csv  += 1
                    if r['t'] in db_done:
                        db_data         += 1
                not_classified  = total_tables - classified
                dd_s['total_comp']  += 1
                dd_s['total_table']  += total_tables
                dd_s['total_classified']  +=classified
                dd_s['total_not_classified']  +=not_classified
                dd_s['total_ph_csv']  +=ph_csv
                dd_s['total_ph']  +=ph
                dd_s['total_csv']  +=csv
                dd_s['total_db']  += db_data
            except:
                print_exception()
        for k, v in dd_s.items():
            print k, v

    def copy_from_preview(self, ijson):
        paths            = ['/mnt/eMB_db/Thales/20/PREVIEW_OUTPUT/SalesByDestination_48_PanEuropeanEquity_N_bak', '/mnt/eMB_db/Thales/20/PREVIEW_OUTPUT/SalesByDestination_48_PanEuropeanEquity_Y_bak']
        paths            = ['/mnt/eMB_db/Thales/20/PREVIEW_OUTPUT/QuarterOrderIntakeAndSales_102_PanEuropeanEquity_N_bak/', '/mnt/eMB_db/Thales/20/PREVIEW_OUTPUT/QuarterOrderIntakeAndSales_139_PanEuropeanEquity_N_bak/', '/mnt/eMB_db/Thales/20/PREVIEW_OUTPUT/QuarterOrderIntakeAndSales_102_PanEuropeanEquity_Y_bak/', '/mnt/eMB_db/Thales/20/PREVIEW_OUTPUT/QuarterOrderIntakeAndSales_139_PanEuropeanEquity_Y_bak/']
        project_id      = ijson['project_id']
        deal_id         = ijson['deal_id']
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()
        f_ar    = []
        for path in paths:
            tdata   = []
            if os.path.exists(path):
                try:
                    env1            = lmdb.open(path, readonly=True)
                    txn             = env1.begin()
                except:
                    txn = {}
                tdata           = txn.get('res')
                if tdata:
                    res = eval(tdata)
                    
                    for ii, rr in enumerate(res[0]['data']):
                        print '\n======================================'
                        print rr['t_id'], rr['t_l']
                        dd  = {}
                        for ph in res[0]['phs']:
                            if ph['k'] not in rr:continue
                            table_id    = rr[ph['k']]['t']
                            xml_id      = rr[ph['k']]['x']
                            print '\t',[rr[ph['k']]['v'], rr[ph['k']]['t'], rr[ph['k']]['x']]
                            tk   = self.get_quid(table_id+'_'+xml_id)
                            c_id        = txn_m.get('XMLID_MAP_'+tk)
                            print 'c_id ', c_id
                            dd[table_id]  = 1
                        if dd:
                            f_ar.append((len(dd.keys()), dd))
        f_ar.sort(key=lambda x:x[0])
        for r in f_ar:
            print r

    def read_template_ptype_update(self, ijson):
        company_name    = ijson['company_name']
        mnumber         = ijson['model_number']
        model_number    = mnumber
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        company_id      = "%s_%s"%(project_id, deal_id)
        tmp_path            = '/mnt/eMB_db/%s/%s/PREVIEW_OUTPUT/%s_%s/'%(company_name, model_number, 'TEMPLATE_PH_UPDATE', ijson['template_id'])
        res = []
        if os.path.exists(tmp_path):
            env1        = lmdb.open(tmp_path)
            txn1        = env1.begin()
            t_col_grp_d = eval(txn1.get('res'))
            phs         = []
            phs.append({'k':'t', 'n':'Table'})
            phs.append({'k':'c', 'n':'Column'})
            phs.append({'k':'phs', 'n':'PHS'})
            phs.append({'k':'ptypes', 'n':'Period Types'})
            phs.append({'k':'taxo', 'n':'Taxonomy'})
            phs.append({'k':'tt', 'n':'Table Type'})
            phs.append({'k':'grpid', 'n':'Group ID'})
            phs.append({'k':'t_id', 'n':'Taxo ID'})
            data    = []
            done_d  = {}
            for k, v in t_col_grp_d.items():
                for col, info in v.items():
                    f   = 0
                    if not '~'.join(info['ptypes'].keys()).strip():
                        continue
                    taxos   = {}
                    for pptype, taxo_d in info['ptypes'].items():
                        taxos.update(taxo_d)
                    for ttaxo in taxos.keys():
                        dd  = {'t':k, 'c':col, 'phs':'~'.join(info['phs'].keys()), 'ptypes':'~'.join(info['ptypes'].keys()), 'taxo':ttaxo}
                        for ttinfo, vinfo in  info.get('update', {}).items():
                            tt, grpid, taxo = ttinfo.split('^^')
                            tmpdd       = copy.deepcopy(dd)
                            tmpdd['tt']    = tt
                            tmpdd['grpid']    = grpid
                            tmpdd['t_id']    = taxo
                            if vinfo[2]:
                                tmpdd['color']  = 'G'
                            if len(info['ptypes'].keys()) > 1:
                                tmpdd['color']  = 'R'
                            tmptup  = (k, col, '~'.join(info['phs'].keys()), '~'.join(info['ptypes'].keys()), ttaxo, tt, grpid) #, taxo)
                            if tmptup not in done_d:
                                data.append(tmpdd)
                                done_d[tmptup]  = 1
                            f   = 1
                        if f == 0:
                            if len(info['ptypes'].keys()) > 1:
                                dd['color'] = 'R'
                            data.append(dd)
            data.sort(key=lambda x:{'R':0}.get(x.get('color', ''), 1))
            res = [{"message":'done', 'data':data, 'phs':phs}]
                    
        return res

    def get_table_txts(self, table_id, doc_id, txn_m):
        t_ar    = []
        row     = 0
        col     = 0
        pnos    = {}
        for l in ['GH', 'VGH', 'HGH', 'GV']:
            k   = l+'_'+table_id
            ks  = txn_m.get(k)
            if not ks:continue
            ks  = ks.split('#')
            ks.sort(key=lambda x:tuple(map(lambda x1:int(x1), x.split('_'))))
            for c_id in ks:
                t, r, c = c_id.split('_')
                row     = max(row, int(r))
                col     = max(col, int(r))
                xs      = txn_m.get('XMLID_'+c_id)
                for x in xs.split(':@:'):
                    for x1 in x.split('#'):
                        if '_' in x1:
                            pnos[int(x1.split('@')[0].split('_')[1])]    = 1
                t   = binascii.a2b_hex(txn_m.get('TEXT_'+c_id))
                t_ar.append(t)
        t   = self.get_quid(''.join(t_ar))
        pnos    = pnos.keys()
        pnos.sort()
        return (row, col, doc_id, tuple(pnos), t)
        
            
    def remove_duplicate_tables(self, ijson):
        company_name    = ijson['company_name']
        model_number    = ijson['model_number']
        deal_id         = ijson['deal_id']
        project_id      = ijson['project_id']
        dbinfo      = {'user':'root', 'password':'tas123', 'host':'172.16.20.229', 'db':'tfms_urlid_%s_%s'%(project_id, deal_id)} 
        conn, cur   = conn_obj.MySQLdb_connection(dbinfo)
        sql             = "select document_id, source_type from ir_document_master where source_type is not NULL"
        cur.execute(sql)
        tres            = cur.fetchall()
        read_qry = """ SELECT norm_resid, source_table_info FROM norm_data_mgmt; """
        cur.execute(read_qry)
        t_data = cur.fetchall()
        conn.close()
        tmap_data    = {}
        for row in t_data:
            if row[1]:
                tmap_data[str(row[0])]    = eval(row[1])
        tmpd            = {}
        for tr in tres:
            document_id, source_type    = tr
            tmpd.setdefault(source_type.split('^')[0], {})[str(document_id)]  = tr
        
        done_tabel_grp  = {}
        db_name = ''
        for dbname, docs in tmpd.items():
            db_name = dbname
            #udate_obj.update_aecn_inc_from_to_data(dbname, docs)
            dbinfo      = {'user':'root', 'password':'tas123', 'host':'172.16.20.52', 'db':dbname} 
            m_conn, m_cur = conn_obj.MySQLdb_connection(dbinfo)
            #docs    = ['4504', '15561']
            read_qry = """ SELECT docid, pageno, groupid FROM db_data_mgmt_grid_slt WHERE docid in (%s) and groupid <=1000 and active_status='Y'"""%(', '.join(docs))
            m_cur.execute(read_qry)
            mt_data  = m_cur.fetchall()
            for tr in mt_data:
                docid, pageno, groupid  = tr
                done_tabel_grp[(str(docid), str(pageno), str(groupid))] = 1
        lmdb_path1  =  "/var/www/html/fill_table/%s_%s/table_info"%(project_id, deal_id)
        env         = lmdb.open(lmdb_path1, readonly=True)
        txn_m       = env.begin()
        import data_builder.db_data as db_data
        obj = db_data.PYAPI()
        m_tables, rev_m_tables, doc_m_d, table_type_m = self.get_main_table_info(company_name, model_number)
        ijson['taxo_flg']   = 1
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        sql             = "select table_type, table_id from mt_data_builder"
        cur.execute(sql)
        res             = cur.fetchall()
        dd  = {}
        for r in res:
            table_type, table_id    = r
            if table_id and table_type:
                dd.setdefault(table_type, {})[str(table_id)]    = 1
        rm_tables   = {}
        #print rev_m_tables.keys()
        #sys.exit()
        for table_type, table_ids in rev_m_tables.items():
            if not table_type:continue
            td  = {}
            for table_id in table_ids.keys():
                table_sig   = self.get_table_txts(table_id, doc_m_d[table_id], txn_m)
                td.setdefault(table_sig, {})[table_id]  = 1
            ks  = td.keys()
            ks  = filter(lambda x: len(td[x].keys())> 1, ks)
            tids    = dd.get(table_type, {})
            for k in ks:
                print k, td[k]
                table_ids   = []
                for t in td[k].keys():
                    f_db    = tids.get(t, 0)
                    sinfo   = tmap_data[t]
                    f_inc   = done_tabel_grp.get(sinfo, 0)
                    f   = 0
                    if f_db == 1 and f_inc == 1:
                        f   = 3
                    elif f_db:
                        f   = 2
                    elif f_inc:
                        f   = 1
                        
                    table_ids.append((t, f))
                table_ids.sort(key=lambda x:x[1], reverse=True)
                print table_ids[0]
                for r in table_ids[1:]:
                    print '\t', r
                    rm_tables[r[0]]   = 1
        print rm_tables
        import classification_insert_new_inc as pyf
        s_Obj = pyf.PYAPI()
        tlist   = []
        docids  = {}
        for tid in rm_tables.keys():
            sinfo   = tmap_data[tid]
            doc_id, pageno, grid_id = tmap_data[tid]
            docids[str(doc_id)]  = 1
            tlist.append((doc_id, '', pageno, grid_id))
        if 0:
            table_source_map    = {}
            tlist               = []
            docids              = {}
            for k, v in dd.items():
                k   = 'NonperformingAssets'
                for tid in v.keys():
                    sinfo   = tmap_data[tid]
                    table_source_map[tid] = str(sinfo)
                    doc_id, pageno, grid_id = tmap_data[tid]
                    docids[str(doc_id)]  = 1
                    tlist.append((doc_id, k, pageno, grid_id))
        print table_source_map
        print tlist
        #s_Obj.insert_auto_inc_classification('%s_%s'%(project_id, deal_id), '#'.join(docids.keys()), table_source_map, tlist, db_name)
        return
        db_file         = self.get_db_path(ijson)
        conn, cur       = conn_obj.sqlite_connection(db_file)
        sql = "delete from mt_data_builder where table_id in (%s)"%(', '.join(rm_tables.keys()))
        print sql
        cur.execute(sql)
        conn.commit()
        conn.close()
            
            
                
        


if __name__ == '__main__':
   obj  = PYAPI()
   #print obj.convert_floating_point('0.644423434', 4)
